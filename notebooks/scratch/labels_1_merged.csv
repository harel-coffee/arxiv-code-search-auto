id,pattern,update_date,label,para
2011.05411,dataset,04/21/22,1,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-users‚Äô devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.05411,"dataset provided, data",04/22/22,2,5.3. Rights of Data Subject
2011.05411,dataset,04/23/22,3,"Although gradient descent-based optimisation methods were successfully engaged in various ML algorithms, they have recently re-gained much attention since the emergence of large-scale distributed learning, including FL [16, 29]. In these scenarios, a complex model, e.g., a deep neural network (DNN) with millions of parameters, is trained on a very large dataset across multiple nodes. These nodes are"
2011.05411,"dataset, data",04/24/22,4,"Another approach to preserve data privacy and security in ML is to utilise homomorphic encryption techniques, particularly in centralised systems, e.g., cloud servers, wherein data is collected and trained at a server without disclosing the original information. Homomorphic encryption enables the ability to perform computation on an encrypted form of data without the need for the secret key to decrypt the ciphertext [44]. Results of the computation are in encrypted form and can only be decrypted by the requester of the computation. In addition, homomorphic encryption ensures that the decrypted output is the same as the one computed on the original unencrypted dataset."
2011.05411,"dataset, data",04/25/22,5,"As aforementioned, a trained ML model contains unintended features that can be utilised to extract personal information. Thus, local ML model parameters from a federated optimisation algorithm can be exploited by an adversary to infer personal information, particularly when combining with related information such as model data structure and meta-data. This information can be either original training data samples (i.e., reconstruction attack) [38, 111, 81, 4, 57, 96, 112, 6, 93, 130, 43] or membership tracing (i.e., to check if a given data point belongs to a training dataset) [15, 112, 86]."
2011.05411,"dataset, data",04/26/22,6,"Data anonymisation or de-identiÔ¨Åcation is a technique to hide (e.g., hashing) or remove sensitive attributes, such as personally identiÔ¨Åable information (PII), so that a data subject cannot be identiÔ¨Åed within the modiÔ¨Åed dataset (i.e., the anonymous dataset) [92]. As a consequence, data anonymisation has to balance well between privacy-guarantee and utility because hiding or removing information may reduce the utility of the dataset. Furthermore, when combined with auxiliary information from other anonymous datasets, a data subject might be re-identiÔ¨Åed, subjected to a privacy attack called linkage attack [40]. To prevent from linkage attack, numerous techniques have been proposed such as k-anonymity [116], l-diversity [79], a k-anonymity-based method, and tcloseness - a technique built on both k-anonymity and l-diversity that preserves the distribution of sensitive attributes in a dataset so that it reduces the risk of re-identifying a data subject in a same quasi-identiÔ¨Åer group [73]."
2011.05411,dataset,04/27/22,7,"FL is well-suited for sorts of ML models that are formulated as minimisation of some objective functions (loss functions) on a training dataset for parameter estimation, particularly for gradient-based optimisation algorithms [70]. The minimisation objective can be formulated as follows:"
2011.05411,"data, data available",04/28/22,8,"FL settings. Training data in FL is unbalanced and non-IID, which is scattered across millions of personal mobile devices with signiÔ¨Åcant higher-latency, lower-throughput connections compared to the traditional techniques working on a cloud-centric data server. In addition, the data and computing resources in personal devices are only intermittently available for training. Therefore, to actualise FL, optimisation algorithms must be well adapted and eÔ¨Éciently performed for federated settings (i.e., federated optimisation [70])."
2011.05411,"dataset, data",04/29/22,9,"Furthermore, the local nodes can leverage the perturbation method to prevent a coordination server and other adversaries from disclosing model parameters updates and original training dataset. The idea of employing perturbation technique to FL is that a local node adds random noise to its local model parameters in order to obscure certain sensitive attributes of the model before sharing. As a result, adversaries, in case it can successfully derive such model parameters, is unable to accurately reconstruct the original training data or infer some related information. In other words, the perturbation method could prevent adversaries from carrying out inference attacks on a local model trained by a particular client. This privacy-preservation method typically adopts diÔ¨Äerential privacy technique that adds random noises to either training dataset or model parameters, oÔ¨Äering statistical privacy guarantees for individual data [35, 33, 7]. Indeed, before the proposal of FL, diÔ¨Äerential privacy with SMC has been suggested as a privacy-preserving technique"
2011.05411,"dataset, data",04/30/22,10,"Generally, there are three gradient descent methods that are categorised based on the amount of training data used in the gradient calculation of the objective function ùëì (ùúÉ) [103]. The Ô¨Årst category is batch gradient descent, in which the gradients are computed over the entire training dataset Óà∞ for one update. The second category is stochastic gradient descent (SGD), that, in contrast to batch gradient descent, randomly selects a sample (or a subset) from Óà∞ and performs the parameters update based on the gradient of this sample only (one sample per step, the whole process sweeps through the entire dataset). The third one is mini-batch gradient descent in which the dataset is subdivided into mini-batches of ùëõ training samples (ùëõ is the batch-size); the parameters update is then performed on every mini-batch (single minibatch per step)."
2011.05411,"dataset, data",05/01/22,11,"Geyer et al. in [48] have developed another method to implement diÔ¨Äerential privacy for federated optimisation in FL settings that conceals the participation of a user in a training task; as a result, the whole local training dataset of the user is protected against diÔ¨Äerential attacks. This approach is diÔ¨Äerent from the batch-level one, which aims at protecting a single data point in a training task. The proposed method utilises a similar concept of privacy accounting from [1] that allows a coordination server to monitor the accumulated privacy budget by observing the moment accountant and privacy loss proposed in [1]. The training process is halted once the accumulated privacy budget reaches a pre-deÔ¨Åned threshold, implying that the privacy guarantee is no further tolerated. The Gaussian mechanism is also used to generate random noise which is then added to distort the sum of gradients updates to protect the whole training data. The proposed method has been experimented on MNIST dataset, and the results show that with a suÔ¨Éciently large number of participants (e.g., about 10,000 clients), the accuracy of the FL trained model almost achieves as high as the nondiÔ¨Äerential-privacy baseline while a certain level of privacy guarantee over the local training data still holds."
2011.05411,"dataset provided, data",05/02/22,12,"In this article, we conduct a survey on existing FL studies with an emphasis on privacy-preserving techniques from the GDPR-compliance perspective. Firstly, we brieÔ¨Çy review the challenges on data privacy preservation in conventional centralised ML approaches (Section 2) and introduce FL as a potential approach to address the challenges (Section 3). Secondly, the state-of-the-art privacy-preserving techniques for centralised FL are described with the analysis of how these solutions can mitigate data security and privacy risks (Section 4). Thirdly, we provide an insightful deliberation with potential solution approaches of how an FL system can be implemented in order to comply with the EU/UK GDPR (Section 5). Unsolved challenges hindering an FL system from complying with the GDPR are also speciÔ¨Åed along with the future research directions."
2011.05411,"dataset, data",05/03/22,13,"In this regard, FL is an alternative for the cloud-centric ML technique that facilitates an ML model to be trained collaboratively while retaining original personal data on their devices, thus potentially mitigates data privacy-related vulnerabilities. It is a cross-disciplinary technique covering multiple computer science aspects including ML, distributed computing, data privacy and security that enables end-users‚Äô devices (i.e., local nodes) to locally train a shared ML model on local data. Only parameters in the training process are exchanged for the model aggregation and updates. The diÔ¨Äerence between FL and the standard distributed learning is that in distributed learning, local training datasets in compute nodes are assumed to be independent and identically distributed data (IID) whose their sizes are roughly the same. FL is, thus, as an advancement of distributed learning as it is designed to work with unbalanced and non-independent identically-distributed data (non-IID) whose sizes may span several orders of magnitude. Such heterogeneous datasets are resided at a massive number of scattering mobile devices under unstable connectivity and limited communication bandwidth [81, 80, 65]."
2011.05411,"dataset, data",05/04/22,14,"In traditional ML approaches, this sort of algorithms performs a vast number of fast iterations over a large dataset homogeneously partitioned in data servers. Such algorithms require super low-latency and high-throughput connections to the training data [80]. Therefore, solving this optimisation problem in the context of FL is diÔ¨Äerent from the traditional ML approaches as such conditions do not hold in"
2011.05411,dataset,05/05/22,15,"It is worth to emphasise that the separation of the four steps in the cycle is not a strict requirement in every training round. For instance, an asynchronous SGD algorithm can be used in which results of the local training can be immediately applied to update the local model before obtaining updates from other participants [21]. This asynchronous approach is typically utilised in distributed training for deep learning models on a large-scale dataset as it maximises the rate of updates [29, 25]. However, in FL settings, the synchronous approach, which requires the coordination from a centralised server, has substantial advantages over the asynchronous ones in terms of both communication eÔ¨Éciency and security because it allows advanced technologies to be integrated such as aggregation compression, secure aggregation with SMC, and diÔ¨Äerential privacy [80, 71, 55, 120]."
2011.05411,"data, data available",05/06/22,16,"Normally, DPAs might require a variety of information with a detailed explanation from Data Controller to perform the analysis including documents of organisational and technical measures related to the implementation the GDPR requirements as well as independent DPIA and PIA reports frequently conducted by the Data Controller. DPAs may also require to be given access to data server infrastructure and management system including personal data that is being processed. In this respect, besides the legal basis such as consents from end-users, an FL service provider can only provide documentation of how FL-related mechanisms are implemented along with privacy-preserving technical measures such as secure aggregation, diÔ¨Äerential privacy, and homomorphic encryption. Other inquiries from DPAs such as direct access to the FL model training operations and inspection of individual local model parameters from a particular end-user are technically infeasible for any FL systems."
2011.05411,dataset,05/07/22,17,"Proposed by Dwork et al. in 2006, diÔ¨Äerential privacy [34] is an advanced solution of the perturbation privacy-preserving technique in which random noise is added to true outputs using rigorous mathematical measures [40]. As a result, it is statistically indistinguishable between an original aggregate dataset and a diÔ¨Äerentially additive-noise one. Thus, a single individual cannot be identiÔ¨Åed as any (statistical) query results to the original dataset is practically the same regardless of the existence of the individual [34, 33, 35]. However, there is a trade-oÔ¨Ä between privacy-guarantee and utility as adding too much noise and improper random Nguyen Truong et al.: Preprint submitted to Elsevier"
2011.05411,"dataset, data",05/08/22,18,"Reconstruction attacks using MI and GANs are only feasible if and only if all class members in an ML model are analogous which entails a similarity between the MI/GANreconstructed outputs and the training data (e.g., facial recognition of a speciÔ¨Åc person, or MNIST dataset for handwritten digits6 used in [4]). Fortunately, this precondition is less practical in most of the FL scenarios."
2011.05411,"dataset, data",05/09/22,19,"Regarding the Fairness and Transparency requirements, as AI/ML algorithms like deep learning are normally operated in a black-box fashion, it is limited of transparency of how certain decisions are made, as well as limited understanding of the bias in data samples and training process [30, 83, 3, 91]. An FL system is not an exception. Generally, if the training data is poorly collected or intentionally prejudicial and fed to an ML, including FL, system, the results apparently turn out to be biased. If the trained model is then utilised for an automated decision-making system, then it probably leads to discrimination and injustice. Furthermore, the nature of preventing service providers from accessing original training dataset as well as the inability to inspect individuals‚Äô locally trained ML model due to Secure Aggregation mechanism ampliÔ¨Åes the lack of transparency and fairness in FL systems. As a result, an FL system Ô¨Ånds it problematic to transparently execute the training operations as well as to ensure any automated decisions from the system are impartially performed. This, consequently, induces the impracticality for any FL systems and fails to fully comply with the GDPR requirements of fairness and transparency."
2011.05411,"dataset, data",05/10/22,20,"SMC is beneÔ¨Åcial to data privacy preservation in distributed learning wherein compute nodes collaboratively perform model training on their local dataset without revealing such dataset to others. Indeed, SMC has been employed in numerous ML algorithms such as secure two-party computation (S2C) in linear regression [31], Iterative Dichotomiser3 (ID3) decision tree learning algorithm [78], and k-means clustering algorithm for distributed data mining [62]. However,most of SMC protocols impose non-trivial overheads which require further eÔ¨Éciency improvements with practical deployment."
2011.05411,"dataset, data",05/11/22,21,"SMC, also known as multi-party computation (MPC) or privacy-preserving computation, was Ô¨Årstly introduced by Yao in 1986 [126] and further developed by numerous researchers. Its catalyst is that a function can be collectively computed over a dataset owned by multiple parties using their own inputs (i.e., a subset of the dataset) so that any party learns nothing about others‚Äô data except the outputs [51, 18, 27]. SpeciÔ¨Åcally, ùëõ parties ùëÉ1, ùëÉ2, .., ùëÉùëõ own ùëõ pieces of pri, respectively to collectively comvate data ùëã1, ùëã2, ..., ùëãùëõ pute a public function ùëì (ùëã1, ùëã2, .., ùëãùëõ) = (ùëå1, ùëå2, .., ùëåùëõ). The only information each party can obtain from the computation is the result (ùëå1, ùëå2, .., ùëåùëõ) and its own inputs ùëãùëñ . Classical secret sharing such as Shamir‚Äôs secret sharing [109, 17] and veriÔ¨Åable secret sharing (VSS) schemes [26] are the groundwork for most of the SMC protocols."
2011.05411,dataset,05/12/22,22,"Shokri and Shmatikov in [111] have proposed a communication eÔ¨Écient privacy-preserving SGD algorithm for deep learning in distributed settings in which local gradient parameters are asynchronously shared among participants with an option of adding noise to such updates for the differentially private protection of the individual model parameters. In this algorithm, participants can choose a fraction of parameters (randomly selected or following a strategy) to be updated at each round so that their local optimal can converge faster while being more accurate. In order to integrate diÔ¨Äerential privacy technique into the algorithm, the ùúÄ total privacy budget parameter and the sensitivity of gradient are taken into account to control Œîùëìùëñ the trade-oÔ¨Ä between the diÔ¨Äerential privacy protection and the model accuracy. Laplacian mechanism is used to generate noise during both parameter selection and exchange processes based on the estimation of the Œîùëìùëñ sensitivity and the allocated ùúÄ privacy budget. The proposed algorithm has experimented on MNIST and SVHN datasets showing the trade-oÔ¨Ä between strong diÔ¨Äerential privacy guarantees and high accuracy of the training model. However, with a large number of participants sharing a large fraction of gradients, the accuracy of the proposed algorithm with diÔ¨Äerential privacy is better than the standalone baseline. It is worth noting that in this algorithm, local gradients can be exchanged directly or via a central server, which can feasibly be implemented in the FL settings."
2011.05411,"dataset provided, data",05/13/22,23,"The GDPR clearly diÔ¨Äerentiates three participant roles, namely: Data Subject, Data Controller and Data Processor, along with associated requirements and obligations under the EU/UK data protection law. While serving as a better privacy and security framework, the GDPR also aims at protecting data ownership by obligating Data Controllers to provide fundamental rights for Data Subjects to control over their data (""How?"" in Fig. 1). For these purposes, the GDPR introduces and sets high-standard for the consent lawful basis in which Data Controller shall obtain consent from Data Subject in order to process data. Data Controller takes full responsibility to regulate the purposes for which and the methods in which, personal data is processed under the Terms and Conditions deÔ¨Åned in the consent."
2011.05411,"dataset provided, data",05/14/22,24,"The GDPR diÔ¨Äerentiates three participant roles, namely Data Subject, Data Controller and Data Processor, and designates associated obligations for these roles under the EU data protection law. Data Controllers are subject to comply with the GDPR by determining the purposes for which, and the method in which, personal data is processed by Data Processors - who will be responsible for processing the data on behalf of Data Controllers. Furthermore, Data Controllers should take appropriate measures to provide Data Subjects with information related not only to how data is shared but also to how data is processed in the manner ensuring security and privacy of personal data. The GDPR also clearly speciÔ¨Åes rights of Data Subjects, giving data owners the rights to inspect information about how the personal data is being processed (e.g., Right to be informed and Right of access) as well as to fully control the data (e.g., Right of rectiÔ¨Åcation and erasure, and Right to restriction of processing)."
2011.05411,"data, publicly available",05/15/22,25,"The GDPR establishes supervisory authorities in each member state which are independent public authorities called Data Protection Authorities (DPAs). DPAs are responsible for supervising and inspecting whether a Data Controller is compliant with the data protection regulations whilst the Data Controller is responsible for demonstrating the compliance. The questions are judiciously raised: How can an FL system be investigated and validated by DPAs, and how can it demonstrate the compliance?"
2011.05411,"dataset, data",05/16/22,26,"The authors in [1] have proposed an SGD algorithm integrated with diÔ¨Äerential privacy performing over some batches (a group) of data samples. This algorithm estimates the gradient of the group by taking the average of the gradient loss of these batches and adds noise (generated by Gaussian mechanism) to the group to protect the privacy. This algorithm is implemented to train on the MNIST and CIFAR-10 datasets showing sensible results as it achieves only 1.3% and 7% less accurate compared to the non-diÔ¨Äerentially private conventional baseline algorithms on the same datasets, respectively. Similar to the mechanism proposed by Shokri and Shmatikov in [111], the authors have proposed a mechanism to monitor the total privacy budget (i.e., privacy accounting)"
2011.05411,"data, publicly available",05/17/22,27,"The new GDPR legislation has come into force from May 2018 in all European Union (EU) countries which is a major update to the EU Data Protection Directive (95/46/EC) (DPD-95) introduced in the year 1995. The GDPR aims to protect personal data (more comprehensive range depicted in ""Which?"" - Fig. 1) with the impetus that ""personal data can only be gathered legally, under strict conditions, for a legitimate purpose"". The full regulation is described in detail across 99 articles covering principles, and both technical and admin requirements around how organisations need to process personal data. The GDPR creates a legal data protection framework throughout the EU/UK member states which has impacted commercial and public organisations worldwide processing EU/UK residents‚Äô data (""Global"" in Fig. 1)."
2011.05411,"dataset provided, data",05/18/22,28,"To meet stringent requirements of the GDPR, conventional ML-based applications and services are required to implement measures that eÔ¨Äectively protect and manage personal data adhering to the six data protection principles in the GDPR, as well as to provide mechanisms for data subjects to fully control their data. Although ML-based systems are strengthened by several privacy-preserving methods, implementing these obligations in a centralised MLbased system is non-trivial, sometimes technologically impractical [119, 53]."
2011.05411,"dataset, data",05/19/22,29,"To overcome such challenges, Federated Learning (FL), proposed by Google researchers in 2016, has appeared as a promising solution and attracted attention from both industry and academia [70, 71, 81, 80]. Generally, FL is a technique to implement an ML algorithm in decentralised collaborative learning settings wherein the algorithm is executed on multiple local datasets stored at isolated data sources (i.e., local nodes) such as smart phones, tablet, PCs, and wearable devices without the need for collecting and processing the training data at a centralised data server. FL allows local nodes to collaboratively train a shared ML model while retaining both training dataset and computation at internal sites [70]. Only results of the training (i.e., parameters) are exchanged at a certain frequency, which requires a central server to coordinate the training process (centralised FL) or utilises a peer-to-peer underlying network infrastructure (i.e., decentralised FL) to aggregate the training results and calculate the global model."
2011.05411,dataset,05/20/22,30,"[92] Narayanan, A., Shmatikov, V., 2008. Robust de-anonymization of large sparse datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008), IEEE. pp. 111‚Äì125."
2011.05411,dataset,05/21/22,31,"as accumulated privacy loss by observing privacy loss random variables. Based on the experiment, the authors also indicate that privacy loss is minimal for large group size (with a large number of datasets)."
2011.05411,"dataset, data, data available",05/22/22,32,"called compute nodes and grouped into clusters. For efÔ¨Åciency, the calculations in the training process should be parallelised using concurrency methods such as model parallelism and data parallelism [24]. Model parallelism distributes an ML model into diÔ¨Äerent computing blocks; available computing nodes are then be assigned to compute some speciÔ¨Åc blocks only. Model parallelism requires mini-batch data is replicated at computing nodes in a cluster, as well as regular communication and synchronisation among such nodes [29]. Data parallelism, instead, keeps the completeness of the model on each computing node but partitions the training dataset into smaller equal size shards (also known as sharding), which are then distributed to computing nodes in each cluster [8]. The computing nodes then train the model on their subset as a mini-batch, which is especially eÔ¨Äective for SGD variants because most operations over mini-batches are independent in these algorithms. Data parallelism can be found in numerous modern ML frameworks including TensorFlow3 and Pytorch4. The two parallelism techniques can also be combined (so-called Hybrid parallelism) to intensify the advantages while mitigating the drawbacks of each one; as a result, a hybrid system can achieve better eÔ¨Éciency and scalability [25]."
2011.05411,"dataset, data",05/23/22,33,"learning system target two main objectives: (i) privacy of the training dataset and (ii) privacy of the local model parameters (from an optimisation algorithm such as a gradient descent variant) which are exchanged with other nodes and/or a centralised server [111]. In this respect, prominent privacy-preserving techniques in ML include data anonymisation [92], diÔ¨Äerential privacy [34], secure multi-party computation (SMC) [126], and homomorphic encryption [44]."
2011.05411,dataset,05/24/22,34,"ness will signiÔ¨Åcantly depreciate reliability and usability of the dataset [33, 35, 40]."
2011.05411,dataset,05/25/22,35,"where the training dataset is in form of a set of input-output pairs (ùë•ùëñ, ùë¶ùëñ), ùë•ùëñ ‚àà ‚Ñùùëë and ùë¶ùëñ ‚àà ‚Ñù, ‚àÄùëñ ‚àà {1, 2, .., ùëõ}. In Equation 2, ùëõ is the number of samples in the dataset, ùë§ ‚àà ‚Ñùùëë is the parameter vector, and ùëìùëñ(ùë§) is a loss function. This formulation covers both linear and logistic regressions, support vector machines, as well as complicated non-convex problems in ArtiÔ¨Åcial Neural Networks (ANN) including Deep Learning [70]. This problem requires an optimisation process that can be eÔ¨Éciently computed by using a gradient descent algorithm with back-propagation technique [105, 101] for minimising the overall loss with respect to each model parameters."
2103.12883,github,05/26/22,36,1https://github.com/ricardoGrando/hydrone_deep_rl_icra
2103.12883,python,05/27/22,37,"The whole system was implemented using ROS and Gazebo frameworks. The Deep-RL approaches were implemented using Python programming language, while the vehicles‚Äô related plugins were partially implemented in C++ and Python. The implementation of the neural networks was carried out with the PyTorch2 library. The performance of our approaches can also be observed in a complementary video3."
2103.12883,package,05/28/22,38,"[31] M. M. M. ManhÀúaes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, ‚ÄúUuv simulator: A gazebo-based package for underwater intervention and multi-robot simulation,‚Äù in OCEANS MTS/IEEE Monterey. IEEE, 2016, pp. 1‚Äì8."
2103.12883,package,05/29/22,39,"‚Ä¢ We demonstrate that, with our approaches, the robot is capable to arrive at the desired target avoiding collisions. However, with a geometrically dependent tracking controller, the robot is unable to bypass the drilling risers. We also provide a completely built ROS package with a real-world described HUAUV."
2108.09408,dataset,05/30/22,40,"Abstract. Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at diÔ¨Äerent scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edgebased U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods."
2108.09408,dataset,05/31/22,41,Datasets
2108.09408,dataset,06/01/22,42,"Datasets and implementation details. The model is trained on the DUTSTR with 10553 images. In detail, we trained the model using the SGD optimizer with initial learning rate 3e-5, 0.9 momentum, 5e-4 weight decay, and batch size 16. Because the ResNet-50 parameters are pre-trained on ImageNet, the learning rate of this part is a tenth of the randomly initialized parts which is set as 3e-5. Then, the trained model is tested on Ô¨Åve datasets, including DUTS-TE with 5019 images, DUT-OMROM with 5168 images, HKU-IS with 4447 images, ECSSD with 1000 images and PASCAL-S with 850 images."
2108.09408,dataset,06/02/22,43,Datasets mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em Metrics .745 .049 .862 .860 .868 .044 .911 .914 .692 .064 .809 .837 .871 .038 .907 .937 BMPM[30] .751 .059 .839 .861 .889 .059 .893 .914 .713 .062 .814 .846 .874 .045 .888 .931 RAS[3] .785 .057 .834 .867 .914 .040 .910 .929 .747 .063 .815 .850 .893 .036 .895 .939 R3Net[5] PiCANet[13] .749 .051 .867 .852 .885 .044 .917 .910 .710 .065 .835 .834 .870 .039 .908 .934 MLMSNet[26] .799 .045 .856 .882 .914 .038 .911 .925 .735 .056 .817 .846 .892 .034 .901 .945 .777 .051 .854 .869 .906 .042 .912 .920 .736 .066 .824 .853 .882 .037 .903 .940 PAGE[23] .805 .043 .869 .886 .917 .037 .918 .925 .747 .056 .825 .866 .891 .034 .905 .944 CPD[28] .756 .048 .866 .884 .880 .037 .916 .921 .756 .056 .836 .869 .895 .032 .909 .946 BASNet[19] .840 .035 .888 .902 .925 .033 .924 .927 .766 .053 .838 .870 .840 .062 .855 .859 F3Net[24] .799 .040 .879 .881 .910 .042 .917 .921 .739 .055 .832 .858 .885 .032 PoolNet[12] .941 .767 .048 .865 .879 .880 .040 .918 .922 .739 .059 .837 .854 .878 .038 .907 .942 TDBU[22] .815 .039 .875 .891 .920 .041 .918 .927 .755 .052 .818 .867 .898 .031 .918 .948 EGNet[31] .792 .044 .861 .886 .892 .033 .928 .924 .761 .054 .847 .871 .896 .031 .916 .948 U2Net[18] .828 .037 .884 .917 .924 .033 .925 .953 .756 .055 .833 .873 .908 .028 .920 .961 MINet[16] .855 .034 .892 .910 .930 .034 .924 .925 .773 .051 .838 .873 .914 .027 .919 .954 LDF[25] .870 .031 .904 .917 .936 .028 .934 .929 .790 .052 .851 .881 .917 .026 .925 .956 Ours
2108.09408,dataset,06/03/22,44,"Quantitative comparison. Table. 3 shows the quantitative evaluation results of the SOTA methods mentioned above and our model in terms of mF , M AE, Sm, and Em. The proposed method consistently performs better than all the competitors across four metrics on four datasets. In terms of Em, our method achieves the second best overall performance, which is slightly inferior to MINet. It is worth noting that MEUNet achieves the best performance in terms of the mean F-measure and structure quality evaluation Sm."
2108.09408,dataset,06/04/22,45,Table 3. Quantitative comparison with state-of-the-art methods on Ô¨Åve datasets. The best results are highlighted in bold. The best and the second best results are highlighted in red and green respectively.
2108.09408,dataset,06/05/22,46,"‚Ä¢ We build an eÔ¨Écient framework to fully combine and fuse edge information, detailed information and semantic clues. Many experiments are conducted to illustrate the validity of our algorithm and this model could surpass most models on four large-scale salient object detection datasets."
2011.05411,data,,,"1. A systematic description of data processing operations, associated purposes, along with clariÔ¨Åcation and justiÔ¨Åcation of the operations. For instance, the operation of asking Data Subject‚Äôs consent for local ML training and sending the ML model parameters to a coordination server should be documented in detail."
2011.05411,data,,,10https://ico.org.uk/for-organisations/guide-to-data-protection/guide to-the-general-data-protection-regulation-gdpr/lawful-basis-forprocessing/
2011.05411,data,,,11https://ico.org.uk/for-organisations/guide-to-data-protection/guideto-the-general-data-protection-regulation-gdpr/individual-rights/right-tobe-informed
2011.05411,data,,,2.2.1. Data Anonymisation
2011.05411,data,,,"3. An assessment of the data security and privacy risks that might be induced by each operation, along with the technical measures implemented to mitigate and manage the risks. For instance, in an FL system, the operation of sending local ML model parameters to a coordination server for global ML model update might be the target of inference attacks, thus, inducing privacy leakage. The measures called Secure Aggregation and Homomorphic Encryption mechanisms are implemented along with the technical report. Even though such privacy-preserving methods are implemented to strengthen FL systems, there exist some risks that can be exploited for illegitimate purposes such as model poisoning with back-door sub-tasks. These possible attacks, which lead to non-compliance with the GDPR, should be addressed."
2011.05411,data,,,"3. Federated Learning: A Distributed Collaborative Learning Approach In many scenarios, the traditional cloud-centric ML approaches are no longer suitable due to the challenges of complying with strict data protection regulations on vast aggregation and processing personal data. By nature, most personal data is generated at the edge by end-users‚Äô devices (e.g., smart phones, tablets, and wearable devices) which are equipped with increasingly powerful computing capability"
2011.05411,data,,,"4. Privacy-Preservation in Centralised Federated Learning Framework As an ML model can be cooperatively trained while retaining training data and computation on-device, FL naturally oÔ¨Äers privacy-guarantee advantages compared to the traditional ML approaches. Unfortunately, although personal data is not directly sent to a coordination server in its original form, the local ML model parameters still contain sensitive information because some features of the training data samples are inherently encoded into such models [5, 81, 4, 96, 86]. For example, authors in [5] have shown that during the training process, correlations implied in the training data are concealed inside the trained models, and personal information can be subsequently extracted. Melis et al. have also pointed out that modern deep-learning models conceal internal representations of all kinds of features, and some of them are not related to the task being learned. Such unintended features can be exploited to infer some information about the training data samples. FL systems, consequently,"
2011.05411,data,,,5.2.3. Data Minimisation
2011.05411,data,,,"A Data Subject is assumed to have the right ""not to be subject to a decision based solely on automated processing, including proÔ¨Åling"" - Article 22(1), the GDPR. Therefore, an FL client, as a Data Subject, has the right to receive meaningful information and explanation about whether the result of the processing (i.e., a global ML model) used in an automated decision-making system will produce legal eÔ¨Äects concerning the client or similarly signiÔ¨Åcantly aÔ¨Äects the client. Unfortunately, due to the black-box operation model and the limitation of the transparency in ML, including FL, training process, results (e.g., a global ML model in FL) are generally generated without any proper explanation [119]. Thus, it is infeasible to predict whether outcomes of an ML model might aÔ¨Äect the legal status or legal rights of the Data Subject, or negatively impact on its circumstances, behaviour or choices. Consequently, any FL system fails to comply with the GDPR requirements of the data subject‚Äôs right in control of automated decision making. Fortunately, this requirement can be neglected if a Data Controller explicitly mentions the lack of automated decision making and proÔ¨Åling right when asking for Data Subject‚Äôs consent to process personal data."
2011.05411,data,,,"AI/ML-based applications and services are high on the agenda in most sectors. However, the unregulated use or misuse of personal data is dramatically spreading, resulting in severe concerns of data privacy. A series of severe personal data breaches such as Facebook‚Äôs Cambridge Analytica scandal, along with urgent mobile applications during the SARS-CoV2 pandemic for large-scale contact tracing and movement tracking [61] trigger worldwide attention respecting to a variety of privacy-related aspects including algorithm bias, ethics, implications of politic settings, and legal responsibility. This leads to a critical demand for eÔ¨Äective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users‚Äô data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diÔ¨Äerential privacy, and encrypted transfer learning methods [106]. We present a systematic summary of the threat models, possible attacks, and the privacy-preserving techniques in FL systems, along with the analysis of how these techniques can mitigate the risk of privacy leakages. Furthermore, insightful analysis of how an FL-system complies with the GDPR is also provided. Obligations and appropriate measures for a service provider to implement a GDPR-compliant FL system are examined in details following the rational guidelines of the GDPR six principles."
2011.05411,data,,,"According to the Ô¨Årst principle, a service provider providing an FL application, as a Data Controller, must specify its legal basis in order to request end-users to participate in the FL training. There are total six legal bases required by the GDPR namely (1) Consent, (2) Contract, (3) Legal Obligation, (4) Vital Interest, (5) Public Task, and (6) Legitimate Interest (deÔ¨Åned in Article 6 of the GDPR in detail). These lawful bases might need to come along with other separate conditions for lawfully processing some special cate gory data including healthcare data, biometric data, racial and ethnic origin. Depending on speciÔ¨Åc purposes and context of the processing, the most appropriate one should be determined and documented before starting to process personal data."
2011.05411,data,,,"Along with the privacy-preserving techniques such as Secure Aggregation, diÔ¨Äerential privacy, and Homomorphic Encryption designated for protecting local ML parameters, the FL client application installed at end-users‚Äô devices must be secure to prevent from unauthorised access, cyber-attack, or data breach directly from the devices or from the communications between the users‚Äô devices and a coordination server. This precondition is same as any other systems in which a variety of security and privacy techniques are readily integrated into FL applications, as well as secure communications protocols such as IPSec, SSL/TLS and HTTPS"
2011.05411,data,,,"As FL is in the early stage, a fruitful area of multi-disciplinary research is commenced in order to Ô¨Çourish the technology and to comply with the GDPR fully. Firstly, eÔ¨Écient cryptographic and privacy primitives for decentralised collaborative learning must be further developed, particularly for counteracting model poisoning and inference attacks. Furthermore, as these privacy-preserving techniques such as SMC impose non-trivial performance overheads, further eÔ¨Äort on how to eÔ¨Éciently utilise such techniques on FL applications are required. Secondly, research on transparency, interpret-ability and algorithm fairness in FL systems should be profoundly carried out. Even though a sub stantial amount of research has been conducted in centralised AI/ML settings, there is still an open question whether these approaches could be employed and how to sensibly adapt them to the decentralised settings where training data is highly skewed non-IID and unevenly distributed across sources. The sampling constraints should be investigated to see how much extend they aÔ¨Äect and how to mitigate the bias of the global training model. For instance, the agnostic FL framework introduced in [89] naturally yields good-intent fairness as it modelled the target distribution as an unknown mixture of the distributions instead of the uniform distribution in typical FL training algorithms. This agnostic FL framework, as a result, can control for bias in the training objective. Thirdly, it requires more research on interpretable and unbiased ML models and algorithms that can be employed over encrypted settings to well consolidate with advanced encryption schemes in FL systems. Besides, the trade-oÔ¨Äs between privacy utility, accuracy, interpretability, and fairness in an FL framework need to be thoroughly explored."
2011.05411,data,,,"As depicted in Table 2, in FL settings, personal data is regarded as local model parameters, not the original data samples as in traditional cloud-based ML systems. A service provider, who implements an FL system, is Data Controller and Data Processor combined as the service provider (i) dictates end-users (i.e., Data Subject) to train an ML model using their local training data and to share such locally trained model, (ii) processes the local model parameters sent from end-users (i.e., aggregates and updates the global model), and (ii) disseminates the global models to all end-users and requests the end-users to update their local models. Furthermore, in centralised FL settings, a service provider can only"
2011.05411,data,,,"As illustrated in Fig. 8, the investigation of non-compliance and decision of punishment are carried out by DPAs once there is a suspicion or a claim Ô¨Åled by a customer. The compliance inspection will conduct some analysis to see whether a suspicious organisation follows the legal requirement of Privacy&Security-by-design approach and satisÔ¨Åes some standard assessments such as Data Protection Impact Assessment (DPIA) and Privacy Impact Assessment (PIA), which are essential parts of the GDPR accountability obligations."
2011.05411,data,,,"Attackers might carry out model inversion (MI) attack to extract sensitive information contained in training data samples, for instance, by reconstructing representatives of classes which characterising features in classiÔ¨Åcation ML models [38]. MI attacks do not require the attacker to actively participate in the training process (i.e., black-box or passive attacks). For example, it is possible to recover images from a facial recognition model for a particular person (i.e., all class members depict this person) using MI by deriving a correct weighted probability estimation for the target feature vectors [112, 43]. In this scenario, the experiment results show that this MI attack can reconstruct images that are visually similar to the victim‚Äôs photos [38]."
2011.05411,data,,,"Basically, this principle ensures that a Data Controller does not keep personal data for longer if the data is no longer needed for the claimed purposes. In this case, data should be erased or anonymised. There is an exception for data retention only if the Data Controller keeps the data for public interest archiving, scientiÔ¨Åc or historical research, or statistical purposes."
2011.05411,data,,,"Data Poisoning [11, 84, 124, 68, 23, 63]"
2011.05411,data,,,"Depending on encryption schemes and classes of computational operations that can be performed on an encrypted form, homomorphic encryption techniques are divided into diÔ¨Äerent categories such as partial, somewhat (SWHE), and fully homomorphic encryption (FHE)[2]. Some classic encryption techniques, including Rivest‚ÄìShamir‚ÄìAdleman (RSA), is SWHE wherein simple addition and multiplication oper ations can be executed [2]. FHE, Ô¨Årstly proposed by Graig et al. in [45, 46], enables any arbitrary operations (thus, enables any desirable functionality) over cipher-text, yielding results in encrypted forms. In FHE, computation on the original data or the cipher-text can be mathematically transferred using a decryption function without any conÔ¨Çicts."
2011.05411,data,,,"DiÔ¨Äerential privacy technique has been widely employed in various ML algorithms such as linear and logistic regression [19], Support Vector Machine (SVM) [102] and deep learning [20, 1], as well as in ML-based applications such as data mining [39] and signal processing with continuous data [108]."
2011.05411,data,,,"Dr. Kai Sun is the Operation Manager of the Data Science Institute at Imperial College London. She received the MSc degree and the Ph.D degree in Computing from Imperial College London, in 2010 and 2014, respectively. From 2014 to 2017, she was a Research Associate at the Data Science Institute at Imperial College London, working on EU IMI projects including U-BIOPRED and eTRIKS, responsible for translational data management and analysis. She was the manager of the HNA Centre of Future Data Ecosystem in 20172018. Her research interests include translational research management, network analysis and decentralised systems."
2011.05411,data,,,"Dr. Nguyen B.Truong is currently a Research Associate at Data Science Institute, Imperial College London, United Kingdom. He received his Ph.D, MSc, and BSc degrees from Liverpool John Moores University, United Kingdom, Pohang University of Science and Technology, Korea, and Hanoi University of Science and Technology, Vietnam in 2018, 2013, and 2008, respectively. He was a Software Engineer at DASAN Networks, a leading company on Networking Products and Services in South Korea in 2012-2015. His research interest is including, but not limited to, Data Privacy, Security, and Trust, Personal Data Management, Distributed Systems, and Blockchain."
2011.05411,data,,,"Dr. Yike Guo (FREng, MAE) is the director of the Data Science Institute at Imperial College London and the Vice-President (Research and Development) of Hong Kong Baptist University. He received the BSc degree in Computing Science from Tsinghua University, China, in 1985 and received the Ph.D in Computational Logic from Imperial College London in 1993. He is a Professor of Computing Science in the Department of Computing at Imperial College London since 2002. He is a fellow of the Royal Academy of Engineering and a"
2011.05411,data,,,"Even though homomorphic encryption oÔ¨Äers rigorous privacy-guarantee to individuals as the original data in plaintext has never been disclosed, there is a practical limitation in performing computation over cipher-text due to the tremendous computational overhead. As a consequence, employing homomorphic encryption in large-scale data training remains impractical [49]."
2011.05411,data,,,"Federated Learning Systems FL emerges a new approach to tackle data privacy challenges in ML-based applications by decoupling of data storage and processing (i.e., local model training) at end-users‚Äô devices (i.e., local nodes) and the aggregation of a global ML model at a service provider‚Äôs server (i.e., a coordination server). The privacy-preservation advantage of FL compared to the traditional centralised ML approaches is undeniable: It enables to train an ML model whilst retaining personal training data on end-users‚Äô devices. Only locally trained model parameters, which contain the essential amount of information required to update the global model, are shared with a coordination server. Nevertheless, such model parameters still enclose some sensitive features that can be exploited to reconstruct or to infer related personal information as depicted in Section 4. Subsequently, an FL system still retains within the GDPR and is liable for complying with obligatory requirements. This section closely examines whether a GDPR requirement should be complied or inapplicable and should be waived in FL settings. Unsolved challenges on fully complying with the GDPR are also determined and discussed."
2011.05411,data,,,"Furthermore, local nodes not only passively contribute local training results but also get updated about intermediate stages of a global training model from a coordination server. This practice enables the opportunity for malicious participants to manipulate the training process by providing arbitrary updates in order to poison the global model [41, 9], which calls for an investigation on security models along with insightful analysis of privacy guarantees for a centralised FL framework. Accordingly, the FL framework then needs to be strengthened by employing further privacy and security mechanisms to protect personal data eÔ¨Äectively and to comply with intricate data protection legislation like the GDPR. A summary of related articles in terms of attack models with associated privacy preservation methods in centralised FL is depicted in Table 1. Detailed descriptions along with analysis are carried out in the following subsections."
2011.05411,data,,,"Furthermore, the requirements of purpose limitation and data minimisation are not always feasibly carried out in MLbased systems. The majority of ML algorithms heavily rely on data quality and quantity, thus researchers tend to collect as much related data as possible. Therefore, determining 1) the purposes of data collection as well as 2) what data is adequate, limited, and relevant only to the claimed purposes before executing such ML algorithms are problematic challenges. These requirements overly restrict the natural operations of ML-based services and applications to a smaller range than ever before."
2011.05411,data,,,"In most of the real-world scenarios, data, particularly personal data, is generated and stored in data silos, either end-users‚Äô devices or service providers‚Äô data centres. Most of conventional ML algorithms are operated in a centralised fashion, requiring training data to be fused in a data server. Essentially, collecting, aggregating and integrating heterogeneous data dispersed over various data sources as well as securely managing and processing the data are non-trivial tasks. The challenges are not only due to transporting highvolume, high-velocity, high-veracity, and heterogeneous data across organisations but also the industry competition, the complicated administrative procedures, and essentially, the"
2011.05411,data,,,"In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with diÔ¨Éculties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not suÔ¨Écient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR."
2011.05411,data,,,"In this federated setting, minimising the number of iterations in the optimisation algorithms is paramount of importance as there is limited communication capability of the local nodes. In the same paper, Koneƒçn`y et al. proposed a novel distributed gradient descent by combining the Stochastic Variance Reduced Gradient (SVRG) algorithm [64, 72] with the Distributed Approximate Newton algorithm (DANE) [110] for distributed optimisation called Federated SVRG (FSVRG) [70]. The FSVRG computes gradients based on data on each local node ùëò, obtains a weighted average of ‚Ñôùëò the parameters from all the ùêæ local nodes, and updates new parameters for each node after round. This algorithm is then experimented based on public Google+ posts, clustered by about 10, 000 users as local nodes, for predicting whether a post will receive any comments. The results show that the FSVRG outperforms the native gradient descent algorithm as it converges to the optimum within only 30 iterations."
2011.05411,data,,,"In this paper, we examine the centralised FL in which there exists a centralised server (i.e., service provider) requests to coordinate the whole training process. SpeciÔ¨Åcally, this coordination server (i) determines a global model to be trained, (ii) selects participants (i.e., local nodes) for each training round, (iii) aggregates local training results sent by the participants, (iv) updates the global model based on the aggregated results, (v) disseminates the updated model to the participants, and (vi) terminates the training when the global model satisÔ¨Åes some requirements (e.g., accurate enough). Local nodes passively train the model over their local data as requested, and send the training results back to the server whenever possible. The workÔ¨Çow cycle in a centralised FL framework consists of four steps (illustrated in Fig. 2) as follows:"
2011.05411,data,,,"It is worth noting that standard distributed ML algorithms are generally designed to train independent identically-distributed (IID) data, and this assumption does not hold in federated settings due to the signiÔ¨Åcant diÔ¨Äerences of the"
2011.05411,data,,,Keywords: Federated Learning Data Protection Regulation GDPR Personal Data Privacy Privacy Preservation
2011.05411,data,,,"Large-scale data collection, aggregation and processing at a central server in such ML-based systems not only entail the risks of severe data breaches due to single-point-offailure but also intensify the lack of transparency, data misuse and data abuse because the service providers are in full control of the whole data lifecycle [118]. In addition, as ML algorithms operate in a black-box manner, it is also challenging to provide insightful interpretation of how the algorithms execute and how certain decisions are made [83, 91]. Consequently, most of the ML-based systems Ô¨Ånd it diÔ¨Écult to satisfy the requirements of transparency, fairness, and automated decision-making in the GDPR."
2011.05411,data,,,"ML is a disruptive technology for designing and building intelligent systems that can automatically learn and improve from experience to accomplish a task without being explicitly programmed. For this purpose, an ML-based system builds up a mathematical model (i.e., model training process) based on a sample set (i.e., training data) whose parameters are to be optimised during this training process. As a result, the system can perform better predictions or decisions on a new, unseen task. Typically, an ML task can be formulated as a mathematical optimisation problem whose goal is to Ô¨Ånd the extremum of an objective function. Thus, an optimisation method is of paramount importance in any ML-based systems."
2011.05411,data,,,"Model poisoning attacks are always inherent in collaborative learning including FL. As shown by Bagdasaryan et in [6], just by controlling less than 1% Byzantine paral. ticipants, an adversary can successfully insert a backdoor functionality into a global model without reducing much accuracy, preventing the coordination server from detecting the attack. Solutions to mitigate model poisoning attack at server-side have to detect and Ô¨Ålter out poisoned model updates from malicious clients (i.e., model anomaly detection) [41, 63]. For this purpose, the server needs to access either participants‚Äô training data or parameter model updates, which breaks the privacy-preservation catalyst of FL. Besides, Secure Aggregation protocol is assumed to be implemented at both client- and server-side, which prevents the server from inspecting individual model updates; consequently, ruling out any solutions for model poisoning attacks [41]. Indeed, no resolutions have been proposed that eÔ¨Äectively tackle model poisoning attacks at server-side, which imposes as a critical research topic for FL."
2011.05411,data,,,"Mr. Florian Guitton received a BSc in Software Engineering from Epitech (France) in 2011 and a MSc in Advanced Computing from the University of Kent (United Kingdom) in 2012. In 2012 he joined the Discovery Sciences Group at Imperial College London where he became Research Assistant working on iHealth, eTRIKS and IDEA-FAST EU programs. He is currently a PhD candidate at Data Science Institute, Imperial College London working on distributed data collection and analysis pipeline in mixed-security environments with the angle of optimising user facing experiences."
2011.05411,data,,,"Mr. Siyao Wang is a PhD student of the Data Science Institute at Imperial College London. He received the BSc degree in Computer Science and Technology from the University of Chinese Academy of Sciences in 2018. He received the MRes degree in Medical Robotics and ImageGuided Intervention from Imperial College London in 2019. His research interests include machine learning, deep learning, computer vision and artiÔ¨Åcial intelligence applications in healthcare."
2011.05411,data,,,"Nevertheless, both centralised and decentralised architectures are required to acquire model consistency, particularly when data parallelism is employed. There are numerous strategies to update parameters in order to maintain the consistency of a global model, respected to a synchronisaIn this regard, Asyntion model among compute nodes. chronous Parallel (ASP) [99, 29], Bulk Synchronous Parallel (BSP) [47], and Stale Synchronous Parallel (SSP) [58] are the most common approaches to update parameters in a distributed learning system. The BSP and the ASP update parameters once receiving all gradients from a bulk of compute nodes (barrier synchronisation) and from just any node (no synchronisation), respectively. Generally, the BSP is relatively slow due to the stall time of waiting whereas ASP is faster as it does not perform any synchronisation; as a tradeoÔ¨Ä, the convergence in BSP is guaranteed but uncertain in"
2011.05411,data,,,"One of the fundamentals of FL is eÔ¨Écient optimisation algorithms for federated settings wherein training data is nonIID, massively and unevenly distributed across local nodes, Ô¨Årst introduced by Koneƒçn`y et al. in 2016 [70]. The distributed settings for the federated optimisation is formulated as follows. Let ùêæ be the number of local nodes, ‚Ñôùëò be the set of data samples stored on node ùëò ‚àà {1, 2, .., ùêæ}, and be the number of data samples stored on node ùëõùëò = |‚Ñôùëò| ùëò. As personal data in each local node is diÔ¨Äerent, we can assume that ‚Ñôùëò ‚à© ‚Ñôùëô = ‚àÖ if ùëò ‚â† ùëô and ‚àëùêæ ùëò=1 ùëõùëò = ùëõ. The distributed problem formulation for the minimisation objective is deÔ¨Åned as:"
2011.05411,data,,,"One of the privacy-preserving objectives of centralised FL is that a coordination server is unable to inspect the data or administer the training process at a local node. This, however, prohibits the transparency of the training process; thus, imposes a new vulnerability of a new type of attack called model poisoning [12, 87, 22, 41, 9, 6]. Generally, model poisoning attacks aim at manipulating training process by feeding poisoned local model updates to a coordination server. This type of attack is diÔ¨Äerent from data poisoning [11, 84, 124, 68, 23, 63], which is less eÔ¨Äective in FL settings [9, 6] because the original training data is never shared with a server. Thus, this section is mainly dedicated to analysing the model poisoning attacks in FL."
2011.05411,data,,,"Original training data End-users Service Provider Service Provider, Third-parties"
2011.05411,data,,,Personal Data Data Subject Data Controller Data Processor
2011.05411,data,,,"Standing on these federated optimisation research works, McMahan et al. proposed a variation of the SGD called FederatedSGD along with the Federated Averaging algorithm that can train a deep network at 100 times fewer communications compared to the naive FSVRG [81, 80]. The catalyst of such algorithms is to leverage the increasingly powerful processors in modern personal mobile devices to perform high-quality updates than simply calculating gradient steps. SpeciÔ¨Åcally, each client not only calculates the gradients but also computes the local model for multiple times; the coordination server only performs aggregation of the local models from the clients. This results in fewer training rounds iterations (thus fewer communications) while producing a decent global model. These proposed algorithms well suited for scenarios that are highly limited communication bandwidth with high jitter and latency. In these scenarios, the naive FSVRG algorithms proposed in [70, 71] are not eÔ¨Écient enough. Indeed, the algorithms are utilised for a realworld application for text prediction in Google keyboard in Android smartphones (i.e., G-board)5 [125]. In this system setting, the FederatedSGD is executed locally on the smartphone to compute gradient descent using local data. The gradient is then sent to an aggregation server. This server performs the FederatedAveraging algorithm which randomly selects a fraction of smartphones for each training round, and takes the average of all gradients sent from the selected participants to update the global model. This updated global model is distributed to all participants; the local nodes will then update their local models accordingly."
2011.05411,data,,,The GDPR deÔ¨Ånes 6-core principles as rational guidelines for service providers to manage personal data as illustrated in Fig. 7 (The GDPR Articles 5-11). These principles are broadly similar to the principles in the Data Protection Act 1998 with the accountability that obligates Data Controllers to take responsibility for complying with the principles and implementing appropriate measures to demonstrate the compliance.
2011.05411,data,,,"The GDPR requires Data Controllers to provide the following rights for Data Subjects if capable (The GDPR Articles 12-23): (1) Right to be informed, (2) Right of access, (3) Right to rectiÔ¨Åcation, (4) Right to erasure (Right to be forgotten), (5) Right to restrict processing, (6) Right to data portability, (7) Right to object, and (8) Rights in relation to automated decision making and proÔ¨Åling."
2011.05411,data,,,"The challenge to provide this right to Data Subjects is that the GDPR demands the Data Controller to concisely, intelligibly, and speciÔ¨Åcally specify what and how the local ML model is used in the FL training, along with expected outputs of the mechanism11. Same as many complex ML mechanisms, FL is as a black-box model; thus, it cannot be precisely interpreted of how it works and predicting the outcomes. The GDPR supervisory board recognises the challenges and relaxes the requirement for AI/ML mechanisms by accepting a general explanation as an indication of how and what personal data is going to be processed. As a result, for an FL system, the right to be informed is achieved as privacy information including purposes for processing local ML model (i.e., to build a global ML model), retention periods (i.e., no longer in use after each training round), and who it will be shared with (only the coordination server) can be determined as in Terms and Conditions when a client accepts to participate in an FL system."
2011.05411,data,,,"The data minimisation principle in the GDPR necessitates a Data Controller (e.g., a service provider) to collect and process personal data that is adequate, limited, and relIn traditional centralised evant only to claimed purposes. ML algorithms, this data minimisation requirement is a challenge as it is not always possible to envision what data and the minimal amount of data are necessary for training an ML model. In this regard, FL appears as a game-changer as an FL system does not need to collect and process original training data; instead, a service provider only needs to gather local ML models from participants for assembling the global model. Generally, with privacy-preserving techniques introduced in Section 4, an FL system can assure that the coordination server obtains aggregated local model parameters from participants for global model updates only (i.e., the claimed purposes) while acquiring nothing about individual‚Äôs contribution. The aggregation mechanism also assures that the global model itself contains no individual sensitive features that can be exploited by adversaries to extract or infer any personal information."
2011.05411,data,,,"The natural advantage of FL compared to the traditional cloud-centric ML approaches is the ability to reassure data privacy and (presumably) comply with the GDPR because personal data is stored and processed locally, and only model parameters are exchanged. In addition, the processes of pa 2https://gdpr-info.eu/"
2011.05411,data,,,"The nature of decoupling between data storage and processing at client-side and global ML model aggregation at server-side in centralised FL leads to the unnecessity of providing the (2) Right of access, (3) Right to rectiÔ¨Åcation, (4) Right to erasure, (5) Right to restrict processing, (6) Right to data portability, and (7) Right to object. For instance, regarding the ""Right to erasure"", if a user requests to delete its data (i.e., local ML model parameters sent to an FL server), literally, the only way to fulÔ¨Ål the user‚Äôs request is to thoroughly re-train the global model without using user‚Äôs data from the round that the user Ô¨Årst participates [50]. This is unnecessary and impractical in FL settings as only local ML model parameters (possibly privacy guarantee-strengthened with diÔ¨Äerential privacy) in aggregated encrypted forms (by using Secure Aggregation and other advanced cryptography techniques) are shared with a coordination server. Consequently, it is worthless for a Data Subject to have control over its local ML model as (i) the model parameters are protected by privacy-preserving techniques from inference attacks; (ii) the server is unable to separate the user‚Äôs data from the others, the server also does not store the model once it is aggregated to update the global model; and (iii) the global model is wholly anonymised and cannot be exploited to extract or infer any individual information."
2011.05411,data,,,"The purpose of this principle is to ensure that a Data Controller should keep personal data correctly, updated, and not misleading any matter of fact. In centralised FL settings, a coordination server does not store any individual locally trained ML model parameters except the aggregated results from a batch of participants, and the anonymised global ML model. This information is stored and processed (i.e., for updating global model) in its original form without any changes, and updated for every training round. For these reasons, FL systems automatically satisfy the GDPR accuracy principle."
2011.05411,data,,,"This principle obligates Data Controllers to implement appropriate measures in place to eÔ¨Äectively protect personal data. Thus, in order to comply with this principle, a centralised FL system requires to implement security and privacy mechanisms not only at a coordination server but also at end-users‚Äô devices as the FL system itself does not guarantee security and privacy."
2011.05411,data,,,"This purpose limitation principle can be interpreted that an FL service provider needs to clearly inform clients about the purpose of a global ML model training as well as how clients‚Äô local personal data and devices‚Äô computation are used to locally train a requested ML model provided by the service provider. The principle also states that the service provider can further process the data for other compatible purposes. In this respect, FL systems fully satisfy with the principles if suÔ¨Écient privacy-preserving mechanisms such as Secure Aggregation and diÔ¨Äerential privacy are readily implemented into the systems. This is because locally trained ML models from clients are aggregated only for the global model updates and cannot be individually extracted and exploited (by the coordination server) for other purposes."
2011.05411,data,,,This research was supported by the HNA Research Centre for Future Data Ecosystems at Imperial College London and the Innovative Medicines Initiative 2 IDEA-FAST project under grant agreement No 853981.
2011.05411,data,,,"To ensure privacy, an FL system is designed in a way that does not let the service provider (i.e., the coordination server) to directly access and obtain either original training data or locally trained ML models at end-users‚Äô devices. Instead, end-users, as participants in the FL system, will only send the results back to the coordination server when they are ready. An FL client-side application should oÔ¨Äer several options for clients to participate in the training process proactively that allows a client to fully control the local training as well as of the sending/receiving ML model updates to/from a coordination server. Furthermore, FL systems only process data (i.e., local ML model parameters) for an explicit purpose (i.e., aggregates results and updates a global model), which is in ways that clients would reasonably expect whilst having minimal privacy impact. For these reasons, either Consent or Legitimate Interest legal basis can be appropriate for an FL application10."
2011.05411,data,,,"We are now living in a data-driven world where most of applications and services such as health-care and medical services, autonomous cars, and Ô¨Ånance applications are based on artiÔ¨Åcial intelligence (AI) technology with complex data-hungry machine learning (ML) algorithms. AI has been showing advances in every aspect of lives and expected to ""change the world more than anything in the history of mankind. More than electricity.‚Äù 1. However, the AI technology is yet to reach its full potential, also the realisation of such AI/ML-based applications has been still facing longstanding challenges wherein centralised storage and computation is one of the critical reasons."
2011.05411,data,,,"[108] Sarwate, A.D., Chaudhuri, K., 2013. Signal processing and machine learning with diÔ¨Äerential privacy: Algorithms and challenges for continuous data. IEEE signal processing magazine 30, 86‚Äì94."
2011.05411,data,,,"[118] Truong, N.B., Sun, K., Lee, G.M., Guo, Y., 2019. Gdpr-compliant personal data management: A blockchain-based solution. IEEE Transactions on Information Forensics and Security 15, 1746‚Äì1761. [119] Wachter, S., Mittelstadt, B., Floridi, L., 2017. Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7, 76‚Äì 99."
2011.05411,data,,,"[124] Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F., 2015. Is feature selection secure against training data poisoning?, in: International Conference on Machine Learning, pp. 1689‚Äì1698. [125] Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., Beaufays, F., 2018. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903 ."
2011.05411,data,,,"[128] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V., arXiv preprint Federated learning with non-iid data."
2011.05411,data,,,"[14] Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K., 2016. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482 ."
2011.05411,data,,,"[23] Chen, X., Liu, C., Li, B., Lu, K., Song, D., 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 ."
2011.05411,data,,,"[24] Chen, X.W., Lin, X., 2014. Big data deep learning: challenges and"
2011.05411,data,,,"[31] Du, W., Han, Y.S., Chen, S., 2004. Privacy-preserving multivariate statistical analysis: Linear regression and classiÔ¨Åcation, in: Proceedings of the 2004 SIAM international conference on data mining, SIAM. pp. 222‚Äì233."
2011.05411,data,,,"[34] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M., 2006. Our data, ourselves: Privacy via distributed noise generation, in: Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer. pp. 486‚Äì503."
2011.05411,data,,,"[36] Dwork, C., Smith, A., Steinke, T., Ullman, J., 2017. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application 4, 61‚Äì84."
2011.05411,data,,,"[39] Friedman, A., Schuster, A., 2010. Data mining with diÔ¨Äerential privacy, in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 493‚Äì502. [40] Fung, B.C., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: A survey of recent developments. ACM Computing Surveys (Csur) 42, 1‚Äì53."
2011.05411,data,,,"[44] Gentry, C., 2010. Computing arbitrary functions of encrypted data."
2011.05411,data,,,"[49] Gilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M., Wernsing, J., 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy, in: International Conference on Machine Learning, pp. 201‚Äì210."
2011.05411,data,,,"[50] Ginart, A., Guan, M., Valiant, G., Zou, J.Y., 2019. Making ai forget you: Data deletion in machine learning, in: Advances in Neural Information Processing Systems, pp. 3518‚Äì3531."
2011.05411,data,,,"[59] Horvitz, E., Mulligan, D., 2015. Data, privacy, and the greater good."
2011.05411,data,,,"[5] Ateniese, G., Mancini, L.V., Spognardi, A., Villani, A., Vitali, D., Felici, G., 2015. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classiÔ¨Åers. International Journal of Security and Networks 10, 137‚Äì150."
2011.05411,data,,,"[61] Ienca, M., Vayena, E., 2020. On the responsible use of digital data to tackle the covid-19 pandemic. Nature medicine 26, 463‚Äì464. [62] Jagannathan, G., Wright, R.N., 2005. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 593‚Äì599."
2011.05411,data,,,"[78] Lindell, Y., Pinkas, B., 2000. Privacy preserving data mining, in: Annual International Cryptology Conference, Springer. pp. 36‚Äì54. [79] Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, l-diversity: Privacy beyond k-anonymity. ACM Trans M., 2007. actions on Knowledge Discovery from Data (TKDD) 1, 3‚Äìes. [80] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017a. Communication-eÔ¨Écient learning of deep networks from decentralized data, in: ArtiÔ¨Åcial Intelligence and Statistics, pp. 1273‚Äì 1282."
2011.05411,data,,,"can be generated by injecting a hidden backdoor model intentionally, as illustrated in Fig. 4. Compromised participants analyse the targeted global model; the poisoned model is then trained on backdoor data samples using dedicated techniques such as constrain-and-scale accordingly, and feed the parameters to a coordination server as other honest participants. The objective of this attack is that the global model is replaced by a joint model consisting of both original task and the injected backdoor sub-task while retaining high accuracy on the two. The backdoor training at the adversary can be empowered by modifying minimisation strategies such as constrain-and-scale, which optimises both gradients of the loss and the backdoor objective [6]. A parameter estimation mechanism is then used for generating parameters submitted to the coordination server for honest participants‚Äô updates. As secure aggregation is used for preventing the server from inspecting individual models, this poisoning model is unable to detect [9, 6]."
2011.05411,data,,,"clarify the whole data management processes along with the necessity and proportionality of these processes. Such assessments are important tools for accountability and essential to eÔ¨Éciently manage the data security and privacy risks, to demonstrate the compliance, as well as to determine the measure have been taken to address the risks. However, carrying out a DPIA or PIA is not mandatory for every data processing operation. It is only required when the operation is ""likely to result in a high risk to the rights and freedoms of natural persons"" (Article 35(1)). The guideline for the criteria on the DPIA/PIA obligatory is described under Article 35(3), 35(4) which are adopted by DPAs to carry out such assessments."
2011.05411,data,,,"data protection regulations and restrictions such as the EU General Data Protection Regulation (GDPR)2 [59]. In traditional ML algorithms, large-scale data collection and processing at a powerful cloud-based server entails the singlepoint-of-failure and the risks of severe data breaches. Foremost, centralised data processing and management impose limited transparency and provenance on the system, which could lead to the lack of trust from end-users as well as the diÔ¨Éculty in complying with the GDPR [118]."
2011.05411,data,,,"data; instead, inferring attributes or membership of the original trained data from local model parameters can also induce serious privacy leakage [42, 85, 86, 93, 94] (e.g., an attacker can Ô¨Ågure out whether a speciÔ¨Åc data sample (of a patient) is used to train a model of a disease). This is the baseline for the membership attack. Authors in [85, 86, 94] have investigated membership attacks in FL and demonstrated the capability of these attacks in both passive and active approaches. For instance, the gender of a victim can be inferred with a very high accuracy of 90% when conducting this attack in a binary gender classiÔ¨Åer on the FaceScrub dataset7. Other features, which are uncorrelated with the main task, can also be inferred such as race and facial appearance (e.g., whether a face photo is wearing glasses) [86]. Nasr et al. proposed an active attack approach called gradient ascent by exploiting the privacy vulnerabilities of SGD optimisation algorithms. This attack based on the correlation between the local gradients of the loss and the direction and the amount of changes of model parameters when minimising the loss to Ô¨Åt a model to train data samples in the SGD algorithms. This active membership attack was conducted on the CIFAR100"
2011.05411,data,,,"for the aggregation of independently trained neural networks in [95]. Since then, this technique has been improved to return statistically indistinguishable results among participants while ensuring that such noise-added model parameters do not aÔ¨Äect much on the accuracy of the global model in FL settings [111, 4, 48, 1, 114, 82]. As a consequence, adversaries cannot distinguish individual records in the FL training process and do not know whether or not a targeted client participating in the training; thus, preserving data privacy and protecting against the inference attacks. Generally, there are two types of employing diÔ¨Äerential privacy techniques for local nodes in FL settings: batch-level and userlevel where random noise is added by measuring parameters‚Äô sensitivity from data points in a mini-batch and users themselves, respectively."
2011.05411,data,,,"ing set without having access to the original data. For instance, Hitaj et al. based on GANs have developed an attack at user-level which allows an insider to infer information from a victim just by analysing the shared model parameters in some consecutive training rounds [57]. This attack can be accomplished at client-side without interfering the whole FL procedure, even when the local model parameters are obfuscated using DP technique. A malicious coordination server can also recover partial personal data by inspecting the proportionality between locally trained model parameters sent to the server and the original data samples [4, 122]."
2011.05411,data,,,"member of the Academia Europaea. His research interests are in the areas of data mining for largescale scientiÔ¨Åc applications including distributed data mining methods, machine learning and informatics systems."
2011.05411,data,,,"methods. Frontiers in Applied Mathematics and Statistics 3, 9. [73] Li, N., Li, T., Venkatasubramanian, S., 2007. t-closeness: Privacy beyond k-anonymity and l-diversity, in: 2007 IEEE 23rd International Conference on Data Engineering, IEEE. pp. 106‚Äì115. [74] Li, O., Liu, H., Chen, C., Rudin, C., 2017. Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions. arXiv preprint arXiv:1710.04806 ."
2011.05411,data,,,"number of data samples and data distributions among personal mobile devices. Training over non-IID data has been shown to be much less accurate as well as slower convergence than IID data in federated settings [128]. Koneƒçn`y with his colleagues at Google went further on improving the eÔ¨Éciency of the FSVRG algorithms in distributed settings by minimising the information in parameter update to be sent to an orchestration server [71]. Two types of updates are considered called structured updates and sketched updates in which the number of variables used in an ML model is minimised as many as possible, along with the compression of the information in the full model updates. Another ambitious federated optimisation approach is that local nodes are independently trained diÔ¨Äerent ML models as a task in a multi-learning objective simultaneously [113]. Generally, local nodes generate data under diÔ¨Äerent distributions which naturally Ô¨Åt separate learning models; however, these models are structurally similar resulting in the ability to model the similarity using a multi-tasking learning (MTL) framework. Therefore, this approach improves performance when dealing with non-IID data as well as guarantees the learning convergence [113]."
2011.05411,data,,,"rameters updates and aggregation between local nodes and a central coordination server are strengthened by privacypreserving and cryptography techniques, which enhance data security and privacy [48, 123, 14, 15, 96]. The FL capability could potentially inaugurate new opportunities for service providers to implement some sorts of ML algorithms for their applications and services without acquiring clients‚Äô personal data, hence naturally complying with data protection regulations like the GDPR. Unfortunately, despite the distributed collaborative learning model of FL empowered by privacy-preserving measures, personal information can be stealthily extracted from local training parameters [4, 96, 130, 57, 86]. As a consequence, FL-based service providers still stay within the regulatory personal data protection framework and are still liable for implementing GDPR-compliant mechanisms when dealing with EU/UK citizens."
2011.05411,data,,,"share a global ML model, which can be considered as anonymous information, with third-parties as it does not possess any other personal data (e.g., original training data as in traditional ML systems). Therefore, Data Processors in FL settings are also the service providers, but not other players (i.e., third-parties). The processing mechanisms in FL are also uncomplicated compared to the traditional ones as they are only related to the aggregation of the local ML models as well as the update of the global ML model."
2011.05411,data,,,"sonal data, any previous infringement, and the nature, gravity, and duration of the current infringement. For instance, Facebook and Google were hit with a collective $8.8 billion lawsuit (Facebook, 3.9 billion euro; Google, 3.7 billion euro) by Austrian privacy campaigner, Max Schrems, alleging violations of GDPR as it pertains to the opt-in/opt-out clauses. SpeciÔ¨Åcally, the complaint alleges that the way these companies obtain user consent for privacy policies is an ""all-ornothing"" choice, asking users to check a small box allowing them to access services. It is a clear violation of the GDPR‚Äôs provisions per privacy experts and the EU/UK. A list of Ô¨Ånes and notices (with non-compliance reasons) issued under the GDPR can be found on Wikipedia12"
2011.05411,data,,,to protect data in transit between clients and the server.
2011.05411,data,,,‚Ä¢ Provide insightful examination on pros and cons of the existing privacy-preserving techniques as well as prospective solution approaches in order for a FL-based service to comply with the EU/UK General Data Protection Regulation (GDPR).
2103.12883,data,,,"Abstract‚Äî Since the application of Deep Q-Learning to the continuous action domain in Atari-like games, Deep Reinforcement Learning (Deep-RL) techniques for motion control have been qualitatively enhanced. Nowadays, modern Deep-RL can be successfully applied to solve a wide range of complex decision-making tasks for many types of vehicles. Based on this context, in this paper, we propose the use of Deep-RL to perform autonomous mapless navigation for Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs), robots that can operate in both, air or water media. We developed two approaches, one deterministic and the other stochastic. Our system uses the relative localization of the vehicle and simple sparse range data to train the network. We compared our approaches with a traditional geometric tracking controller for mapless navigation. Based on experimental results, we can conclude that Deep-RLbased approaches can be successfully used to perform mapless navigation and obstacle avoidance for HUAUVs. Our vehicle accomplished the navigation in two scenarios, being capable to achieve the desired target through both environments, and even outperforming the geometric-based tracking controller on the obstacle-avoidance capability."
2103.12883,data,,,"In this work, we aim to navigate our described HUAUV autonomously from a starting point to a target point in a different environment without any environmental knowledge, preventing any collision with the scenario by using only range readings and the vehicle localization data. Its translation function, therefore, is deÔ¨Åned as:"
2103.12883,data,,,"Our proposal differs from the discussed works by only using the vehicle‚Äôs relative localization data and not its explicit localization data. We also propose two Deep-RL approaches which we called 3DNDRL-D and 3DNDRL-S, a deterministic one and also a bias-stochastic one to further enhance our work. We compare them with the mapless navigation performed by a traditional geometric tracking controller that can be used for mapless navigation [16]."
2103.12883,data,,,"This work contains the following contributions: ‚Ä¢ We propose two new approaches based on state-ofart Deep-RL algorithms for ground robots that can successfully perform goal-oriented mapless navigation for HUAUVs, using only range data readings and the vehicles‚Äô relative localization data."
2103.12883,data,,,"[26] K. Kang, S. Belkhale, G. Kahn, P. Abbeel, and S. Levine, ‚ÄúGeneralization through simulation: Integrating simulated and real data into deep reinforcement learning for vision-based autonomous Ô¨Çight,‚Äù in Int. Conf. on Robotics and Automation (ICRA). IEEE, 2019, pp. 6008‚Äì6014."
2103.12883,data,,,"agents also learned to bypass obstacles to arrive at a target, where traditional algorithms fail. We have validated two powerful approaches, which managed to understand the complex behaviors in realistic simulation and that can be used in the real world if guarantying range Ô¨Åndings and relative localization data."
2103.12883,data,,,"both air and underwater environments can be obtained using sensors like a lidar and a Sonar, respectively. As well as for the vehicle‚Äôs localization data, which could be obtained by a combination of sensors like GPS and USBL for example."
2108.09408,data,,,"In general, there is no need to down-sample the feature at 7 √ó 7 scale because it has a suÔ¨Éciently large receptive Ô¨Åeld. However, the experiment results in Table. 2 is somewhat counterintuitive. It shows that there is still some diÔ¨Äerent global semantic information waiting to be discovered. The version for comparison is a U-shaped vanilla model, which is trained with the initial learning rate 10‚àí4. Here we use the same learning rate to train the parameters in the backbone and modules designed in our method. The rise of M AE, mF , Sm and Em reÔ¨Çects ADM‚Äôs reliability and validity. Fig. 4 shows the comparison of visualized results between the vanilla network with and without ADM. UENs generate multiscale features on account of the input level and integrate the features with edge information, which guides the model to pay attention to the boundary areas. The experimental data also reveals that the UENs make greater improvements in various aspects."
2108.09408,data,,,"Multi-scale feature aggregation based. The problems mentioned above are also partly caused by inappropriate aggregation and fusion. Thus, many articles looked for more eÔ¨Écient and eÔ¨Äective methods to aggregate low-level features with detailed information and high-level features with semantic clues. Feng et al. [8] employ a Dilated Convolutional Pyramid Pooling (DCPP) module to generate a coarse prediction based on global contextual knowledge. In MINet [16], the aggregate interaction module can eÔ¨Éciently utilize the features from adjacent layers through mutual learning, while the self-interaction module makes the network adaptively extract multi-scale information from data and better deal with scale variation. Li et al. [10] present a residual reÔ¨Ånement network with semantic context features, including a global attention module, a multireceptive block module, and a recurrent residual module. These methods are mainly innovative in feature extraction and fusion, but they do not explore features with more possibilities."
