{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "device count: 1\n",
      "device name: NVIDIA GeForce GTX 980M\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "print(\"device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/colinlagator/pytorch-bert-multi-label/notebook\n",
    "\n",
    "https://discuss.huggingface.co/t/download-models-for-local-loading/1963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pattern</th>\n",
       "      <th>token_count</th>\n",
       "      <th>update_date</th>\n",
       "      <th>label</th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710.02907</td>\n",
       "      <td>data, dataset</td>\n",
       "      <td>280</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>0</td>\n",
       "      <td>Experiment 2: In this set of experiments, we e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1811.11012</td>\n",
       "      <td>data</td>\n",
       "      <td>195</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>0</td>\n",
       "      <td>This section of the technical report is focuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1811.11012</td>\n",
       "      <td>data, dataset</td>\n",
       "      <td>70</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>0</td>\n",
       "      <td>volunteers’ vehicles were mounted with BSM-bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1912.09582</td>\n",
       "      <td>dataset</td>\n",
       "      <td>13</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>0</td>\n",
       "      <td>for small datasets–a case with Dutch book revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1912.09582</td>\n",
       "      <td>dataset</td>\n",
       "      <td>15</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Table 4: Sentiment Analysis accuracy scores on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        pattern  token_count update_date  label  \\\n",
       "0  1710.02907  data, dataset          280  2022-04-21      0   \n",
       "1  1811.11012           data          195  2022-04-21      0   \n",
       "2  1811.11012  data, dataset           70  2022-04-21      0   \n",
       "3  1912.09582        dataset           13  2022-04-21      0   \n",
       "4  1912.09582        dataset           15  2022-04-21      1   \n",
       "\n",
       "                                                para  \n",
       "0  Experiment 2: In this set of experiments, we e...  \n",
       "1  This section of the technical report is focuse...  \n",
       "2  volunteers’ vehicles were mounted with BSM-bro...  \n",
       "3  for small datasets–a case with Dutch book revi...  \n",
       "4  Table 4: Sentiment Analysis accuracy scores on...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_folder = Path().cwd().parent.parent / \"data\"\n",
    "path_interim_folder = path_data_folder / \"interim\"\n",
    "path_label_folder = path_data_folder / \"processed\" / \"labels\" / \"labels_complete\"\n",
    "\n",
    "# load the labels.csv from the path_label_folder\n",
    "df = pd.read_csv(path_label_folder / \"labels.csv\", dtype={\"id\": str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so we can solve the dual comparison problem (18) using any eﬃcient svm solver, such as libsvm (chang & lin 2011). we used the r interface in the kernlab package (karatzoglou et al. 2004), and our code is available in the ranksvmcompare package on github.\n"
     ]
    }
   ],
   "source": [
    "text = \"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github.\"\n",
    "\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('/home/tvhahn/scibert_scivocab_uncased') # hpc\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')  # local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "['so', 'we', 'can', 'solve', 'the', 'dual', 'comparison', 'problem', '(', '18', ')', 'using', 'any', '[UNK]', 'svm', 'solver', ',', 'such', 'as', 'lib', '##svm', '(', 'chang', '&', 'lin', '2011', ')', '.', 'we', 'used', 'the', 'r', 'interface', 'in', 'the', 'kern', '##lab', 'package', '(', 'kar', '##atz', '##og', '##lo', '##u', 'et', 'al', '.', '2004', ')', ',', 'and', 'our', 'code', 'is', 'available', 'in', 'the', 'ranks', '##vm', '##compare', 'package', 'on', 'gi', '##th', '##ub', '.']\n",
      "[564, 185, 300, 5191, 111, 4793, 2029, 1167, 145, 1178, 546, 487, 843, 101, 11422, 14699, 422, 555, 188, 8147, 22228, 145, 1044, 894, 3158, 5228, 546, 205, 185, 501, 111, 182, 3396, 121, 111, 5092, 4253, 7526, 145, 7402, 8665, 247, 609, 30120, 365, 186, 205, 6706, 546, 422, 137, 580, 2737, 165, 1427, 121, 111, 18949, 12986, 26513, 7526, 191, 4706, 266, 284, 205]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  text,\n",
    "  max_length=512,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  padding='max_length',\n",
    "  return_attention_mask=True,\n",
    "  truncation=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "# encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'so',\n",
       " 'we',\n",
       " 'can',\n",
       " 'solve',\n",
       " 'the',\n",
       " 'dual',\n",
       " 'comparison',\n",
       " 'problem',\n",
       " '(',\n",
       " '18',\n",
       " ')',\n",
       " 'using',\n",
       " 'any',\n",
       " '[UNK]',\n",
       " 'svm',\n",
       " 'solver',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'lib',\n",
       " '##svm',\n",
       " '(',\n",
       " 'chang',\n",
       " '&',\n",
       " 'lin',\n",
       " '2011',\n",
       " ')',\n",
       " '.',\n",
       " 'we',\n",
       " 'used',\n",
       " 'the',\n",
       " 'r',\n",
       " 'interface',\n",
       " 'in',\n",
       " 'the',\n",
       " 'kern',\n",
       " '##lab',\n",
       " 'package',\n",
       " '(',\n",
       " 'kar',\n",
       " '##atz',\n",
       " '##og',\n",
       " '##lo',\n",
       " '##u',\n",
       " 'et',\n",
       " 'al',\n",
       " '.',\n",
       " '2004',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'our',\n",
       " 'code',\n",
       " 'is',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ranks',\n",
       " '##vm',\n",
       " '##compare',\n",
       " 'package',\n",
       " 'on',\n",
       " 'gi',\n",
       " '##th',\n",
       " '##ub',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# for hpc (need to manually download model)\n",
    "# model = AutoModel.from_pretrained('/home/tvhahn/scibert_scivocab_uncased')\n",
    "\n",
    "# for local computer\n",
    "# model = AutoModel.from_pretrained('/home/tvhahn/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "['so', 'we', 'can', 'solve', 'the', 'dual', 'comparison', 'problem', '(', '18', ')', 'using', 'any', '[UNK]', 'svm', 'solver', ',', 'such', 'as', 'lib', '##svm', '(', 'chang', '&', 'lin', '2011', ')', '.', 'we', 'used', 'the', 'r', 'interface', 'in', 'the', 'kern', '##lab', 'package', '(', 'kar', '##atz', '##og', '##lo', '##u', 'et', 'al', '.', '2004', ')', ',', 'and', 'our', 'code', 'is', 'available', 'in', 'the', 'ranks', '##vm', '##compare', 'package', 'on', 'gi', '##th', '##ub', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words, etc\n",
    "stop = stopwords.words('english')\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "\n",
    "# text = text.lower()\n",
    "# text = text.apply(lambda x: x.split(' '))\n",
    "# text = text.apply(lambda x: [item for item in x if item not in stop])\n",
    "# text = text.apply(lambda x: ' '.join(x))\n",
    "# text = text.apply(lambda x: re.sub('[^A-Za-z\\s]+', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub('\\n', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub(r'^\\s', '', x))\n",
    "# text = text.apply(lambda x: re.sub(r'\\s$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original study , authors reported results grid-independence study justify spatial temporal grid resolutions used parametric study . compared force coefficients , profiles velocity components , profiles fluctuating kinetic energy , distances vortical structures near wake , obtained different grid resolutions . , also report results grid-independence study moving results parametric study . use domain size original study : 30c × 25c × 25c ( c chord length wing ) . root wing ( around plate undergoes rolling/pitching motion ) located center computational domain . keep spatial grid uniform ( highest resolution ) sub-area domain covers motion wing . outside area , also add extra uniform layer grid-spacing size ∆x = 0.05c , sub-domain [ −2c , 6c ] × [ −3c , 3c ] × [ −1c , 2c ] , covers near-wake region . ( opted smooth transition two uniform regions , grid-cell widths stretched constant ratio 1.1 directions , except streamwise direction behind wing used ratio 1.03 . ) finally , grid-cell width stretched external boundaries constant ratio 1.2. readers interested inspecting geometric characteristics grids used present study : used python scripts codify grid parameters saved petibm-readable yaml files ( available github repository ) . present study , model wing flat elliptical surface , discretized lagrangian markers uniformly distributed surface ( similar resolution grid-spacing size background eulerian grid ) . original study , consider case circular wing ( ar = 1.27 ) reynolds number = 200 , strouhal number st = 0.6 , phase-difference angle ψ = 90o , assess independence numerical results . investigated effect grid-spacing size , time-step size , convergence criterion iterative solvers , numerical solution . assess effect grid-spacing size ∆x vicinity wing solution , computed five flapping cycles three grids : coarse ( ∆x = 0.03c ) , nominal ( ∆x ='"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n",
      "['in', 'the', 'original', 'study', ',', 'the', 'authors', 'reported', 'the', 'results', 'of', 'a', 'grid', '-', 'independence', 'study', 'to', 'justify', 'the', 'spatial', 'and', 'temporal', 'grid', 'resolutions', 'used', 'for', 'the', 'parametric', 'study', '.', 'they', 'compared', 'force', 'coefficients', ',', 'profiles', 'of', 'the', 'velocity', 'components', ',', 'profiles', 'of', 'the', 'fluctuating', 'kinetic', 'energy', ',', 'and', 'distances', 'between', 'vor', '##tical', 'structures', 'in', 'the', 'near', 'wake', ',', 'obtained', 'with', 'different', 'grid', 'resolutions', '.', 'here', ',', 'we', 'also', 'report', 'the', 'results', 'of', 'our', 'grid', '-', 'independence', 'study', 'before', 'moving', 'on', 'to', 'the', 'results', 'of', 'the', 'parametric', 'study', '.', 'we', 'use', 'the', 'same', 'domain', 'size', 'as', 'in', 'the', 'original', 'study', ':', '30', '##c', '×', '25', '##c', '×', '25', '##c', '(', 'where', 'c', 'is', 'the', 'chord', 'length', 'of', 'the', 'wing', ')', '.', 'the', 'root', 'of', 'the', 'wing', '(', 'around', 'which', 'the', 'plate', 'undergoes', 'the', 'rolling', '/', 'pitch', '##ing', 'motion', ')', 'is', 'located', 'at', 'the', 'center', 'of', 'the', 'computational', 'domain', '.', 'we', 'keep', 'the', 'spatial', 'grid', 'uniform', '(', 'with', 'highest', 'resolution', ')', 'in', 'the', 'sub', '-', 'area', 'of', 'the', 'domain', 'that', 'covers', 'the', 'motion', 'of', 'the', 'wing', '.', 'outside', 'this', 'area', ',', 'we', 'also', 'add', 'an', 'extra', 'uniform', 'layer', 'with', 'grid', '-', 'spacing', 'size', '∆', '##x', '=', '0', '.', '05', '##c', ',', 'in', 'the', 'sub', '-', 'domain', '[', '−2', '##c', ',', '6', '##c', ']', '×', '[', '−3', '##c', ',', '3', '##c', ']', '×', '[', '−1', '##c', ',', '2', '##c', ']', ',', 'which', 'covers', 'the', 'near', '-', 'wake', 'region', '.', '(', 'we', 'opt', '##ed', 'for', 'a', 'smooth', 'transition', 'between', 'the', 'two', 'uniform', 'regions', ',', 'in', 'which', 'the', 'grid', '-', 'cell', 'widths', 'are', 'stretched', 'with', 'a', 'constant', 'ratio', 'of', '1', '.', '1', 'in', 'all', 'directions', ',', 'except', 'in', 'the', 'stream', '##wise', 'direction', 'behind', 'the', 'wing', 'where', 'we', 'used', 'a', 'ratio', 'of', '1', '.', '03', '.', ')', 'finally', ',', 'the', 'grid', '-', 'cell', 'width', 'is', 'stretched', 'to', 'the', 'external', 'boundaries', 'with', 'a', 'constant', 'ratio', 'of', '1', '.', '2', '.', 'to', 'the', 'readers', 'interested', 'in', 'further', 'insp', '##ecting', 'the', 'geometric', 'characteristics', 'of', 'the', 'grids', 'used', 'in', 'the', 'present', 'study', ':', 'we', 'used', 'python', 'scripts', 'to', 'cod', '##ify', 'the', 'grid', 'parameters', 'and', 'saved', 'them', 'into', 'pet', '##ib', '##m', '-', 'read', '##able', 'yam', '##l', 'files', '(', 'available', 'on', 'the', 'gi', '##th', '##ub', 'repository', ')', '.', 'in', 'the', 'present', 'study', ',', 'we', 'model', 'the', 'wing', 'with', 'a', 'flat', 'elliptical', 'surface', ',', 'discretized', 'with', 'lagrangian', 'markers', 'uniformly', 'distributed', 'on', 'its', 'surface', '(', 'with', 'a', 'similar', 'resolution', 'as', 'the', 'grid', '-', 'spacing', 'size', 'of', 'the', 'background', 'euler', '##ian', 'grid', ')', '.', 'as', 'in', 'the', 'original', 'study', ',', 'we', 'consider', 'the', 'case', 'of', 'a', 'circular', 'wing', '(', 'ar', '=', '1', '.', '27', ')', 'with', 'reynolds', 'number', 're', '=', '200', ',', 'stro', '##uh', '##al', 'number', 'st', '=', '0', '.', '6', ',', 'and', 'phase', '-', 'difference', 'angle', 'ψ', '=', '90', '##o', ',', 'to', 'assess', 'independence', 'in', 'the', 'numerical', 'results', '.', 'we', 'investigated', 'the', 'effect', 'of', 'the', 'grid', '-', 'spacing', 'size', ',', 'the', 'time', '-', 'step', 'size', ',', 'and', 'the', 'convergence', 'criterion', 'of', 'the', 'iterative', 'solvers', ',', 'on', 'the', 'numerical', 'solution', '.', 'to', 'assess', 'the', 'effect', 'of', 'the', 'grid', '-', 'spacing', 'size', '∆', '##x', 'in', 'the', 'vicinity', 'of', 'the', 'wing', 'on', 'the', 'solution', ',', 'we', 'computed', 'five', 'flap', '##ping', 'cycles', 'on', 'three', 'grids', ':', 'coarse', '(', '∆', '##x', '=', '0', '.', '03', '##c', ')', ',', 'nominal', '(', '∆', '##x', '=']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/tvhahn/arxiv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_tokens = tokenizer.batch_encode_plus(text, pad_to_max_length=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2441, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[102, 259, 103,  ...,   0,   0,   0],\n",
       "        [102, 146, 103,  ...,   0,   0,   0],\n",
       "        [102, 103,   0,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [102, 412, 103,  ...,   0,   0,   0],\n",
       "        [102, 103,   0,  ...,   0,   0,   0],\n",
       "        [102, 275, 103,  ...,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17e082919eb97a8b1648db68459a0548143f50884a45122adabc4767e3d2dece"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
