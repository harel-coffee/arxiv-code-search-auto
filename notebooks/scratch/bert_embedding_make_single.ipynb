{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchmetrics import PrecisionRecallCurve\n",
    "from transformers import AutoModel\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path().cwd().parent.parent\n",
    "\n",
    "path_label_dir = proj_dir / \"data\" / \"interim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "device count: 1\n",
      "device name: NVIDIA GeForce GTX 980M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "print(\"device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scratch_path = Path.home() / \"scratch\"\n",
    "if scratch_path.exists():\n",
    "    tokenizer = BertTokenizer.from_pretrained(proj_dir / \"bert_cache_dir\")\n",
    "    model = AutoModel.from_pretrained(proj_dir / \"bert_cache_dir\")\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pattern</th>\n",
       "      <th>token_count</th>\n",
       "      <th>update_date</th>\n",
       "      <th>label</th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1602.06797</td>\n",
       "      <td>data</td>\n",
       "      <td>6</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>of data with given labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1602.06797</td>\n",
       "      <td>data</td>\n",
       "      <td>9</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Guan. sparse co-occurrence data. Computer Soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1602.06797</td>\n",
       "      <td>data</td>\n",
       "      <td>14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Figure 5: Inﬂuence of labeled data, where the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1602.06797</td>\n",
       "      <td>data</td>\n",
       "      <td>20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>In other words, µk is equal to the mean of all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1602.06797</td>\n",
       "      <td>data</td>\n",
       "      <td>25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>metric-learn-ave-vec also uses the metric lear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id pattern  token_count update_date  label  \\\n",
       "0  1602.06797    data            6         NaT    0.0   \n",
       "1  1602.06797    data            9         NaT    0.0   \n",
       "2  1602.06797    data           14         NaT    0.0   \n",
       "3  1602.06797    data           20         NaT    0.0   \n",
       "4  1602.06797    data           25         NaT    0.0   \n",
       "\n",
       "                                                para  \n",
       "0                         of data with given labels.  \n",
       "1  Guan. sparse co-occurrence data. Computer Soci...  \n",
       "2  Figure 5: Inﬂuence of labeled data, where the ...  \n",
       "3  In other words, µk is equal to the mean of all...  \n",
       "4  metric-learn-ave-vec also uses the metric lear...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\n",
    "    path_label_dir / \"labels_to_not_include_in_final\" / \"labels_3.ods\",\n",
    "    parse_dates=[\"update_date\"],\n",
    "    engine=\"odf\",\n",
    "    dtype={\"id\": str},\n",
    "    )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_single_embedding(text, model, tokenizer, device=None, max_len=512):\n",
    "    \"\"\"Create a single embedding for a given text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to create embedding for.\n",
    "    model : BertModel\n",
    "        Bert model to use. The model should already be sent to the appropriate device.\n",
    "    tokenizer : BertTokenizer\n",
    "        Bert tokenizer to use.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_masks = encoding[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # from https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "        last_hidden_states = model(\n",
    "            input_ids=input_ids.to(device),\n",
    "            attention_mask=attention_masks.to(device)\n",
    "        )\n",
    "\n",
    "        features =last_hidden_states[0][:, 0, :].cpu().numpy().flatten()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By increasing the value of α, we acquired progressive improvements, and reached to the peak point at α=0.01. After that, the performance dropped. Therefore, we choose α=0.01 in the following experiments. This results also indicate that the unlabeled data are useful for the text representation learning process.\n"
     ]
    }
   ],
   "source": [
    "text = df.iloc[9][\"para\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = creat_single_embedding(text, model, tokenizer, device=device)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase text\n",
    "text = text.lower()\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_masks = encoding[\"attention_mask\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # from https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    last_hidden_states = model(\n",
    "        input_ids=input_ids.to(device),\n",
    "        attention_mask=attention_masks.to(device)\n",
    "    )\n",
    "\n",
    "    features =last_hidden_states[0][:, 0, :].cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6885415315628052,\n",
       " -1.1852481365203857,\n",
       " 0.006578156724572182,\n",
       " -0.288088321685791,\n",
       " -0.12499744445085526,\n",
       " 0.08755375444889069,\n",
       " -0.5094060897827148,\n",
       " -0.5622881650924683,\n",
       " -1.049770474433899,\n",
       " -0.10232977569103241,\n",
       " 0.5875229835510254,\n",
       " 1.242366909980774,\n",
       " 1.1124123334884644,\n",
       " -0.3905874788761139,\n",
       " 0.6971225738525391,\n",
       " -0.26959753036499023,\n",
       " -2.309335708618164,\n",
       " -0.9504613280296326,\n",
       " 0.4319698214530945,\n",
       " -0.6324054002761841,\n",
       " -0.19918538630008698,\n",
       " -0.07609106600284576,\n",
       " -0.6810355186462402,\n",
       " -0.1331252157688141,\n",
       " 2.2282874584198,\n",
       " 1.1921900510787964,\n",
       " -0.020399512723088264,\n",
       " -0.40858981013298035,\n",
       " 0.3790270984172821,\n",
       " 0.0471184104681015,\n",
       " -0.3291703760623932,\n",
       " -1.633091688156128,\n",
       " 0.6017197370529175,\n",
       " -0.8544878959655762,\n",
       " 0.5962883830070496,\n",
       " -0.12913057208061218,\n",
       " 0.006662297993898392,\n",
       " -0.8041810989379883,\n",
       " -0.0015627562534064054,\n",
       " 1.02269446849823,\n",
       " -0.10568635165691376,\n",
       " 1.0959454774856567,\n",
       " -0.0675923153758049,\n",
       " -0.830102264881134,\n",
       " -0.20767192542552948,\n",
       " 0.47641345858573914,\n",
       " -0.6356093287467957,\n",
       " -0.025422390550374985,\n",
       " 0.3281707763671875,\n",
       " -0.168486088514328,\n",
       " 0.8419046998023987,\n",
       " -0.8563369512557983,\n",
       " -0.18048861622810364,\n",
       " 0.9929128289222717,\n",
       " -0.32228901982307434,\n",
       " 0.7717149257659912,\n",
       " -0.06710052490234375,\n",
       " -1.6367243528366089,\n",
       " 0.010116956196725368,\n",
       " 0.17078833281993866,\n",
       " -1.1578019857406616,\n",
       " -0.43772628903388977,\n",
       " 0.7935740351676941,\n",
       " -0.17802584171295166,\n",
       " -0.13641275465488434,\n",
       " 0.14233751595020294,\n",
       " 0.24615149199962616,\n",
       " 1.2664577960968018,\n",
       " -0.012992322444915771,\n",
       " 1.202650785446167,\n",
       " 0.08506684750318527,\n",
       " 0.3339981436729431,\n",
       " 0.059947576373815536,\n",
       " -0.9237575531005859,\n",
       " 1.3801298141479492,\n",
       " 1.055428147315979,\n",
       " 1.1562857627868652,\n",
       " 1.0770680904388428,\n",
       " -0.765812873840332,\n",
       " 0.3168524205684662,\n",
       " 0.4956485331058502,\n",
       " 0.7924830913543701,\n",
       " -0.6948912739753723,\n",
       " -1.0842514038085938,\n",
       " -0.6216822862625122,\n",
       " 0.545027494430542,\n",
       " 0.3493122458457947,\n",
       " 0.45315638184547424,\n",
       " 0.7394849061965942,\n",
       " 0.20666392147541046,\n",
       " 0.3382292687892914,\n",
       " 0.3272426724433899,\n",
       " 0.04982610046863556,\n",
       " -0.08604227751493454,\n",
       " 1.031298041343689,\n",
       " -0.31824833154678345,\n",
       " 0.5995824337005615,\n",
       " -0.38253673911094666,\n",
       " -1.1801507472991943,\n",
       " 1.5425701141357422,\n",
       " -0.852250337600708,\n",
       " 0.06337668746709824,\n",
       " -0.4165599048137665,\n",
       " 0.19238591194152832,\n",
       " 1.5059905052185059,\n",
       " 0.8769985437393188,\n",
       " 0.23356623947620392,\n",
       " -0.38044998049736023,\n",
       " -0.48832347989082336,\n",
       " 0.16217178106307983,\n",
       " -0.0674164667725563,\n",
       " -1.1965117454528809,\n",
       " 0.49132466316223145,\n",
       " -1.147221326828003,\n",
       " -0.5439664125442505,\n",
       " -1.6638362407684326,\n",
       " -0.14353272318840027,\n",
       " -0.46010851860046387,\n",
       " -1.2653625011444092,\n",
       " 1.5030816793441772,\n",
       " -0.4923928678035736,\n",
       " -0.24917659163475037,\n",
       " 0.05133293941617012,\n",
       " 0.1506734937429428,\n",
       " 1.78203547000885,\n",
       " 0.2648921012878418,\n",
       " 0.5222933292388916,\n",
       " 0.08107389509677887,\n",
       " 1.6165764331817627,\n",
       " -1.8207249641418457,\n",
       " -0.940691351890564,\n",
       " 0.09336737543344498,\n",
       " 0.08578629791736603,\n",
       " 0.4163794219493866,\n",
       " 1.3049449920654297,\n",
       " 0.175584614276886,\n",
       " 0.21364641189575195,\n",
       " -0.04614570364356041,\n",
       " -0.18284176290035248,\n",
       " 0.3559581935405731,\n",
       " 0.6171337962150574,\n",
       " 1.0121943950653076,\n",
       " 1.356575608253479,\n",
       " -0.25652897357940674,\n",
       " 0.32371893525123596,\n",
       " 0.6708090305328369,\n",
       " 0.008158973418176174,\n",
       " -1.0730655193328857,\n",
       " 0.41896653175354004,\n",
       " -1.0336717367172241,\n",
       " -0.0836608037352562,\n",
       " -0.0787825658917427,\n",
       " -1.1392874717712402,\n",
       " -0.004438845440745354,\n",
       " -0.1747008115053177,\n",
       " -0.021229922771453857,\n",
       " -0.006423158571124077,\n",
       " -0.06027334928512573,\n",
       " 0.005648142192512751,\n",
       " 0.24498677253723145,\n",
       " -0.30121487379074097,\n",
       " -0.44203004240989685,\n",
       " 1.1601811647415161,\n",
       " 0.18291854858398438,\n",
       " -0.8221859335899353,\n",
       " -0.8496862053871155,\n",
       " 0.7857117652893066,\n",
       " 0.41482946276664734,\n",
       " 0.19889108836650848,\n",
       " 0.5107666850090027,\n",
       " -0.22777977585792542,\n",
       " -0.15128381550312042,\n",
       " 0.32631954550743103,\n",
       " 0.5376771688461304,\n",
       " 0.41706305742263794,\n",
       " -0.526764452457428,\n",
       " -0.5586445927619934,\n",
       " 0.5145229697227478,\n",
       " 0.037204161286354065,\n",
       " 0.010613443329930305,\n",
       " -1.2105143070220947,\n",
       " 0.1964915245771408,\n",
       " -0.4713321626186371,\n",
       " -0.6479560136795044,\n",
       " 0.5012726187705994,\n",
       " -0.7151186466217041,\n",
       " 0.3500572741031647,\n",
       " -0.5603892803192139,\n",
       " 0.43089669942855835,\n",
       " -1.4697847366333008,\n",
       " -0.21398919820785522,\n",
       " -0.5546921491622925,\n",
       " 1.2182215452194214,\n",
       " 0.11018591374158859,\n",
       " -0.555453896522522,\n",
       " -0.13554495573043823,\n",
       " -0.7370015382766724,\n",
       " 0.07565461099147797,\n",
       " -0.3066079020500183,\n",
       " -0.33389589190483093,\n",
       " 0.7131171822547913,\n",
       " 0.5985294580459595,\n",
       " -0.3033373951911926,\n",
       " -0.6915034055709839,\n",
       " -0.09528369456529617,\n",
       " -0.11088239401578903,\n",
       " 0.4401446282863617,\n",
       " -0.8341637849807739,\n",
       " 0.3783465027809143,\n",
       " 0.5998908281326294,\n",
       " -0.3875581622123718,\n",
       " -0.5312781929969788,\n",
       " -0.7038910984992981,\n",
       " 0.13546591997146606,\n",
       " -0.4662320613861084,\n",
       " 0.5851140022277832,\n",
       " -0.405103474855423,\n",
       " 0.49629491567611694,\n",
       " 0.05179161578416824,\n",
       " 1.3476091623306274,\n",
       " -0.23656907677650452,\n",
       " 0.13873985409736633,\n",
       " -0.2947312593460083,\n",
       " -1.1811589002609253,\n",
       " 0.03527544438838959,\n",
       " -0.3644363284111023,\n",
       " 0.21632491052150726,\n",
       " 0.5923197269439697,\n",
       " 0.13182112574577332,\n",
       " 0.24295908212661743,\n",
       " -0.6926175355911255,\n",
       " 0.027991635724902153,\n",
       " 0.8810684680938721,\n",
       " 0.3640211522579193,\n",
       " -0.375697523355484,\n",
       " -0.9499608278274536,\n",
       " 0.22632135450839996,\n",
       " -0.14566366374492645,\n",
       " -0.6129156947135925,\n",
       " -0.12535561621189117,\n",
       " -0.5111672282218933,\n",
       " 0.6518141627311707,\n",
       " 0.1757473647594452,\n",
       " 0.4567694365978241,\n",
       " -1.1876511573791504,\n",
       " 0.785579264163971,\n",
       " 0.3473443388938904,\n",
       " 0.26907098293304443,\n",
       " 0.2993678152561188,\n",
       " 0.5193493962287903,\n",
       " -0.14836229383945465,\n",
       " -0.042162325233221054,\n",
       " 0.5660104751586914,\n",
       " -0.052139006555080414,\n",
       " 0.3924163579940796,\n",
       " -0.5291892886161804,\n",
       " -0.5718972682952881,\n",
       " -0.5811564922332764,\n",
       " 0.1777428537607193,\n",
       " 0.09920218586921692,\n",
       " -0.1664101779460907,\n",
       " -0.6972619295120239,\n",
       " 0.5007185339927673,\n",
       " 1.6025078296661377,\n",
       " 1.708419680595398,\n",
       " -0.1307191103696823,\n",
       " -0.7528232336044312,\n",
       " 0.07203234732151031,\n",
       " -0.4245683550834656,\n",
       " 0.48306310176849365,\n",
       " 0.6589756608009338,\n",
       " -0.5423982739448547,\n",
       " 0.2031228393316269,\n",
       " -0.5670100450515747,\n",
       " 0.00166788836941123,\n",
       " 0.7075050473213196,\n",
       " 0.21842969954013824,\n",
       " -0.5829882621765137,\n",
       " -0.740324079990387,\n",
       " -1.6480947732925415,\n",
       " -1.0933663845062256,\n",
       " -1.9165716171264648,\n",
       " 0.024975312873721123,\n",
       " -0.8827764391899109,\n",
       " -0.7865466475486755,\n",
       " -0.6271674036979675,\n",
       " -0.3012799024581909,\n",
       " -0.4237726032733917,\n",
       " -0.4744959771633148,\n",
       " 0.20419706404209137,\n",
       " 0.0710420235991478,\n",
       " -0.13060912489891052,\n",
       " 0.9004432559013367,\n",
       " -0.09555405378341675,\n",
       " -0.7293030619621277,\n",
       " -0.35842448472976685,\n",
       " -0.927204966545105,\n",
       " -0.21901853382587433,\n",
       " -0.03598944470286369,\n",
       " 0.5692564845085144,\n",
       " 0.5394388437271118,\n",
       " 0.7171228528022766,\n",
       " 0.5391322374343872,\n",
       " 1.4734246730804443,\n",
       " -0.6302911043167114,\n",
       " 0.19138416647911072,\n",
       " 0.3229186236858368,\n",
       " -0.2763531506061554,\n",
       " 0.10397763550281525,\n",
       " 1.348530888557434,\n",
       " 0.6821385622024536,\n",
       " -0.3642873167991638,\n",
       " 1.4658713340759277,\n",
       " 0.01091200951486826,\n",
       " 0.05273221805691719,\n",
       " 0.11850118637084961,\n",
       " -0.1906803548336029,\n",
       " 0.7269793152809143,\n",
       " -0.1619691252708435,\n",
       " -0.4196259677410126,\n",
       " -0.21787245571613312,\n",
       " 0.216311514377594,\n",
       " -0.19788150489330292,\n",
       " -0.30358052253723145,\n",
       " -0.26355040073394775,\n",
       " 0.5740327835083008,\n",
       " -0.32060253620147705,\n",
       " 0.3652629852294922,\n",
       " 0.2622824013233185,\n",
       " -0.35940659046173096,\n",
       " 0.31852805614471436,\n",
       " -0.6562258005142212,\n",
       " -0.1291380524635315,\n",
       " 0.929007887840271,\n",
       " 0.09622664749622345,\n",
       " -0.18938744068145752,\n",
       " -0.7377411127090454,\n",
       " -0.41072002053260803,\n",
       " -0.1323348432779312,\n",
       " 0.4945335388183594,\n",
       " -0.3091853857040405,\n",
       " -0.33254268765449524,\n",
       " 0.13305813074111938,\n",
       " -0.4818447530269623,\n",
       " -0.7796055674552917,\n",
       " -1.597564458847046,\n",
       " -0.4662097692489624,\n",
       " 0.19442734122276306,\n",
       " -0.04830747842788696,\n",
       " 0.2961011826992035,\n",
       " 0.21035081148147583,\n",
       " -0.04033461958169937,\n",
       " 0.06658000499010086,\n",
       " 0.10792379081249237,\n",
       " -0.40595000982284546,\n",
       " -0.48635172843933105,\n",
       " -0.42554086446762085,\n",
       " -0.9538953900337219,\n",
       " 0.8337950706481934,\n",
       " 0.4706241488456726,\n",
       " 0.3044312298297882,\n",
       " -0.8110408186912537,\n",
       " -0.1064145639538765,\n",
       " 0.22034738957881927,\n",
       " 0.6742476224899292,\n",
       " -0.9739952683448792,\n",
       " 0.18117357790470123,\n",
       " -0.3546602725982666,\n",
       " -0.12067880481481552,\n",
       " -0.47070789337158203,\n",
       " -0.40378299355506897,\n",
       " 0.36012494564056396,\n",
       " -0.22705870866775513,\n",
       " 0.017116062343120575,\n",
       " -0.4888263940811157,\n",
       " -0.1825370192527771,\n",
       " -0.516604483127594,\n",
       " -0.1689046323299408,\n",
       " 1.659907341003418,\n",
       " -0.16828817129135132,\n",
       " 0.8680158853530884,\n",
       " 0.21267633140087128,\n",
       " -0.6181543469429016,\n",
       " -0.15228860080242157,\n",
       " -0.5997354984283447,\n",
       " -0.3410223126411438,\n",
       " 1.4034454822540283,\n",
       " 1.1349684000015259,\n",
       " 0.40359199047088623,\n",
       " -0.877474844455719,\n",
       " 1.319218635559082,\n",
       " 0.680509090423584,\n",
       " -0.3827017843723297,\n",
       " 0.15184371173381805,\n",
       " 0.9858534336090088,\n",
       " -0.47202858328819275,\n",
       " -0.12765291333198547,\n",
       " 0.01280411146581173,\n",
       " 0.4152657985687256,\n",
       " 0.9573320746421814,\n",
       " -0.036382563412189484,\n",
       " -0.35672077536582947,\n",
       " -0.9218661785125732,\n",
       " -0.8067999482154846,\n",
       " -1.3630599975585938,\n",
       " -0.4462498724460602,\n",
       " 1.2664997577667236,\n",
       " -0.12793785333633423,\n",
       " 0.742725670337677,\n",
       " 0.2670873701572418,\n",
       " -0.19522394239902496,\n",
       " -0.007121783681213856,\n",
       " 0.3400888442993164,\n",
       " -0.47266462445259094,\n",
       " -0.4000788629055023,\n",
       " 1.2272284030914307,\n",
       " -0.03264647722244263,\n",
       " -0.566821277141571,\n",
       " -0.4759663939476013,\n",
       " 0.8706602454185486,\n",
       " -0.3664492070674896,\n",
       " 0.9951768517494202,\n",
       " 13.49837875366211,\n",
       " 0.4085155129432678,\n",
       " -0.31273379921913147,\n",
       " 0.41975149512290955,\n",
       " 0.14265859127044678,\n",
       " 2.0494115352630615,\n",
       " -0.5426515340805054,\n",
       " -0.08439252525568008,\n",
       " -0.22968994081020355,\n",
       " 0.8109138607978821,\n",
       " -0.7686477899551392,\n",
       " -0.5555481910705566,\n",
       " -0.533790111541748,\n",
       " 0.3115556836128235,\n",
       " 0.13135457038879395,\n",
       " 0.5025301575660706,\n",
       " -1.0557150840759277,\n",
       " 0.6727415323257446,\n",
       " -0.28032803535461426,\n",
       " -0.47728976607322693,\n",
       " 0.33585163950920105,\n",
       " 0.5720061659812927,\n",
       " 0.7482643723487854,\n",
       " -0.14962288737297058,\n",
       " -0.3766079843044281,\n",
       " 0.6493350863456726,\n",
       " -0.8464983105659485,\n",
       " -0.21906669437885284,\n",
       " 0.5254009962081909,\n",
       " -0.9153851866722107,\n",
       " -0.03885535150766373,\n",
       " -0.9999122023582458,\n",
       " 0.8823726177215576,\n",
       " 0.9596043229103088,\n",
       " -1.5285252332687378,\n",
       " -0.44021880626678467,\n",
       " -0.28207212686538696,\n",
       " 0.1543382704257965,\n",
       " -0.13257208466529846,\n",
       " 0.6701139807701111,\n",
       " -0.013704020529985428,\n",
       " 0.5006194114685059,\n",
       " 1.2551608085632324,\n",
       " 0.9515016078948975,\n",
       " -0.03586090728640556,\n",
       " 0.8263263702392578,\n",
       " -1.442615270614624,\n",
       " 0.30107736587524414,\n",
       " 0.37404975295066833,\n",
       " -0.8497971296310425,\n",
       " -0.049081798642873764,\n",
       " 0.6133363842964172,\n",
       " 0.06888643652200699,\n",
       " 0.8832569718360901,\n",
       " 1.2904481887817383,\n",
       " -0.047346893697977066,\n",
       " -0.025349579751491547,\n",
       " -0.5427201986312866,\n",
       " -0.7921054363250732,\n",
       " 0.0686427503824234,\n",
       " 0.09524746984243393,\n",
       " -0.31275007128715515,\n",
       " 0.3077334761619568,\n",
       " 0.3393700420856476,\n",
       " -0.3437284231185913,\n",
       " 0.33331722021102905,\n",
       " 0.8999888896942139,\n",
       " -1.0359808206558228,\n",
       " 0.6417641043663025,\n",
       " 0.29951590299606323,\n",
       " -1.3246865272521973,\n",
       " 0.7189956903457642,\n",
       " 0.03327665477991104,\n",
       " -0.5646447539329529,\n",
       " -0.16668714582920074,\n",
       " -0.3690458834171295,\n",
       " 0.20457257330417633,\n",
       " -1.3813382387161255,\n",
       " -0.14116542041301727,\n",
       " -0.972861647605896,\n",
       " -0.15726852416992188,\n",
       " 0.4416707456111908,\n",
       " -0.2723451852798462,\n",
       " -0.44370579719543457,\n",
       " -0.3806169629096985,\n",
       " -0.3048091530799866,\n",
       " 0.4555416703224182,\n",
       " 0.8239558339118958,\n",
       " -0.35481998324394226,\n",
       " -0.6841470003128052,\n",
       " -1.6781830787658691,\n",
       " -0.5850866436958313,\n",
       " -1.0657883882522583,\n",
       " 1.2037808895111084,\n",
       " 0.011977307498455048,\n",
       " -0.5623293519020081,\n",
       " -0.17012782394886017,\n",
       " -0.8595936894416809,\n",
       " -0.8540768027305603,\n",
       " 1.1652789115905762,\n",
       " 0.38240474462509155,\n",
       " 0.19860121607780457,\n",
       " 0.10136500746011734,\n",
       " 0.46296530961990356,\n",
       " -0.18948060274124146,\n",
       " 1.3077099323272705,\n",
       " 1.1993573904037476,\n",
       " 0.15566568076610565,\n",
       " -0.8595134019851685,\n",
       " 0.09887746721506119,\n",
       " 0.5301305651664734,\n",
       " -0.433894544839859,\n",
       " -0.39752063155174255,\n",
       " 1.0176422595977783,\n",
       " -0.23711180686950684,\n",
       " -0.4569476842880249,\n",
       " -0.7433081269264221,\n",
       " -0.11558069288730621,\n",
       " 0.8482357263565063,\n",
       " -0.05592094361782074,\n",
       " 0.5534425973892212,\n",
       " 1.2488406896591187,\n",
       " 0.48249930143356323,\n",
       " 0.31253883242607117,\n",
       " -0.433806836605072,\n",
       " 0.17198391258716583,\n",
       " 1.124908447265625,\n",
       " 0.2754937708377838,\n",
       " 0.051501963287591934,\n",
       " -0.13577955961227417,\n",
       " 0.38371938467025757,\n",
       " 0.5308123826980591,\n",
       " -0.34553855657577515,\n",
       " 1.2983150482177734,\n",
       " -0.030453141778707504,\n",
       " 0.02729361690580845,\n",
       " 0.395668625831604,\n",
       " -0.43727508187294006,\n",
       " 0.756399929523468,\n",
       " 0.3938788175582886,\n",
       " -0.19347254931926727,\n",
       " -0.9080213308334351,\n",
       " -0.7271621227264404,\n",
       " 0.616596519947052,\n",
       " -0.4574045240879059,\n",
       " 0.7796199321746826,\n",
       " -0.6089551448822021,\n",
       " -0.5693050622940063,\n",
       " -0.12213446199893951,\n",
       " 0.2804642617702484,\n",
       " 1.2966499328613281,\n",
       " -1.1247390508651733,\n",
       " 0.8942590951919556,\n",
       " -0.2763602137565613,\n",
       " 0.09457764774560928,\n",
       " 0.10071718692779541,\n",
       " 0.49792036414146423,\n",
       " 1.3541051149368286,\n",
       " -0.00793920923024416,\n",
       " 0.7571437954902649,\n",
       " -0.18812502920627594,\n",
       " -0.39550960063934326,\n",
       " -0.39640793204307556,\n",
       " 0.41811254620552063,\n",
       " 0.10285483300685883,\n",
       " -0.3316284716129303,\n",
       " 0.6532801985740662,\n",
       " -0.21072731912136078,\n",
       " 0.4710276126861572,\n",
       " -1.704412817955017,\n",
       " -0.06841910630464554,\n",
       " -0.3493275046348572,\n",
       " 0.20813094079494476,\n",
       " 0.365250825881958,\n",
       " 0.8486369252204895,\n",
       " -0.3422035872936249,\n",
       " -0.7672909498214722,\n",
       " 1.541256308555603,\n",
       " -1.700058937072754,\n",
       " -0.09162453562021255,\n",
       " -0.25648170709609985,\n",
       " 1.4236128330230713,\n",
       " -0.6807458996772766,\n",
       " 1.1941652297973633,\n",
       " 1.672939419746399,\n",
       " -0.13710804283618927,\n",
       " 0.4984748065471649,\n",
       " -0.007506873924285173,\n",
       " -0.38568630814552307,\n",
       " 0.6427146196365356,\n",
       " 0.1314217448234558,\n",
       " 0.4888237714767456,\n",
       " -0.6754450798034668,\n",
       " -1.1376595497131348,\n",
       " -0.2442406713962555,\n",
       " -0.17100469768047333,\n",
       " -1.0162664651870728,\n",
       " 0.16705220937728882,\n",
       " -0.3895556926727295,\n",
       " -0.34902337193489075,\n",
       " -0.049229733645915985,\n",
       " -0.213917076587677,\n",
       " -0.9203080534934998,\n",
       " -0.4543467164039612,\n",
       " -0.6945171356201172,\n",
       " -0.599467933177948,\n",
       " -0.7278764247894287,\n",
       " -0.8111569285392761,\n",
       " -0.05515986680984497,\n",
       " 0.37436074018478394,\n",
       " -0.34779855608940125,\n",
       " 0.530694305896759,\n",
       " 1.1685521602630615,\n",
       " -0.5073329210281372,\n",
       " -0.4389544725418091,\n",
       " 0.16042101383209229,\n",
       " -0.28212088346481323,\n",
       " 0.4171668291091919,\n",
       " 0.09446871280670166,\n",
       " 0.9993342757225037,\n",
       " -0.09141095727682114,\n",
       " -0.8628888726234436,\n",
       " -0.15805955231189728,\n",
       " 0.54259192943573,\n",
       " 0.013297357596457005,\n",
       " 0.37684646248817444,\n",
       " 1.2776657342910767,\n",
       " -1.3920936584472656,\n",
       " -0.24753044545650482,\n",
       " 1.2867941856384277,\n",
       " 0.40825867652893066,\n",
       " -0.8234648108482361,\n",
       " -0.20674967765808105,\n",
       " 0.035317741334438324,\n",
       " -0.8290390968322754,\n",
       " 0.09359928220510483,\n",
       " 0.05287550762295723,\n",
       " -1.6096856594085693,\n",
       " -1.1054322719573975,\n",
       " 0.489565372467041,\n",
       " 1.028503179550171,\n",
       " -0.5611831545829773,\n",
       " -0.2400369495153427,\n",
       " 0.4678288400173187,\n",
       " -0.5731858015060425,\n",
       " -0.5926796197891235,\n",
       " 0.8515748977661133,\n",
       " -0.8891689777374268,\n",
       " -0.7893875241279602,\n",
       " -0.27993690967559814,\n",
       " 0.49902278184890747,\n",
       " 0.8290292620658875,\n",
       " -0.2933872640132904,\n",
       " -0.3018434941768646,\n",
       " -0.21556584537029266,\n",
       " -0.32568541169166565,\n",
       " 0.15763817727565765,\n",
       " -0.47152915596961975,\n",
       " 1.1293367147445679,\n",
       " 0.21257497370243073,\n",
       " 0.6260605454444885,\n",
       " -0.29519468545913696,\n",
       " -0.4770844280719757,\n",
       " -0.2813037037849426,\n",
       " -0.7336164116859436,\n",
       " -0.7392904162406921,\n",
       " 0.21883749961853027,\n",
       " -1.0810080766677856,\n",
       " 0.2085726261138916,\n",
       " 0.12769395112991333,\n",
       " -1.2295458316802979,\n",
       " 0.7956453561782837,\n",
       " 0.4723266363143921,\n",
       " 0.2611335515975952,\n",
       " 0.17268626391887665,\n",
       " -0.5888897180557251,\n",
       " 0.5255546569824219,\n",
       " 0.035495106130838394,\n",
       " 0.7580876350402832,\n",
       " 0.14835350215435028,\n",
       " 0.3001020550727844,\n",
       " 0.8205124735832214,\n",
       " 0.9798730611801147,\n",
       " -1.6320958137512207,\n",
       " -0.00817919336259365,\n",
       " -1.2653006315231323,\n",
       " -0.9872461557388306,\n",
       " -0.08697093278169632,\n",
       " 0.18111486732959747,\n",
       " 0.401066392660141,\n",
       " -0.29868167638778687,\n",
       " -0.03899232670664787,\n",
       " 0.07531388849020004,\n",
       " 0.6807926893234253,\n",
       " -1.1294808387756348,\n",
       " 0.5709237456321716,\n",
       " 0.6323016881942749,\n",
       " -0.3314819931983948,\n",
       " 0.7200196981430054,\n",
       " -0.6290964484214783,\n",
       " 0.002875692443922162,\n",
       " 1.0298655033111572,\n",
       " -0.36976030468940735,\n",
       " -0.26485756039619446,\n",
       " -0.312485009431839,\n",
       " -0.03898293152451515,\n",
       " -0.14272749423980713,\n",
       " -0.4691990315914154,\n",
       " 0.3407350182533264,\n",
       " -0.13967978954315186,\n",
       " 0.1513889878988266,\n",
       " -0.07880549877882004,\n",
       " -0.8587508201599121,\n",
       " 1.8128646612167358,\n",
       " -1.1489936113357544,\n",
       " 0.5053878426551819,\n",
       " -1.0650461912155151,\n",
       " 0.08873741328716278,\n",
       " 0.5689138174057007,\n",
       " -0.5793405175209045,\n",
       " -0.4448745846748352,\n",
       " -0.06770750135183334,\n",
       " -0.5889660716056824,\n",
       " 0.22414246201515198,\n",
       " 0.05625990033149719,\n",
       " -0.3070116639137268,\n",
       " 0.015387075953185558,\n",
       " 0.8627844452857971,\n",
       " -0.7337772250175476,\n",
       " -1.1789525747299194,\n",
       " -0.7573941349983215,\n",
       " 0.7778122425079346,\n",
       " -0.9991126656532288,\n",
       " 0.23384593427181244,\n",
       " -0.13053040206432343,\n",
       " 0.9368691444396973,\n",
       " 0.8102743029594421,\n",
       " 0.4828862249851227,\n",
       " -1.5336949825286865,\n",
       " -1.0453155040740967,\n",
       " 0.056573159992694855,\n",
       " 0.4387841820716858,\n",
       " -0.28581464290618896,\n",
       " -0.9162880182266235,\n",
       " 0.49346575140953064,\n",
       " 0.03749694675207138]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000094?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m creat_single_embedding(text, model, tokenizer, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[1;32m/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb Cell 5'\u001b[0m in \u001b[0;36mcreat_single_embedding\u001b[0;34m(text, model, tokenizer, device, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=30'>31</a>\u001b[0m attention_masks \u001b[39m=\u001b[39m encoding[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=32'>33</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=33'>34</a>\u001b[0m     \u001b[39m# from https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=34'>35</a>\u001b[0m     last_hidden_states \u001b[39m=\u001b[39m model(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=35'>36</a>\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=36'>37</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_masks\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=37'>38</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=39'>40</a>\u001b[0m     features \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=40'>41</a>\u001b[0m         last_hidden_states[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m, :]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=41'>42</a>\u001b[0m     )  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/Documents/arxiv-code-search/notebooks/scratch/bert_embedding_make_single.ipynb#ch0000092?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "features = creat_single_embedding(text, model, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/colinlagator/pytorch-bert-multi-label/notebook\n",
    "\n",
    "https://discuss.huggingface.co/t/download-models-for-local-loading/1963\n",
    "\n",
    "Much of notebook is from: https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=-FWG7kBm372V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders, etc\n",
    "class ArxivDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, labels, tokenizer, max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    text_orig = str(self.texts[item])\n",
    "    text = str(self.texts[item]).lower()\n",
    "    label = self.labels[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      truncation=True,\n",
    "      return_tensors='pt', # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'texts': text,\n",
    "      'texts_orig': text_orig,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_masks': encoding['attention_mask'].flatten(),\n",
    "      'labels': torch.tensor(label, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = ArxivDataset(\n",
    "    texts=df.para.to_numpy(),\n",
    "    labels=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# for hpc (need to manually download model)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# send model to gpu\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_folder = Path().cwd().parent.parent / \"data\"\n",
    "path_interim_folder = path_data_folder / \"interim\"\n",
    "path_label_folder = path_data_folder / \"processed\" / \"labels\" / \"labels_complete\"\n",
    "\n",
    "# load the labels.csv from the path_label_folder\n",
    "df = pd.read_csv(path_label_folder / \"labels.csv\", dtype={\"id\": str})\n",
    "df[\"para\"] = df[\"para\"].str.lower()\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x > 0 else 0) # binary labels\n",
    "print(df.shape)\n",
    "print(df[\"label\"].unique())\n",
    "\n",
    "\n",
    "# loop\n",
    "train_data_loader = create_data_loader(df, tokenizer, 512, 20)\n",
    "\n",
    "dfh_list = []\n",
    "for i, data in enumerate(train_data_loader):\n",
    "\n",
    "  labels = data['labels']\n",
    "  with torch.no_grad():\n",
    "    last_hidden_states = model(data[\"input_ids\"].to(DEVICE), attention_mask=data[\"attention_masks\"].to(DEVICE))\n",
    "    features = last_hidden_states[0][:,0,:].cpu().numpy() # from https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    df_h = pd.DataFrame(labels, columns=[\"label\"])\n",
    "    df_h[\"para\"] = data[\"texts_orig\"]\n",
    "    df_h[\"h\"] = features.tolist()\n",
    "    df_h['h'] = df_h['h'].apply(lambda x: np.array(x))\n",
    "    dfh_list.append(df_h)\n",
    "  \n",
    "  if i % 5 == 0:\n",
    "    print(i*20)\n",
    "\n",
    "dfh = pd.concat(dfh_list)\n",
    "# save dfh as a pickle file\n",
    "with open(path_interim_folder / \"dfh.pkl\", \"wb\") as f:\n",
    "  pickle.dump(dfh, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dfh.pickle\n",
    "with open(path_interim_folder / \"dfh.pkl\", \"rb\") as f:\n",
    "  dfh = pickle.load(f)\n",
    "dfh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dfh\n",
    "Load dfh in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>para</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1710.02907</td>\n",
       "      <td>experiment 2: in this set of experiments, we e...</td>\n",
       "      <td>[-0.7371358871459961, -1.4070982933044434, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1811.11012</td>\n",
       "      <td>this section of the technical report is focuse...</td>\n",
       "      <td>[-0.3564741313457489, 0.018136806786060333, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1811.11012</td>\n",
       "      <td>volunteers’ vehicles were mounted with bsm-bro...</td>\n",
       "      <td>[-0.7548128366470337, -0.35174882411956787, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1912.09582</td>\n",
       "      <td>for small datasets–a case with dutch book revi...</td>\n",
       "      <td>[-1.4487942457199097, -0.013197386637330055, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1912.09582</td>\n",
       "      <td>table 4: sentiment analysis accuracy scores on...</td>\n",
       "      <td>[-0.8141533136367798, 0.016403447836637497, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          id                                               para  \\\n",
       "0      0  1710.02907  experiment 2: in this set of experiments, we e...   \n",
       "1      0  1811.11012  this section of the technical report is focuse...   \n",
       "2      0  1811.11012  volunteers’ vehicles were mounted with bsm-bro...   \n",
       "3      0  1912.09582  for small datasets–a case with dutch book revi...   \n",
       "4      1  1912.09582  table 4: sentiment analysis accuracy scores on...   \n",
       "\n",
       "                                                   h  \n",
       "0  [-0.7371358871459961, -1.4070982933044434, -0....  \n",
       "1  [-0.3564741313457489, 0.018136806786060333, -0...  \n",
       "2  [-0.7548128366470337, -0.35174882411956787, -0...  \n",
       "3  [-1.4487942457199097, -0.013197386637330055, 0...  \n",
       "4  [-0.8141533136367798, 0.016403447836637497, -0...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_dir = Path().cwd().parent.parent / \"data\"\n",
    "embeddings_dir = path_data_dir / \"processed/embeddings\"\n",
    "\n",
    "# load dfh.pickle\n",
    "with open(embeddings_dir / \"df_embeddings.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the case study used for testing the methodology was chosen from the commercial reference buildings database [26] of the us department of energy (doe). a secondary school located in san francisco (california) and constructed after the year of 1980 was selected. data about the energy load demands (whose hourly values are shown in figure 4) were calculated by means of energyplus simulation software [27] and then imported and processed in matlab. hourly temperatures of the typical meteorological year of san francisco, which are shown in figure 5, were considered.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == 1].iloc[9][\"para\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([i for i in dfh[\"h\"].values])\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([i for i in dfh[\"h\"].values])\n",
    "y = dfh['label'].values\n",
    "\n",
    "# split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_clf = GaussianNB()\n",
    "gnb_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316326530612245"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniconda3/envs/arxiv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a naive bayes classifier\n",
    "gnb = GaussianNB()\n",
    "y_train = dfh[\"label\"].values\n",
    "\n",
    "gnb.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "x_test = np.array([i for i in dfh[\"h\"].values])\n",
    "x_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>para</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>experiment 2: in this set of experiments, we e...</td>\n",
       "      <td>[-0.7371358871459961, -1.4070982933044434, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>this section of the technical report is focuse...</td>\n",
       "      <td>[-0.3564741313457489, 0.018136806786060333, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>volunteers’ vehicles were mounted with bsm-bro...</td>\n",
       "      <td>[-0.7548128366470337, -0.35174882411956787, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>for small datasets–a case with dutch book revi...</td>\n",
       "      <td>[-1.4487942457199097, -0.013197386637330055, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>table 4: sentiment analysis accuracy scores on...</td>\n",
       "      <td>[-0.8141533136367798, 0.016403447836637497, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               para  \\\n",
       "0      0  experiment 2: in this set of experiments, we e...   \n",
       "1      0  this section of the technical report is focuse...   \n",
       "2      0  volunteers’ vehicles were mounted with bsm-bro...   \n",
       "3      0  for small datasets–a case with dutch book revi...   \n",
       "4      1  table 4: sentiment analysis accuracy scores on...   \n",
       "\n",
       "                                                   h  \n",
       "0  [-0.7371358871459961, -1.4070982933044434, -0....  \n",
       "1  [-0.3564741313457489, 0.018136806786060333, -0...  \n",
       "2  [-0.7548128366470337, -0.35174882411956787, -0...  \n",
       "3  [-1.4487942457199097, -0.013197386637330055, 0...  \n",
       "4  [-0.8141533136367798, 0.016403447836637497, -0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings = pd.read_pickle(embeddings_dir / \"dfh.pkl\")\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_embeddings[\"h\"].values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([i for i in a])\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in a:\n",
    "    l.append(int(i.shape[0]))\n",
    "\n",
    "# only keep unique values in l\n",
    "l = list(set(l))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(a, (-1, a.shape[0]))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape(-1, a.shape[-1])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh[\"h\"].to_numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_h['h'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test creation of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_folder = Path().cwd().parent.parent / \"data\"\n",
    "path_interim_folder = path_data_folder / \"interim\"\n",
    "path_label_folder = path_data_folder / \"processed\" / \"labels\" / \"labels_complete\"\n",
    "\n",
    "# load the labels.csv from the path_label_folder\n",
    "df = pd.read_csv(path_label_folder / \"labels.csv\", dtype={\"id\": str})\n",
    "# lowercase \"para\" column in df\n",
    "df[\"para\"] = df[\"para\"].str.lower()\n",
    "\n",
    "\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x > 0 else 0) # binary labels\n",
    "# df = df.drop(columns=[\"label\"])\n",
    "print(df.shape)\n",
    "print(df[\"label\"].unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, labels, tokenizer, max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.texts[item]).lower()\n",
    "    text_orig = str(self.texts[item])\n",
    "    label = self.labels[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      truncation=True,\n",
    "      return_tensors='pt', # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'texts': text,\n",
    "      'texts_orig': text_orig,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_masks': encoding['attention_mask'].flatten(),\n",
    "      'labels': torch.tensor(label, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = ArxivDataset(\n",
    "    texts=df.para.to_numpy(),\n",
    "    labels=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df, tokenizer, 512, 2)\n",
    "\n",
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hpc (need to manually download model)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# send model to gpu\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put h into a dataframe with the labels\n",
    "df_h = pd.DataFrame(labels, columns=[\"label\"])\n",
    "df_h[\"para\"] = data[\"texts_orig\"]\n",
    "df_h[\"h\"] = h.tolist()\n",
    "df_h['h'] = df_h['h'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_folder = Path().cwd().parent.parent / \"data\"\n",
    "path_interim_folder = path_data_folder / \"interim\"\n",
    "path_label_folder = path_data_folder / \"processed\" / \"labels\" / \"labels_complete\"\n",
    "\n",
    "# load the labels.csv from the path_label_folder\n",
    "df = pd.read_csv(path_label_folder / \"labels.csv\", dtype={\"id\": str})\n",
    "# lowercase \"para\" column in df\n",
    "df[\"para\"] = df[\"para\"].str.lower()\n",
    "\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x > 0 else 0) # binary labels\n",
    "# df = df.drop(columns=[\"label\"])\n",
    "print(df.shape)\n",
    "print(df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"para\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df[\"para\"].apply((lambda x: tokenizer.encode(\n",
    "    x,\n",
    "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,\n",
    "    # return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    truncation=True,\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# for hpc (need to manually download model)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader -- inspired by https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
    "class ArxivDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, labels, tokenizer, max_len):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    text = str(self.texts[item]).lower()\n",
    "    label = self.labels[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      truncation=True,\n",
    "      return_tensors='pt', # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'texts': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'labels': torch.tensor(label, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.1, random_state=12)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = ArxivDataset(\n",
    "    texts=df.para.to_numpy(),\n",
    "    labels=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')  # local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, pre_trained_model_name):\n",
    "        super(ArxivClassifier, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained(pre_trained_model_name)\n",
    "\n",
    "        self.dense_1 = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.dense_2 = torch.nn.Linear(768, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = self.dense_1(pooled_output)\n",
    "        pooled_output = torch.nn.ReLU()(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.dense_2(pooled_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArxivClassifier(4)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best_model_state.bin pytorch model\n",
    "model.load_state_dict(torch.load(\"best_model_state.bin\"))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "input_ids = data['input_ids'].to(DEVICE)\n",
    "attention_mask = data['attention_mask'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = F.softmax(model(input_ids, attention_mask), dim=1)\n",
    "labels = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try pr-auc curve with torchmetrics\n",
    "# https://torchmetrics.readthedocs.io/en/v0.8.2/classification/precision_recall_curve.html\n",
    "# https://torchmetrics.readthedocs.io/en/v0.8.2/classification/binned_precision_recall_curve.html\n",
    "\n",
    "pr_curve = PrecisionRecallCurve(num_classes=4)\n",
    "precision, recall, thresholds = pr_curve(pred.cpu(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    labels = d[\"labels\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      labels = d[\"labels\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, labels)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == labels)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    DEVICE, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    DEVICE, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(history['train_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(history['train_acc']), label='train accuracy')\n",
    "plt.plot(torch.tensor(history['val_acc']), label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github.\"\n",
    "\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('/home/tvhahn/scibert_scivocab_uncased') # hpc\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')  # local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  text,\n",
    "  max_length=512,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  padding='max_length',\n",
    "  return_attention_mask=True,\n",
    "  truncation=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "# encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# for hpc (need to manually download model)\n",
    "model = AutoModel.from_pretrained('/home/tvhahn/scibert_scivocab_uncased')\n",
    "\n",
    "# for local computer\n",
    "# model = AutoModel.from_pretrained('/home/tvhahn/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words, etc\n",
    "stop = stopwords.words('english')\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stop]\n",
    "\n",
    "# text = text.lower()\n",
    "# text = text.apply(lambda x: x.split(' '))\n",
    "# text = text.apply(lambda x: [item for item in x if item not in stop])\n",
    "# text = text.apply(lambda x: ' '.join(x))\n",
    "# text = text.apply(lambda x: re.sub('[^A-Za-z\\s]+', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub('\\n', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "# text = text.apply(lambda x: re.sub(r'^\\s', '', x))\n",
    "# text = text.apply(lambda x: re.sub(r'\\s$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer.batch_encode_plus(text, pad_to_max_length=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17e082919eb97a8b1648db68459a0548143f50884a45122adabc4767e3d2dece"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
