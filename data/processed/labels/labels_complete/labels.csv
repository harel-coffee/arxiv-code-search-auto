id,pattern,token_count,update_date,label,para
1710.02907,"data, dataset",280,2022-04-21,0,"Experiment 2: In this set of experiments, we evaluated the ability of the proposed method to compress high dimensional data. For these experiments, the proposed compression method was deployed on each dimension and the performance metrics are averaged over all the dimensions. In this regard, Dataset 2 ( comprising of three medical images and the color version of lena.jpg image) was used to benchmark ZT-based algorithm with those of DCT and FWHT. It can be observed in Figs. 11a and 12a that ZT-based transform have competitive performance on Scan1 data for block sizes 4 128, performance of ZT-based 8, 32 algorithm plummeted in comparison with DCT and FWHT-based approaches. Experimentation with Scan2 image shows that ZT consistently compresses the MRI data better and faster than its counterpart for most of the block sizes considered as shown in Figs. 11b and 12b. With block size of 128 128, the performance of FWHT improves and became comparable with ZT. Better performance in compression and speed of implementation is again observed on the average for ZT-based method using the Scan3 image, and the performance is more pronounced when the block size is 128 128 as shown in Figs. 11c and 12c. Again the proposed algorithm using color version of Lena image was compared with those of DCT and FWHT. It was observed that for most of the block sizes, ZT consistently compresses better and faster than its counterparts as shown in Figs. 11d and 12d. For block size 4 and 8, zipper transform has slightly better compression capability and"
1811.11012,data,195,2022-04-21,0,"This section of the technical report is focused on two similar desktop applications, which will here be referred to as Intersection Viewer (abbreviated IV) Version 1 and Intersection Viewer Version 2. IV v1 (developed by Nick Hodge and J.T. Blevins, with maintenance by Noah Carter) parses SPaT data and displays it in an intuitive graphical user interface. IV v2 (developed by Jacob Hoyos, Matthew Dale, and Noah Carter) goes a step further by not only displaying the GUI but also acting as a server and relaying the SPaT data to a single speciﬁed client machine over a local network. In IV v2, said client then displays the GUI based on the received SPaT data. The intended use of these applications was to serve as a learning experience and a prototype for the application described in section 3 of this report, IV v3. In IV v3 (itself yet another prototype), the client-server relationship was transitioned to the cloud, and made accessible from all personal devices over LTE (and the Internet generally)."
1811.11012,"data, dataset",70,2022-04-21,0,"volunteers’ vehicles were mounted with BSM-broadcasting devices. (See the ﬁnal section of this technical report for a basic analysis of that particular dataset.) One aspect of such data analysis is visualization and simulation. Allowing an analyst to see the data in a helpful and intuitive format enables the analyst to make more informed and strategic decisions [2], [26]."
1912.09582,dataset,13,2022-04-21,0,for small datasets–a case with Dutch book reviews. arXiv preprint 1910.00896.
1912.09582,dataset,15,2022-04-21,1,Table 4: Sentiment Analysis accuracy scores on the 110k Dutch Book Reviews Dataset.
1912.09582,github,26,2022-04-21,0,"2https://deepset.ai/german-bert 3https://github.com/cl-tohoku/bert-japanese 4A monolingual Dutch model has also been made available at http://textdata.nl, but this this model was consistently"
2005.07667,"data, database, dataset",180,2022-04-21,0,"For that purpose, in recent years, with the extensive development of deep learning techniques, especially convolutional and recurrent neural networks, the results are drastically improving. There have been quite a few researches attempting to generate data processing results by directly linking records in the tables to the semantic meaning of the natural language input, such as [6] and [7]. However, these attempts are not scalable to big tables and are not reusable when the database schema is changed. More recent approaches use only the natural language input and the database schema and metadata to generate the queries. We review the most recent approaches in our research. Furthermore, the release of large annotated datasets containing questions and the corresponding database queries has additionally enhanced the ability to use deep learning or supervised techniques to tackle this problem. This has enabled the problem to evolve into a more complex task where the approaches should be domain independent and involving multiple tables with complex queries."
2005.07667,dataset,45,2022-04-21,0,"Dataset ATIS [8] GeoQuery [9] Restaurants [10], [19] Academic [11] Scholar [12] Yelp [13] IMDB [13] WikiSQL [15] Advising [14] Spider [16]"
2005.08622,dataset,90,2022-04-21,0,"In literature, multi-label classiﬁcation is an important ﬁeld in machine learning and it is strongly related to many realworld applications for example, in biomedical images annotation, document categorization and whatever problem which the instances inside the classes are not disjoint but they keep a hierarchical structure. In this work, we have conducted four empirical studies on different datasets to prove by experimental results the effectiveness and robustness of our proposed model, that can be applied as an extension to any Convolutional Neural Network."
2007.12442,"data available, data",74,2022-04-21,0,"Third, edge-based pub/sub middleware usually lacks any mechanism of access control, e.g., the topics are public. An attacker with knowledge of the publish/subscribe topics could inject carefully crafted information while receiving potentially sensitive data sent by the clients (AV-3). By leveraging access control mechanisms, we can also revoke access to byzantine nodes, hence providing a simple defense mechanism against replay attacks towards the broker."
2007.12442,open-source,14,2022-04-21,0,[54] The Eclipse Foundation. Eclipse Mosquitto - An open source MQTT
2008.01391,code,155,2022-04-21,0,"An SMT system with a code-switched parallel corpus was studied by Menacer et al. (2019) and Fadaee and Monz (2018) for Arabic-English language pair. The authors have manually translated or used back translation method to translate foreign words. The identiﬁcation of the language of the word is based on the orthography. Chakravarthi et al. (2018) used the same approach for Dravidian languages; they used the improved MT for creating WordNet, showing improvement in the results. For English-Hindi, Dhar et al. (2018) manually translated the code-switched component and shown improvements. Machine translation of social media was studied by Rijhwani et al. (2016) where they tackle the code-mixing for Hindi-English and Spanish-English. The same approach translated the main language of the sentence using Bing Translate API (Niu et al., 2018)."
2011.09069,data repos,121,2022-04-21,0,"In case of Facebook platform, the coverage levels of the two aggregators are found to be quite similar, though PlumX has an edge over Altmetric.com. Ortega (2020a) has also found that in collecting mentions from Facebook and Mendeley, Altmetric.com has performed poorly as compared to PlumX. One possible reason for Altmetric.com recording slightly lesser Facebook mentions is that Altmetric.com captures only public pages19 and excludes likes, shares, and comments, whereas PlumX includes shares, likes, and comments along with the interactions in user’s closed network. A similar argument is also given by Zahedi & Costas (2018b) behind the low Facebook mentions captured by Altmetric.com."
2011.09069,data repos,4,2022-04-21,0,(c) Mendeley
2101.00522,dataset,104,2022-04-21,0,"Table 3: The percentage of shift in pixel labels during adaptation for the cardiac dataset. A cell (i, j) in the table has three values. The ﬁrst value represents the percentage of pixels labeled i that are labeled j after adaptation. The second value represents the percentage of switching pixels whose true label is i - lower is better. The third value represents the percentage of switching pixels whose true label is j - higher is better. Bolded cells denote label shift where more than 1% of pixels migrate from i to j."
2103.00265,dataset,169,2022-04-21,0,"λ1 . The metric tensor is considered to be poorly conditioned if the condition number has a high value. (Odena et al., 2018) proposed to eschew the issue of computing the complete spectrum, which could be quite challenging, in favor of sampling random directions (essentially sampling small random values for εvk and empirically computing 1.45, and then adding a penalty to encourage these values to fall within a speciﬁc range. In practice they achieved good results by setting λmin to 1.0 and λmax to 20.0. Making λmax too small could have the eﬀect of making it too hard for the model to be responsive to the latent variables and setting λmin to be too large could have the eﬀect of making it impossible for the model to learn to give large regions in the space relatively constant density. In practice these hyperparameters would likely need to be tuned depending on the dataset in accordance with these concerns."
2103.15335,dataset,2,2022-04-21,0,3.1 Datasets
2103.16349,dataset,148,2022-04-21,0,"The very ﬁrst observation is that MLP itself is a also a strong baseline, which outperforms HI and state-of-the-art models across almost all datasets and prediction lengths. Regardless of this point, we take MLP as a basic model, and evaluate the the ensemble of MLP and HI. We operate weighted summation over MLP’s and HI’s outputs to get the ﬁnal prediction. The weights of two models are set as 0.5/0.5. From Table 4 and Table 5, it could be concluded that this hybrid model can obtain better results in many cases, which is especially evidential for the task of univariate forecasting. MLP + HI brings up to 32% relative improvement over HI and 45% relative improvement over MLP on MSE, and 20%, 27% relative improvement on MAE."
2104.02307,dataset,4,2022-04-21,0,Target (dataset)
2104.09994,"data, dataset",170,2022-04-21,0,"sources (network data but also sensor readings, operating system logs and telemetry data) about a network containing several IoT/IIoT devices. In [27], the authors propose a dataset collecting benign and volumetric attacks traﬃc traces for 27 IoT devices. The main purpose of this dataset is to evaluate volumetric attacks perpetrated against a network containing real commercial IoT devices. The dataset proposed in [28] was generated with the traﬃc of 2 home IoT devices under multiple attack scenarios. It also includes simulated Mirai traﬃc appearing to come from the IoT devices. Finally, IoT23 [29] is a dataset consisting of 20 captures that include malware activity as well as 3 captures of benign IoT traﬃc. To conclude, it is worthy to mention that there is a lack of dataset suitable for FL approaches detecting malware in IoT devices. Existing FL-based solutions must consider split centralized datasets in order to apply federated techniques."
2104.09994,database,42,2022-04-21,0,"be used as a decentralized database where each client would share its local model and retrieve the models of other clients when performing the aggregation. Thus, the framework would be totally decentralized without an entity coordinating the generated models."
2104.09994,"publicly available, dataset",7,2022-04-21,0,Table 1 Public IoT network datasets.
2105.04903,dataset,145,2022-04-21,0,"REFERENCES [1] Omar Adjali, Romaric Besançon, Olivier Ferret, Hervé Le Borgne, and Brigitte Grau. 2020. Building a Multimodal Entity Linking Dataset From Tweets. In Proceedings of the 12th Language Resources and Evaluation Conference. 4285–4292. [2] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2020. Open-Domain Question Answering Goes Conversational via Question Rewriting. arXiv preprint arXiv:2010.04898 (2020). [3] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 344–354."
2105.04903,dataset,177,2022-04-21,1,"Generating annotation candidates. We employ a pooling approach to generate an extended set of candidate mentions and entities. Three EL tools were used to annotate the dialogues: TagMe [28], WAT [46], and REL [57]. Each tool was employed in two ways: (i) the turn method, which annotates a single turn, irrespective of the conversation history, and (ii) the history method, which annotates each turn given the conversation history up to that turn. For the CAsT dataset, only user utterances were given to the EL tool, while for other datasets both system and user utterances were considered as conversation history. This is due to relatively long system utterances in the CAsT dataset, which makes infeasible for the EL tools to annotate the whole conversation history. To further improve the recall of our pool, we included the top-10 Wikipedia search results, using mentions as queries sent to the MediaWiki API.4"
2105.04903,dataset,35,2022-04-21,1,"The resources provided in this paper allow for further investigation of entity linking in conversational settings, can be used for evaluation or training of conversational EL systems, and complement existing conversational datasets."
2105.10148,dataset,12,2022-04-21,0,"valid dataset with a ratio of 9:1, train each algorithm on"
2105.10148,dataset,56,2022-04-21,0,"We then evaluate all the algorithms on the hard dataset in both BSuite and DM Control environments. The episodes are generated from a mixture of partially trained policies from a diﬀerent run, and the distribution of states is likely to have a quite diﬀerent coverage from the distribution generated by the target distribution."
2106.07691,"data, dataset",107,2022-04-21,1,"only 6.09% of T5base’s attempts were AP T . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5base would generate better paraphrases, as measured by a higher chance of passing AP T on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table 2, which shows that T5base was able to generate an AP T passing paraphrase for 84.8% of the sentences in TwitterPPDB."
2106.13764,"data, data https",51,2022-04-21,0,"[35] Sybu Data. Sybu javascript blocker – google chrome extension. https: //sybu.co.za/wp/projects/js-blocker/, 2016. Accessed: 2020-05-02. [36] Houssein Djirdeh. Javascript | 2019 | the web almanac by http archive. https://almanac.httparchive.org/en/2019/javascript, 2019. Accessed: 2020-01-2."
2106.13764,database,138,2022-04-21,0,"SlimWeb’s JS classiﬁcation service crawls popular web pages to identify JS elements used in these pages, and then employs the classiﬁer to label these elements and store their categories in a database. It periodically updates and shares the labels with the users’ browser plugins. On the other hand, SlimWeb’s browser plugin is responsible for blocking noncritical JS elements. These elements are identiﬁed based on the labels received from the service. When a web page is requested by the user, the plugin ﬁrst checks if a label is locally available for each JS element, such that non-critical elements are immediately blocked. In the case of a label absence, the plugin considers the corresponding JS element as critical and requests it from the web."
2107.10483,"data, dataset provided",135,2022-04-21,0,"Assumption 4 ENCO relies on neural networks to determine the conditional data distributions ...). Hence, for providing a guarantee, we assume that in the graph learning step the neural p(Xi| networks have been sufﬁciently trained such that they accurately model all possible conditional ...). In practice, the neural networks might have a slight error. However, as long as distribution p(Xi| enough data, network complexity, and training time is provided, it is fair to assume that the difference between the modeled distribution and the true conditional is smaller than an arbitrary constant (cid:15), based on the universal approximation theorem (Hornik et al., 1989). For the limited data setting, see Appendix B.2.3."
2108.02756,dataset,140,2022-04-21,0,"Inspired by the defense mechanism presented in [20], we could use our method to generate boundary samples, which are examples that fall near the decision boundary of a pre-trained classiﬁer. Such examples can be used for adversarial training to enhance robustness or for knowledge distillation (e.g., see [20, 21]). Here, we focus on synthesizing such examples regardless of the application of interest. Unlike [20], we achieve this without the need to train a CGAN with a full dataset, and with the additional ﬂexibility of choosing a desired soft output pd (e.g., uniform over only a subset of the classes). The ﬁrst four synthesized samples in the bottom row of Figure 1 are all examples of near-decision-boundary samples."
2108.02756,dataset,89,2022-04-21,0,"We augment a repeated version of vector z to create a small training dataset and utilize the back-propagation algorithm [14]. Given the two objectives of BOSS, and the utilization of the adjustable parameters of network h, φ, we introduce the surrogate losses Lh(p(g(z ; φ) ; θ), pd) and Lg(g(z ; φ), xd), and use the back-propagation algorithm to optimize φ based on the minimization"
2109.08237,"data, dataset",225,2022-04-21,0,"In the ﬁrst experiment, the DL algorithm was trained on the different datasets. Figure 6 displays an example from the test set, which shows the gold standard images and the DL reconstructions for data undersampled with R = 4. Generally the visual quality of all the images (both gold standard and reconstructed ones) reduces with increased JPEG compression level (left-to-right in Figure 6); this is expected from compressed data. However, the NRMSE metric shows an unexpected effect: it improves with the compression, i.e. the reconstruction error reduces although the image visual quality degrades. The reason for this phenomenon is that in retrospective experiments the reconstruction quality is measured w.r.t. to a ""gold standard"" image that is also processed (see the pipeline in Figure 1c); the error metric is therefore blind to data processing. Strikingly, the NRMSE could show a misleading improvement even when the human eye cannot see any difference, as demonstrated in the left two columns of Figure 6: although the reconstructions from NC and QF = 75 are visually similar, the NRMSE of the latter is lower by 30%. This reﬂects the subtle bias induced by the pipeline of subtle data crime II."
2109.08237,dataset,81,2022-04-21,0,"[21] A. D. Desai, A. M. Schmidt, E. B. Rubin, C. M. Sandino, M. S. Black, V. Mazzoli, K. J. Stevens, R. Boutin, C. Re, G. E. Gold, et al., “SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation,” in Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021."
2109.12907,dataset,10,2022-04-21,0,Table 3: Usage of relations in the dataset.
2110.05428,"data, dataset",151,2022-04-21,0,"Evaluation Metrics To measure the identiﬁability of latent causal variables, we compute Mean Correlation Coefﬁcient (MCC) on the validation dataset, a standard metric in the ICA literature for continuous variables. MCC reaches 1 when latent variables are perfectly identiﬁable up to permutation and componentwise invertible transformation in the noiseless case (we use Pearson correlation and rank correlation for linearly and nonlinearly related latent processes, respectively). To evaluate the recovery performance on causal relations, we use different approaches for (1) linear and (2) nonlinear transitions: (1) the entries of estimated state transition matrices are compared with the true ones after permutation, signs, and scaling are adjusted, and (2) the estimated causal skeleton is compared with the true data structure, and Structural Hamming Distance (SHD) is computed."
2110.05428,dataset,64,2022-04-21,1,"KiTTiMask The KiTTiMask dataset consists of pedestrian segmentation masks sampled from the autonomous driving vision benchmark KiTTi-MOTS. For each given frame, the position (vertical and horizontal) and the scale of the pedestrian masks are set using measured values. The difference in the sample time (e.g., ∆t = 0.15s) generates the sparse Laplacian innovations between frames."
2110.11575,"github, repo",38,2022-04-21,0,"Make sure the target repo (the repo to be analyzed, not the repo of this tool) is on your machine. In this demo, the target repo is downloaded from a GitHub repo:"
2110.11575,repo,67,2022-04-21,0,# make sure [ repo path ] is the target repo path # the [ output path ] can be anywhere you desire git_stats generate -p [ repo path ] -o [ output path ] # e . g . git_stats generate -p / home / user / git - stats / Weasis -o # / home / user / git - stats / Weasis - analytics
2110.15667,database,45,2022-04-21,0,"[50] L. Deng, “The mnist database of handwritten digit images for machine learning research [best of the web],” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012."
2110.15667,dataset,1,2022-04-21,0,Dataset
2110.15667,dataset,33,2022-04-21,0,"[51] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,” arXiv preprint arXiv:1708.07747, 2017."
2111.03516,"data, dataset",34,2022-04-21,0,"Figure 2 shows the F1 comparisons for each of the data augmentation methods (SMOTE, B SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE, SMOTE-RSB, and CFA) on 25 datasets using"
2111.03516,dataset,16,2022-04-21,1,classifier on the ‘PIMA’ dataset we can see that SMOTE-based methods had better performance
2111.03516,dataset,16,2022-04-21,1,§ Pima Indians Diabetes dataset: A binary-class dataset used to predict whether or not a
2111.03516,dataset,18,2022-04-21,0,"next best with 3 datasets. Baseline, B-SMOTE, and ADASYN had the highest AUC-ROC in 2"
2111.03516,dataset,20,2022-04-21,1,"§ Ecoli dataset: This dataset is a multi-class dataset, with 8 classes. The problem is to classify"
2111.03516,dataset,31,2022-04-21,0,"and Multilayer Perceptron (MLP) models. Several alternative ML models were used because dif ferent models find different decision boundaries for a given dataset, differences that could impact"
2111.04287,python,39,2022-04-21,2,"We ﬁll the gap between the rapid development of algorithms on paper and the lack of practice in real world with the introduction of BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms."
2111.13172,"data, data repository",5,2022-04-21,0,UDM: Unified Data Management
2112.03471,dataset,2,2022-04-21,0,4.1.1 Datasets
2201.00732,database,26,2022-04-21,0,"[49] Luca Biferale, Fabio Bonaccorso, Michele Buzzicotti, and Patricio Clark di Leoni, “TURB-Rot. A large database of 3d"
2201.00732,dataset,106,2022-04-21,0,"FIG. 11. (Main panel) Mean deviation between the predicted and true value of Ω for each of the ten classes considered, the errorbars show the root mean square of the diﬀerence between Ωpred and Ωtrue. (Inset) Mean squared diﬀerence between prediction and true Ω normalized to the square of the correct rotation value for each of the ten classes. The average is taken over the validation dataset, while the predictions are obtained as in Fig. 10, by the neural network, DCNN, or as the mean, ΩBay and most likely, Ω∗"
2201.00732,dataset,3,2022-04-21,0,B. Dataset extraction
2202.12674,github,4,2022-04-21,0,3https://github.com/SC-SGS/PLSSVM/blob/v1.0.1/utility scripts/generate
2202.12674,"github, data, data https, python",59,2022-04-21,0,"[18] H. E. L. Cagnin et al., “A portable OpenCL-based IEEE, 2015. [19] S. Raschka et al., “Machine learning in python: Main developments and technology trends in data science, machine learning, and artiﬁcial intelligence,” MDPI, 2020. https://github.com/ “svm-gpu,”"
1401.8008,"code, package, code available, github, code package",56,2022-04-27,2,"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github."
1401.8008,data,103,2022-05-13,0,"Comparison data also results when considering subjective human evaluations of pairs of items. For example, if each item is a movie, a person might say that Les Misérables is better than Star Wars, and The Empire Strikes Back is as good as Star Wars. Another example is rating food items such as wine, in which a person may prefer one wine to another, but not be able to perceive a diﬀerence between two other wines. In this context, it is important to use a model which can predict no diﬀerence between two items."
1401.8008,data,11,2022-05-13,0,4. Comparison to SVMrank in sushi and simulated data sets
1401.8008,data,112,2022-05-13,0,"Fig. 6. Test AUC for each model used after training on the ﬁrst 4 months of match data in the 8 different 12-month periods. All SVM model AUCs are shown in addition to AUC of the ELO, Glicko scores and trivial model. The plots in black show the trivial AUC calculated by predicting the most common positive label. The plots in dark blue are AUC values obtained from using Glicko or ELO scores only. The plots in light blue show the AUC distribution from models using only ELO and Glicko features and plots in white are AUC values from models using all computed features."
1401.8008,data,113,2022-05-13,0,"The goal of learning to compare is to accurately predict a test set of labeled pairs (2), which includes equality yi = 0 pairs. We test the SVMcompare algorithm alongside two baseline models that use SVMrank (Joachims 2002). We chose SVMrank as a baseline because of its similar large-margin learning formulation, to demonstrate the importance of directly modeling the equality yi = 0 pairs. SVMrank does not directly model the equality yi = 0 pairs, so we expect that the proposed SVMcompare algorithm makes better predictions when these data are present. The diﬀerences between the algorithms are summarized in Table 2:"
1401.8008,data,115,2022-05-13,0,"Lemma 3.1 establishes the fact that one can learn a ranking function r and a corresponding comparison function c1, deﬁned in (5), by solving either the LP (7) or the QP (12). To make corresponding learning problems for non linearly-separable data as deﬁned by the linear separability test in this section, one can add slack variables to either the QP or the LP. In the next subsection, we pursue only the QP, since it leads to a dual problem with a sparse solution that can be solved by any standard SVM solver such as libsvm (Chang & Lin 2011)."
1401.8008,data,138,2022-05-13,0,"In this subsection, we assume the data are not linearly separable, and want to learn a nonlinear ranking function. We deﬁne a positive deﬁnite kernel κ : Rp × Rp → R, which implicitly deﬁnes an enlarged set of features Φ(x) (middle panel of Figure 1). As in (9), we learn a function f (x) = β + u⊺Φ(x) which is aﬃne in the feature space. Let α, α′ ∈ Rm be coeﬃcients such that u = i), and so we have m i=1(αiκ(˜xi, x) + α′ f (x) = β + i, x)). We then use Lemma 3.1 to deﬁne the ranking function"
1401.8008,data,15,2022-05-13,0,"Overall from the sushi data, it is clear that the proposed SVMcompare model performs"
1401.8008,data,15,2022-05-13,0,"generalizes to a test set of data, as measured by the zero-one loss:"
1401.8008,data,161,2022-05-13,0,"In Figure 5 we ﬁxed the number of training pairs n = 400 and varied the proportion ρ of equality pairs for the three simulated squared norm ranking functions r. We select the model with maximum area under the validation set ROC curve, then use test set AUC to evaluate the learned models. All methods perform close to the optimal true ranking function when r(x) = ||x||2 2. For the other patterns, it is clear that all the methods perform similarly when there are mostly inequality pairs (ρ = 0.1), since SVMrank was designed for this type of training data. In contrast, when there are mostly equality pairs (ρ = 0.9), the compare and rank2 methods clearly outperform the rank method, which ignores the equality pairs. It is also clear that the rank2 and compare methods perform similarly in terms of test AUC."
1401.8008,data,19,2022-05-13,0,"Joachims, T. (2002), Optimizing search engines using clickthrough data, in ‘KDD’."
1401.8008,data,199,2022-05-13,0,"In the supervised learning to rank problem (Li 2011), we are given labeled pairs of items x, x′, where the label y ∈ {−1, 1} indicates which item in the pair should be ranked higher. The goal is to learn a ranking function r(x) ∈ R which outputs a real-valued rank for each item. In this paper we consider a related problem in which the expanded label space y ∈ {−1, 0, 1} includes the y = 0 label which indicates that there should be no rank diﬀerence. In this context the goal is to learn a comparison function c(x, x′) ∈ {−1, 0, 1}. Comparison data naturally arise from competitive two-player games in which the space of possible outcomes includes a draw (neither player wins). In games such as chess, draws are a common result between highly skilled players (Elo 1978). To accurately predict the outcome of such games, it is thus important to learn a model that can predict a draw."
1401.8008,data,2,2022-05-13,0,training data
1401.8008,data,23,2022-05-13,0,"predicted (Shashua & Levin 2003). In this article we propose SVMcompare, a support vector algorithm for these data."
1401.8008,data,29,2022-05-13,0,"Overall, our analysis of the chess match data suggests that the proposed SVMcompare model performs with a higher AUC than the existing state-of-the-art ELO and Glicko results."
1401.8008,data,31,2022-05-13,0,"Overall from the simulations, it is clear that when the data contain equality pairs, it is advantageous to use a model such as the proposed SVMcompare method which learns"
1401.8008,data,32,2022-05-13,0,"Taken together, these imply ˆµ = −1/β. Now, by deﬁnition of the ﬂipped data (8), we can re-write the max margin QP (9) as"
1401.8008,data,51,2022-05-13,0,"These data are geometrically represented in the top panel of Figure 1. Pairs with equality labels yi = 0 are represented as line segments, and pairs with inequality labels yi = {−1, 1} are represented as arrows pointing to the item with the higher rank."
1401.8008,data,57,2022-05-13,0,"For future work, it will be interesting to see if the same results are observed in learning to rank data from search engines. For scaling to these very large data sets, we would like to try algorithms based on smooth discriminative loss functions, such as stochastic gradient descent with a logistic loss."
1401.8008,data,67,2022-05-13,0,"The optimal decision boundaries r(x) ∈ {−1, 1} and margin boundaries r(x) ∈ {−1 ± µ, 1 ± µ} are drawn in Figure 2. Note that ﬁnding a feasible point for this LP is a test of linear separability. If there are no feasible points then the data are not linearly separable."
1401.8008,data,7,2022-05-13,0,3.2. Kernelized QP for non-separable data
1401.8008,data,77,2022-05-13,0,"Fig. 2. The separable LP and QP comparison problems. Left: the difference vectors x′ − x of the original data and the optimal solution to the LP (7). Middle: for the unscaled ﬂipped data ˜x′ − ˜x (8), the LP is not the same as the QP (9). Right: in these scaled data, the QP is equivalent to the LP."
1401.8008,data,89,2022-05-13,0,"1 where x ∈ R2. Left: the training data Fig. 3. Application to a simulated pattern r(x) = ||x||2 are n = 100 pairs, half equality (segments indicate two points of equal rank), and half inequality (arrows point to the higher rank). Others: level curves of the learned ranking functions. The rank model does not directly model the equality pairs, so the rank2 and compare models recover the true pattern better."
1401.8008,data,89,2022-05-13,0,"Our experimental results on simulated and real data clearly showed the importance of directly modeling the equality pairs, when they are present. We showed in Figure 5 that when there are few equality pairs, as is the usual setup in learning to rank problems, the baseline SVMrank algorithm performs as well as our proposed SVMcompare algorithm. However, when there are many equality pairs, it is clearly advantageous to use a model such as SVMcompare which directly learns from the equality pairs."
1401.8008,data,94,2022-05-13,0,"3.1. LP and QP for separable data In our learning setup, the best comparison function is the one with maximum margin. We will deﬁne the margin in two diﬀerent ways, which correspond to the linear program (LP) and quadratic program (QP) discussed below. To illustrate the diﬀerences between these max-margin comparison problems, in this subsection we assume that the training data are linearly separable. Later in Section 3.2, we propose an algorithm for learning a nonlinear function from non linearly-separable data."
1401.8008,"data, database",181,2022-05-13,0,"Ranking data sets are often described not in terms of labeled pairs of inputs (xi, x′ i, yi) but instead single inputs xi with ordinal labels yi ∈ {1, . . . , k}, where k is the number of integral output values. Support Vector Ordinal Regression (Chu & Keerthi 2005) has a large-margin learning formulation speciﬁcally designed for these data. Another approach is to ﬁrst convert the inputs to a database of labeled pairs, and then learn a ranking model such as the SVMcompare model we propose in this paper. Van Belle et al. (2011) observed that directly using a regression model gives better performance than ranking models for survival data. However, in this paper we limit our study to models for labeled pairs of inputs, and we focus on answering the question, “how can we adapt the Support Vector Machine to exploit the structure of the equality yi = 0 pairs when they are present?”"
1401.8008,"data, dataset",101,2022-05-13,0,"The notation and organization of this article is as follows. We use bold uppercase letters for matrices such as X, K, and bold lowercase letters for their row vectors xi, ki. In Section 2 we discuss links with related work on classiﬁcation and ranking, then in Section 3 we propose a new algorithm: SVMcompare. We show results on 3 illustrative simulated data sets and 2 real by learning to rank a sushi data set and a chess dataset in Section 4 and 5. We then discuss future work in Section 6."
1401.8008,"data, dataset",159,2022-05-13,0,"In ranking problems, the goal is to learn a ranking function r(x) ∈ R from Summary. labeled pairs x, x′ of input points. In this paper, we consider the related comparison problem, where the label y ∈ {−1, 0, 1} indicates which element of the pair is better (y = −1 or 1), or if there is no signiﬁcant difference (y = 0). We cast the learning problem as a margin maximization, and show that it can be solved by converting it to a standard SVM. We use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed SVMcompare algorithm outperforms SVMrank when there are equality y = 0 pairs. In addition, we show that SVMcompare outperforms the ELO rating system when predicting the outcome of chess matches."
1401.8008,"data, dataset",68,2022-05-13,0,"Fig. 5. Area under the ROC curve (AUC) for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. For each data set we picked n = 400 pairs, varying the proportion ρ of equality pairs. We plot mean and standard deviation of AUC over 4 test sets."
1401.8008,"data, dataset",78,2022-05-13,0,"Another way to formulate the comparison problem is by ﬁrst performing a change of variables, and then solving a binary SVM QP. The idea is to maximize the margin between signiﬁcant diﬀerences yi ∈ {−1, 1} and equality pairs yi = 0. Let Xy, X′ y be the |Iy| × p matrices formed by all the pairs i ∈ Iy. We deﬁne a new “ﬂipped” data set with"
1401.8008,"data, dataset",86,2022-05-13,0,"2.2. SVMrank for comparing In this subsection we explain how to apply the existing SVMrank algorithm to a comparison data set. The goal of SVMrank is to learn a ranking function r : Rp → R. When r(x) = w⊺x (where ⊤ denotes the transpose) is linear, the primal problem for some cost parameter C ∈ R+ (where R+ is a set of all non-negative real numbers) is the following quadratic program (QP):"
1401.8008,"data, dataset",87,2022-05-13,0,"Fig. 4. Test error for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. We randomly generated data sets with ρ = 1/2 equality and 1/2 inequality pairs, then plotted test error as a function of data set size n (a vertical line shows the data set which was used in Figure 3). Lines show mean and shaded bands show standard deviation over 4 test sets."
1401.8008,"data, dataset",95,2022-05-13,0,"For each experiment, there are train, validation, and test sets each drawn from the same data set of examples. We ﬁt a 10 × 10 grid of models to the training set (cost parameter C = 10−3, . . . , 103, Gaussian kernel width 2−7, . . . , 24), and select the model using the validation set. We use two evaluation metrics to judge the performance of the models: zero-one loss and area under the ROC curve (AUC)."
1401.8008,"data, dataset, data https",164,2022-05-13,1,"5.1. Data source and processing We downloaded the chess match dataset from Chessmetrics (http://www.chessmetrics.com/cm/), containing 1.8 million games played over the 11year period from 1999–2009 by 54205 chess players. For each of the years 1999–2006, we consider the ﬁrst four months (Jan–Apr) as a train set, and the last eight months as a test set (May–Dec). We removed all matches containing a player who had less than 10 matches against other players in the train set, to prevent our data set from containing players with very little information. We also removed all matches that contained a player’s ﬁrst match from the train set as we would have no information about this player. Before pre-processing, 30.9% of matches were a draw and 69.1% of matches resulted in a win or loss. After pre-processing, the median percentage of draws and win-loss results"
1401.8008,"data, dataset, data https",233,2022-05-13,1,"4.2. Learning to rank sushi data We downloaded the sushi data set of Kamishima et al. (2010) from kamishima (http://www.kamishima.net/sushi). We used the sushi3b.5000.10.score from kamishima, which consist of 100 diﬀerent sushis rated by 5000 diﬀerent people. Each person rated 10 sushis on a 5 point scale, which we convert to 5 preference pairs, for a total of 17,832 equality yi = 0 and 7,168 inequality yi ∈ {−1, 1} pairs. For each pair i we have features i ∈ R14 consisting of 7 features of the sushi and 7 features of the person. Sushi xi, x′ features are style, major, minor, oily, eating frequency, price, and selling frequency. Person features are gender, age, time, birthplace and current home (we converted Japanese prefecture codes to latitude/longitude coordinates). As in the simulations of Section 4.1, we picked train, validation, and test sets, each with the same number of pairs n and the same proportion ρ of equality pairs. We ﬁt a grid of models to the training set, select the model with minimal zero-one loss on the validation set, and then use the test set to estimate the generalization ability of the selected model."
1401.8008,"dataset, package",127,2022-05-13,0,"was 44.7% and 55.3% respectively over each of the 8 datasets. For each match i, we i ∈ R16 consisting of ELO scores, Glicko scores, if the player had computed features xi, x′ the initial move, the percentage of instances where a player either lost to a lower ranked player, or won against a higher ranked player, the average score diﬀerence of opponents, win/loss/draw/games played raw values and percentages in addition to various other statistics. ELO scores were initially set at 1200 for all players and FIDE rules were applied to score calculations. ELO and Glicko scores were updated after every match using the PlayerRatings R package (Stephenson & Sonas 2016)."
1401.8008,github,3,2022-05-13,0,https://github.com/tdhock/compare-paper
1401.8008,package,13,2022-05-13,0,"Player Ratings Estimation (R package version 1.0-1)’, CRAN ."
1401.8008,package,26,2022-05-13,0,"Karatzoglou, A., Smola, A., Hornik, K. & Zeileis, A. (2004), ‘kernlab – an S4 package"
1605.07495,code,14,2022-05-13,0,Algorithm 1: Pseudo code of selection method based on non-dominated relative crowding distance
1605.07495,code,16,2022-05-13,0,Algorithm 3: Pseudo code of calculation methods of CR and LR in (15)
1605.07495,code,233,2022-05-13,0,"(i.e., ξcd) and the relative crowding distance (i.e., ξrcd) of all the 6 solutions are given out in TABLE II. Then, with nondominated CDV, the Θ2, Θ3 and Θ4 are qualiﬁed to be the candidates. Thus, the global best solution will be selected out according to their ξrcd (i.e., the selected ones will be marked with circle). For instance, since with the largest ξrcd, the Θ4 has the highest priority to be selected as global best in any case, while the Θ2 is with the lowest priority (only when Y = 3) for its smallest ξrcd. Finally, we assume there are 100 particles in total and we give out the result of particle dividing when Y = 1, 2, 3 (i.e., the numbers of particles in the group assigned to each selected solution are given out after each circle). We notice that the Θ5 is not be selected for any case though its relative crowding distance higher than that of the Θ2, it is because that the CDV of the Θ5 is dominated by the Θ3, which excluding its possibility to be selected as global best. We also give out the pseudo code of this selection method in Algorithm 1."
1605.07495,code,65,2022-05-13,0,"more non-dominated solutions. Meanwhile, each sub-swarm aims to ﬁnd better values for its assigned objective function and all sub-swarms work together in order to get a bigger PF . The pseudo code of MOPSO-NRCD is given at Algorithm 2 (the calculation method of objective functions is given in Algorithm 3, whose detail will be given in the next section)."
1605.07495,code,7,2022-05-13,0,Algorithm 2: Pseudo code of MOPSO-NRCD
1606.01039,data,10,2022-05-13,0,(b) Signal used for ﬁlling missing-data gaps.
1606.01039,data,106,2022-05-13,0,Results using (16) are presented in Figures 5(e)-5(f). We see that in Figure 5(f) the posterior mean describes properly the periodic behaviour and amplitude envelope smooth evolution of the modelled signal. We observe that prediction on the decay gap using (16) is closer to the actual data (red dots) than the results obtained with (15) as well as (14). This is reﬂected in the smallest RMS error in table 2. This is because (16) allows to describe periodic functions that
1606.01039,data,119,2022-05-13,0,"To face the issue of modelling time dynamics we modiﬁed the previous covariance function (15), by multiplying it with an exponentiated quadratic kernel (14). This allows to “smooth” the strictly periodic behaviour of (15). The resulting kernel corresponds to (16). From Figure 4(b) we see that although the posterior mean of the predictive distribution does not exactly ﬁt the data, the model is able to learn the pitch of each of the three sound events with a smaller RMS error (Table 1), as well as the time dynamics or variations in the amplitude envelope of the signal."
1606.01039,data,131,2022-05-13,0,"Experiments were done over real audio. We evaluated diﬀerent kernel conﬁgurations on a pitch estimation task, and on a missing data imputation task. All experiments assume we previously know the number of change-windows and its locations. In the pitch estimation task all the parameters of the covariance function are known, except those related with the fundamental frequency of each sound event, i.e. the value of ωm in (15) and (16) when using these kernels in the general model (10). Thus, we focus on optimizing only these model hyperparameters from the data. In the missing data imputation task the score of the modelled piece of music audio is used for tuning manually the model hyperparameters."
1606.01039,data,132,2022-05-13,0,"In music information research, the aim of audio content analysis is to estimate musical concepts which are present but hidden in the audio data [17]. With this purpose, diﬀerent signal processing techniques are applied to music signals for extracting useful information and descriptors related to the musical concepts. Here, musical concepts refers to parameters related to written music, such as pitch, melody, chords, onset, beat, tempo and rhythm. Then, perhaps the most general application is one which involves the prediction of several musical dimensions, that of recovering the score of a music track given only the audio signal [10]. This is known as automatic music transcription (AMT) [5]."
1606.01039,data,132,2022-05-13,0,"We performed regression on the signal shown in Figure 3(a) using the kernel (15). Figure 4(a) shows the posterior mean of the predictive distribution after training (blue continuous line). The black circle points correspond to observed data. We see the trained model is able to estimate the pitch for each sound event with a RMS error of 0.6282 semitones (Table 1). On the other hand, the amplitudeenvelope evolution of the signal is beyond the scope of the structure that this kernel can model. This is because this covariance function can only describe constant amplitude-envelope, periodic signals, with a fundamental frequency and several harmonics (see Figure 2(f))."
1606.01039,data,142,2022-05-13,0,"In this study we used two short audio excerpts, in order to explore the method, so that we can eﬃciently ﬁt models and search in the hyperparameter space. The excerpt used for pitch estimation experiments corresponds to 0.7 seconds of the song Black Chicken 37 by Buena Vista Social Club. This segment of audio contains three notes of a bass melody (Figure 3(a)). In the missing data imputation task we used polyphonic audio corresponding to 1.14 seconds of Chopin’s Nocturne Op. 15 No. 1, where more than one note occur at the same time. The segments of signal in red in Figure 3(b) represent gaps of missing data. We reduced the sample frequency of both audio excerpts from 44.1KHz to 8KHz."
1606.01039,data,144,2022-05-13,0,"In this article we discussed a Gaussian processes regression framework for modelling music audio. We compared diﬀerent models in pitch estimation as well as in prediction of missing data. We showed which kernels were more appropriate for describing properties of music signals, speciﬁcally: nonstationarity, dynamics, and spectral harmonic content. The advantage of this approach is that by designing a proper kernel we can introduce prior knowledge and beliefs about the properties of music signals, and use all that prior information to improve prediction. The presented work could be extended using eﬃcient representations of GPs in order to model larger audio signals. Other kernels could be studied, as the spectral mixture for modelling harmonic content [2], and Latent Force models [1] for describing mechanistic characteristics of the signal."
1606.01039,data,153,2022-05-13,0,"We compared three diﬀerent models predicting missing-data gaps. We studied kernels (14), (15), and (16). In Figure 3(b) ﬁrst gap (red segment) contains the transient (onset and attack [3]) of a sound event, whereas the second gap is located in a more stable segment of the data (smooth decay). Figures 5(a)5(b) depict the prediction using (14). These ﬁgures correspond to zoom in small sections of the signal where the gaps occur (Figure 3(b)). We see that the model using this kernel overﬁts the data, i.e. the posterior mean (blue line) ﬁts all the observed data (black dots) with high conﬁdence (grey shaded area), but the"
1606.01039,data,183,2022-05-13,0,"Figures 5(c)-5(d) show the prediction using covariance function (15). In the transient gap (Figure 5(c)) the posterior mean (blue line) does not follows the data, this is because transients are short intervals during which the signal evolves in a nonstationary, nontrivial and unpredictable way [3]. opposite to this, the model using kernel (15) can only describe the behaviour of constant amplitudeenvelope periodic stochastic functions. In the second gap (Figure 5(d)) the posterior mean describes properly the periodic behaviour of the data, but it does not follow the amplitude-envelope of the observations. This is because this covariance function is able to describe periodic functions that have several harmonic components. The drawback of this kernel is that it assumes constant the amplitude of the periodic stochastic functions that describes. These diﬀerent performance on the prediction is reﬂected on the RMS error obtained for each gap (Table 2)."
1606.01039,data,2,2022-05-13,0,3.1 Data
1606.01039,data,219,2022-05-13,0,"R where the time input variable t ∈ R, we model the whole function f (t) as a GP. That is, instead of putting a prior over the function parameters η, we introduce a prior over the function f (t) itself [16]. Learning in GP regression corresponds to computing the posterior distribution over the function f (t) conditioned on the observed data y = [y1, · · · , yN ] [15, 13]. The underlying idea in GP regression is that the correlation function introduces dependences between function f (t) values at diﬀerent inputs. Thus, the function values at the observed points give information also of the unobserved points [14]. The structure of the kernel (1) captures high-level properties of the unknown function f (t), which in turn determines how the model generalizes or extrapolates to new test time instants [9]. This is quite useful because we can introduce prior knowledge about what we believe the proprieties of music signals are, by choosing a proper kernel that reﬂects those characteristics. In section 2.2 we study in more detail the design of kernels."
1606.01039,data,264,2022-05-13,0,"In [20] GPs are used for time-frequency analysis as probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct timefrequency components, with the analytic goal being to infer these components by applying Bayes’ rule [20]. GPs have also been used for underdetermined audio source separation. In [8] the mixture signal is modelled as a linear combination of independent convolved versions of latent GPs or sources. The model splits the mixture signal in frames also considered independent, by using weightfunctions. Thus each source is modelled as a series of concatenated locally stationary frames, each one with its corresponding covariance function. With this assumption the resulting signal is supposed to be non-stationary [8]. On the other hand, despite the approach we present also assumes the latent GPs fm in (10) as non-correlated, the observed signal is not framed into independent segments. Instead of using weight-functions that act over the observed data, we introduce change-windows φm inﬂuencing each latent GP ending up with latent processes representing speciﬁc sound events that happen at certain segments of time. Therefore the proposed model keeps the correlation between the observations throughout all the signal. That is what allows to make prediction in gaps of missing data (section 4.2). GPs have been used also for estimating spectral envelope and fundamental frequency of singing voice [21], and for time-domain audio source separation [22]."
1606.01039,data,42,2022-05-13,0,"Figure 3: (a) analysed audio (blue line), change-windows (dashed lines). (b) observed data (blue line), missing-data gaps (red line), change-windows (dashed lines)."
1606.01039,data,45,2022-05-13,0,"Figure 5: Zoom in a portion of missing-data gaps. In each ﬁgure the continuous blue line represent the posterior mean, grey shaded areas correspond to the posterior variance, red dots are missing data, whereas black dots are observed data."
1606.01039,data,8,2022-05-13,0,4.2 Filling gaps of missing data in audio
1606.01039,data,8,2022-05-13,0,"and Data Analysis. Wiley, 1988."
1606.01039,data,91,2022-05-13,0,"The covariance function (1) used for computing the prior distribution (5) allows us to introduce in the model all the knowledge and beliefs we have about the properties of the data. We are trying to model music signals, and some of the broad properties of audio signals are non-stationarity, rich spectral content, dynamics (locally periodic, non constant amplitude envelope), mechanistic behaviour, and music structure. Therefore we seek covariance functions that can describe or reﬂect these properties."
1606.01039,"data, data available",57,2022-05-13,0,"conﬁdence decreases and the prediction is quite poor in the input space zones where the data is not available (red dots). Also, we see that the model using (14) does not expect any periodic behaviour in the gaps. The RMS error for both gaps is presented in Table 2."
1606.01039,"data, dataset",117,2022-05-13,0,"The regression problem concerns the prediction of a continuous quantity [12], here a function f (t), given a data set D = {(ti, yi)}N i=1, where yi are assumed as noisy measurements of f (t) at typically regularly-spaced time instants ti (though GP regression framework allows for irregular sampling or missing data), i.e. yi = f (ti)+ǫi, where ǫi ∼ N (0, σ2 noise). In GP regression for mono channel audio signals, instead of estimating parameters η of ﬁxed-form functions f (t, η) : R 7→"
1608.04885,code,13,2022-05-13,0,in the synthesis of code which mimics proposed service behaviour replacing the real
1608.04885,code,13,2022-05-13,0,"– First line speciﬁes HTTP version, three-digit code and a text string"
1608.04885,code,14,2022-05-13,0,Listing A.2 contains the Java code for the symmetric ﬁeld identiﬁcation class which can
1608.04885,code,14,2022-05-13,0,indicates the operation code that the client requests to or to which the server
1608.04885,code,14,2022-05-13,0,"interact with distributed software systems, while it interacts with local code in lieu"
1608.04885,code,15,2022-05-13,0,The java source code of the implementation of modifying the centroid response is listed in
1608.04885,code,15,2022-05-13,0,the code of mock objects are typically coupled with the code of the software under
1608.04885,code,17,2022-05-13,0,for both identiﬁed symmetric ﬁelds are shown in Table 5.3. The java source code of the
1608.04885,code,2,2022-05-13,0,Java Code
1608.04885,code,27,2022-05-13,0,Listing A.3 contains the Java code for the ﬁeld substitution method that performs sym metric ﬁeld substitution to modify a recorded response for generating a response.
1608.04885,code,3,2022-05-13,0,A Java Code
1608.04885,code,4,2022-05-13,0,APPENDIX A. JAVA CODE
1608.04885,"code, code available",14,2022-05-13,0,environment with the code under investigation is only possible if the code itself is
1608.04885,"code, code package, code available",13,2022-05-13,0,Listing A.1 contains the Java code for describing the symmetric ﬁeld information.
1608.04885,data,1,2022-05-13,0,data
1608.04885,data,10,2022-05-13,0,Table 7.8: Response Accuracy for Clusters with Noisy Data
1608.04885,data,10,2022-05-13,0,deﬁnes temporal rules expressing data dependencies among exchanged messages.
1608.04885,data,10,2022-05-13,0,signiﬁcant diﬀerence between this form and ASN.1 on-the-wire data representation
1608.04885,data,11,2022-05-13,0,EncodingDecodingApplication data representationmessageApplication data representationmessagemessageCHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,11,2022-05-13,0,attach location data to tweets and discover tweets & locations.
1608.04885,data,11,2022-05-13,0,"hierarchical data structure, for instance, a tree structure."
1608.04885,data,12,2022-05-13,0,approach. This approach adopted clustering techniques and data mining techniques to
1608.04885,data,13,2022-05-13,0,A reordered dissimilarity matrix image indicates cluster tendency in the data by dark
1608.04885,data,13,2022-05-13,0,One of the most common transformation of network data is from the representation
1608.04885,data,13,2022-05-13,0,and management of data in distributed applications. The middleware service protocol is
1608.04885,data,13,2022-05-13,0,databases. In Proceedings of the 20th International Conference on Very Large Data
1608.04885,data,13,2022-05-13,0,"twitter data, rather than changing/updating data. Although the current client program"
1608.04885,data,13,2022-05-13,0,"– Compound data types can be constructed by nesting primitive types, shown"
1608.04885,data,13,2022-05-13,0,• Built on a client-server architecture and uses separate control and data connections
1608.04885,data,14,2022-05-13,0,3. A variable-length SMB Data contain 2-byte length ﬁeld that indicates the size
1608.04885,data,14,2022-05-13,0,Textual protocols are built around the notion of message ﬁelds encoded with text data
1608.04885,data,14,2022-05-13,0,and transmitting structured data over a network connection. It is used primarily to
1608.04885,data,14,2022-05-13,0,"application integration, data integration, message oriented middleware (MOM), object"
1608.04885,data,14,2022-05-13,0,binary and textual methods. The binary encoding method targets to a standardized data
1608.04885,data,14,2022-05-13,0,"command type identiﬁer, and value represents data for the command. The most"
1608.04885,data,14,2022-05-13,0,how primitive data types and compound data structure are encoded so they can be
1608.04885,data,14,2022-05-13,0,individual machines. The same issues of complex setup and data availability exist.
1608.04885,data,14,2022-05-13,0,primitive data types and compound data structures. Both X DR and ASN.1 specify
1608.04885,data,14,2022-05-13,0,"speciﬁc data representation, we need to design speciﬁc rules to obtain the embedded"
1608.04885,data,14,2022-05-13,0,the data it wants to transmit from representation it uses internally into a message
1608.04885,data,14,2022-05-13,0,"the oﬄine processing, utilising data mining techniques. It can dramatically reduce the"
1608.04885,data,14,2022-05-13,0,"transmit data between a server and web applications, serving as an alternative of"
1608.04885,data,14,2022-05-13,0,• Uses ASN.1 to deﬁne the data types used to build an SNMP message
1608.04885,data,15,2022-05-13,0,"In this chapter, we have utilised data mining techniques in a large enterprise software"
1608.04885,data,15,2022-05-13,0,"data representations, where ASN.1 and XDR are the most popular ones. For a"
1608.04885,data,15,2022-05-13,0,library into clusters of interactions of the same type. The data mining techniques were
1608.04885,data,15,2022-05-13,0,"schema, which is a speciﬁcation for what JSON data is required for a given"
1608.04885,data,15,2022-05-13,0,• Streaming: Give developers low latency access to Tweet data and other events have
1608.04885,data,16,2022-05-13,0,External Data Representation (XDR) [22] is the network format used to transfer
1608.04885,data,16,2022-05-13,0,JavaScript Object Notation (JSON) [21] is a text-based data interchange format.
1608.04885,data,16,2022-05-13,0,Most of Twitter API operations are used to provide the twitter data for 3rd party Twitter
1608.04885,data,16,2022-05-13,0,Network Data Management Protocol (NDMP) [14] is used to transport data between
1608.04885,data,16,2022-05-13,0,"data on that directory. Once the server received an unbind request, it must unbind"
1608.04885,data,16,2022-05-13,0,"data reorganization by a clustering technique. Operations Research, 20(5):993–1009,"
1608.04885,data,16,2022-05-13,0,"than the Whole Library approach, even though the latter uses all the available data points"
1608.04885,data,16,2022-05-13,0,"• XML deﬁnes a basic syntax for mixing markup with data text, but the designer"
1608.04885,data,17,2022-05-13,0,[82] M. K. Jiawei Han. Data Mining: Concepts and Techniques. Morgan Kaufmann
1608.04885,data,17,2022-05-13,0,"name in their request, followed by a payload, containing the data the service is expected"
1608.04885,data,17,2022-05-13,0,"visual cluster analysis. In Data Mining, 2008. ICDM’08. Eighth IEEE International"
1608.04885,data,18,2022-04-21,0,"[122] A. A. Sofokleous and A. S. Andreou. Automatic, Evolutionary Test Data Generation"
1608.04885,data,18,2022-05-13,0,and semantics of the data. The model step is the ﬁnal stage where we specify how virtual
1608.04885,data,18,2022-05-13,0,"encode data, which is intended or expected to be read by a machine rather than a human"
1608.04885,data,18,2022-05-13,0,"fast as a real service can. To answer this question, we propose and implement a data"
1608.04885,data,19,2022-05-13,0,"2For each LDAP operation, we demonstrate its raw data, as well as the textual representation, which"
1608.04885,data,19,2022-05-13,0,"Data Units (PDU)), exchange of interactions (via service primitives) with service users at"
1608.04885,data,19,2022-05-13,0,"following, we ﬁrst describe how we collect data of six case study protocols in Section. 7.3.1."
1608.04885,data,19,2022-05-13,0,"that can be transmitted over the network; that is, the data is encoded into a message."
1608.04885,data,19,2022-05-13,0,"– Represents each data item with a triple of the form <tag, length, value>,"
1608.04885,data,20,2022-05-13,0,"[62] S. Elbaum, G. Rothermel, S. Karre, and M. F. II. Leveraging user-session data"
1608.04885,data,20,2022-05-13,0,work in pattern matching sequences of data. One is the n − gram approach [72] and the
1608.04885,data,21,2022-05-13,0,VAT [35][148] is a technique that exists in data mining for the visual assessment of cluster
1608.04885,data,21,2022-05-13,0,approach (cf. Section 5.4.2 in Chapter 5) to data sets of all six case study protocols. Having
1608.04885,data,22,2022-05-13,0,"Given a trace library (cf. Deﬁnition 5 in Chapter 4.2), shown as Table 6.1, a data clustering"
1608.04885,data,22,2022-05-13,0,"[147] D. Yuan, Y. Yang, X. Liu, and J. Chen. A data placement strategy in scientiﬁc"
1608.04885,data,23,2022-05-13,0,"protocols, they can be further divided based on the multi-byte order and data representa tions, which are illustrated as follows:"
1608.04885,data,24,2022-05-13,0,7.8 Response Accuracy for Clusters with Noisy Data . . . . . . . . . . . . . . . 118
1608.04885,data,25,2022-05-13,0,Abstract Syntax Notation One (ASN.1) [20] is an standard that deﬁnes a rep resentation for data sent over a network.
1608.04885,data,26,2022-05-13,0,"[97] G. J. McLachlan, K.-A. Do, and C. Ambroise. Analyzing Microarray Gene Expres sion Data. Wiley-Interscience, 2004."
1608.04885,data,27,2022-05-13,0,3.2 Encoding and decoding application data . . . . . . . . . . . . . . . . . . . . 34
1608.04885,data,28,2022-05-13,0,"Most application-level protocols deﬁne message structures containing some form of oper ation or service name in their requests, followed by a payload on what data this service"
1608.04885,data,28,2022-04-21,0,"interaction models. Moreover, by utilizing data mining techniques, the eﬃciency of re sponse generation in the emulation environment has been greatly improved. However the"
1608.04885,data,29,2022-05-13,0,"integrated with many other systems for managing and interpreting data from many busi ness activities. The other systems (called services) include a legacy mainframe program,"
1608.04885,data,3,2022-05-13,0,of data.
1608.04885,data,30,2022-05-13,0,"Binary protocols rely on speciﬁc data structure; and hence, transmitted messages usu ally resort to ﬁxed-length ﬁelds or to a special notation to indicate the length of variable"
1608.04885,data,30,2022-05-13,0,"There are two popular network data representations (i.e. External Data Represen tation (XDR) and Abstract Syntax Notation One (ASN.1)), proposed to encode"
1608.04885,data,31,2022-05-13,0,"• XML syntax provides for a nested structure of tag/value pairs, which is equiv alent to a tree structure for the represented data. This is similar to XDR and"
1608.04885,data,34,2022-05-13,0,"[104] R. T. Ng and J. Han. Eﬃcient and eﬀective clustering methods for spatial data min ing. In Proceedings of the 20th International Conference on Very Large Data Bases,"
1608.04885,data,34,2022-05-13,0,"[140] L. Wang, X. Geng, J. Bezdek, C. Leckie, and R. Kotagiri. Enhanced visual anal ysis for cluster tendency assessment and data partitioning. Knowledge and Data"
1608.04885,data,35,2022-05-13,0,"[138] L. Wang, J. C. Bezdek, C. Leckie, and R. Kotagiri. Selective sampling for approxi mate clustering of very large data sets. International Journal of Intelligent Systems,"
1608.04885,data,43,2022-05-13,0,"as AgileLoad [1] and Selenium [10]. AgileLoad [39] features functions like auto matic modeling, real time data correlation, anomaly diagnostic and recommenda tions, enabling automatic identiﬁcation of performance bottlenecks in the systems."
1608.04885,data,5,2022-05-13,0,2. Network data representations
1608.04885,data,7,2022-05-13,0,• External Data Representation (XDR)
1608.04885,data,8,2022-05-13,0,3.2.2 Network Data Management Protocol (NDMP)
1608.04885,data,8,2022-05-13,0,Figure 3.2: Encoding and decoding application data
1608.04885,data,8,2022-05-13,0,data between diﬀerent kinds of computer systems.
1608.04885,data,83,2022-05-13,0,Protocol Taxonomy      NFS NDMP SNMP LDAP SMB HTTP FTP SMTP POP3 IRC JMS SOAP Message Format Textual   √ √ √ √ √ √ Binary √ √ √ √ √   √ Multi-byte Transmission Order Big-Endian √ √ √ √     Little-Endian   √    Data Representation XDR √ √      ASN.1  √ √     Stateful Stateful v4 √ √ √ √ √ √ √  Stateless v1-v3 √  √    Layer Low √ √ √ √ √ √ √ √ √ √  High      √ √ CHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,9,2022-04-21,0,data is transformed into messages sent and received.
1608.04885,data,9,2022-05-13,0,"– Unstructured body data follows, with speciﬁed size"
1608.04885,data,9,2022-05-13,0,• All requests are four characters followed by data
1608.04885,"data, data available",12,2022-05-13,0,require physical relocation and manual reconﬁguration. Real system data may not
1608.04885,"data, data https",14,2022-05-13,0,[22] XDR: External Data Representation Standard. 2006. http://tools.ietf.org/
1608.04885,"data, data https",17,2022-05-13,0,[14] NDMP: Network Data Management Protocol. Network Working Group. 1996. http:
1608.04885,"data, dataset",16,2022-05-13,0,"As shown in Figure 5.4, we randomly partitioned the original interactions’ data set into"
1608.04885,"data, dataset",16,2022-05-13,0,a statistical analysis will generalise to an independent data set. For the purpose of our
1608.04885,"data, dataset",16,2022-05-13,0,results of a statistical analysis will generalise to an independent data set. For the purpose
1608.04885,"data, dataset",18,2022-05-13,0,"similar search requests in our data set, some of them resulting in responses with zero or one"
1608.04885,database,12,2022-05-13,0,widely utilised in distributed database systems for the vertical partition of large
1608.04885,database,13,2022-05-13,0,of relationships between proposed services and a database. This information is used
1608.04885,database,15,2022-05-13,0,which is a database term for a speciﬁcation of how to interpret a collection of
1608.04885,database,16,2022-05-13,0,[130] M. Tamer ˝Ozsu and P. Valduriez. Principles of Distributed Database Systems.
1608.04885,dataset,13,2022-05-13,0,Our experimental results using the 6 message trace datasets demonstrate that our approach
1608.04885,dataset,13,2022-05-13,0,has two additional datasets: a dataset with textual representation converted from the
1608.04885,dataset,13,2022-05-13,0,• The datasets were obtained by randomly generating client requests for services of
1608.04885,dataset,14,2022-05-13,0,accuracy overall for the datasets tested. The combined approach achieves 100% accuracy
1608.04885,dataset,14,2022-05-13,0,datasets. The Accuracy Ratio column is calculated by dividing the number of valid
1608.04885,dataset,15,2022-05-13,0,client-to-server load. They require detailed knowledge of target protocols and suit able datasets.
1608.04885,dataset,15,2022-04-21,0,"represent the average times spent generating requests, for all the requests in the datasets"
1608.04885,dataset,15,2022-05-13,0,than those of our datasets. Further testing on real system interactions are warranted.
1608.04885,dataset,15,2022-05-13,0,• Our evaluation was performed on six datasets from four protocols. Given the great
1608.04885,dataset,16,2022-05-13,0,The impact of the entropy weightings can only be observed for the LDAP binary dataset.
1608.04885,dataset,18,2022-05-13,0,higher accuracy than f = 1. For the other datasets the threshold had no impact on the
1608.04885,dataset,18,2022-05-13,0,the LDAP (binary) dataset the thresholds of f = 0.5 and f = 0.8 produced signiﬁcantly
1608.04885,dataset,19,2022-05-13,0,(i.e. 0% noise) of interaction messages by operation type for the six datasets tested.
1608.04885,dataset,19,2022-05-13,0,"for four of the datasets, and 99.95% and 99.34% for the remaining two (LDAP binary"
1608.04885,dataset,20,2022-05-13,0,"binary dataset (denoted by LDAP text (1)), and another textual dataset that was used in"
1608.04885,dataset,21,2022-05-13,0,"accuracy stays above 97% for all datasets, when the noise ratio is 5%. As the noise ratio"
1608.04885,dataset,29,2022-05-13,0,"datasets, no impact from the weightings can be observed, as the consensus sequence pro totype by itself (Consensus Only) already produces 99-100% accuracy."
1608.04885,dataset,31,2022-05-13,0,"responses, indeed much faster than the real services being emulated. The response genera tion time is comparable to the Cluster Centroid approach, being faster for some datasets,"
1608.04885,dataset,35,2022-05-13,0,"from the trace library (for three datasets Consensus+Weighting is signiﬁcantly more ac curate, for two it has the same accuracy, for one it is slightly lower). The reason for the"
1608.04885,dataset,4,2022-05-13,0,the other datasets.
1608.04885,dataset,6,2022-05-13,0,7.5 Sample protocol message trace datasets
1608.04885,dataset,8,2022-05-13,0,Table 7.5: Sample protocol message trace datasets
1608.04885,"dataset, used dataset",17,2022-05-13,0,"We have used one message trace dataset for each of these protocols. In addition, LDAP"
1608.04885,github,9,2022-05-13,0,[9] Mockery. https://github.com/padraic/mockery.
1608.04885,open-source,13,2022-05-13,0,ActiveMQ is an open source message broker which fully implements the Java Message
1608.04885,open-source,22,2022-05-13,0,"[142] T. Wang, G. Yin, X. Li, and H. Wang. Labeled topic detection of open source"
1608.04885,package,12,2022-05-13,0,1 package com . ca . calabs . bilby . substitution ;
1608.04885,publicly available,13,2022-05-13,0,"However, the disadvantage is that the communication contract is not always publicly"
1608.04885,publicly available,15,2022-05-13,0,publicly available so that we can use this knowledge to deﬁne criteria for validation.
1608.04885,python,20,2022-05-13,0,"including Java, C#, Groovy, Perl, PHP, Python and Ruby so that the tests can"
1701.07853,code,6,2022-05-13,0,"Elisa Mussumecia, Fl´avio Code¸co Coelhoa"
1701.07853,data,10,2022-05-13,0,empirical data. The adjacency matrix A is given by
1701.07853,data,133,2022-05-13,0,"In this work we decided to look at the spread of news stories over the internet characterizing the resulting spread network and the dynamics of the spread. We start by looking at an actual case of news spread, and estimate the spread network by applying ideas of temporal networks and topic Modeling, connecting similar articles within the bounds of temporal window of inﬂuence. Then we postulate that the spread dynamics approximates an epidemic process and model it using a Network SIR model[3]. The spread of ideas as an epidemic process is not a new idea[4], but here we Propose new tools to estimate the spread network from data and compare it with simulated networks produced by an SIR epidemic model."
1701.07853,data,28,2022-05-13,0,"From the simulation (ﬁgure 11) we obtain the state matrix, which we use to compare the simulated infection distribution with the original data. Then"
1701.07853,data,4,2022-05-13,0,2.1. Data sources
1701.07853,"data, database",91,2022-05-13,0,"The data used for this study was obtained from the Media Cloud Brasil project (MCB) which collects news articles from thousands of sources in the Brazilian Internet since 2013. From the MCB database we obtained 2129 articles talking about the Charlie Hebdo terrorist attack in February 2015. The articles span from the day of the attack to the end of march of 2015. The data include the full text of the article, the URL of publication and the date and time of the publication."
1701.07853,dataset,52,2022-05-13,0,"Figure 10: Total number of articles infected between 0 < λ < 0.00005. The blue area is the area where the peak of the simulation is the same as the peak of the dataset distribution, threfore is the area where the λ values were tested for our simulation."
1701.07853,dataset,63,2022-05-13,0,"where NXY is the number of times an article from publisher X (the publisher of article i), has infected an article from publisher Y (the publisher of article j) and NY Is the total number of articles from publisher Y that have been infected, regardless of publisher. These counts are derived from the empirical dataset."
1701.07853,"dataset, database",51,2022-05-13,0,"The dataset used is the result of a very speciﬁc search on a news articles database, therefore we can expect to the articles to display a great similarity among themselves. Figure 4, shows the distribution of pairwise similarities that were used to construct the empirical inﬂuence network."
1803.06456,code,61,2022-05-13,0,"Here, the baselines are the same as the TE schema. However, we are not able to compare the PAN winner methods with our model as they have not published their code and many implementation details are left unknown to us in their reports to reimplement their methods. We call our second model PRNN in our reports."
1803.06456,data,140,2022-05-13,0,"methods as long as we keep the test and training sets the same as theirs. TE methods: We perform the Transformation Encoder (TE) on a problem P = (DS, DT ) that its documents are represented under one feature set with one feature value assignment to compute the transformation error. We then leverage the error rates taking from (at most) F = 7 feature sets (Section 3.2) of TE to form the ﬁnal TE feature vector (V ). Indeed, Each of the dimensions captures the transformation loss of one feature set. We apply the TE to both training and test data. Two well-known GNB and DT classiﬁers are used for veriﬁcation. We indicate them as TE+GNB and TE+DT respectively in our experiments."
1803.06456,data,176,2022-05-13,0,"where zs ∈ Rd is the reconstructed input and must be transformed into the target (zs ≈ xt). This can be done by setting TE’s objective function as the minimization of the transformation loss. We set the TE transformation loss Er to be the cross-entropy between reconstructed input (zs) and the target input (xt) as: Er(xt, zs) = − (cid:80)d i + (1 − xt Now, we assign our authorship veriﬁcation problem into the proposed Transformation Encoder. It is intuitively expected that TE shows diﬀerent manner when it transforms the source into the target while both having many features in common compared to the case where they have less common features. Here, the goal is to utilize TE for the AV problems that suﬀer from restricted labeled data. So, we put a document expansion method on top of TE as an initial step to overcome the restriction to some extent."
1803.06456,data,236,2022-05-13,0,"In this paper we deﬁne two diﬀerent schemas to study the AV problem. Under the ﬁrst schema we address the following challenges: 1- writing samples of available authors are quite limited during the training step as the length of the given text documents is short (a few hundred to a few thousand words) and size of the training set is so small (from 10 to 200 examples). So, it is quite hard to infer the same or diﬀerent-authorship status of given pairs. 2- The test and train documents are from diﬀerent genera and/or topics which makes the learning and prediction process much harder as the word distribution might diﬀer considerably. 3- No writing samples of the future authors is speciﬁed to us during the training and we may have seen no samples by the future authors at all. Under the second schema the scale of the training data is larger compared to the ﬁrst schema. However, we address the problem of identifying the diﬀerence in documents from identical domains in two ways: 1- authorship diversity in similar contents by utilizing Amazon reviews from 300 distinct authors. 2- Scientiﬁc documents from the same area of research by diﬀerent authors who have almost identical level of expertise in the ﬁeld. It also can be considered as an application of plagiarism detection."
1803.06456,data,4,2022-05-13,0,1 http://pan.webis.de/data.html
1803.06456,data,43,2022-05-13,0,"of the two classes decreases as the transformation encoder updates its weights in each epoch. However, there is a diﬀerence between the transformation loss of the positive and negative data in both diagrams. The loss of negative data is less"
1803.06456,data,44,2022-05-13,0,"Fig. 4: t-SNE plot of two folds of output of the fusion layer for PAN2015 in 5-fold CV. +: positive training data, ×: positive test data, ◦: negative training data, •: negative test data"
1803.06456,"data, dataset",188,2022-05-13,0,"Document Expansion for Small Scale Datasets Neural networks need suﬃcient amounts of data during their learning process to avoid the over-ﬁtting problem to produce the desired output. So, we propose a document expansion method to make use of the existing labeled training data of small scale datasets such as PAN to a great extent. A sliding window with the length of l sentences moves forward through each text document by one sentence per step making a smaller document each time. More speciﬁcally, a document with n sentences will be distributed into n − l + 1 smaller documents. New line characters, as well as empty sentences, are ignored here. So, using this expansion technique each problem P = (S, T ) in the small datasets will be converted to P = (DS, DT ) j}lT where DS = {ds j=1 are the set of all shorter documents after expansion of S and T (source and target). lS and lT denote the size of DS and DT respectively."
1803.06456,"data, dataset",330,2022-05-13,0,"However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy. The best accuracy belongs to the Amazon dataset where we have the largest dataset. It can be inferred that when the scale of the underlying dataset is large enough, the network learns the relation between the two language models of its given inputs well. It should be noted that for the two PAN2013 and PAN2014E even after CV the network cannot converge and the validation loss increases after each epoch. To avoid it we increase the total number of document pairs by splitting each document into two smaller ones with an equal number of sentences and making new pairs. This technique decreases the validation loss during training. However, it still suﬀers from lack of labeled examples and causes weakest results compared to the other larger datasets. To illustrate how PRNN discriminate writing styles we provide the t-SNE plot of the output of the fusion layer in a 5-fold CV classiﬁcation for two folds of PAN2015 (Figure 4). According to Figure 4, both classes have almost similar distribution in the test and training data. But, in some rare parts, the positive and negative points are close. They are probably the portion of the data that mislead the classiﬁer during the training step or will be misclassiﬁed in predictions."
1803.06456,"data, dataset",344,2022-05-13,0,"than the positive’s. In other words, it is easier to transform one document into the other while they are a diﬀerent-authorship pair (negative pair) compared to a same-authorship pair (positive pair). It makes the results of reconstruction loss to be counterintuitive. The reason is that we represent both documents of each problem under vector space model and only based on the vocabulary of the source document. So, the exclusive features of the target document, the features that only belong to the target but no to the source document, will be ﬁltered under this document representation model. Moreover, it is expected that the documents written by diﬀerent authors have fewer features in common and have more exclusive features than the ones written by the same author. This fact makes the target document of diﬀerent-authorship pair sparser than that of the same-authorship pair. And transforming the source document into a sparse document (its vector is sparse) makes less error than to a dense document (its vector is dense). This feature diﬀerentiates the positive and negative data and exists for both training and test sets and makes the transformation loss a distinctive feature for the veriﬁcation. A more direct way to classify the documents (instead of using classiﬁers) is to simply thresholding the reconstruction error. However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy."
1803.06456,"data, dataset",46,2022-05-13,0,"Fig. 3: Transformation loss for 50 epochs: averaged over all problems in the PAN2015 dataset. (A): training data (100 problems), (B): test data (500 problems), Feature set: unigram."
1803.06456,"data, dataset, dataset provided",344,2022-05-13,0,"For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes. The underlying feature set is unigram. The ﬁgures show that in both training and test sets the TE loss"
1803.06456,"data, dataset, dataset provided",349,2022-05-13,0,"TE schema The classiﬁcation results are compared in Table 3. The highest accuracy is indicated in bold and the second highest is underlined. For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes."
1803.06456,dataset,10,2022-05-13,0,Experimental results on evaluation datasets show that both methods achieve
1803.06456,dataset,153,2022-05-13,0,"Let P = (S, T ) denotes a pair of documents, indicating S as the source and T as the target. Here, the task is to investigate whether S and T are written by the same author. We map this problem into a binary classiﬁcation paradigm. Accordingly, if S and T are authored by the same person, P belongs to the positive class. Nevertheless (S and T have diﬀerent authors) P belongs to the negative class. In the ﬁrst step, we explain the Transformation Encoder which is a feature extraction-based method designed for the small-scale datasets with 200 labeled samples at most. However, many AV problems might have a larger scale with much more examples. So, we introduce the Parallel Recurrent Neural Network (PRNN) for large scale datasets in the second step."
1803.06456,dataset,167,2022-05-13,0,"We analyze authorship veriﬁcation on several datasets with binary structure. To our knowledge this amount of analysis has not been done in authorship veriﬁcation on diverse types of datasets. Two models are proposed. First, a Transformation Encoder (TE) to model error feature vectors for classiﬁcation inspired by the idea of autoencoders. TE is compatible with the AV problems with small-scale training sets. Giving a pair of input documents, TE transforms one input into the other. In this process, the transformation loss is observed as a reasonable measure of closeness of the two inputs to be used by a classiﬁer. The second model is a parallel recurrent neural network (PRNN) that is inspired by the popular similarity measures in Statistical Machine Learning (ML). Being based on language models, it is mostly applicable for relatively larger datasets. PRNN compares the proximity of the language model of its two input sequences"
1803.06456,dataset,2,2022-05-13,0,3.1 Dataset
1803.06456,dataset,2,2022-05-13,0,Dataset Train
1803.06456,dataset,222,2022-05-13,1,"Abstract. We propose two models for a special case of authorship veriﬁcation problem. The task is to investigate whether the two documents of a given pair are written by the same author. We consider the authorship veriﬁcation problem for both small and large scale datasets. The underlying small-scale problem has two main challenges: First, the authors of the documents are unknown to us because no previous writing samples are available. Second, the two documents are short (a few hundred to a few thousand words) and may diﬀer considerably in the genre and/or topic. To solve it we propose transformation encoder to transform one document of the pair into the other. This document transformation generates a loss which is used as a recognizable feature to verify if the authors of the pair are identical. For the large scale problem where various authors are engaged and more examples are available with larger length, a parallel recurrent neural network is proposed. It compares the language models of the two documents. We evaluate our methods on various types of datasets including Authorship Identiﬁcation datasets of PAN competition, Amazon reviews and machine learning articles. Experiments show that both methods achieve stable and competitive performance compared to the baselines."
1803.06456,dataset,234,2022-05-13,1,"case no writing samples of a questioned author are speciﬁed and they are unknown to us. No general solution has been oﬀered for the veriﬁcation problem under this assumption till 2014 [7]. Since then, a few works can be found in the literature: Koppel and Winter [7] propose an almost unsupervised method for the blog corpus dataset using “impostors” method. Optimized Classiﬁcation Trees, the winner method of PAN2014 Essays dataset, optimizes a decision tree based on various types of features and diﬀerent comparison methods including cosine similarity, correlation coeﬃcient and euclidean distance [8]. Multi-headed RNN is a character-level RNN and contains a common recurrent state among all authors with an independent softmax output per author [9]. Fuzzy C-Means clustering, the winner of the PAN2014 competition for novels dataset, adopts C-Means clustering and lexical features for the task [10]. Recently, an approach based on the compression models has been evaluated on PAN datasets [11]. Their method achieves promising results for the two years of PAN competitions but not for the other two datasets. Our methods is similar to these methods and considers the problems with the binary structure but we examine them on all PAN small-scale datasets as well as two large scale datasets."
1803.06456,dataset,260,2022-05-13,0,"for PAN datasets. All other parameters are selected based on pilot experiments. We report accuracy, the Area Under Receiver Operating Characteristic (ROC) curve [4] (AUC) and Score=AUC× Acc in TE experiments. The higher AUC and Score indicate more eﬀective classiﬁcation. PRNN schema The plain text of each document is used as the input of PRNN. The features sets for the baselines are the same as the TE baselines. However, we did not use the original training and test sets of the PAN datasets as the size of the training set is too small to be used for PRNN. To avoid overﬁtting problem we perform 5-fold Cross Validation (CV) for the PAN2015, Amazon and MPLA* where we have suﬃcient amount of examples in training folds. And for the PAN2013, PAN2014E and PAN2014N datasets that are relatively smaller we perform 10-fold CV to increase the size of the training folds. This setting is applied for PRNN as well as the baselines. We use Theano to implement PRNN. All classiﬁer’s parameters are the same as the TE schema. The back-propagation is done using stochastic gradient descent with learning rate=0.001, batch size=1, and dropout rate=0.2. We use the Glove pre-trained vectors5 as an initial value for the embedding vectors when there is a match. Otherwise, a random vector from a continuous uniform distribution over [0, 1) is used."
1803.06456,dataset,3,2022-05-13,0,Dataset Positive Negative
1803.06456,dataset,30,2022-05-13,0,Table 4: Classiﬁcation accuracy for PRNN schema using 5 and 10-fold CV across different datasets. The input for the baselines are empowered by the proposed similarity vector.
1803.06456,dataset,334,2022-05-13,0,"Here, the comparison methods are presented in three categories: baseline, PAN winners and our TE method. The details are provided as follows. Baseline: We connect several Machine Learning reliable classiﬁers widely used in the area with the seven similarity measures to set strong baselines (Table2). Since each example in our underlying dataset structure comprises two documents, we need to adapt it to the structure of an ordinary classiﬁer input by converting them to one single entity. A simple direct way is to concatenate their feature vectors. However, our experiments show it provides weak results mostly equal to the random label assignment. So, we deﬁne the summary vector as a single unit representative of each example/problem P = (DS, DT ) by utilizing several similarity measures. The summary vector comprises a class of several metrics each measures one aspect of the closeness of the two documents (DS and DT ) of the pair for all underlying feature sets. For any two feature vector documents x, y their summary vector is sum(x, y) = [simj i (x, y)1≤i≤M,1≤j≤F computes the ith similarity metric of M metrics in Table 2 under jth of F = 7 feature sets (Section 3.2) between x, y. Then, we use a classiﬁer including SVM, Gaussian Naive Bayes (GNB), K-Nearest Neighbor (KNN), Logistic Regression (LR), Decision Tree (DT) and Multi-Layer Perception (MLP) to predict the class label. PAN winners: We compare our method with the top methods of PAN AV competition between 2013 and 2015. The results of each method for one year of the competition are available and we report them here. So, our comparisons are not impacted by diﬀerent parameter setting and implementation details of these"
1803.06456,dataset,348,2022-05-13,1,"PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random. On the other hand, the negative candidate set is made of all unique and possible combinations of review pairs having diﬀerent authors. The negative class having equal size with the positive class is created by random selection from the negative candidate set."
1803.06456,dataset,348,2022-05-13,1,"for PAN2013), and literally the second document includes one piece of writing. Two documents of a pair might be from signiﬁcantly various genres and topics. The length of a document changes from a few hundred to a few thousand words. PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random."
1803.06456,dataset,5,2022-05-13,0,Table 1: Datasets information
1803.06456,dataset,86,2022-05-13,1,TE schema To evaluate the Transformation Encoder we use all available authorship identiﬁcation datasets released by PAN 1 (Table 1). Each PAN dataset consists of a training and test corpus and each corpus has a various number of distinct problems. One problem is a pair of two documents: the ﬁrst document of a problem composed of up to ﬁve writings (even as few as one) by a single person (implicitly disjoint For PAN2014 and PAN2015 and explicitly disjoint
1803.06456,dataset,88,2022-05-13,0,"PRNN is designed to solve the AV problem for relatively large scale datasets. The structure of the problem is the same as TE’s. We model a pair of documents using a simple parallel recurrent architecture. The overall model is shown in Figure 2. In general, PRNN consists of three components: two parallel columns of identical layers, one shared fusion layer and a SoftMax layer as the output. We proceed to describe the network in the following paragraphs."
1803.06456,dataset,93,2022-05-13,0,"to investigate their authorship. We also propose the summary vector to adapt our problem to a common binary classiﬁcation style to create strong baselines as there are limited studies in authorship veriﬁcation according to the literature. Applying this adaptation we are able to employ the recognized classiﬁers as well as similarity measures that are widely used in ML to build our baselines. Besides, the two pre-existing datasets, Amazon reviews and MPLA-400, are mapped to the binary structure to be used for our large scale AV problem."
1803.06456,"dataset, github",17,2022-05-13,0,2 https://github.com/dainis-boumber/MLP-400-datasets 3 we use scikit-learn software for all linguistic features 4 http://deeplearning.net/software/theano/
1803.06456,python,106,2022-05-13,0,"3. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. Journal of Machine Learning Research 12 (2011) 2825–2830 4. Egan, J.P.: Signal detection theory and {ROC} analysis. Academic Press (1975) 5. Japkowicz, N., Myers, C., Gluck, M., et al.: A novelty detection approach to"
1807.01857,code,147,2022-05-13,0,"Fig. 1 shows the schematic diagram of our proposed approach for IDE-based web search. Once the developer selects an exception from the Error Log or Console View of Eclipse IDE, our approach collects necessary information about it such as error message, stack trace and source code context. Then, it collects the results from three reliable search engines (e.g., Google, Bing and Yahoo) and one programming Q & A site (e.g., StackOverﬂow) through API endpoints against the error message and develops the corpus. The proposed approach then considers the context of the occurred error or exception, popularity and search engine recommendation of the collected results and calculates the proposed metrics to determine their acceptability and relevance with the target exception. Once the ﬁnal scores are calculated from those metrics, the results"
1807.01857,code,148,2022-05-13,0,"A sites, forums or discussion boards in their program directly or with minor modiﬁcations. Therefore, a result link containing source code snippet similar to the surrounding code block of the selected error or exception location is likely to discuss relevant issues that the developer needs. We consider three lines before and after the affected line in the source ﬁle as the source code context of the error or exception and extract the code snippets from result links though HTML scrapping. Then, we apply SimHash Algorithm on both code contexts and generate their SimHash values. We use equation (1) to determine Source Code Context Matching Score for each result link. The score values from zero to one and it indicates the relevance of the result link with the target error in terms of the context of source code."
1807.01857,code,252,2022-05-13,0,"To summarize, we propose a novel IDE-based web search solution that (1) exploits the search and ranking capabilities of three reliable search engines and a programming Q & A site through their API endpoints, (2) considers not only the content of the search (i.e., query keywords) but also the problem context such as stack trace and source code context, link popularity and link recommendation from the search engines, and (3) provides search result within the context of IDE with web browsing capabilities. We conduct an experiment with 25 runtime errors and exceptions related to Eclipse plugin development. Our approach recommended solutions with 96% accuracy which necessarily outperforms the traditional keyword-based search. In order to validate the results, we conduct a user study involving ﬁve prospective participants which gave a response agreement of 64.28%. Given that the relevance checking of a solution against the selected error is completely a subjective process, the preliminary results are promising. However, the proposed approach needs to be further validated with more errors and exceptions followed by an extensive user study to establish itself as a complete IDE-based web search solution. We also have plans to enable multiprocessing for the application and host it as a web service API so that others can readily use it with real time experience and also can use the API in their own IDEs rather than Eclipse."
1807.01857,code,47,2022-05-13,0,"[2] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer. Two studies of opportunistic programming: interleaving web foraging, learning, and writing code. In Proc. SIGCHI, pages 1589–1598, 2009."
1807.01857,code,66,2022-05-13,0,"Title Matching Score measures the content similarity between search query and result title. Stack Trace Matching Score and Source Code Context Matching Score determine the relevance of the result link based on its contextual similarity with that of the selected error or exception; therefore, they constitute the Context Relevance Score, Scxt. We get this score using equation (5)."
1807.01857,code,81,2022-05-13,0,"[5] M. Goldman and R. C. Miller. Codetrail: Connecting source code and web resources. J. Vis. Lang. Comput., 20(4):223–235, August 2009. [6] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns. What makes a good code example?: A study of programming Q & A in Stack Overﬂow. In Proc. ICSM, pages 25–34, 2012."
1807.01857,code,86,2022-05-13,0,"4) Source Code Context Matching Score (Scc): Sometimes, stack trace may not be enough for problem ﬁxation and developers post related source code in forums and discussion boards for clariﬁcation. We are interested to check if the source code contexts of the discussed errors or exceptions in the result links are similar to that of the selected exception from IDE. The code contextual similarity is possible; because, the developers often reuse code snippets from programming Q &"
1807.01857,code,98,2022-05-13,0,"In our experiment, we select 25 runtime errors and exceptions related to Eclipse plug-in development, and collect associated information such as error or exception messages, stack traces and source code context. We then use those information (e.g., error content and context) to search for solution using our approach. We also perform extensive web search manually with different available search engines and ﬁnd out the solutions for all errors and exceptions. We should note that we choose the most appropriate solution as the accepted one for each exception or error"
1807.01857,"code, package",187,2022-05-13,0,"In this paper, we propose an Eclipse IDE-based search solution called SurfClipse to the encountered errors or exceptions which addresses the concerns identiﬁed in case of existing approaches. We package the solution as an Eclipse plug-in which (1) exploits the search and ranking algorithms of three reliable web search engines (e.g., Google, Bing and Yahoo) and a programming Q & A site (e.g., StackOverﬂow) through their API endpoints, (2) provides a content (e.g., error message), context (e.g., stack trace and surrounding source code of the subject error), popularity and search engine recommendation (of result links) based ﬁltration and ranking on the extracted results of step one, (3) facilitates the most recent solutions, accesses the complete and extensible solution set and pulls solutions from numerous forums, discussion boards, blogs, programming Q & A sites and so on, and (4) provides a real web surﬁng experiences within the IDE context using Java based browser."
1807.01857,data,102,2022-05-13,0,"discussion boards and Q & A sites with the help of search enginies, but also ensures the access to the most recent content of StackOverﬂow through API access. However, the existing approaches by Cordeiro et al. [4] and Ponzanelli et al. [7] provide results from a single and ﬁxed sized data dump of StackOverﬂow and therefore, the results do not contain the most recent posts (i.e., discussing the most recent errors or exceptions) from StackOverﬂow as well as the promising solutions from other programming Q & A sites."
1807.01857,data,103,2022-05-13,0,"8) Search Trafﬁc Rank Score (Sstr): The amount of search trafﬁc to a site can be considered as an important indicator of its popularity. In this research, we consider the relative popularity of the result links found in the corpus. We use the statistical data from two popular site trafﬁc control companiesAlexa and Compete through their provided APIs and get the average ranking for each result link. Then, based on their ranks, we provide a normalized Search Trafﬁc Rank Score between zero and one considering minimum and maximum search trafﬁc ranks found."
1807.01857,data,216,2022-05-13,0,"Abstract—Traditional web search forces the developers to leave their working environments and look for solutions in the web browsers. It often does not consider the context of their programming problems. The context-switching between the web browser and the working environment is time-consuming and distracting, and the keyword-based traditional search often does not help much in problem solving. In this paper, we propose an Eclipse IDE-based web search solution that collects the data from three web search APIs– Google, Yahoo, Bing and a programming Q & A site– StackOverﬂow. It then provides search results within IDE taking not only the content of the selected error into account but also the problem context, popularity and search engine recommendation of the result links. Experiments with 25 runtime errors and exceptions show that the proposed approach outperforms the keyword-based search approaches with a recommendation accuracy of 96%. We also validate the results with a user study involving ﬁve prospective participants where we get a result agreement of 64.28%. While the preliminary results are promising, the approach needs to be further validated with more errors and exceptions followed by a user study with more participants to establish itself as a complete IDE-based web search solution."
1807.01857,data,282,2022-05-13,0,"Traditional web search forces the developers to leave the working environment (i.e., IDE) and look for the solution in the web browsers. In contrast, if the developer chooses SurfClipse, it allows to check the search results within the context of IDE (e.g., Fig. 1-(b)). Once she selects an error message using context menu option (e.g., Fig. 1-(a)), the plugin pulls results from three reliable search engines and one programming Q & A site against that error message. Then, it calculates the proposed metrics for each result related to the error content, error context, popularity and search engine recommendation to determine its relevance with the occurred error or exception, and then sorts and displays the results. Moreover, the plug-in allows the developer to browse the solution on a Java-based web browser (e.g., Fig. 1-(c)) without leaving the context of the IDE which makes it time-efﬁcient and ﬂexible to use. The plug-in by Cordeiro et al. [4] also shows the results within the context of the IDE; however, (1) the result set is limited (i.e., only from StackOverﬂow and does not consider the whole web), (2) cannot address newly introduced issues (i.e., ﬁxed corpus and subjected to the availability of StackOverﬂow data dump), (3) only considers stack trace information as problem context, and (4) the developer cannot enjoy the web browsing experience."
1807.01857,data,318,2022-05-13,0,"1) Search Engine Weight Based Score (Ssew): According to Alexa1, one of the widely recognized web trafﬁc data providers, Google ranks second, Yahoo ranks fourth and Bing ranks sixteenth among all websites in the web this year. While these ranks indicate their popularity (e.g., site trafﬁc) and reliability (i.e., users’ trust) as information service providers, it is reasonable to think that search results from different search engines of different ranks have different levels of acceptance. We conduct an experiment with 75 programming task and exception related queries2 against those search engines and a programming Q & A site (e.g., StackOverﬂow) to determine the relative weights or acceptance. We collect the top 15 search results for each query from each search tool and get their Alexa ranks. Then, we consider the Alexa ranks of all result links provided by each search tool and calculate the average rank for a result link provided by them. The average rank for each search tool is then normalized and inversed which provides a value between 0 and 1. We get a normalized weight of 0.41 for Google, 0.30 for Bing, 0.29 for Yahoo and 1.00 for StackOverﬂow. The idea is that if a result link against a single query is found in all three search engines, it gets the search engine scores (i.e., conﬁdence) from all three of them which sum to 1.00. StackOverﬂow has drawn the attention of a vast community (1.7 million3) of programmers and software professionals, and it also has a far better average Alexa rank than that of the search engines; therefore, the results returned from StackOverﬂow are provided a search engine score (i.e., conﬁdence) of 1.00."
1807.01857,"data, code",117,2022-05-13,0,"Existing studies related to our research focus on integrating commercial-off-the-shelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying within IDE environment [4, 7], embedding web browser inside the IDE [3] for code example recommendation and so on. In this paper, we propose a novel approach that exploits result data from the state of art web search APIs and provides ﬁltered and ranked search results taking problem content, context, result link’s popularity and search engine recommendation about the result links into consideration. Our proposed approach not only collects solution posts from a large set of forums,"
1807.01857,"data, code, dataset provided",293,2022-05-13,0,"for the solution in the web browsers. The context-switching between IDE and the web browser is distracting and timeconsuming. Moreover, checking relevance from hundreds of search results is a cognitive burden on the novice developers. Existing studies focus on integrating commercial-off-theshelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying them within the IDE environment [4], embedding web browser inside the IDE [3] and so on. Cordeiro et al. [4] propose an IDE-based recommendation system for runtime exceptions. They extract the question and answer posts from StackOverﬂow data dump and suggest posts relevant to the occurred exceptions considering the context from the stack trace information generated by the IDE. They also suggest a nice solution to the context-switching issue through visualization of the solution within the IDE. However, the proposed approach suffers from several limitations. First, they consider only one source (e.g., StackOverﬂow Q & A site) rather than the whole web for information and thus, their search scope is limited. Second, the developed corpus cannot be easily updated and is subjected to the availability of the data dump. For example, they use the StackOverﬂow data dump of September 2011, that means it cannot provide help or suggestions to the recently introduced software bugs or errors after September 2011. Third, the visualization of the solutions is not efﬁcient as it uses plain text to show the post contents such as source code, stack trace and discussion. Thus the developers do not really experience the style and presentation of a web page."
1807.04488,code,115,2022-04-21,0,"Baseline Query Selection: We select the title of a bug report as the baseline query for our experiments, as was also selected by earlier studies [21, 28, 49]. However, we discard such queries that (i.e., in verbatim titles) already return their ﬁrst correct results within the Top-10 positions, they possibly do not need query reformulation [21]. Finally, we ended up with a collection of 1,675 baseline queries. We perform the same preprocessing steps as were done on the source documents (Section II-C), on the queries before using them for code search in our experiments."
1807.04488,code,117,2022-04-21,0,"Answering RQ3–Do Document Structures Matter? While most of the earlier reformulation techniques miss or ignore the structural aspect of a source document, we consider such aspect as an important paradigm of our technique. We consider a source document as a collection of structured entities (e.g., signatures, methods, ﬁelds) [38] rather than a regular text document. Thus, we make use of method signatures and ﬁeld signatures rather than the whole source code for query reformulation given that they are likely to contain more salient terms and less noise [23]. Fig. 4 demonstrates how incorpora Method signature Field signature Both signatures Both signatures"
1807.04488,code,13,2022-05-13,0,"Code Elements in Informal Documentation. ICSE, pages 832–841, 2013."
1807.04488,code,132,2022-05-13,0,"earlier [32], we apply a heuristic threshold of 0.0001 for the convergence checking. The algorithm captures importance of a source term not only by estimating its local impact but also by considering its global inﬂuence over other terms. For example, the term, “Classpath”, Fig. 1, occurs in multiple structured tokens (Listing 1), complements the semantics of ﬁve other terms, and thus is highly important within the term graph (i.e., Fig. 1). Once the iterative computation is over, each of the terms from the graph is found with a numeric score. We consider these scores as the relative weight or importance of the corresponding terms from the source code."
1807.04488,code,146,2022-04-21,0,"To summarize, we propose a novel technique–ACER–for improved query reformulation for concept location. It takes an initial query as input, identiﬁes appropriate search terms from the source code using a novel term weight, and then suggests the best reformulation to the initial query using document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight systems report that our technique can improve 71% of the baseline queries and preserve 26% of them, which are highly promising. Comparison with ﬁve closely related approaches including the state-of-the-art not only validates our empirical ﬁndings but also demonstrates the high potential of our technique. In future, we plan to apply our term weighting method, CodeRank, to other SE text retrieval tasks involving source code such as bug localization and traceability recovery."
1807.04488,code,146,2022-04-21,0,"source subject systems show that our technique can improve 71% (and preserve 26%) of the baseline queries which are highly promising according to relevant literature [13, 21, 34]. Our suggested queries return correct results for 64% of the queries in the Top-100 results. Our ﬁndings report that CodeRank is a more effective term weighting method than the traditional methods (e.g., TF, TF-IDF) for search query reformulation in the context of source code. Our ﬁndings also suggest is an important paradigm for both term weighting and query reformulation. Comparison with ﬁve closely related existing approaches [13, 21, 23, 43, 49] not only validates our empirical ﬁndings but also demonstrates the superiority of our technique. Thus, the paper makes the following contributions:"
1807.04488,code,149,2022-05-13,0,"In this paper, we propose a novel technique–ACER–for automatic query reformulation for concept location in the context of software change tasks. We (1) ﬁrst introduce a novel graph-based term weight –CodeRank– for identifying important terms from the source code, and then (2) apply that term weight and source document structures (e.g., method signatures) to our technique for automatic query reformulation. CodeRank identiﬁes important terms not only by analyzing salient structured entities (e.g., camel case tokens), but also by exploiting the co-occurrences among the terms across various entities. Our technique–ACER–accepts a natural language query as input, develops multiple candidate queries from two different important contexts, (1) method signatures and (2) ﬁeld signatures of the source documents independently using CodeRank, and then suggests the best reformulation ( based"
1807.04488,code,163,2022-05-13,0,"[14] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker. Mining Source Code to Automatically Split Identiﬁers for Software Analysis. In Proc. MSR, pages 71–80, 2009. [15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The Vocabulary Problem in Human-system Communication. Commun. ACM, 30(11):964–971, 1987. [16] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the Use of Relevance Feedback in IR-based Concept Location. In Proc. ICSM, pages 351–360, 2009. [17] S. Haiduc and A. Marcus. On the Use of Domain Terms in Source Code. In Proc. ICPC, pages 113–122, 2008. [18] S. Haiduc and A. Marcus. On the Effect of the Query In Proc. ICPC, pages"
1807.04488,code,167,2022-05-13,0,"CodeRank: PageRank [10] is one of the most popular algorithms for web link analysis which was later adapted by Mihalcea and Tarau [32] for text documents as TextRank. In this research, we adapt our term weighting method from TextRank [9, 32, 41] for source code, and we call it CodeRank. To date, only traditional term weights (e.g., TF, TFIDF [21, 43, 49]) are applied to source code which were originally proposed for regular texts [26] and are mostly based on isolated frequencies. On the contrary, CodeRank not only analyzes the connectivity (i.e., incoming links and outgoing links) of each source term, but also the relative weight of the connected terms from the graph recursively, and calculates the term weight, S(Vi), as follows (Step 6, Fig. 2):"
1807.04488,code,168,2022-05-13,0,"There exist a number of studies in the literature that reformulate a given query for concept location in the context of software change tasks. Existing studies apply relevance feedback from developers [16], pseudo-relevance feedback from IR tools [21], partial phrasal matching [23, 44], and machine learning [21, 34] to query reformulation. They also make use of context of query terms from source code [25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Hill et al. [23] consider the presence of query terms in the method or ﬁeld signatures as an indicator of their relevance, and suggest natural language phrases from them as reformulated queries. Sisman and Kak [49] choose such terms for query reformulation that frequently co-occur with query terms within"
1807.04488,code,17,2022-05-13,0,Fig. 1. An example term graph generated by CodeRank for source code of Listing 1
1807.04488,code,17,2022-05-13,0,Listing 1. Source code used for automatic query reformulation (abridged from [3])
1807.04488,code,179,2022-05-13,0,"In order to suggest meaningful reformulations to an initial query, feedback on the query is required. Gay et al. [16] ﬁrst reformulate queries based on explicit feedback from the developers. Although such feedback could be useful, gathering them is often time-consuming and sometimes infeasible. Hence, a number of recent studies [13, 21, 40, 41] apply pseudorelevance feedback as a feasible alternative. The top ranked results returned by the code search tool for an initial query are considered as the pseudo-relevance feedback for the query. We ﬁrst reﬁne an initial query by removing the punctuation marks, numbers, special symbols and stop words (Step 1, Fig. 2). Then we collect the Top-K (i.e., K = 10, best performing heuristic according to our experiments) search results returned by the query, and use them as the source for our candidate terms for query reformulation (Steps 2, 3, Fig. 2)."
1807.04488,code,185,2022-04-21,0,"Abstract—During software maintenance, developers usually deal with a signiﬁcant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique –ACER– that takes an initial query, identiﬁes appropriate search terms from the source code using a novel term weight –CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with ﬁve closely related existing techniques in query reformulation not only validates our empirical ﬁndings but also demonstrates the superiority of our technique."
1807.04488,code,190,2022-05-13,0,"Stop word and Keyword Removal: Since our structured tokens comprise of natural language terms, we discard stop words from them as a common practice (Step 4, Fig. 2). We use a standard list [6] hosted by Google for stop word removal. Programming keywords can often be considered as the equivalence of stop words in the source code which are also discarded from our analysis. Since we deal with Java source code, the keywords of Java are considered for this step. As suggested by earlier study [21], we also discard insigniﬁcant source terms (i.e., having word length< 3) from our analysis. Stemming: It extracts the root (e.g., “send”) out of a word (e.g., “sending”). Although existing studies suggest contradictory [28, 45] or conﬂicting [24] evidences for stemming with the source code, we investigate the impact of stemming with RQ4 where Snowball stemmer [24, 37] is used for stemming."
1807.04488,code,219,2022-05-13,0,"Thus, to answer RQ1, the reformulation of ACER improves the baseline queries signiﬁcantly both in terms of query effectiveness and retrieval performance. ACER improves 71% of the baseline queries with 64% Top-100 retrieval accuracy. Answering RQ2–CodeRank vs. Traditional Term Weighting Methods: Table VII shows the comparative analysis between CodeRank and two traditional term weights– TF and TF-IDF– which are widely used in the text retrieval contexts [13, 28, 43]. While TF estimates the importance of a term based on its occurrences within a document, TF-IDF additionally captures the global occurrences of the term across all the documents of the corpus [26]. On the contrary, CodeRank employs a graph-based scoring mechanism that determines the importance of a term based on its co-occurrences with other important terms within a certain context. From Table VII, we see that CodeRank performs signiﬁcantly better than both TF (i.e., paired t-test, p-value=0.005<0.05) and TF-IDF (i.e., p-value<0.001) in identifying important search terms from source code, especially from the method signatures. Considering the whole source code rather than signatures improves the performance of both TF (i.e., 56% query improvement) and"
1807.04488,code,228,2022-05-13,0,"Fig. 3 shows how CodeRank and traditional term weights perform in reformulating the baseline queries with their (a) Top-10 and (b) Top-30 terms. We see that TF reaches its peak performance pretty quickly (i.e., K = 3), and then shows a stationary or irregular behaviour. That means, TF identiﬁes frequent terms for query reformulation, and few of them (e.g., Top-3) could be highly effective. On the contrary, our method– CodeRank– demonstrates a gradual improvement in the performance up to Top-12 terms (i.e., K=12, Fig. 3-(b)), and crosses the performance peak of TF with a large margin (i.e., paired t-test, p-value=0.004<0.05, Cohen’s D=3.77>1.00 (large)), for K=10 to K=15). CodeRank emphasizes on the votes from other important terms (i.e., by leveraging co-occurrences) for determining weight of a term, and as demonstrated in Fig. 3, this weight is found to be more reliable than TF. TF-IDF is found relatively less effective according to our investigation. Thus, to answer RQ2, CodeRank performs signiﬁcantly better than traditional methods in identifying effective terms for query reformulation from the source code."
1807.04488,code,23,2022-04-21,0,• RQ3: Does employment of document structure improve ACER’s suggestion on good quality search terms from the source code?
1807.04488,code,231,2022-05-13,0,"Candidate Token Mining: Developers often express their intent behind the code and encode domain related concepts in the identiﬁer names and comments [17]. However, code comments are often inadequate or outdated [51]. All identiﬁer types also do not have the same level of importance. For example, while the signature of a method encodes the high level its body focuses on granular level implementation details and thus possibly contains more noisy terms [23]. In fact, Hill et al. [23] ﬁrst analyze method signatures and ﬁeld signatures to suggest natural language phrases as queries for code search. In the same vein, we thus also consider method signatures (msig) and ﬁeld signatures (f sig) as the source for our candidate reformulation terms. We extract structured identiﬁer names from these signatures using appropriate regular expressions [42] (Step 4, Fig. 2). Since different contexts of a source document might convey different types or levels of semantics (i.e., developers’ intent), we develop a separate candidate token set (CTsig) for each of the two signature types (sig ∈ {msig, f sig}) from the feedback documents (∀d ∈ DRF ) as follows: CTsig ="
1807.04488,code,248,2022-05-13,0,"Candidate Reformulation Selection: Algorithms 1 and 2 show the pseudo-code of our query reformulation technique– ACER–for concept location. We ﬁrst collect pseudo-relevance feedback for the initially provided query (Q) where Top-K source documents are returned (Lines 3–5, Algorithm 1). Then we collect method signatures and ﬁeld signatures from each of the documents (∀d ∈ DRF ), and extract structured tokens from them. We prepare three token sets–CTmsig, CTf sig and CTcomb from these signatures (Lines 6–12, Algorithm 1, Step 4, Fig. 2) where CTcomb combines tokens from both signatures. Then we perform limited natural language preprocessing on each token set where Samurai algorithm [14] is used for token splitting. We develop separate term graph for each of these token sets where individual terms are represented as vertices, and term co-occurrences are encoded as connecting edges (Lines 3–7, Algorithm 2, Step 5, Fig. 2). We apply CodeRank term weighting to each of the graphs which provides a ranked list of terms based on their relative importance. Then we select Top-K (e.g., K = 10) important terms from each of the three graphs, and prepare three reformulation candidates (Lines 8– 12, Algorithm 2, Steps 6, 7, 8, Fig. 2). Algorithm 1 ACER: Proposed Query Reformulation"
1807.04488,code,26,2022-05-13,0,[3] Example code snippet. URL https://goo.gl/WSZHiC. [4] Samurai preﬁx and sufﬁx list. URL https://hiper.cis.udel.
1807.04488,code,263,2022-05-13,0,"Once candidate tokens are extracted from method signatures and ﬁeld signatures, and are splitted into candidate terms, we develop source term graphs (e.g., Fig. 1) from them (Step 5, Fig. 2). Developers often encode their intent behind the code and domain vocabulary into the carefully crafted identiﬁer names where multiple terms are concatenated. For example, the method name–getChatRoomBots–looks like a natural language phrase–“get chat room bots”–when splitted properly. Please note that each of these three terms–“chat”, “room” and “bots”– co-occur with each other to convey an important concept– a robotic technology, and thus, they are semantically connected. On the other hand, the remaining term–“get”– cooccurs with them due to a temporal relationship (i.e., develops a verbal phrase). Similar phrasal representations (reﬁned with lexical matching) were directly returned by Hill et al. for query reformulation. However, their approach could be limited due to the added constraint (e.g., warrants query terms in signatures). We thus perform further analysis on such phrases, and exploit the co-occurrences among the terms for our graph based term weighting. In particular, we encode the term co-occurrences into connecting edges (E) in the term graph (G(V, E)) where the individual terms (Vi) are denoted as vertices (V )."
1807.04488,code,281,2022-05-13,0,"Studies show that about 80% of the total efforts is spent in software maintenance [36] where developers deal with a signiﬁcant number of software issues [35, 45, 52]. Software issue reports (a.k.a., change requests) discuss both unexpected (or erroneous features such as bugs) and expected but nonexistent features (e.g., new functionality). For both bug resolution and new feature implementation, a developer is required to map the concepts discussed in the issue report to appropriate source code within the project which is widely known as concept location [29, 31, 40]. Developers generally choose one or more important keywords from the report texts, and then use a search method (e.g., regular expression) to locate the source code entities (e.g., classes, methods) that need to be changed. Unfortunately, as the existing studies [28, 30] report, developers regardless of their experience perform poorly in choosing appropriate search terms for software change tasks. According to Kevic and Fritz [28], only 12.20% of the search terms chosen by the developers were able to locate relevant source code entities for the change tasks. Furnas et al. [15] also suggest that there is a little chance (i.e., 10%–15%) that developers guess the exact words used in the source code. One way to assist the developers in this regard is to automatically suggest helpful reformulations (e.g., complementary keywords) to their initially chosen queries."
1807.04488,code,29,2022-05-13,0,"• A novel term weighting method –CodeRank– for source code that identiﬁes the most important terms from a given code entity (e.g., class, method)."
1807.04488,code,29,2022-05-13,0,"• RQ2: Does CodeRank perform better than traditional term weighting methods (e.g., TF, TF-IDF) in identifying effective search terms from the source code?"
1807.04488,code,3,2022-04-21,0,Preprocessing Code search
1807.04488,code,31,2022-05-13,0,"[49] B. Sisman and A. C. Kak. Assisting Code Search with Automatic Query Reformulation for Bug Localization. In Proc. MSR, pages 309–318, 2013."
1807.04488,code,311,2022-05-13,0,"Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]. Thus, they might also fail to identify the appropriate terms from the structured source documents for query reformulation."
1807.04488,code,320,2022-05-13,0,"We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig. 5 shows how (a) Top-10 and (b) Top-30 reformulation terms improve the baseline queries. We see that"
1807.04488,code,324,2022-05-13,0,"Table I shows an example change request [2] submitted for eclipse.jdt.debug system, and it refers to “debugger source lookup” issue of Eclipse IDE. Let us assume that the developer chooses important keywords from the request title, and formulates a generic initial query–“debugger source lookup.” Unfortunately, the query does not perform well, and the 79th position of the returns the ﬁrst correct result at result list. Further extension–“debugger source lookup work variables”–also does not help, and returns the result at the 77th position. The existing technique – RSV [13]– extends the query as follows–“debugger source lookup work variables launch conﬁguration jdt java debug”–where the new terms are collected from the project source using TF-IDF based term weight. This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig."
1807.04488,code,328,2022-05-13,0,"This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig. 1) based on term co-occurrences from each signature type, apply CodeRank term weighting, and extract multiple candidate reformulations with the highly weighted terms (e.g., orange coloured, Fig. 1). Then we analyze the quality of the candidates using their quality measures [19], apply machine learning, and suggest the best reformulation to the initial query. Thus, our technique (1) ﬁrst captures salient terms from the source documents by analyzing their structural aspects (i.e., unlike bag of words approaches [46]) and an appropriate term weight–CodeRank, and (2) then suggests the best query reformulation using document structures (i.e., multiple candidates derived from various signatures), query quality analysis and machine learning [19]. Experiments using 1,675 baseline queries from eight open"
1807.04488,code,33,2022-05-13,0,"[38] M. M. Rahman and C. K. Roy. On the Use of Context in Recommending Exception Handling Code Examples. In Proc. SCAM, pages 285–294, 2014."
1807.04488,code,335,2022-05-13,0,"* = Statistically signiﬁcant difference between two measures from the same signature, MRD = Mean Rank Difference between ACER and baseline queries tion of document structures into a technique could be useful for query reformulations. We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig."
1807.04488,code,342,2022-05-13,0,"reformulation tasks. They also make use of context of query terms from source code [23, 25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]."
1807.04488,code,35,2022-04-21,0,"Fig. 2 shows the schematic diagram of our proposed technique–ACER–for automatic query reformulation. We use a novel graph-based metric of term importance–CodeRank– for source code, and apply source document structures, query"
1807.04488,code,36,2022-05-13,0,"[23] E. Hill, L. Pollock, and K. Vijay-Shanker. Automatically Capturing Source Code Context of NL-queries for Software Maintenance and Reuse. In Proc. ICSE, pages 232–242, 2009."
1807.04488,code,37,2022-04-21,0,"[25] M. J. Howard, S. Gupta, L. Pollock, and K. VijayShanker. Automatically Mining Software-based, Semantically-Similar Words from Comment-Code Mappings. In Proc. MSR, pages 377–386, 2013."
1807.04488,code,38,2022-05-13,0,"[31] A. Marcus, A. Sergeyev, V. Rajlich, and J.I. Maletic. An Information Retrieval Approach to Concept Location in Source Code. In Proc. WCRE, pages 214–223, 2004."
1807.04488,code,4,2022-05-13,0,C. Source Code Preprocessing
1807.04488,code,4,2022-05-13,0,URL https://code.google.com/p/
1807.04488,code,49,2022-05-13,0,"[22] S. Haiduc, G. De Rosa, G. Bavota, R. Oliveto, A. De Lucia, and A. Marcus. Query Quality Prediction and the Refoqus Reformulation for Source Code Search: Tool. In Proc. ICSE, pages 1307–1310, 2013."
1807.04488,code,61,2022-04-21,0,"[43] J.J. Rocchio. The SMART Retrieval System—Experiments in Automatic Document Processing. Prentice-Hall, Inc. [44] M. Roldan-Vega, G. Mallet, E. Hill, and J. A. Fails. CONQUER: A Tool for NL-based Query Reﬁnement and In Proc. ICSM, Contextualizing Code Search Results. pages 512–515, 2013."
1807.04488,code,7,2022-05-13,0,B. Corpus Indexing & Source Code Search
1807.04488,code,7,2022-05-13,0,Tasks to Source Code. 2014.
1807.04488,code,7,2022-04-21,0,that structure of a source code document
1807.04488,code,77,2022-05-13,0,"[51] C. Vassallo, S. Panichella, M. Di Penta, and G. Canfora. CODES: Mining Source Code Descriptions from Developers Discussions. In Proc. ICPC, pages 106–109, 2014. [52] I. Vessey. Expertise in Debugging Computer Programs: An Analysis of the Content of Verbal Protocols. TSMC, 16(5):621–637, 1986. [53] J. Yang and L. Tan."
1807.04488,code,87,2022-05-13,0,"Here sig(d) extracts all tokens from method signatures or ﬁeld signatures, and structured(t) determines whether the token t ∈ Tsig is structured or not. Although we deal with Java source code in this research where the developers generally use camel case tokens (e.g., MessageType) or occasionally might use same case tokens (e.g., DECIMALTYPE), our approach can be easily replicated for snake case tokens (e.g., reverse traversal) as well."
1807.04488,data,130,2022-05-13,0,"Threats to internal validity relate to experimental errors and biases [55]. Although CodeRank and document structures play a major role, the data resampling step (Section II-F, Step 9, Fig. 2) has a signiﬁcant role behind the high performance of our technique. Unfortunately, to the best of our knowledge, Refoqus [21] does not have such a step. Thus, the performance comparison might look like a bit unfair. Besides, models based on data resampling are sometimes criticized for intrinsic biases [5]. However, we apply data resampling to Refoqus as well (i.e., Refoqussampled), and demonstrate that our technique still performs better in terms of worsening ratio."
1807.04488,data,19,2022-05-13,0,"Index Terms—Query reformulation, CodeRank, term weight ing, query quality analysis, concept location, data resampling"
1807.04488,data,328,2022-05-13,0,"Haiduc et al. [19] suggest that quality of a query with respect to the corpus could be determined using four of its statistical properties– speciﬁcity, coherency, similarity and term relatedness–that comprise of 21 metrics [11]. They apply machine learning on these properties, and separate high quality queries from low quality ones. We thus also similarly apply machine learning on our reformulation candidates (and their metrics), and develop classiﬁer model(s) where Classiﬁcation And Regression Tree (CART) is used as the learning algorithm [19]. Since only the best of the four reformulation candidates (i.e., including baseline) is of our interest, the training data was inherently skewed. We thus perform bootstrapping (i.e., random resampling) [27, 50] on the data multiple times (e.g., 50) with 100% sample size and replacement (Step 9, Fig. 2), train multiple models using the sampled data, and then record their output predictions. Then, we average all the predictions for each test instance from all models, and determine their average probability of being the best candidate reformulation. Thus, we identify the best of the four candidates using our models, and suggest the best reformulation to the initial query (Lines 16–20, Algorithm 1, Steps 10, 11, Fig. 2). Bassett and Kraft [8] suggest that repetition of certain query terms might improve retrieval performance of the query. If none of the candidates is likely to improve the initial query according to the quality model (i.e., baseline itself is the best), we repeat all the terms from the initial query as the reformulation. Algorithm 2 getQRCandidate: Get a candidate reformulation (cid:46) CTsig: extracted"
1807.04488,data,33,2022-05-13,0,"[54] J. Yao, B. Cui, L. Hua, and Y. Huang. Keyword Query Reformulation on Structured Data. In Proc. ICDE, pages 953–964, 2012."
1807.04488,data,36,2022-05-13,0,"[50] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online Defect Prediction for Imbalanced Data. In Proc. ICSE, volume 2, pages 99–108, 2015."
1807.04488,data,37,2022-04-21,0,"[7] A. Bachmann and A. Bernstein. Software Process Data Quality and Characteristics: A Historical View on Open and Closed Source Projects. In Proc. IWPSE, pages 119– 128, 2009."
1807.04488,data,4,2022-04-21,0,Quality metric data resampling
1807.04488,"data, code",319,2022-05-13,0,"a ﬁxed size of window in the code. Rocchio [43] and RSV [13] determine importance of a term using TF-IDF based metrics. Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source."
1807.04488,"data, code",338,2022-05-13,0,"Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source. Thus, while earlier studies adopt various methodologies or information sources, our technique not only employs a novel and promising term weight –CodeRank, but also exploits structures of the source documents for identifying the best reformulation to a given query for improved concept location."
1807.04488,"data, data https",32,2022-04-21,0,[1] ACER experimental data. URL https://goo.gl/ZkaNvd. [2] Debbugger source lookup does not work with variables. URL https://bugs.eclipse.org/bugs/show bug.cgi? id=31110.
1807.04488,"data, data https",8,2022-04-21,0,Replication: All experimental data and relevant materials
1807.04488,"data, dataset",308,2022-04-21,0,"From Table IX, we see that RSV and Refoqus perform better than the other existing approaches. They improve about 55% and about 53% of the baseline queries respectively. Such ratios are also pretty close to the originally reported performances by Haiduc et al. on a different dataset, which possibly validates the correctness of our implementation. While 55% query improvement is the maximum performance provided by any of the existing approaches, our technique–ACER–improves about 70% of the baseline queries (i.e., 1% difference between Table V and Table IX due to rounding error) which is signiﬁcantly higher, i.e., paired t-test, p-value=6.663e-06<0.05, Cohen’s D=2.43>1.00 (large). Refoqus adopts a similar methodology like ours. Unfortunately, the approach is limited due to possibly the low performance of its candidate reformulations. One might argue about the data resampling step (i.e., Step 9, Fig. 2) of ACER for the high performance. However, we also apply data resampling to Refoqus using the same settings as ours for further investigation. We see that Refoqussampled has a similar improvement ratio like ours, but it still worsens a signiﬁcant amount of queries, 29%, compared to our 3.40%. Thus, our technique still performs better than Refoqus in the equal settings. Our quantile measures and mean ranks are more promising than those from the baseline or competing methods as reported in Table IX. Table V and RQ1 also suggest that our queries have high potential for reducing human efforts. We also experiment with an extended dataset (i.e., 1,755=1,675 + 8x10) containing 80 very good queries. As reported in Table"
1807.04488,"data, dataset, open-source",55,2022-05-13,0,"Data Collection: We collect a total of 1,675 bug reports from eight open source subject systems (i.e., ﬁve Eclipse systems and three Apache systems) for our experiments. Table III shows the experimental dataset. We ﬁrst extract resolved bug reports (i.e., marked as RESOLVED) from BugZilla and"
1807.04488,database,32,2022-05-13,0,"[55] T. Yuan, D. Lo, and J. Lawall. Automated Construction of a Software-speciﬁc Word Similarity Database. In Proc. CSMR-WCRE, pages 44–53, 2014."
1807.04488,dataset,13,2022-04-21,0,TABLE III EXPERIMENTAL DATASET System #CR #Classes ecf–279.279 log4j–1.2.18 sling–9.0 tomcat70–7.0.73
1807.04488,dataset,4,2022-05-13,0,A. Experimental Dataset
1807.04488,"dataset, code, github",133,2022-04-21,0,"JIRA repositories, and then collect corresponding bug-ﬁxing commits from GitHub version control histories of these eight systems. Such approach was regularly adopted by the relevant literature [8, 21, 41, 49], and we also follow the same. In order to ensure a fair evaluation or validation, we discard the bug reports from our dataset for which no source code ﬁles (e.g., Java classes) were changed or no relevant source ﬁles exist in the system snapshot collected for our study. We also discard such bug reports that contain stack traces using appropriate regular expressions [33]. They do not represent a typical change request (i.e., mostly containing natural language texts) from the regular software users."
1807.04488,open-source,7,2022-05-13,0,from eight open source subject systems.
1807.04488,publicly available,103,2022-04-21,0,"that their implementations are not publicly available. In the case of Refoqus, we implement 27 metrics (20 pre-retrieval [19] and 7 post-retrieval [21]) that estimate query difﬁculty. We develop a machine learning model using CART algorithm (i.e., as used by them) and 10-fold cross validation. Then, we use the model to return the best reformulation out of four candidates of Refoqus– query reduction, Dice expansion, Rocchio’s expansion and RSV expansion–for each baseline query. Table IX and Fig. 6 summarize our comparative analyses."
1810.03977,data,100,2022-05-13,0,The images are normalized and then given to the model for training.First convolution layer the kernel size used is 3×3 with input shape 32×56×56 with RELU (Rectiﬁed Linear Unit) activation function in the ﬁrst convolution layer and then with max pooling layer of size 32×27×27 we are down sampling the data to half of the original dimension and subsequent layers follow the similar pattern.The brief description of the CNN layers architecture along with the output shape is described in table 1.drop out is 0.25 which means we randoms abandon some of the weights to avoid the over ﬁtting
1810.03977,data,173,2022-05-13,0,"Support vector machines(SVM’s) are the most used machine learning algorithms for the image spam detection and because of high accuracy and robustness to misclassiﬁcations is the reason researcher prefer SVM[6]. SVM is a supervised learning algorithm used for the classiﬁcation of data.It consists of support vectors which divide and classiﬁes the data.It classiﬁes the non linear data using kernel trick in which the non linear data is projected to higher dimensions to make it linearly separable by a plane which is generally referred as hyper plane.It does this using a kernel function and their are diﬀerent types of kernel functions like linear,polynomial,radial basis function(RBF) and GausImage spam detection is a binary classian kernels. siﬁcation problem and two classes are spam and not spam.Using the training data that is collected and labeled according to respective classes model is trained and then the model is tested by giving the test data and performance of model is evaluated."
1810.03977,data,39,2022-05-13,0,"[8] R. Vinayakumar, P. Poornachandran, K. P. Soman, Scalable Framework for Cyber Threat Situational Awareness Based on Domain Name Systems Data Analysis, Springer Singapore, Singapore, 2018, pp. 113–142."
1810.03977,data,50,2022-05-13,0,"Initially, neural networks are used for image spam detection and then now research has shifted focus on applying the deep learning algorithms. Deep learning consists of neural network layers which automatically extracts the features from the data in hierarchical pattern and then predicts and classiﬁes the data."
1810.03977,data,81,2022-05-13,0,Convolutional neural networks (CNN’s) are one of the highly eﬃcient deep learning algorithms used for classifying data (particularly image data) using supervised learning technique. They consist of an Input layer and convolution layer followed by pooling layer and again convolution and pooling layers alternatively based on the size and architecture of the network. The ﬁnal layer is a fully connected layer. Fully connected layer converts the ﬁnal scalar outputs of individual classes
1810.03977,"data, dataset",10,2022-05-13,0,Table 2: Results metrics evaluated on test data set
1810.03977,"data, dataset",144,2022-05-13,0,In this research we have used the convolutional neural network(CNN) which is a deep learning network architecture for image spam detection.The deep learning approach gives better accuracy when compared with the machine learning and other conventional image processing based methods and also avoids the manual feature extraction task by automatically identifying the features by itself reducing the time and eﬀort.Binary classiﬁcation of image is performed the model is trained with existing labelled data set and then tested with the test data then metrics are evaluated.Further research can be carried out by exploring other deep learning algorithms like RNN and LSTM and tuning the architecture and hyper parameters may provide interesting insights.Capsule networks can also be tested on the data set which are giving promising results recently when compared with the convolutional neural networks for image related techniques[10].
1810.03977,"data, dataset",24,2022-05-13,0,"features[5]. Features like sender,meta data,message header are extracted and training dataset is prepared and labeled."
1810.03977,"data, dataset",4,2022-05-13,0,4. Data set
1810.03977,"data, dataset",47,2022-05-13,0,"Total images are split in ratio of 80 percent training data and 20 percent testing data.The model is evaluated after the convolutional neural network is trained on training dataset.It is then tested on the testing data set and result metrics accuracy,precision,recall and f1score"
1810.03977,dataset,32,2022-05-13,0,Dataset is subdivided into both training and testing datasets. Training dataset consists of 742 spam images and 648 normal images.Testing dataset consists of 186 spam images and 162 normal images.
1810.03977,dataset,47,2022-05-13,1,The dataset used in the experiment consist of 928 spam images and 810 normal images which collected from diﬀerent sources[9] and all are RGB images in various dimensions which in preprocessing are reshaped to 56×56 images.The sample images are shown in ﬁgure.1 and ﬁgure.2
1810.03977,dataset,8,2022-05-13,0,Figure 1: Sample spam image from dataset
1810.03977,dataset,8,2022-05-13,0,Figure 2: Sample non-spam image from dataset
1810.03977,dataset,96,2022-05-13,0,Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classiﬁcation achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques.
1812.05067,code,133,2022-05-13,0,"Experimental evaluation We have used our implementation to typecheck a variety of examples, including all the examples from the RelCost paper. Some of the examples, such as the relational analysis of merge sort (msort), have rather complex paper proofs. However, in all cases, the total typechecking time (including existential elimination and SMT solving) is less than 1s, suggesting that the approach is practical. Table 1 shows the experimental results over a subset of our example programs (our appendix lists all our example programs, including their code and experimental results). A “-” indicates a negligible value. Our experiments were performed on a 3.20GHz 4-core Intel Core i5-6500 processor with 16 GB of RAM."
1812.05067,code,239,2022-05-13,0,"Heuristics illustrated with merge sort We explain how our implementation types one example—the standard merge sort function— relationally. The goal of this exercise is primarily to illustrate some of our heuristics. The merge sort function, msort, splits a list into two nearly equal-sized sublists using an auxiliary function we call bsplit, recursively sorts each sublist and then merges the two sorted sublists using another function merge. In their paper on RelCost, Çiçek et al. [14] show that the relative cost of two runs of msort with two input lists of length n that diﬀer in at most α positions is upper-bounded by Q(n, α) = ) · min(α, 2H −i ), where H = ⌈log2(n)⌉ and h is a speciﬁc linear function. This open-form expression lies in O(n · (1 + log2(α))).1 Next, we explain at a high-level how this relative cost is typechecked bidirectionally. We show below the code of the top-level merge sort function msort. ﬁx msort(_).Λ.Λ.λl .case l of nil → nil | h1 :: tl1 → case tl1 of nil → cons(h1, nil) | _ :: _ → let r = bsplit ()[ ] [ ] l in"
1812.05067,code,36,2022-05-13,0,"We do not show the code of the helper functions bsplit and merge, but they have the following types (these types are also checked with BiRelCost; we omit those details here):"
1812.05067,data,154,2022-05-13,0,"Next, we extend relSTLC in two steps inspired by the features of previously proposed relational type systems. Our ﬁrst step, named RelRef, adds relational reﬁnement types over lists (as an example of an inductive data type), and a comonadic type that represents syntactic equality of two values. Our second step, named RelRefU, adds to RelRef the possibility to relate arbitrary programs of possibly dissimilar syntactic structure, thanks to the possibility to switch to a complementary unary type system. Both these extensions add intrinsic nondeterminism to the type system to allow a programmer ﬂexibility in writing programs. The source of nondeterminism in both these systems is non-syntax-directed typing and subtyping rules. RelRef has such rules for relational reﬁnement types and for subtyping, while RelRefU has such a rule for switching to unary typing and more such rules for subtyping."
1812.05067,data,98,2022-05-13,0,"Relational eﬀects [8, 14, 16, 28, 39] are often of a quantitative nature and measure some quantitative diﬀerence between two executions of the two expressions. These relational eﬀects are similar in spirit to their standard unary counterpart [13, 31, 32, 34] but their interpretation is a relation between the eﬀects of the two executions. For example, in diﬀerential privacy, a relational eﬀect is used to measure the level of indistinguishability between the observable outputs on two inputs diﬀering in one data element."
1904.11228,data,112,2022-05-13,0,"The feature matrix of data in the vth view is denoted as Xv = [xv N ]T ∈ RN ×dv , xv 1 ∈ Rdv×1, dv is the dimension of feature in the vth view, N is the number of data samples. We pack the feature matrices in V views {Xv}V v=1 and the overall feature matrix of data can be represented as X = [X1, X2, ..., XV ] ∈ RN ×d, (cid:80)V v=1 dv = d. The objective of unsupervised multi-view feature selection is to identify l most valuable features with only X."
1904.11228,data,145,2022-05-13,0,"The selected features should preserve the dynamically learned similarity structure. Conventional approaches separate the similarity structure construction and feature selection into two independent processes, which will potentially lead to sub-optimal performance. In this paper, we learn the collaborative similarity structure dynamically and further integrate it with feature selection into a uniﬁed framework. Specifically, based on the collaborative similarity structure learning in Eq.(3), we employ sparse regression model to learn a projection matrix P ∈ Rd×k, so that the projected lowdimensional data XP can approximate the relaxed cluster indicator F. To select the features, we impose l2,1 norm penalty on P to force it with row sparsity. The importance of features can be measured by the l2 norm of each row feature in P. The overall optimization formulation can be derived as"
1904.11228,data,15,2022-05-13,0,"ture selection for big data analytics. 32(2):9–15, 2017."
1904.11228,data,173,2022-05-13,0,"N Sj = 1, Sj ≥ 0, WT where Sj ∈ RN ×1 characterizes the similarities between any data points with j, it should be subjected to the constraint that 1TSj = 1, Sj ≥ 0, Wj = [w1 j ]T ∈ RV ×1 is comj , w2 prised of view weights for the jth column of similarities, it is constrained with WT j 1V = 1, W = [W1, W2, ..., WN ] ∈ RV ×N is view weight matrix for all columns in the similarity structures. As indicated in recent work [Nie et al., 2014], a theoretically ideal similarity structure for clustering should have the property that the number of connected components is equal to the number of clusters. The similarity structure with such neighbor assignment could beneﬁt the subsequent feature selection. Unfortunately, the similarity structure learned from Eq.(1) does not have such desirable property."
1904.11228,data,176,2022-05-13,0,"hand, with multi-view features, the data could be characterized more precisely and comprehensively from different perspectives. On the other hand, high-dimensional multiview features will inevitably generate expensive computation cost and cause massive storage cost. Moreover, they may contain adverse noises, outlying entries, irrelevant and correlated features, which may be detrimental to the subsequent learning process [Zhu et al., 2016b; Zhu et al., 2016a; Zhu et al., 2017a]. Unsupervised multi-view feature selection [Wang et al., 2016; Li and Liu, 2017] is devised to alleviate the problem. It selects a compact subset of informative features from the original features by dropping irrelevant and redundant features with advanced unsupervised learning. Due to the independence on semantic labels, high computing efﬁciency and well interpretation capability, unsupervised multiview feature selection has received considerable attention in It becomes a prerequisite component in various literature. machine learning models [Li et al., 2017]."
1904.11228,data,204,2022-05-13,0,"The key problem of multi-view feature selection is how to effectively exploit the diversity and consistency of multi-view features to collaboratively identify the feature dimensions, which could retain the key characteristics of the original features. Existing approaches can be categorized into two major families. The ﬁrst kind of methods ﬁrst concatenates multiview features into a vector and then directly imports it into the conventional single-view feature selection model. The candidate features are generally ranked based on spectral graph theory. Typical methods of this kind include Laplacian Score (LapScor) [He et al., 2005], spectral feature selection (SPEC) [Zhao and Liu, 2007] and minimum redundancy spectral feature selection (MRSF) [Zhao et al., 2010]. Commonly, the pipeline of these methods follows two separate processes: 1) Similarity structure is constructed with ﬁxed graph parameters to describe the geometric structure of data. 2) Sparsity and manifold regularization are employed together to identify the most salient features. Although these methods are reported to achieve certain success, they treat features from different views independently and unfortunately neglect the important view correlations."
1904.11228,data,205,2022-05-13,0,"Another family of methods considers view correlation when performing feature selection. Representative works in       clude adaptive multi-view feature selection (AMFS) [Wang et al., 2016], multi-view feature selection (MVFS) [Tang et al., 2013] and adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013]. These methods ﬁrst construct multiple view-speciﬁc similarity structures1 and then perform the subsequent feature selection based on the collaborative (combined) similarity structure. These two processes are separate and independent. The collaborative similarity structure remains ﬁxed during feature selection. The latently involved data noises and outlying entries in the view-speciﬁc similarity structures will adversely reduce the reliability of the ultimate collaborative similarity structure for feature selection. Furthermore, conventional approaches generally employ knearest neighbors assignment to construct the view-speciﬁc similarity structures and the simple weighted combination for ultimate similarity structure generation. This strategy can hardly achieve the ideal state for clustering that the number of connected components in the ultimate similarity structure is equal to the number of clusters [Nie et al., 2014]. Thus, suboptimal performance may be caused under such circumstance."
1904.11228,data,206,2022-05-13,0,"the important correlation of different feature views. Another kind of methods directly tackles the multi-view feature selection. They consider view correlations when performing feature selection. Adaptive multi-view feature selection (AMFS) [Wang et al., 2016] is an unsupervised feature selection approach which is developed for human motion retrieval. It describes the local geometric structure of data in each view with local descriptor and performs the feature selection in a general trace ratio optimization. In this method, the feature dimensions are determined with trace ratio criteria. Adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013] addresses the feature selection problem for visual concept recognition. It employs l2,1 norm [Nie et al., 2010] based sparse regression model to automatically identify discriminative features. In AUMFS, data cluster structure, data similarity and the correlations of different views are considered for feature selection. Multi-view feature selection (MVFS) [Tang et al., 2013] investigates the feature selection for multi-view data in social media. A learning framework is devised to exploit the relations of views and help each view select relevant features."
1904.11228,data,38,2022-05-13,0,"[Tang et al., 2013] Jiliang Tang, Xia Hu, Huiji Gao, and Huan Liu. Unsupervised feature selection for multi-view data in social media. In SDM, pages 270–278, 2013."
1904.11228,data,69,2022-05-13,0,"[He et al., 2005] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS, pages 507–514, 2005. [Huang et al., 2015] Jin Huang, Feiping Nie, and Heng Huang. A new simplex sparse learning model to measure data similarity for clustering. In IJCAI, pages 3569–3575, 2015."
1904.11228,data,75,2022-05-13,0,"As mentioned above, the data points can be directly partitioned into k clusters if the number of components in the similarity structure S is exactly equal to k. Theorem 1 indicates that this condition can be achieved if the rank of Laplacian matrix is equal to n − k. With the analysis, we add a reasonable rank constraint in Eq.(1) to achieve the condition. The optimization problem becomes"
1904.11228,data,76,2022-05-13,0,"With the advent of big data, multi-view features with high dimensions are widely employed to represent the complex data in various research ﬁelds, such as multimedia computing, machine learning and data mining [Liu et al., 2016; Liu et al., 2017; Zhu et al., 2017b; Zhu et al., 2015; Cheng and Shen, 2016; Cheng et al., 2016]. On the one"
1904.11228,"data, dataset",295,2022-05-13,1,"4 Experiments 4.1 Experimental Datasets 1) MSRC-v1 [Winn and Jojic, 2005]. The dataset contains 240 images in 8 class as a whole. Following the setting in [Grauman and Darrell, 2006], we select 7 classes composed of tree, building, airplane, cow, face, car, bicycle and each class has 30 images. We extract 5 visual features from each image: color moment with dimension 48, GIST with 512 dimension, SIFT with dimension 1230, CENTRIST feature with 210 dimension, and local binary pattern (LBP) with 256 dimension. 2) Handwritten Numeral [van Breukelen et al., 1998]. This dataset is comprised of 2,000 data points from 0 to 9 digit classes. 6 features are used to represent each digit. They are 76 dimensional Fourier coefﬁcients of the character shapes, 216 dimensional proﬁle correlations, 64 dimensional Karhunen-love coefﬁcients, 240 dimensional pixel averages in 2 × 3 windows, 47 dimensional Zernike moment and 6 dimensional morphological features. 3) Youtube [Liu et al., 2009]. This real-world dataset is collected from Youtube. It contains intended camera motion, variations of the object scale, viewpoint, illumination and cluttered background. The dataset is comprised of 1,596 video sequences in 11 actions. 4) Outdoor Scene [Monadjemi et al., 2002]. The outdoor scene dataset contains 2,688 color images that belong to 8 outdoor scene categories. 4 visual features are extracted from each image: color moment with dimension 432, GIST with dimension 512, HOG with dimension 256, and LBP with dimension 48."
1904.11228,dataset,1,2022-05-13,0,Dataset
1905.12665,code,82,2022-05-13,0,"This work was ﬁnanced in part by the S˜ao Paulo Research Foundation (FAPESP) under grants No. 2016/199476 and No. 2017/16597-7, the Brazilian National Council for Scientiﬁc and Technological Development (CNPq) under grant No. 307425/2017-7, and the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior—Brasil (CAPES)—Finance Code 001. We acknowledge the support of NVIDIA Corporation for the donation of a Titan X Pascal GPU used in this research."
1905.12665,"code, code available",1,2022-05-13,0,Code
1905.12665,data,102,2022-05-13,0,"Abstract Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classiﬁcation, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings."
1905.12665,data,22,2022-05-13,0,Presented at the ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data Copyright 2019 by the author(s).
1905.12665,data,37,2022-05-13,0,"Berg, R. v. d., Kipf, T. N., and Welling, M. Graph convolutional matrix completion. ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), 2018."
1905.12665,data,49,2022-05-13,0,"Donnat, C., Zitnik, M., Hallac, D., and Leskovec, J. Learning structural node embeddings via diffusion wavelets. In ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), pp. 1320–1329. ACM, 2018."
1905.12665,data,73,2022-05-13,0,"Bai, Y., Ding, H., Bian, S., Chen, T., Sun, Y., and Wang, W. SimGNN: A neural network approach to fast graph similarity computation. In ACM Inter. Conf. Web Search Data Min. (WSDM), WSDM ’19, pp. 384–392, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-5940-5."
1905.12665,data,94,2022-05-13,0,"Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In Gangemi, A., Navigli, R., Vidal, M.-E., Hitzler, P., Troncy, R., Hollink, L., Tordai, A., and Alam, M. (eds.), Semantic Web Conf. (ESWC), pp. 593–607, Cham, 2018. Springer International Publishing."
1905.12665,dataset,140,2022-05-13,1,"In this work, we evaluate our model as an edge classiﬁer, and simulate its performance as a graph generator by inputting noise as features and predicting on them. We perform experiments on three synthetic datasets that consist of images with Geometric Figures for segmentation, 3D surface function, and Community dataset (see Appendices A.1, A.2, and A.3, respectively). For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. Our evaluation metric is the Maximum Mean Discrepancy (MMD) measure (You et al., 2018), which measures the Wasserstein distance over three statistics of the graphs: degree (Deg), clustering coefﬁcients (Clus), and orbits (Orb)."
1905.12665,dataset,147,2022-05-13,0,"For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. For both models, we use the following settings. Our activation functions, σl, are sigmoid for all layers, except for the Eq. 7 where σl is a hyperbolic tangent. We use L = 5 layers to extract the ﬁnal adjacency and embeddings. The feature dimension, dl, is 32 for all layers. The learning rate is set 10−5 for the Community dataset, and in the rest of datasets, the learning rate is set 5 × 10−6. Additionally, the number of epochs changes depending on the experiment. Thus in the experiments of Communities, Surfaces and Geometrical Figures we use 150, 200 and 150 times respectively and, the number"
1905.12665,dataset,2,2022-05-13,0,A. Datasets
1905.12665,dataset,39,2022-05-13,0,"Figure 2. Results of the dissimilarity (MMD) between the prediction and ground truth (smaller values are better) while varying the number of recurrent steps, on the 3D Surface dataset (Surf400)."
1905.12665,dataset,4,2022-05-13,0,A.3. Community Dataset
1905.12665,dataset,40,2022-05-13,0,"We made the Geometric Figures dataset for the task of image segmentation within a controlled environment. Segmentation is given by the connected components of the graph ground-truth. Here, we provide RGB images and their expected segmentations."
1905.12665,dataset,44,2022-05-13,0,"Additionally, in Table 2, we present an ablation analysis of our model’s loss functions and regularization components on the Geometric Figures dataset. We emphasize a stable training and a fast convergence when we minimize both loss functions simultaneously."
1905.12665,dataset,5,2022-05-13,0,A.1. Geometric Figures Dataset
1905.12665,dataset,5,2022-05-13,0,A.2. 3D Surfaces Dataset
1905.12665,dataset,52,2022-05-13,0,"The Geometric Figures dataset contains 3000 images of size n×n, that are generated procedurally.1 Each image contains circles, rectangles, and lines (dividing the image into two parts). We also add white noise to the color intensity of the images to perturb and mixed their regions."
1905.12665,dataset,56,2022-05-13,0,"We generated 200 versions of each surface by randomly applying a set of transformations (from scaling, translation, rotation, reﬂection, or shearing) to the curve, moreover, two versions of the Surface dataset were created, Surf100 and Surf400 that use 100 and 400 vertices per surface, respectively."
1905.12665,dataset,68,2022-05-13,0,"We perform experiments on a synthetic dataset (Community dataset) that comprises two sets with C = 2 and C = 4 communities with 40 and 80 vertices each, respectively, created with the caveman algorithm (Watts, 1999), where each community has 20 people. Besides, Community C = 4 and C = 2 have 500 and 300 samples respectively."
1905.12665,dataset,69,2022-05-13,0,"Table C.1. Comparison of GLN, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric are accuracy (Acc), intersection-over-union (IoU), Recall (Rec), and Precision (Prec) shown row-wise per method, where larger numbers denote better performance."
1905.12665,dataset,73,2022-05-13,0,"Finally, in Fig. F.1, we present an application, even fundamental, on segmentation where each of the connected components represents different objects. For this, we apply our GLN model on Geometric Image dataset, using size image of 20 × 20. Besides, the white edges represent correct predictions, and light blue dashed edges are false negatives (i.e., not predicted edges)."
1905.12665,dataset,78,2022-05-13,0,"Figure E.1. Results on Community dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,78,2022-05-13,0,"Knowing the depth of the recursive model (i.e., the number of iterations) is not a trivial task since we must ﬁnd a tradeoff between the efﬁciency and effectiveness of the model. In Fig. 2, we show the dissimilarity metrics (MMD) while varying the number of applications of our proposed block on the 3D Surface dataset. According to our experiment, using ﬁve recurrent steps provides the right trade-off."
1905.12665,dataset,79,2022-05-13,0,"Figure D.1. Results on 3D Surface dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,81,2022-05-13,0,"To evaluate our method we needed a highly structured dataset with intricate relations and with easily understandable features. Hence, we convert parts of 3D surfaces into a mesh by sampling them. Each point in the mesh is translated into a node of the graph, with its position as a feature vector. We have a generator2 that creates different conﬁgurations for this dataset based on a number of nodes per surface, and transformation on it."
1905.12665,dataset,83,2022-05-13,0,"In Fig. D.1, we show the qualitative result of GLN for the 3D Surface dataset. We show the prediction on the elliptic hyperboloid, elliptic paraboloid, torus, saddle, and ellipsoid, all using 100 nodes (Surf100). We normalized the graphs (w.r.t. scale and translation) for better visualization. Besides, the red edges represent false negatives (i.e., not predicted edges) and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"In Fig. E.1, we predict the adjacency matrix the of Community dataset on two and four communities, C = 2 and C = 4 respectively (even rows). Note, our node embedding obtained after apply the λl function, shows a good grouping of individuals in the hyperspace (odd rows). Furthermore, the red edges represent false negatives (i.e., not predicted edges), and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"structure given a set of points and their feature embeddings, respectively. (ii) A recurrent architecture that deﬁnes our iterative process and our prediction functions. (iii) An endto-end learning framework for predicting graphs’ structure given a family of graphs. (iv) Additionally, we introduce a synthetic dataset, i.e., 3D surface functions, that contains patterns that can be controlled and mapped into graphs to evaluate the robustness of existing methods."
1905.12665,dataset,84,2022-04-21,0,"Table 1. Comparison of GLN against deep generative models, GraphRNN (G.RNN), Kronecker (Kron.), and MMSB, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric is MMD for degree (D), cluster (C), and orbits (O) shown row-wise per method, where smaller numbers denote better performance."
1905.12665,github,4,2022-05-13,0,at https://gitlab.com/mipl/
1905.12665,supplementary data,10,2022-05-13,0,Graph Learning Network: A Structure Learning Algorithm SUPPLEMENTARY MATERIAL
1906.08554,data,105,2022-05-13,0,"The smart devices are increasing exponentially day by day in the whole world. They provide much more facility to the end users and also attach with their daily life. Smart devices can connect to the internet easily for sending and receiving data within the network. The smart devices are not just smart phones, it may be smart refrigerator, Smart home automation entry point, smart air conditioners, Smart hubs, Smart thermostat, Color changing smart LEDs, Smart Watches and smart Tablets etc. in internet of things framework they are connected to each other through internet."
1906.08554,data,119,2022-05-13,0,"The objective of this research is to create the new reliable communication framework for the smart cities using the Tactile Internet a next revolution of internet of things. This research is based on low-latency, ultra-high availability and high-performance concepts of Tactile Internet. The framework provides QoS through reducing the latency (1ms in round trip) also the variety of the quantity of smart devices. In this research we consider idle state in order to makes our examination more efficient, at that point the general execution regarding the overall performance of the framework is evaluated. The framework will monitor and analyze the real-time data collected from network and then taking the action."
1906.08554,data,12,2022-05-13,0,Table.2: Comparison of peak data rate and latency [3]
1906.08554,data,127,2022-05-13,0,"The proposed research entitled “Tactile Internet based reliable communication framework for Smart cities in 5G” is a step forward in wireless networking and IoT where we propose new reliable framework based on Tactile Internet. The Wireless communication is the key of Internet of things and Tactile Internet. It is expected to exceed 50 billion connected devices by 2020 and most of these nodes cannot be connected by wireline. In order to enable critical applications such as smart factories or smart buildings, the networking protocols have to deal with the non-deterministic nature of wireless links. In the 5th generation communication system, the secure and reliable data packets will rely on the network with high availability and low latency."
1906.08554,data,169,2022-05-13,0,"The proposed research plan builds research on extending the performance of communication in internet of things using tactile internet. The transfer data from one configuration to another using wireless networks starts from 1973 in the form of packets radio network. They were able to communicate with another same configuration devices. Recent work is continuing on a project called the Serval Project. It provides networks facility to android devices for communication in infrastructure less network. Whereas our research is concerned about the high-performance communication in internet of smart devices for smart cities. The main contribution of this research is the creation of the reliable communication framework and provide secure, reliable and fast communication using Tactile Internet among the internet of smart devices. The previous studies have been focused on the creation and optimization the framework for communication, but such research doesn’t perform the full framework for secure and reliable communication among internet of smart devices for smart cities."
1906.08554,data,270,2022-05-13,0,"The main contribution of this research is designing a framework for ultra-reliable, low latency and high availability communication in Internet of smart devices for future smart cities using the Tactile Internet. The proposed framework is specifically appropriate for applications in which data is periodically transmitted in internet of smart devices environment. In these applications, on one hand, packets are being produced based on a certain periodic time pattern. On the other hand, service time is always a random variable with general distribution. Therefore, service time might temporarily exceed the period time which, as an inevitable consequence some packets might encounter a busy channel and be dropped. We solve this problem by proposing the new communication framework. We demonstrate that proposed reliable framework, not only increases the throughput, but also the direct connection between the generation (sensors) and communication packet systems are eliminated which make the system far more stable. Moreover, in order to enhance the proposed model, we have employed retransmission scheme, variable packet length, and saturated traffic condition. The solution of this research is summarized as follows. The implementation of proposed framework for communication among internet of smart devices in 5G will be programmed to execute on to the internet of things using Tactile Internet concepts. The idea will focus into three main concepts, these concepts are Reliability, Security and availability. The proposed study supports the wireless networking technology to establish a reliable framework among internet of devices for smart cities."
1906.08554,database,110,2022-05-13,0,"[27]. Aljohani, Mohammed, and Tanweer Alam. ""An algorithm for accessing traffic database using wireless technologies."" In Computational Intelligence and Computing Research (ICCIC), 2015 IEEE International Conference on, pp. 1-4. IEEE, 2015. DOI: https://doi.org/10.1109/iccic.2015.7435818  [28]. Alam, Tanweer, and Mohammed Aljohani. ""Design a new middleware for communication in ad hoc network of android smart devices."" In Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies, p. 38. ACM, 2016. DOI: https://doi.org/10.1145/2905055.2905244"
1912.0313,data,125,2022-05-13,0,"We observe that the ST-DIM based pre-trained models can easily be ﬁne-tuned only with small amount of downstream data. In this classiﬁcation task, our model can classify a randomly chosen time-series as a sample of VAR or SVAR. Note, with very few samples, models based on the pre-trained encoder (FPT and UFPT) outperform supervised models. However, as the number of samples grows, the accuracy achieved with or without pre-training levels out. We also notice that autoencoder based self-supervised pretraining does not assist in VAR vs. SVAR classiﬁcation. Consequently, we use only ST-DIM based pre-training for all the real data experiments. Refer to Figure 4 for the results of simulation experiments."
1912.0313,data,212,2022-05-13,0,"Unsupervised pre-training is a well-known technique to get a head start for the deep neural network. It may be considered as a regularizer which compares to classical regularizers (i.e. L1/L2) may not vanish even with more data and could ﬁnd a robust local minima for better generalization [6]. Classical methods are Deep Beliefs Networks (DBMs) [7] and stacked denoising autoencoders (SDAE) [8]. Unsupervised pre-training has broad implications in ﬁelds such as computer vision [9], natural language processing (NLP) (GPT2 [10], BERT [11], Word2Vec [12]) and automatic speech recognition (ASR) (with SDAE [13], with DBN-HMMs [14]). However, this unsupervised pre-training is considered to be less popular in ﬁelds other than NLP [15]. Speciﬁcally, in computer vision, researchers usually use the model which is pre-trained in supervised fashion on Imagenet as a starting point for downstream tasks. Furthermore, given enough data and technical strategies, it is possible to achieve better results on COCO object detection without supervised pre-training on Imagenet [16]."
1912.0313,data,25,2022-05-13,0,Simulations Real Data 16 Training Batch Size 200 Validation Batch Size 200 Test Batch Size Initial Learning Rate 3e-4 Learning Rate Scheduler None Max Epochs
1912.0313,data,30,2022-05-13,0,Figure 8. ICA time courses are computed from the resting state fMRI data. Results contain statistically independent spatial maps (top) and their corresponding time courses.
1912.0313,data,36,2022-05-13,0,"[19] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019."
1912.0313,data,38,2022-05-13,0,"namics generalizes across different data distributions, as our model pre-trained on healthy adults shows improvements in children and elderly. The generality of the approach is also demonstrated in an application to the keyword detection problem."
1912.0313,data,42,2022-05-13,0,"To simulate the data, we generate multiple 10-node graphs with 10 × 10 stable transition matrices. Using these we generate multivariate time series with autoregressive (VAR) and structural vector autoregressive (SVAR) models [44]."
1912.0313,data,42,2022-05-13,0,"[2] Vince D Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adalı. The chronnectome: time-varying connectivity networks as the next frontier in fmri data discovery. Neuron, 84(2):262–274, 2014."
1912.0313,data,45,2022-05-13,0,"[28] Vince D Calhoun, Tulay Adali, Godfrey D Pearlson, and JJ Pekar. A method for making group inferences from functional MRI data using independent component analysis. Human brain mapping, 14(3):140–151, 2001."
1912.0313,data,48,2022-05-13,0,"[31] R Devon Hjelm, Eswar Damaraju, Kyunghyun Cho, Helmut Laufs, Sergey M Plis, and Vince D Calhoun. Spatio-temporal dynamics of intrinsic networks in functional magnetic imaging data using recurrent neural networks. Frontiers in neuroscience, 12:600, 2018."
1912.0313,data,51,2022-05-13,0,"application [33–35] is considered as a way to enable learning from data and thus improve results in downstream classiﬁcation. To achieve improved performance, another idea is the data generating approach [36] which uses synthetic data generator for pre-training, relieving the scarcity of data."
1912.0313,data,61,2022-05-13,0,"U. Mahmood1, M. M. Rahman1, A. Fedorov2 , Z. Fu1, V. D. Calhoun1, 2, 3, S. M. Plis1 Tri-institutional Center for Translational Research in Neuroimaging and Data Science: 1Georgia State University, 2Georgia Institute of Technology, 3Emory University Atlanta, GA, USA {umahmood1,mrahman21}@student.gsu.edu afedorov@gatech.edu"
1912.0313,data,70,2022-05-13,0,"[50] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer, Mirella Dapretto, et al. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular psychiatry, 19(6):659, 2014."
1912.0313,data,96,2022-05-13,0,"As we are more interested in subjects for classiﬁcation task, we feed each time series (ICA time courses) into the encoder in the form of a sequence of windows. The encoder encodes the windows of input data into latent representation. The latent representation of the entire time series is then concatenated and passed to a biLSTM with hidden dimension of size 200. The output of biLSTM is then used as input to a feed forward network of two linear layers with 200 and 2 units to perform binary classiﬁcation."
1912.0313,"data, data https",154,2022-05-13,0,"We preprocessed the fMRI data using statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) under MATLAB 2016 environment. A rigid body motion correction was performed using the toolbox in SPM to correct subject head motion, followed by the slice-timing correction to account for timing difference in slice acquisition. The fMRI data were subsequently warped into the standard Montreal Neurological Institute (MNI) space using an echo planar imaging (EPI) template and were slightly resampled to 3 × 3 × 3 mm3 isotropic voxels. The resampled fMRI images were ﬁnally smoothed using a Gaussian kernel with a full width at half maximum (FWHM) = 6 mm. After the preprocessing. We included subjects in the analysis if the subjects have head motion ≤ 3◦ and ≤ 3 mm, and with functional data providing near full brain successful normalization [52]."
1912.0313,"data, data repository",11,2022-05-13,1,"2These data were downloaded from the Function BIRN Data Repository,"
1912.0313,"data, data repository",61,2022-05-13,0,"[48] David B Keator, Theo GM van Erp, Jessica A Turner, Gary H Glover, Bryon A Mueller, Thomas T Liu, James T Voyvodic, Jerod Rasmussen, Vince D Calhoun, Hyo Jong Lee, et al. The function biomedical informatics research network data repository. Neuroimage, 124:1074–1079, 2016."
1912.0313,"data, data repository, data https",100,2022-05-13,1,"Data for Schizophrenia classiﬁcation was used in this study were downloaded from the Function BIRN Data Repository (http://bdr.birncommunity.org:8080/BDR/), supported by grants to the Function BIRN (U24-RR021992) Testbed funded by the National Center for Research Resources at the National Institutes of Health, U.S.A. and from the COllaborative Informatics and Neuroimaging Suite Data Exchange tool (COINS; http://coins.trendscenter.org) and data collection was performed at the Mind Research Network, and funded by a Center of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 from the NIH to Dr.Vince Calhoun."
1912.0313,"data, dataset",106,2022-05-13,0,"COBRE The dataset has total 157 subjects — a collection of 68 HC and 89 affected with SZ. Like FBIRN, each subject has 53 non-noise components in its ICA time courses with 140 time points. We use two hold-out sets of size 32 each respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Unlike FBIRN, it has been impossible to increase the number of subjects for downstream training due to insufﬁciency of data."
1912.0313,"data, dataset",111,2022-05-13,0,"15255075100Number of Subjects Per Class0.30.40.50.60.70.80.91.0AUCSpeech CommandsFPTUFPTNPTFBIRNCOBREABIDEOASISHCPpre-trainingapplyingautismschizophreniaADdatasets contain labeled Schizophrenia (SZ) and Healthy Control (HC) subjects. FBIRN The dataset has total 311 subjects consisting of 150 HC and 161 affected with SZ. Each subject has 53 non-noise components with 140 time points. We use two hold-out sets with sizes 32 and 64 respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with 50% overlap along time dimension resulting in 13 windows for each subject. The details of the results are shown in Figure 9."
1912.0313,"data, dataset",111,2022-05-13,0,"As we see, the AUC scores of all the three models for 15 subjects is ∼ 0.5, which can be treated as merely random guess. However, as the number of subjects increases, pretrained models gradually start performing better than NPT which, in fact, even with 120 subjects fails to learn from the data. We suspect that the reason why pre-trained models do not work for 15 subjects is that the data set is much different than HCP. The big age gap between subjects of HCP and OASIS is a major difference and 15 subjects are even not enough for pre-trained models."
1912.0313,"data, dataset",126,2022-05-13,0,"Figure 7. Datasets used for pre-training and classiﬁcation tasks. Healthy controls from the HCP [5] are used for encoder pre-training guided by data dynamics alone1. The pre-trained encoder is then used in downstream classiﬁcation tasks of 3 different diseases, 4 independently collected datasets, many of which contain data from a number of sites, and consist of populations with signiﬁcant age difference. The age distributions in the datasets have the following means, medians and standard deviations: HCP: 29.31, 29.00, 3.667; ABIDE: 17.04, 15.40, 7.29; COBRE: 37.96, 37, 12.90; FBIRN: 37.87, 38, 11.25; OASIS: 67.67, 68, 8.92."
1912.0313,"data, dataset",131,2022-05-13,0,"In this section we study the performance of our model on both, synthetic and real data. To compare and show the advantage of pre-training on large unrelated dataset we use three different kind of models — 1) FPT (Frozen Pre Encoder for simulation experiment consists of 4 1D convolutional layers with output features (32, 64, 128, 64), kernel sizes (4, 4, 3, 2) respectively, followed by ReLU [41] after each layer followed by a linear layer with 256 units. For real data experiments, we use 3 1D convolutional layers with output features (64, 128, 200), kernel sizes (4, 4, 3) re 3"
1912.0313,"data, dataset",136,2022-05-13,0,"Recent advances in unsupervised learning using selfsupervised methods with mutual information objectives have reduced the gap between supervised and unsupervised learning on standard computer vision classiﬁcation datasets [17– 21] and scaled pre-training to very deep convolutional networks (e.g., 50-layer ResNet). Furthermore, it inﬂuences the neuroimaging ﬁeld for classiﬁcation of progression to Alzheimer’s disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identiﬁcation [24]. Speciﬁcally, authors [19] have shown that contrastive based self-supervised pre-training can outperform supervised methods by a large margin in case of small data (e.g., 13 images per class in ImageNet [25])."
1912.0313,"data, dataset",164,2022-05-13,0,"Differentiating multivariate dynamic signals is a difﬁcult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m (cid:29) n problem. In this paper, we show that the source of the problem—signal dynamics—can be used to our advantage and noticeably improve classiﬁcation performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets."
1912.0313,"data, dataset",17,2022-05-13,0,data experiments. Refer to Figure 7 for the details of the datasets for disease classiﬁcation.
1912.0313,"data, dataset",217,2022-05-13,0,"subjects provides beneﬁts that transfer across datasets, collection sites, and multiple disease classiﬁcation with varying age gap. Learning dynamics of fMRI helps to improve classiﬁcation results for schizophrenia, autism, Alzheimer’s dieseases and speed up the convergence of the algorithm on small datasets, that otherwise do not provide reliable generalizations. Although the utility of these results is highly promising by itself, we conjecture that direct application to spatio-temporal data will warrant beneﬁts beyond improved classiﬁcation accuracy in the future work. Working with ICA components is a smaller and thus easier to handle space that exhibits all dynamics of the signal, in future we will move beyond ICA pre-processing and work with fMRI data directly. We expect model introspection to yield insight into the spatio-temporal biomarkers of schizophrenia. In future work, we will use the same analogously pre-trained encoder on datasets with various other mental disorders such as MCI and bipolar. We are optimistic about the outcome because the proposed pre-training is oblivious of the downstream use and is done in a manner quite different from the classiﬁer’s work. It may indeed be learning crucial information about dynamics that might contain important clues into the nature of mental disorders."
1912.0313,"data, dataset",221,2022-05-13,0,"Mental disorders manifest in behavior that is driven by disruptions in brain dynamics [1, 2]. Functional MRI captures the nuances of spatio-temporal dynamics that could potentially provide clues to the causes of mental disorders and enable early diagnosis. However, the obtained data for a single subject is of high dimensionality m and to be useful for learning, and statistical analysis, one needs to collect datasets with a large number of subjects n. Yet, for any kind of a disorder, demographics or other types of conditions, a single study is rarely able to amass datasets large enough to go out of the m (cid:29) n mode. Traditionally this is approached by handcrafting features [3] of much smaller dimension, effectively reducing m via dimensionality reduction. Often, the dynamics of brain function in these representations vanishes into proxy features such as correlation matrices of functional network connectivity (FNC) [4]. Efforts that pull together data from various studies and increase n do exist, but it is difﬁcult to generalize to study of smaller and more speciﬁc disease populations that cannot be shared to become a part of these pools or are too different from the data in them."
1912.0313,"data, dataset",71,2022-05-13,0,"points. We use two hold-out sets of size 100 each respectively for validation and test purpose. The remaining data are used for downstream training i.e., autism vs. HC classiﬁcation. Like COBRE dataset, with 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Refer to Figure 11 for the details of the experimental results."
1912.0313,"data, dataset",93,2022-05-13,0,"For experiments on the downstream tasks, a hold out is selected for testing and is never used through the training/validation phase. For each downstream task, the number of subjects used for supervised training is gradually increased within a range to observe the effectiveness of pretraining in downstream task with varied number of training subjects . For each experiment, 10 trials are performed to ensure random selection of training subjects and, in each case, the performance is evaluated on the hold out dataset (test data)."
1912.0313,"data, dataset",94,2022-05-13,0,"For brain data, each of the models (FPT, UFPT, NPT) yields the best results based on its respective gain value of Xavier [43] initialization used for biLSTM. To ﬁnd the best gain value for each model, 20 values in the range 0 − 1 are tried with an increment of 0.05. For each value, 10 experiments are performed and best value is chosen based on the results on validation dataset. Refer to Table 1 for more parametric details of the models."
1912.0313,"data, dataset",98,2022-05-13,0,"15306090120Number of Subjects Per Class0.30.40.50.60.70.80.9AUCSTDIM vs AutoencoderNPTUFPT_STDIMFPT_STDIMUFPT_AutoencoderFPT_AutoencoderLibriSpeech+Mel-SpectrogramCoffee ShopBackgroundSpeech Commands    (Cat)Coffee ShopBackground+Mel-SpectrogramPre-TrainingTraining00.511.52Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dB+0 dB1111TTTT/2051015Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dBResearch Excellence) [49] project, from the release 1.0 of ABIDE (Autism Brain Imaging Data Exchange3) [50] and from release 3.0 of OASIS (Open Access Series of Imaging Studies4) [51]. Written informed consent was obtained from all participants of each dataset under protocols approved by the institutional review board (IRB)."
1912.0313,"data, dataset provided",53,2022-05-13,1,"Data for Alzheimer’s was provided by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, UL1TR000448, R01AG043434, R01EB009352. AV-45 doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly."
1912.0313,"data, dataset provided",60,2022-05-13,1,"Data for healthy subjects was provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University."
1912.0313,"data, dataset provided",82,2022-05-13,1,"15255075100120Number of Subjects Per Class0.400.450.500.550.600.650.70AUCOASISFPTUFPTNPTAutism data was provided by ABIDE. We acknowledge primary support for the work by Adriana Di Martino provided by the (NIMH K23MH087770) and the Leon Levy Foundation and primary support for the work by Michael P. Milham and the INDI team was provided by gifts from Joseph P. Healy and the Stavros Niarchos Foundation to the Child Mind Institute, as well as by an NIMH award to MPM (NIMH R03MH096321)."
1912.0313,"data, dataset, publicly available, data available",158,2022-05-13,1,"Our goal is to enable the direct study of systems dynamics in smaller datasets. In the case of brain data it, in turn, can enable an analysis of brain function. In this paper, we show how one can achieve signiﬁcant improvement in classiﬁcation directly from dynamical data on small datasets by taking advantage of publicly available large but unrelated datasets. We demonstrate that it is possible to train a model in a self-supervised manner on dynamics of healthy control subjects from the Human Connectome Project (HCP) [5] and apply the pre-trained encoder to a completely different data collected across multiple sites from healthy controls and patients. We show that pre-training on dynamics allows the encoder to generalize across a number of datasets and a wide range of disorders: schizophrenia, autism, and Alzheimer’s disease. Importantly, we show that learnt dy 1"
1912.0313,"data, dataset, used dataset",70,2022-05-13,1,"Next, we apply the same unsupervised pre-training method to brain imagining data. For encoder pre-training, we use HCP [5] consortium dataset and apply the pre-trained encoder for further downstream tasks. We apply the same pre-trained encoder for three different types of diseases spanning four datasets to classify schizophrenia, autism and Alzheimer’s diseases. We use resting fMRI data for all brain"
1912.0313,database,11,2022-05-13,0,"L. Fei-Fei. Image Database. In CVPR09, 2009."
1912.0313,dataset,10,2022-05-13,0,"Learnt dynamics generalizes across tasks, datasets, and populations"
1912.0313,dataset,101,2022-05-13,0,"The dataset OASIS [51] has total 372 subjects with equal number (186) of HC and AZ patients. We use two holdout sets each of size 64 respectively for validation and test purpose. The remaining are used for supervised training. Unlike other datasets described earlier, it has only 120 time points though the number of non-noise componets is same (53) as other datasets. With 120 time points, we use six 53 × 20-sized non-overlapping windows for each subject. Refer to Figure 12 for the details of the experiments."
1912.0313,dataset,104,2022-05-13,0,"Figure 5. Left: For pre-training, we combine audio ﬁles from LibriSpeech dataset with background noise of coffee shop. T is the length of audio ﬁles which ranges from 1 to 20 seconds. Right: For training, we superimpose a T/2 length audio of word ""cat"" padded with T/2 zeros onto a background noise of coffee shop of length T (T = 2). For both pre-training and training, we calculate the mel-spectrogram of the combined audio ﬁles that results in a matrix of size components × time courses for each audio ﬁle."
1912.0313,dataset,111,2022-05-13,0,"First, we generate 50 VAR times series with size 10 × 20000. Then we split our dataset to 50 × 10 × 14000 samples for training, 50 × 10 × 4000 —for validation and 50 × 10 × 2000 — for testing. Using these samples, We pre-train an encoder to learn consecutive windows (positive examples) from the same VAR time series. As mentioned in Section 3.1.2, we also use autoencoder for pre-training the same encoder and show the effectiveness of ST-DIM to learn time-series dynamics in self-supervised manner. After pretraining, we use our pre-trained encoder for complete-time series classiﬁcation."
1912.0313,dataset,141,2022-05-13,0,"In most cases, due to practical reasons, researchers in brain imaging are constrained to work with small datasets. In addition, earlier work [26, 27] in brain imaging have been based on unsupervised methods to learn the dynamics and structure of the brain while supervised approaches are used to perform predictions at individual level. Such unsupervised methods include models as linear ICA [28], HMM framework [29]. Moreover, some other nonlinear approaches are also proposed to capture the dynamics. Examples include using Restricted Boltzman Machines (RBMs) [30], RNN modiﬁcation of ICA [31], and reconstructions by recurrent U-Net architecture [32]. In some cases, where dataset is very small, transfer learning as observed in some neuroimaging"
1912.0313,dataset,142,2022-05-13,0,"Let D = {(ut, vs) : 1 ≤ t, s ≤ N, t (cid:54)= s} be a dataset of pairs of values at time point t and s sampled from sequence with length N . Then D+ = {(ut, vs) : 1 ≤ t ≤ N − 1, s = t + 1} is called a dataset of positive pairs and D− = {(ut, vs) : 1 ≤ t, s ≤ N, s (cid:54)= t + 1} — of negative pairs. The dataset D+ refers to a joint distribution and D− — a marginal distribution. Eventually, the lower bound with InfoNCE estimator [37] If (D+) is deﬁned as:"
1912.0313,dataset,19,2022-05-13,0,[46] Pete Warden. Speech commands: A dataset for limited vocabulary speech recognition. 2018.
1912.0313,dataset,2,2022-05-13,0,4.4.1 Datasets
1912.0313,dataset,23,2022-05-13,1,"For schizophrenia classiﬁcation, we conduct experiments on two different datasets, FBIRN [48] and COBRE [49]. The"
1912.0313,dataset,30,2022-05-13,1,"Four datasets used in this study are collected from the FBIRN (Function Biomedical Informatics Research Network2) [48] project, from the COBRE (Center of Biomedical"
1912.0313,dataset,32,2022-05-13,0,"As we have demonstrated, self-supervised pre-training of a spatiotemporal encoder gives signiﬁcant improvement on the downstream tasks in both keyword detection and brain imaging datasets. Pre-training on fMRI of healthy"
1912.0313,dataset,34,2022-05-13,0,"Figure 11. AUC scores for all the three models on ABIDE dataset. Like experiments on FBIRN and COBRE, it is evident that the pre-trained models consistently perform better than NPT."
1912.0313,dataset,34,2022-05-13,0,"The dataset ABIDE has total 569 subjects, of which, 255 are HC and 314 are affected with autism. Like other datasets, each subject has 53 non-noise components with 140 time"
1912.0313,dataset,43,2022-05-13,0,"Our method is two fold. We ﬁrst pre-train our encoder on large unrelated dataset to learn improved representation of the latent factors, and then use the pre-trained encoder for downstream task. We explain both steps in the following sections."
1912.0313,dataset,49,2022-05-13,0,"Figure 10. AUC scores for all the three models on COBRE dataset. It is obvious that even with 15 subjects for training, FPT outperforms NPT noticeably, that is, the difference between two median AUC scores is remarkable ((cid:39) 0.15)."
1912.0313,dataset,51,2022-05-13,0,"Figure 9. AUC scores for all the three models (Refer to Figure 3) on FBIRN dataset. It is noticeable that even with only 15 subjects for supervised training, the median AUC scores of FPT and NPT differ by a large margin (10%)."
1912.0313,dataset,53,2022-05-13,0,"Figure 12. AUC scores for all the models on OASIS dataset. As we continue increasing the number of subjects, the pre-trained models start learning and thus improve their respective scores. However, notice that the NPT model even with 120 subjects didn’t signiﬁcantly improve its predictability."
1912.0313,dataset,56,2022-05-13,0,"Figure 4. Area Under Curve (AUC) scores for VAR vs. SVAR time-series classiﬁcation using ST-DIM and autoencoder based pre-training methods. ST-DIM based pre-training greatly improves the performance of downstream task with small datasets. On the other side, autoencoder based pre-training fails to learn dynamics and thus exhibits poor performance."
1912.0313,dataset,59,2022-05-13,0,"Out of all the available subjects, we select 416 which have large number of time points (N (cid:62) 20k). We use 300 subjects for training and 116 for validation. For pretraining, we use the algorithm as described in section 3.1.1 and achieve accuracy of ∼ 0.95 on the validation dataset."
1912.0313,dataset,61,2022-05-13,0,15255075100Number of Subjects Per Class0.500.550.600.650.700.750.80AUCFBIRNFPTUFPTNPT152540Number of Subjects Per Class0.40.50.60.70.8AUCCOBREFPTUFPTNPT15255075100150Number of Subjects Per Class0.350.400.450.500.550.600.650.70AUCABIDEFPTUFPTNPT7.29 years. Refer to Figure 7 for the demographic information of all the datasets. The dissimilarity in the age range is supposed to cause signiﬁcant difference between these two datasets as the structure of brain and thought process of children is obviously different than adults.
1912.0313,dataset,69,2022-05-13,0,"For each dataset, 100 ICA components as shown in Figure 8 are acquired using the same procedure described in [52]. However, only 53 non-noise components as determined per slice (time point) are used in pre-training of encoder and on downstream task. For experiments, including both pretraining and classiﬁcation the fMRI sequence is divided into windows of 20 time points."
1912.0313,dataset,89,2022-05-13,0,"Trained): The pre-trained encoder is not further trained on the dataset of downstream task, 2) UFPT (Unfrozen PreTrained): The pre-trained encoder is further trained on the dataset of downstream task and 3) NPT (Not Pre-trained): The encoder is not pre-trained at all and only trained on the small dataset of downstream task. The models are shown in Figure 3. In each experiment, we compare all three models to demonstrate the effectiveness of unsupervised pre-training."
1912.0313,dataset,94,2022-05-13,0,"To show the broad implications of unsupervised pretraining, we ﬁrst apply it to a simple problem of keyword detection in audio ﬁles. We choose this problem as it has many practical applications (e.g., virtual assistants in smart phones, robots). We use LibriSpeech ASR corpus [45] for pre-training and Speech Commands Dataset [46] for supervised training. The audio ﬁles of both datasets are combined with a background noise of coffee shop collected from [47] to make pre-training and classiﬁcation harder."
1912.0313,"dataset, used dataset",108,2022-05-13,0,"As seen in the ﬁgure, same pre-trained encoder performs reasonably better than NPT for autism vs. HC classiﬁcation and thus reinforces our hypothesis that unsupervised pretraining learns signal dynamics useful for downstream tasks. Note the difference between age ranges of ABIDE and HCP datasets. The age range of ABIDE is much lower than that of HCP dataset used for pre-training. HCP dataset contains subjects of different ages with means 30.01 and 28.48, medians 30 and 28, and standard deviations 3.522 and 3.665 years respectively for female and male, whereas ABIDE dataset has overall mean 17.04, median 15.40 and standard deviation"
1912.09621,"code, code available",88,2022-04-21,0,"Images from digital microscopy are captured under different illumination conditions. To aid our classification architecture, we preprocess the images using a color constancy technique [10, 11] to maintain the color contrast across all images. Later, we resize the images to match with the ResNet and GoogLeNet architectures. Figures 4 and 5 present the results obtained using the color constancy preprocessing for different images [10]. The code utilized for color constancy is available at [11]."
1912.09621,data,156,2022-05-13,0,"Computer-aided detection has been a research area attracting great interest in the past decade. Machine learning algorithms have been utilized extensively for this application as they provide a valuable second opinion to the doctors. Despite several machine learning models being available for medical imaging applications, not many have been implemented in the real-world due to the uninterpretable nature of the decisions made by the network. In this paper, we investigate the results provided by deep neural networks for the detection of malaria, diabetic retinopathy, brain tumor, and tuberculosis in different imaging modalities. We visualize the class activation mappings for all the applications in order to enhance the understanding of these networks. This type of visualization, along with the corresponding network performance metrics, would aid the data science experts in better understanding of their models as well as assisting doctors in their decision-making process."
1912.09621,data,206,2022-05-13,0,"This paper describes a study that supports the apprehension of the results predicted by deep neural networks applied towards medical imaging analysis. Several machine learning and deep learning architectures have been proposed in the literature for automated Computer-Aided Detection (CAD) tools for various applications [1-4]. In the past few years, deep learning networks have been used widely in medical imaging applications [1, 3]. Residual Networks (ResNet) [5] and GoogLeNet [6] are some of the most popular networks used in this field. The availability of a vast variety of networks raises the question of choosing the optimal network for a given disease/condition. In a data science perspective, optimal results could be measured in terms of overall accuracy, confusion matrix, precision, recall, Receiver Operating Characteristics (ROC) curve, or any other performance metric. However, these optimal results might not be satisfactory for the doctors if the results are not interpretable. Determining the Region of Interest (ROI) that contributed to the decision making of the network will enhance the understanding for both data science experts and clinicians."
1912.09621,"data available, dataset",3,2022-05-13,0,5.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,2.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,3.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,4.1. Dataset
1912.09621,"data, dataset",213,2022-05-13,0,"In addition, we have presented a comprehensive study of these algorithms based on their CAM results. This type of CAD system would let the medical expert analyst choose the algorithm based on their discretion in terms of CAM results, overall accuracy, AUC, or any other performance metric along with computation time and memory consumption. CAM results could be utilized by data science experts to further optimize their respective models in terms of architecture and/or preprocessing techniques. This type of CAM study would also assist the researchers in understanding the discriminative regions determined by various architectures in different imaging modalities. For instance, if an architecture achieves a high CAD detection accuracy for a particular dataset but the discriminative region determined using CAM is present in an unrelated area, performance metrics might mislead both data science researchers and medical expert analysts. Hence, CAM visualization would provide the necessary documentation for the medical expert analysts to enhance the trust in CAD system. This research can be further extended by studying the ROI determined by CAM by changing different hyper-parameters and how they evolve throughout the epochs. Another way to extend this research is by fusing CAM regions determined using different architectures."
1912.09621,"data, dataset",59,2022-05-13,0,"For this application, we perform a hold-out validation study. We split the dataset into groups of 80% and 20% for training and testing respectively. We utilize a subset of 10% from our training data for validation purpose in order to fine-tune our hyperparameters. Table 1 presents the distribution of the dataset."
1912.09621,"data, dataset, publicly available",98,2022-05-13,1,"To maintain the homogeneity across all these applications, we solely study the performance of transfer learning approaches using GoogLeNet and ResNet. Figure 1 presents the top-level block diagram of the transfer learning methodology adopted in this study. We implement these techniques for publicly available datasets thereby setting a new benchmark for each application. Results presented for the publicly available datasets would grant the capability for researchers to replicate and enhance them further. This type of analysis would assist the doctors and data science experts in selecting the optimal model of their choice."
1912.09621,dataset,115,2022-05-13,0,"For this application, due to the limited availability of images, we perform a 10-fold validation study. We believe 10-fold validation would give a better estimate of our performance. We train 10 different networks based on 9 folds and test on the remaining fold in each iteration. For each fold, we train and tune our hyper-parameters solely based on the images from training fold. We make sure to exclude the testing fold in any manner to conduct a rigorous study. Note that we utilize the same set of cases in each fold for the architectures implemented. Overall distribution of the dataset is presented in Table 5."
1912.09621,dataset,135,2022-05-13,0,"Figures 21 and 22 present the CAM results obtained for two different cases from the brain tumor dataset. Figure 21 presents the results for the case marked as ‘positive brain tumor’ by the trained clinician and our algorithm accurately predicts the same. The discriminative region is near the tumor portion which would help the doctors pinpoint important features, spatially. Figure 22 presents the results for the case marked as ‘negative brain tumor’ and the visualization behind algorithm’s prediction. CAM visualization for 26 (one test fold) different cases using our approach is available at [17]. This type of automated CAD technology for brain tumor detection would assist the doctors in providing a valuable second opinion and enhancing their workflow."
1912.09621,dataset,147,2022-05-13,0,"In this section, we study the CAM results obtained using GoogLeNet and ResNet. Figures 8 and 9 present the CAM results obtained for two different cases from the malaria dataset using our proposed approach. Figure 8 presents the results for the case marked as parasitized by the expert reader, and our algorithm not only accurately predicts the same but also presents the discriminative region that contributed the most to its decision. The ROI is around the red spot containing plasmodium. Figure 9 presents the results for the case marked as uninfected. CAM visualization for 100 different cases using our presented approach is available at [12]. Typically, ResNet CAM converges to a smaller ROI in comparison to GoogLeNet. This type of automated CAD technology for malaria detection would assist the microscopists and enhance their workflow."
1912.09621,dataset,155,2022-05-13,1,"Figures 14 and 15 present the CAM results obtained for two different test cases from the APTOS dataset using our proposed approach. Figure 14 presents the results for a case marked as ‘positive DR’ by the expert clinicians. It is also interesting to note that CAM results presented by GoogLeNet and ResNet for DR detection have minimal intersection despite exhibiting similar performance, which could affect an expert clinician’s choice of network. Figure 15 presents the results for a case marked as ‘negative DR’ by an expert clinician and the discriminative region for this case is near the retinal portion. CAM visualization for 60 different cases using our presented approach is available at [15]. This type of automated CAD technology for DR detection could be implemented for immediate solutions and could be applied in places with scarcity of such expert clinicians."
1912.09621,dataset,3,2022-05-13,0,Type of Dataset
1912.09621,dataset,49,2022-05-13,0,"Similar to brain tumor detection, due to the limited availability of images, we perform a 10-fold validation study. Overall distribution of the dataset is presented in Table 7. There is no additional preprocessing except converting these images into the input size for the network."
1912.09621,dataset,6,2022-05-13,0,Table 7: Tuberculosis dataset distribution
1912.09621,dataset,66,2022-05-13,0,"Similar to malaria detection (Section 2), we perform a hold-out validation study. We split the dataset into groups of 72%, 8%, and 20% for training, validation, and testing respectively. Table 3 presents the distribution of each dataset. There is no preprocessing except converting these images into the input size of the network."
1912.09621,dataset,67,2022-05-13,0,"The remainder of this paper is organized as follows. Sections 2-5 present the results obtained for CAD of malaria, DR, brain tumor, and tuberculosis respectively. In each of these sections, we describe the dataset along with the experimental results obtained in terms of both performance metrics and CAM results. Finally, discussions and conclusions are offered in Section 6."
1912.09621,dataset,7,2022-05-13,0,Table 1: Malaria dataset distribution.
1912.09621,dataset,7,2022-05-13,0,Table 3: DR dataset distribution.
1912.09621,dataset,8,2022-05-13,0,Table 5: Brain Tumor dataset distribution.
1912.09621,dataset,92,2022-05-13,0,"MRI scans contain text information for some cases, which are not essential for classification and might mislead our deep neural networks. Hence, we preprocess MRI scans by cropping ROI of brain and removing any additional text from the image using simple morphological operations. In addition, we perform histogram equalization to enhance and maintain the contrast across the dataset. Later, we resize the images to match with the input of ResNet and GoogLeNet architectures. Figure 18 presents the results obtained using these preprocessing techniques."
1912.09621,"dataset, data https",17,2022-05-13,1,"[16] Brain Tumor Dataset, https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumordetection, Accessed December 7, 2019."
1912.09621,"dataset, data https",18,2022-05-13,1,"[13] Kaggle Diabetic Retinopathy Dataset, https://www.kaggle.com/c/diabetic-retinopathydetection/overview, Accessed December 7, 2019."
1912.09621,"dataset, data https",21,2022-05-13,1,"[8] Malaria Dataset, National Institutes of Health, https://ceb.nlm.nih.gov/repositories/malariadatasets/ , Accessed December 8, 2019."
1912.09621,"dataset, publicly available, dataset provided, used dataset",48,2022-05-13,1,We make use of the publicly available Shenzhen dataset [19] provided for the classification of chest radiographs. This dataset contains a total of 662 images. Figures 23 and 24 present sample images marked as ‘Normal’ and ‘Tuberculosis’ by radiologists.
1912.09621,"dataset, publicly available, used dataset",136,2022-05-13,1,"We make use of a publicly available dataset provided by the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 on Kaggle [14] to detect DR in retinal images. In this dataset, 3662 retinal images are graded by expert clinicians at Aravind Eye Hospital, India into 5 different categories: (i) Negative DR, (ii) Mild DR, (iii) Moderate DR, (iv) Proliferative DR, and (v) Severe DR. In this research, we solely focus on the detection of DR, hence, we merge all the mild, moderate, proliferative and severe cases into a single category ‘positive DR’. Figures 10 and 11 present sample images marked in different categories by expert clinicians."
1912.09621,"dataset, publicly available, used dataset",68,2022-05-13,1,We make use of a publicly available dataset provided on Kaggle [16] for the classification of MRI scans. The dataset contains a total of 253 images classified into two different categories ‘positive’ and ‘negative’ brain tumor. Figures 16 and 17 present sample images marked as ‘positive brain tumor’ and ‘negative brain tumor’ by trained clinicians.
1912.09621,"dataset, publicly available, used dataset",84,2022-05-13,1,"We make use of a publicly available dataset provided by the National Institutes of Health (NIH) for the classification of cell images [8, 9]. Any cell image that contains plasmodium is marked as ‘parasitized’ by expert analysts and ‘uninfected’ otherwise. The dataset contains a total of 27,558 images with equal distribution of parasitized and uninfected cells. Figures 2 and 3 present sample images marked as parasitized and uninfected by expert readers."
2002.00512,code,55,2022-05-13,0,"[32] M. Torrent, F. Jollet, F. Bottin, G. Zérah;, and X. Gonze, Implementation of the projector augmentedwave method in the ABINIT code: Application to the study of iron under pressure, Computational Materials Science, 42 (2008), pp. 337 – 351."
2002.00512,code,94,2022-05-13,0,"The numerical results using a Julia [1] homemade code are summarized in the following ﬁgures with Z = 3, R = 1 and L = 5. The atomic PAW function φk are the eigenfunctions of the hydrogenoid atom. For the pseudo atomic function (cid:101)φk, continuity of the function and of the ﬁrst four derivatives are enforced (i.e. d = 5). The lowest eigenvalue is computed using a conjugate-gradient algorithm stopped when the norm of the residual is less than 10−5."
2002.00512,data,43,2022-05-13,0,"[17] F. Jollet, M. Torrent, and N. Holzwarth, Generation of Projector Augmented-Wave atomic data: A 71 element validated table in the XML format, Computer Physics Communications, 185 (2014), pp. 1246–1254."
2002.04279,code,15,2022-05-13,0,"● Software that checks text-based documents, source code, or both (Chowdhury &"
2002.04279,data,126,2022-05-13,0,"Since the analysis and interpretation of the data are quite sensitive, we approached this period with the utmost care. As suggested by Guba and Lincoln (1989), member check is an effective technique for establishing the trustworthiness criteria in qualitative studies. Therefore, having analyzed and reported the data, we sent a preprint of the results to the vendors. Team members closely evaluated the issues raised by the vendors. Not all of them were able to be addressed in this paper, but as many as possible were incorporated. Because of the rigorous efforts to establish the validity of the results and the reliability of the study in this process, this study was further delayed."
2002.04279,data,18,2022-05-13,0,"Journal of Data Mining and Knowledge Discovery, 2(2), 50–53. doi: 10.11648/j.ajdmkd.20170202.12"
2002.04279,data,21,2022-05-13,0,This research did not receive any external funding. HTW Berlin provided funding for openly publishing the data and materials.
2002.04279,data,36,2022-05-13,0,"5. Because of European data privacy laws, for higher education institutions in the EU it must be certain that the companies are only using servers in the EU if they are storing material."
2002.04279,data,5,2022-05-13,0,Availability of data and materials
2002.04279,data,64,2022-05-13,0,"Sorokina, D., Gehrke, J., Warner, S., & Ginsparg, P. (2006, December). Plagiarism detection in arXiv. In J. Liu & B. W. Wah (Eds.), Proceedings of the Sixth International Conference on Data Mining (ICDM'06). Hong Kong. (pp. 1070–1075). doi: 10.1109/ICDM.2006.147"
2002.04279,"data, data available",125,2022-05-13,0,"Our testing took place between Nov 2018 and May 2019. During this time, we tested both coverage and usability. An additional test of multi-source document took place between August and November 2019. Since the present research did not benefit from any funding, the researchers were expected to fulfill their institutional workloads during the research period. Considering the size of the project team from various countries, we could make significant progress only during semester breaks, which explains the length of the testing process. It should be noted that we tested what the systems offered at the time of data collection. We used features that were allowed by the access given to us by the vendors."
2002.04279,"data, publicly available, data available",14,2022-05-13,1,Data and materials used in this project are publicly available from http://www.academicintegrity.eu/wp/wg-testing/
2002.04279,database,105,2022-05-13,0,"As has been shown in other investigations (Weber-Wulff et al., 2013) translation plagiarism is very seldom picked up by software systems. The worst performance of the systems in this test was indeed the translation plagiarism, with one notable exception—Akademia. This system is the only one that performs semantic analysis and allows users to choose the translation language. Unfortunately, their database—with respect to the languages of our testing—is much smaller than the database of other systems. However, the performance drop between copypaste and translation plagiarism is much smaller for Akademia than for the other systems."
2002.04279,database,107,2022-05-13,0,"PlagScan presents itself as a plagiarism checker. It is operated by the German company PlagScan GmbH and was launched in 2009. They state that they have more than 1,500 organizations as customers. Although they focus on higher education, high schools, and businesses, PlagScan is also available for single users. They search the internet using MS Bing, published academic articles, their so-called “Plagiarism Prevention Pool”, and optionally a customer’s own database. PlagScan offers multiple pricing plans for each type of customer, there are apparently also now options for a free trial."
2002.04279,database,108,2022-05-13,0,"One aspect of Wikipedia sources that is not adequately addressed by the text-matching software systems is the proliferation of Wikipedia copies on the internet. As discussed in Weber-Wulff et al. (2013), this can lead to the appearance of many smallish text matches instead of one large one. In particular, this can happen if the copy of the ever-changing Wikipedia in the database of the software system is relatively old and the copies on the internet are from newer versions. A careless teacher may draw false conclusions if they focus only on the quantity of Wikipedia similarities in the report."
2002.04279,database,13,2022-05-13,0,"2. Text-matching systems that maintain a database of potential sources, employ"
2002.04279,database,139,2022-05-13,0,"The Croatian researchers Birkić, Celjak, Cundeković, and Rako (2016) tested four tools that are widely used in Europe and have the possibility to be used at the national and institutional level. They compared such criteria as the existence of an API (application programming interface) and the possibility to integrate it as a plug-in for learning management systems, database scope, size of the user community, and other criteria. The researchers tested the tools using two papers for each type of submission: journal articles, conference papers, master’s and doctoral theses, and student papers. However, they did not include different types of plagiarism and evaluated the checking process with a focus on quote recognition, tool limitations, and interface intuitiveness."
2002.04279,database,151,2022-05-13,0,"Turnitin was founded in 1999 by four students and grew to be an internationally known company. In 2014, they acquired the Dutch system Ephorus and “joined forces” (Ephorus, 2015). In 2019 they themselves were taken over by a US investment company, Advance (Turnitin, 2019). With a focus on institutional users only, they are used by 15,000 institutions in 150 countries. Turnitin uses its own crawler to search the web including also an archive of all previously indexed web pages (Turnitin, n.d.). Turnitin further compares the texts against published academic articles, as well as their own database of all assignments which have ever been submitted to the system, and optionally institutional databases. They are also developing many additional software tools for educators to use in teaching and giving feedback."
2002.04279,database,156,2022-05-13,0,"Luparenko (2014) tested 22 tools that were selected as popular ones based on an analysis of scientific literature and web sources. She considered many criteria related to functional specification (such as type, availability of free trial mode, need for mandatory registration at a website, number of users that have access to the program, database, acceptable file formats, etc.) and also checked the performance of the tools using one scientific paper in the Ukrainian language and another one in English. Moreover, the checking was done using three different methods: entering the text in the field of website, uploading a file, and submitting the URL of the article. She measured the checking time and evaluated the quality of the report provided by tools, as well as reported the percentage of unique text found in each of the articles."
2002.04279,database,166,2022-05-13,0,"As for the general testing, the results are highly consistent with the Wikipedia results which contributes the validity of the single-source and multi-source testing. Again, in single-source documents, Urkund obtained the highest score, while PlagAware is the best performing system in multi-source documents. Dupli Checker, DPV and intihal.net obtained the least scores in both categories. Most of the systems demonstrated better performance for multi-source documents than for single-source ones. This is most probably explained by the chances the systems had for having access to a source. If one source was missing in the tool’s database, it had no chance to identify the text match. The use of multiple sources gave the tools multiple chances of identifying at least one of the sources. This points out quite clearly the issue of false negatives: even if a text-matching tool does not identify a source, the text can still be plagiarized."
2002.04279,database,249,2022-05-13,0,"Viper presents itself as a plagiarism checker. It was founded in 2007. Viper focuses on all types of customers; the pricing is based on the pay-as-you-go principle. Currently, it is owned by All Answers Limited (2019), which according to the information at the website, gives an impression of an essay mill. It is interesting to see the progress in the way Viper uses the uploaded content on their “Terms and conditions” page. In 2016 the page stated ""[w]hen you scan a document, you agree that 9 months after completion of your scan, we will automatically upload your essay to our student essays database which will appear on one of our network of websites so that other students may use it to help them write their own essays"" (Viper, 2016). The time span was shortened to 3 months some time afterwards (Viper, 2019a). These paragraphs have been removed from the current version of the page (Viper, 2019b). On a different page, it is noted that ""[w]hen you scan your work for plagiarism using Viper Premium it will never be published on any of our study sites"" (Viper, 2019c). In e-mail communication, Viper claims that they are not using any essay without the author's explicit consent."
2002.04279,database,25,2022-05-13,0,Innovation Centre Kosovo (2018). An Albanian Academic Database and a Qualitative AntiPlagiarism System created by Akademia. Retrieved from https://ickosovo.com/news/post/an-albanian-academic-database-and-a-qualitative-antiplagiarism-system-crea
2002.04279,database,90,2022-05-13,0,"Source-based coverage testing was made using four types of sources; Wikipedia, openaccess papers, a student thesis and online articles. For many students, Wikipedia is the starting point for research (Howard & Davies, 2009), and thus can be regarded as one of the primary sources for plagiarists. Since a Wikipedia database is freely available, it is expected that Wikipedia texts should easily be identifiable. Testing the tools with Wikipedia texts demonstrates the fundamental ability to catch text matches."
2002.04279,database,92,2022-05-13,0,"Copyscape declares itself to be a plagiarism checker. The primary aim is to provide a tool for owners of websites to check if their original content was not used by others. They also provide a service of regular checks and email alerts. Copyscape, which started in 2004 (Greenspan, 2019), is operated by a private company, Indigo Stream Technologies Ltd., which is apparently based in Gibraltar. It does not have its own database but uses Google services to crawl the web."
2002.04279,download,138,2022-05-13,0,"The presentation and understandability of the results reported by the systems were evaluated in a second usability criteria group. Since the systems cannot determine plagiarism, the results must be examined by one or more persons in order to determine if plagiarism is present and a sanction warranted. It must be necessary to download the result reports and to be able to locate them again in the system. Some systems rename the documents, assigning internal numbering to them, which makes it extremely difficult to find the report again. Many systems have different formats for online and downloadable reports. It would be useful for the report review if the system kept the original formatting and page numbers of the document being analyzed in order to ease the load of evaluation."
2002.04279,download,88,2022-05-13,0,"None of the systems was able to get the highest score in the usability group related to the test results. Two systems (PlagScan and Urkund) support almost all features, but six systems support half or fewer features. The most supported features are the possibility to download result reports and highlighting matched passages in the online report. Less supported features are a side-by-side demonstration of evidence in the downloaded report and in the online report, as well as keeping document formatting."
2002.04279,publicly available,11,2022-05-13,0,4–5 pages from a publicly available source in the given language
2002.04279,publicly available,125,2022-05-13,0,"Table 2 shows the aggregated results of the language comparisons based on the language sets. It can be seen that most of the systems performed better for English, Italian, Spanish, and German, whereas the results for Latvian, Slovak, Czech, and Turkish languages are poorer in general. The only system which found a Czech student thesis from 2010 which is publicly available from a university webpage, was StrikePlagiarism.com. The Slovak paper in an open-access journal was not found by any of the systems. Urkund was the only system that found an open-access book in Turkish. It is worth noting that a Turkish system, intihal.net, did not find this Turkish source."
2002.04279,publicly available,16,2022-05-13,0,● 4–5 pages from any publicly available source in a given language with 1/3 copy &
2005.10539,data,109,2022-05-13,0,"The second approach was intended to increase the coordination between each instrument, by combining time (i. e., horizontal) and harmony (i. e., vertically) information. This was achieved by changing the data extraction phase from obtaining separately each complete instrument part to obtain every instrument part at every beat. This allowed us to train a set of instruments at the same time from a concrete set of symphonies. This way, the generated parts present a considerable increment of coordination and it is easier to differentiate each musical phrase, as each instrument part respects or accompanies the others."
2005.10539,data,131,2022-05-13,0,"The goal of this work is to generate music, based on Beethoven’s compositional model, obtaining the conductor’s score with all the orchestra instrument’s parts. Two approaches to the goal were established; ﬁrstly, the system was trained with each instrument individually, to generate all the different instrument’s parts, and then put them all together in a conductor’s score. Nevertheless training each part individually lead to a lack of coordination, so a new approach was addressed. This second approach consisted in training the system with information from different instruments at the same time, extracting the data vertically, to maintain the harmony (vertical) and time (horizontal) information."
2005.10539,data,135,2022-05-13,0,"Introduction Romantic composer Ludwig van Beethoven wrote his Symphonies from 1799 to 1824, when he ﬁnished the No. 9 (Cooper 2000). Although there is no constancy of the existence of the 10th Symphony score, there exists some manuscripts found in Beethoven’s house after his death that are thought to be part of the upcoming Symphony. In 1988 Barry Cooper tried to ﬁnish it, building from 50 of those fragments the ﬁrst movement of the Symphony. Those manuscripts are kept in the museum dedicated to his life in his natal city, Bonn, although they can be seen online 1. The public manuscript is not easy to read and understand, so that existing data will not be used in this paper."
2005.10539,data,138,2022-05-13,0,"Ludwig van Beethoven composed his symphonies between 1799 and 1825, when he was writing his Tenth symphony. As we dispose of a great amount of data belonging to his work, the purpose of this paper is to investigate the possibility of extracting patterns on his compositional model from symbolic data and generate what would have been his last symphony, the Tenth. A neural network model has been built based on the Long Short-Therm Memory (LSTM) neural networks. After training the model, the generated music has been analysed by comparing the input data with the results, and establishing differences between the generated outputs based on the training data used to obtain them. The structure of the outputs strongly depends on the symphonies used to train the network."
2005.10539,data,147,2022-05-13,0,"Since the main goal of this paper is to obtain a score including every orchestra instrument’s part, the input ﬁles format were changed to symbolic data in mxl. This extension refers to a compressed music score, which Music21 easily processes. Mxl ﬁles are the compressed format of the so called MusicXML (Good 2001). In order to represent the output of the training, i.e. the weights of the different notes and durations, the model also returns a HDF5 ﬁle, i. e., Hierarchical Data Format version 5, commonly used to store big quantities of data. After the prediction process, given the obtained weights, Music21 allows us to generate the ﬁnal output in MIDI or MusicXML, formats accepted by Musescore (so the score can be visualised and played)."
2005.10539,data,153,2022-05-13,0,"Our ﬁnal network is composed 3 different types of layers. The most relevant ones are the LSTM layers, which take the sequences and return new ones. Then, the Dropout layers prevent overﬁtting, ignoring randomly selected neurons during the training, setting those inputs to 0. The Dense (Density) layer serves as a full connection mechanism. This layer is the last one, so the system returns the same number of outputs as the different numbers of tuples (note name, note duration) the input data had. Finally, the activation function used for every layer is set, and it determines how each node’s output is represented. In this case, the softmax function (i. e., linear activation) is used, allowing the output to be interpreted as a probability between 0 and 1."
2005.10539,data,160,2022-05-13,0,"Technical background Deep learning: LSTM Networks Included in the ﬁeld of Machine Learning, Deep Learning involves the use of artiﬁcial neural networks (Gulli and Pal 2017). There exists several types of neural networks, such as Deep Neural, Deep Brief and Recurrent Neural Networks (RNN). In this paper we work with the last ones, since we need to process sequential data, assuming that each event depends on previous ones. The most accurate RNN variant is the LSTM. As proved with Figure 1, we need the memory that this type of networks own. On it we ﬁnd the sequence F - F - F, a predictor without memory would return another F, although by learning from the notes before, it can extract that after three equal notes, it is probable that the upcoming note is two tones below the last one."
2005.10539,data,247,2022-05-13,0,"Proposed in 1997, LSTM neural networks can learn longterm dependencies, improving the cells or neurons in the RNN graph. They have the ability to connect previous knowledge to a present task. Each cell has memory, and it decides to store or forget a data based on a given priority (i. e., represented as weights), assigned by the algorithm after the learning process. Figure 3 shows a LSTM cell or neuron. The top line represents the ﬂow of the cell state, which can be altered up to three times. The ﬁrst layer, sigmoid (σ), takes information from the previous state and determines if it is useful or not, returning a value between 0 and 1. As it is shown with the vertical arrow, it directly affects to the ﬂow of the cell state. The second layer is composed of the combination of the sigmoid (σ) and tanh functions, which chooses the data to be updated from the previous state, and creates a vector of candidate values to be added to the current cell state. The ﬁnal sigmoid (σ) layer determines the output, by deciding which parts of the state are more relevant. Those will be combined with a tanh function, converting the current state into values between 1 and -1 (Gulli and Pal 2017)."
2005.10539,data,328,2022-05-13,0,"References [Biles 1994] Biles, J. 1994. Genjam: A genetic algorithm In Proc. of the International for generating jazz solos. Computer Music Conference, 131–137. Aarhus, Denmark: ICMC. [Cohen 1995] Cohen, H. 1995. The further exploits of aaron, painter. Stanford Humanities Review 4(2):141–158. [Colton 2012] Colton, S. 2012. The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04."
2005.10539,data,344,2022-05-13,0,"The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04. [Graves, Mohamed, and Hinton 2013] Graves, A.; Mohamed, A.; and Hinton, G. E. 2013. Speech recognition the with deep recurrent neural networks. International conference on acoustics, speech and signal processing, 6645–6649. Vancouver, Canada: IEEE. [Gulli and Pal 2017] Gulli, A., and Pal, S. 2017. Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow. Birmingham, UK: Packt Publishing Ltd."
2005.10539,data,347,2022-05-13,0,"2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor? AI Magazine 30(3):71–71. [Schwanauer and Levitt 1993] Schwanauer, S., and Levitt, D. 1993. Musact: A connectionist model of musical harmony. In Machine Models of Music. Cambridge, MS, USA: The MIT Press. 497–510. [Tao Li 2011] Tao Li, Mitsunori Ogihara, G. T. 2011. Music Data Mining. Boca Ratn, FL, USA: CRC Press. [Thiemel 2001] Thiemel, M. 2001. Dynamics. New Grove online."
2005.10539,data,35,2022-05-13,0,"Once the model is built and the input and output data are ready, it gets trained, generating an hdf5 ﬁle containing the weights (i. e., input notes’ priorities)."
2005.10539,data,35,2022-05-13,0,"Results The system output differs from the information given to the training, although once with the same trained data, the system predicts the same score, which denotes a lack of variability."
2005.10539,data,350,2022-05-13,0,"[Haynes and Cooke 2001] Haynes, B., and Cooke, P. 2001. Pitch. New Grove online. [Hiley 2001] Hiley, D. 2001. Clef (i). New Grove online. [Hiller and Isaacson 1958] Hiller, Jr., L. A., and Isaacson, L. M. 1958. Musical composition with a high-speed digital computer. J. Audio Eng. Soc 6(3):154–160. [Liang and Gotham 2017] Liang, F. T., and Gotham, M. e. a. 2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor?"
2005.10539,data,43,2022-05-13,0,"Training Finally, we can generate the training data (i. e., sequence input and output). By establishing a certain sequence length, the output for each input sequence will be the ﬁrst note that comes after that sequence."
2005.10539,data,76,2022-05-13,0,"For example, setting a sequence length equal to two, the ﬁrst stage of the system’s work ﬂow (i. e., data extraction) for the Figure 5 input would be the shown in Table 1. It is important to take into account that in case of establishing a big sequence length, the model may generalise, while setting a small sequence length may lead to an overlearning problem."
2005.10539,data,84,2022-05-13,0,"Data representation Several ways of representing the Beethoven Symphonies’ scores have been studied for this paper. Firstly, we used MIDI ﬁles as an input for our system, as it is a data ﬁle which contains information about the sounds: what note is played, when, and how long or loud. Figure 4 shows the problem we boarded with the MIDI input. Music21 was unable to differentiate between the different string instrument’s MIDI channels."
2005.10539,"data, dataset",137,2022-05-13,0,"This paper is structured as follows. Previous work on using Artiﬁcial Intelligence in computational creativity and concretely in music generation is exposed in the State of the art section. After that, musical deﬁnitions needed to understand requirements, limitations and characteristics found in the results are introduced. Later, the work developed for this paper is explained in detail, presenting the Deep Learning technique used, the needed toolkits, and how the data was represented. Then, the Music generation subsection is divided in: dataset creation, training, and prediction. The results section is focused on explaining the reason why the system returns a certain output when trained with a speciﬁc set of symphonies. The conclusions and future work are described in the last sections."
2005.10539,"data, dataset, python",314,2022-05-13,0,"Dataset creation As previously mentioned, the Beethoven symphonies have been converted to an mxl ﬁle, which constitutes the dataset or corpus that we have used to obtain the desired results. Also, the instrument or instruments with which the system works has to be established, so the Python module music21 can divide the mxl score into all the present instruments, and take only the desired parts. This way, in the ﬁrst approach, where the goal is to obtain each instrument’s part individually, the note names and durations are stored in an independent ﬁle, being the different tuples of note names and durations the training data. Nevertheless, as the second approach trains the model with a set of chosen instruments at the same time, we need to store, besides the note name and duration, the offset (i. e., time data relating to the moment in which the note is being played regarding the score) and the name of the instrument that plays it. The offset information will be used to sort the data. After making sure that the events are sorted in a time-line as they are in the original score, it can be removed from the dataset, in order to reduce the data dimensionality and to avoid an overlearning problem in the model. This way, the training data will be composed of the different tuples of note names, note durations and the instrument’s name playing it (introduced in the second approach). At this point, a dictionary to encode each data tuple as a number is created, so the neural network can work with it. This dictionary will be also used for the decoding phase, after the prediction."
2005.10539,"data, python, open-source",129,2022-05-13,0,"This project has been developed in Python. Data extracting and processing from the scores has been performed using the python’s library Music21 (Cuthbert and Ariza 2010), which allows parsing and generating scores in different formats. Furthermore, every musical action and representation that we needed to perform, was made possible using that library. For the Deep Learning engine we have used Keras 3 (Gulli and Pal 2017). Finally, in order to manage the score formats, Musescore 4 (i. e., open source program available for every platform) brought us the possibility to import and export the symphonies, so we could see the score and listen to it at the same time."
2005.10539,dataset,114,2022-05-13,0,"Prediction For this task, the network input is generated again, as in the previous process (see Table 1). Since it needs to work over the same model, it is created again, with the same parameters, but now, instead of training the model, it loads the generated weights from the previous process (i. e., the hdf5 ﬁle). It is important at this point that the network input shapes and the loaded weights have the same dimensions. Once the model is ready, the encoding dictionary built during the dataset creation phase is inverted, for decoding the prediction results."
2005.10539,python,123,2022-05-13,0,"In case of the input, reshaping into a 3 dimension matrix is needed so it is compatible with the LSTM layers, using Python’s numpy module. The ﬁrst dimension or shape of the network is the number of unique different sequences (i. e., sequence in in Table 1) obtained in the last step, the second one is the previously established sequence length and ﬁnally the last dimension is forced to be 1, so it has just one input information per sequence length. After that, the software normalises the input into sequential values, from 0 to 1. In case of the output, it is converted into a categorical model."
2010.02554,"code, github",45,2022-05-13,0,The dependencies of parameters in our Pytorch implementation (https://github.com/pmorenoz/RecyclableGP) are clearly shown and evident from the code structure oriented to objects. It is also amenable for the introduction of new covariance functions and more structured variational approximations if needed.
2010.02554,"code, publicly available, github, code available",11,2022-05-13,2,1The code is publicly available in the repository: github.com/pmorenoz/RecyclableGP/.
2010.02554,data,109,2022-05-13,0,"Stationarity and expressiveness. We assume that the non-linear function f is stationary across subsets of data. If this assumption is relaxed, some form of adaptation or forgetting should be included to match the local GPs. Other types of models can be considered for the ensemble, as for instance, with several latent functions (Lázaro-Gredilla and Titsias, 2011) or sparse multi-output GPs (Álvarez and Lawrence, 2011). The model also accepts GPs with increased expressiveness. For example, to get multi-modal likelihoods, we can use mixture of GP experts (Rasmussen and Ghahramani, 2002)."
2010.02554,data,146,2022-05-13,0,"The data D is assumed to be partitioned into an arbitrary number of K subsets that we aim to observe and process independently, that is, {D1, D2, . . . , DK}. There is not any restriction on the amount of subsets or learning nodes. The subsets {Dk}K k=1 do not need to have the same size, and we only restrict them to be Nk<N . However, since we treat with a huge number of observations, we still consider that Nk for all k ∈ {1, 2, . . . , K} is sufﬁciently large for not accepting exact GP inference due to temporal and computational demand. Notice that k is an index while k(·, ·) refers to the kernel."
2010.02554,data,151,2022-05-13,0,"where, once again, φ∗ = {µ∗, S∗} are the global variational parameters that we aim to learn. One important detail of the sum of expectations in (16) is that it works as an average contrastive indicator that measures how well the global q(u∗) is being ﬁtted to the local experts qk(uk). Without the need of revisiting any distributed subset of data samples, the GP predictive qC(uk) is playing a different role in contrast with the usual one. Typically, we assume the approximate posterior ﬁxed and ﬁtted, and we evaluate its performance on some test data points. In this case, it goes in the opposite way, the approximate variational distribution is unﬁxed, and it is instead evaluated over each k-th local subset of inducing-inputs Zk."
2010.02554,data,161,2022-05-13,0,"To obtain multiple independent approximations to the posterior distribution p(f |D) of the GP function, we introduce K variational distributions qk(f ), one per distributed partition Dk. In particular, each variational distribution factorises as qk(f ) = p(f(cid:54)=uk |uk)qk(uk), with qk(uk) = N (uk|µk, Sk) and p(f(cid:54)=uk |uk) being the standard conditional GP prior distribution given the hyperparameters ψk of each k-th kernel. To ﬁt the local variational distributions qk(uk), we build lower bounds Lk on the marginal log-likelihood (ELBO) of every data partition Dk. Then, we use optimisation methods, typically gradient-based, to maximise the K objective functions Lk, one per distributed task, separately. Each local ELBO is obtained as follows"
2010.02554,data,173,2022-05-13,0,"Heterogeneous single-output GP. Extensions to GPs with heterogeneous likelihoods, that is, a mix of continuous and discrete variables yi, have been proposed for multi-output GPs (Moreno-Muñoz et al., 2018). However, there are no restrictions in our single-output model to accept different likelihoods p(yi|f (xi)) per data point {xi, yi}. An inconvenience of the bound in (1), is that, each i-th expectation term could be imbalanced with respect to the others. For example, if mixing Bernoulli and Gaussian variables, binary outputs could contribute more to the objective function than the rest, due to the dimensionality. To overcome this issue, we ﬁt a local GP model to each heterogeneous variable. We join all models together using the ensemble bound in (6) to propagate the uncertainty in a principled way. Although, data-types need to be known beforehand, perhaps as additional labels."
2010.02554,data,20,2022-05-13,0,DATA ST. (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55)
2010.02554,data,214,2022-05-13,0,"4.3 Heterogeneous tasks. We analysed how ensembles of recyclable GPs can be used if one of the local tasks is regression and the other a GP classiﬁer. (viii) London household data: We have two subsets of input-output variables: the binary contract of houses (leasehold vs. freehold) and the price per latitude-longitude coordinate in the London area. Three quadrants (Q) of the city {Q2, Q3, Q4} are trained with a GP classiﬁer and Q1 as regression. To clarify, Q1 is the right-upper corner given the central axes. Our purpose is to combine the local latent uk, learned with the binary data on {Q2, Q3, Q4} and the uk learned on Q1 via regression. Then, we search the global f to be predict with a Bernoulli likelihood in Q1. The ensemble shows a test NLPD of 7.94 ± 0.01 in classiﬁcation while the recyclable task predicts with an NLPD of 8.00 ± 0.01 in the Q1. We asses that the heterogeneous GP prediction is better in Q1 than the local GP classiﬁer. The mean GP of regression is passed through the sigmoid function to show the multimodality."
2010.02554,data,22,2022-05-13,0,where yt and ft are the true output target and function values. Nt is the number of test data points.
2010.02554,data,27,2022-05-13,0,"J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in Artiﬁcial Intelligence (UAI), pages"
2010.02554,data,290,2022-05-13,0,"A priori, the ensemble GP bound is agnostic with respect to the likelihood model. There is a general derivation in Matthews et al. (2016) of how stochastic processes and their integral operators are affected by projection functions, that is, different linking mappings of the function f (·) to the parameters θ. In such cases, the local lower bounds Lk in (1) might include expectation terms that are intractable. Since we build the framework to accept any possible data-type, we propose to solve the integrals via Gaussian-Hermite quadratures as in Hensman et al. (2015); Saul et al. (2016) and if this is not possible, an alternative would be to apply Monte-Carlo methods. Computational cost and connections. The computational cost of the local models is O(NkM 2 k ), while the global GP reduces to O(((cid:80) k Mk)M 2) and O(M 2) in training and prediction, respectively. The methods in Table 1 typically need O((cid:80) k ) for global prediction. A last theoretical aspect is the link between the global bound in (6) and the underlying idea in Tresp (2000); Deisenroth and Ng (2015). Distributed GP models are based on the application of CI to factorise the likelihood terms of subsets. To approximate the posterior predictive, they combine local estimates, divided by the GP prior. It is analogous to (6), but in the logarithmic plane and the variational inference setup."
2010.02554,data,3,2022-05-13,0,DATA SIZE →
2010.02554,data,339,2022-05-13,0,"All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−5, ηL = 10−7, ηψ = 10−8, ηZ = 10−7} and we used M = 25 inducing-inputs."
2010.02554,data,34,2022-05-13,0,"In our experiments with toy data, we used two versions of the same sinusoidal function, one of them with an incremental bias. The true expressions of f (·) are"
2010.02554,data,343,2022-05-13,0,"viii) London household data: Based on the large scale experiments in Hensman et al. (2013), we obtained the register of properties sold in the Greater London county during the 2017 year (https://www.gov.uk/government/collections/ price-paid-data). All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information."
2010.02554,data,349,2022-05-13,0,"In terms of distributed inference for scaling up computation, that is, the delivery of calculus operations across parallel nodes but not data or independent models, we are similar to Gal et al. (2014). Their approach can be understood as a speciﬁc case of our framework. Alternatively, if we look to the property of having nodes that contain usable GP models (Table 1), we are similar to Deisenroth and Ng (2015); Cao and Fleet (2014) and Tresp (2000), with the difference that we introduce variational approximation methods for non-Gaussian likelihoods. An important detail is that the idea of exploiting properties of full stochastic processes (Matthews et al., 2016) for substituting likelihood terms in a general bound has been previously considered in Bui et al. (2017) and Moreno-Muñoz et al. (2019). Whilst the work of Bui et al. (2017) ends in the derivation of expectation-propagation (EP) methods for streaming inference in GPs, the introduction of the reparameterisation of Gal et al. (2014) makes our inference and performance different from Moreno-Muñoz et al. (2019). There is also the inference framework of Bui et al. (2018) for both federated and continual learning, but focused on EP and the Bayesian approach of Nguyen et al. (2018). A short analysis of its application to GPs is included for continual learning settings but far from the large-scale scope of our paper. Moreover, the spirit of using inducing-points as pseudo-approximations of local subsets of data is shared with Bui and Turner (2014), that comments its potential application to distributed setups. More oriented to dynamical modular models, we ﬁnd the work by Velychko et al. (2018), whose factorisation across tasks is similar to Ng and Deisenroth (2014) but oriented to state-space models."
2010.02554,data,45,2022-05-13,0,"We highlight several use cases for the proposed framework. The idea of recycling GP models opens the door to multiple extensions, with particular attention to the local-global modelling of heterogeneous data problems and the adaptation of model complexity in a data-driven manner."
2010.02554,data,48,2022-05-13,0,"Then, if we have (10), which is the ﬁrst version of our ensemble lower bound LE , we can use the augmented likelihood term p(y|f∞) to introduce the local approximations to f instead of revisiting the data. This is,"
2010.02554,data,52,2022-05-13,0,"We introduced a novel framework for building global approximations from already ﬁtted GP models. Our main contribution is the construction of ensemble bounds that accept parameters from regression, classiﬁcation and heterogeneous GPs with different complexity without revisiting any data. We analysed its performance on synthetic and real data with"
2010.02554,data,67,2022-05-13,0,"Model recycling and use cases. The ability of recycling GPs in future global tasks have a signiﬁcant impact in behavioral applications, where ﬁtted private-owned models in smartphones can be shared for global predictions rather than data. Its application to medicine is also of high interest. If one has personalized GPs for patients, epidemiologic surveys can be built without centralising private data."
2010.02554,data,70,2022-05-13,0,"In this paper, we investigate a general framework for recycling distributed variational sparse approximations to GPs, illustrated in Figure 1. Based on the properties of the Kullback-Leibler divergence between stochastic processes (Matthews et al., 2016) and Bayesian inference, our method ensembles an arbitrary amount of variational GP models with different complexity, likelihood and location of pseudo-inputs, without revisiting any data."
2010.02554,data,8,2022-05-13,0,Appendix C. Combined Ensemble Bounds with Unseen Data
2010.02554,data,85,2022-05-13,0,"A common theme in the previous approaches is the idea of model memorising and recycling, i.e. using the already ﬁtted parameters in another problem or joining it with others for an additional global task without revisiting any data. If we look to the functional view of this idea, uncertainty is still much harder to be repurposed than parameters. This is the point where Gaussian process (GP) models (Rasmussen and Williams, 2006) play their role."
2010.02554,data,89,2022-05-13,0,"Data-driven complexity and recyclable ensembles. One of the main advantages of the recyclable GP framework is that it allows data-driven updates of the complexity. That is, if an ensemble ends in a new variational GP model, it also can be recycled. Hence, the number of global inducing-variables M can be iteratively increased conditioned to the amount of samples considered. A similar idea was already commented as an application of the sparse order-selection theorems by Burt et al. (2019)."
2010.02554,data,9,2022-05-13,0,"Data Privacy and Conﬁdentiality at NeurIPS, 2019."
2010.02554,data,9,2022-05-13,0,Figure 2: Recyclable GPs with synthetic data.
2010.02554,"data, code, data available",178,2022-05-13,2,"In this section, we evaluate the performance of our framework for multiple recyclable GP models and data access settings. To illustrate its usability, we present results in three different learning scenarios: i) regression, ii) classiﬁcation and iii) heterogeneous data. All experiments are numbered from one to nine in roman characters. Performance metrics are given in terms of the negative log-predictive density (NLPD), root mean square error (RMSE) and mean-absolute error (MAE). We provide Pytorch code that allows to easily learn the GP ensembles.1 It also includes the baseline methods. The syntax follows the spirit of providing a list of recyclable_models = [GP1, GP2, GP3], where each GPk contains exclusively parameters of the local approximations. Further details about initialization, optimization and metrics are in the appendix. Importantly, we remark that data is never revisited and its presence in the ensemble plots is just for clarity in the comprehension of results."
2010.02554,"data, code, python, github",121,2022-05-13,2,"The code for the experiments is written in Python 3.7 and uses the Pytorch syntax for the automatic differentiation of the probabilistic models. It can be found in the repository https://github.com/pmorenoz/RecyclableGP, where we also use the library GPy for some algebraic utilities. In this section, we provide a detailed description of the experiments and the data used, the initialization of both variational parameters and hyperparameters, the optimization algorithm for both the local and the global GP and the performance metrics included in the main manuscript, e.g. the negative log-predictive density (NLPD), the root mean square error (RMSE) and the mean absolute error (MAE)."
2010.02554,"data, data available",121,2022-05-13,0,"Local likelihood reconstruction. The augmented likelihood distribution is perhaps, the most important point of the derivation. It allows us to apply conditional independence (CI) between the subsets of distributed output targets. This gives a factorized term that we will later use for introducing the local variational experts in the bound, that is, log p(y|f∞) = (cid:80)K k=1 log p(yk|f∞). To avoid revisiting local likelihood terms, and hence, evaluating distributed subsets of data that might not be available, we use the Bayes theorem but conditioned to the inﬁnite-dimensional augmentation. It indicates that the local variational distributions can be approximated as"
2010.02554,"data, dataset",108,2022-05-13,0,"As we already mentioned in the manuscript, there might be scenarios where it could be not necessary to distribute the whole dataset D in K local tasks or, for instance, a new unseen subset k + 1 of observations might be available for processing. In such case, it is still possible to obtain a combined global solution that ﬁts both to the local GP approximations and the new data. For clarity on this point, we rewrite the principal steps of the ensemble bound derivation in section A but without substituting all the log-likelihood terms by its Bayesian approximation, that is"
2010.02554,"data, dataset",166,2022-05-13,0,"One of the most desirable properties for any modern machine learning method is the handling of very large datasets. Since this goal has been progressively achieved in the literature with scalable models, much attention is now paid to the notion of efﬁciency. For instance, in the way of accessing data. The fundamental assumption used to be that samples can be revisited without restrictions a priori. In practice, we encounter cases where the massive storage or data centralisation is not possible anymore for preserving the privacy of individuals, e.g. health and behavioral data. The mere limitation of data availability forces learning algorithms to derive new capabilities, such as i) distributing the data for federated learning (Smith et al., 2017), ii) observe streaming samples for continual learning (Goodfellow et al., 2014) and iii) limiting data exchange for private-owned models (Peterson et al., 2019)."
2010.02554,"data, dataset",230,2022-05-13,1,"v) Pixel-wise MNIST classiﬁcation: We took images of ones and zeros from the MNIST dataset. To simulate a pixel-wise unsupervised classiﬁcation problem, true labels of images were ignored. Instead, we threshold the pixels to be greater or smaller than 0.5, and labeled as yi = 0 or yi = 1. That is, we turned the grey-scaled values to a binary coding. Then, all pixels were described by a two-dimensional input in the range [−1.0, 1.0], that indicates the coordinate of each output datum. In the case of the zero image, we splitted the data in four areas, i.e. the four corners, as is shown in the subﬁgure (A) of Figure 4. Each one of the local tasks was initialized with an equally spaced grid of Mk = 16 inducing-inputs. The ensemble GP required M = 25 in the case of the number zero and M = 16 for the one. The plotted curves correspond to the test GP predictive posterior at the probit levels [0.2, 0.5, 0.8]. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,"data, dataset",311,2022-05-13,0,"1.1 Background. The ﬂexible nature of GP models for deﬁnining prior distributions over non-linear function spaces has made them a suitable alternative in many probabilistic regression and classiﬁcation problems. However, GP models are not immune to settings where the model needs to adapt to irregular ways of accessing the data, e.g. asynchronous observations or missings input areas. Such settings, together with GP model’s well-known computational cost for the exact solutions, typically O(N 3) where N is the data size, has motivated plenty of aproaches focused on parallelising inference. Regarding the task of distributing the computational load between learning agents, GP models have been inspired by local experts (Jacobs et al., 1991; Hinton, 2002). Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al."
2010.02554,"data, dataset",320,2022-05-13,0,"ii) Distributed GPs: In this second experiment, our goal is to compare the performance of the recyclable framework with the distributed GP methods in the literature (Tresp, 2000; Ng and Deisenroth, 2014; Cao and Fleet, 2014; Deisenroth and Ng, 2015). To do so, we begin by generating toy samples from the sinusoidal function f (x). The comparative experiment is divided in two parts, in one, we observe N = 103 and in the other, N = 104 input-output data points. In the ﬁrst case, we splitted the dataset D in K = 50 tasks with Nk = 200 and Mk = 3 per partition. Any of these distributed subsets were overlapping, and their corresponding input-spaces concatenated perfectly in the range x ∈ [0.0, 5.5]. For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm."
2010.02554,"data, dataset",322,2022-05-13,0,"Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al. (2014) presented a new re-parameterisation of the lower bounds that allows to distribute the computational load accross nodes, also applicable to GPs with stochastic variational inference (Hensman et al., 2013) and with non-Gaussian likelihoods (Hensman et al., 2015; Saul et al., 2016). Out of the sparse GP approach and more inspired in Tresp (2000) and product of experts (Bordley, 1982), the distributed GPs of Deisenroth and Ng (2015) scaled up the parallelisation mechanism of local experts to the range of N >106. Their approach is focused on exact GP regression, not considering classiﬁcation or other non-Gaussian likelihoods. Table 1 provides a description of these different methods and their main properties, also if each distributed node is a GP model itself."
2010.02554,"data, dataset",330,2022-05-13,0,"Ideally, to obtain a global inference solution given the GP models included in the dictionary, the resulting posterior distribution should be valid for all the local subsets of data. This is only possible if we consider the entire data set D in a maximum likelihood criterion setting. Speciﬁcally, our goal now is to obtain an approximate posterior q(f ) ≈ p(f |D) by maximising a lower bound LE under the log-marginal likelihood log p(D) without revisiting the data already observed by the local models. We begin by considering the full posterior distribution of the stochastic process, similarly as Burt et al. (2019) does for obtaining an upper bound on the KL divergence. The idea is to use inﬁnite-dimensional integral operators that were introduced by Matthews et al. (2016) in the context of variational inference, and previously by Seeger (2002) for standard GP error bounds. The use of the inﬁnite-dimensional integrals is equivalent to an augment-and-reduce strategy (Ruiz et al., 2018). It consists of two steps: i) we augment the model to accept the conditioning on the inﬁnite-dimensional stochastic process and ii) we use properties of Gaussian marginals to reduce the inﬁnite-dimensional integral operators to a ﬁnite amount of GP function values of interest. Similar strategies have been used in continual learning for GPs (Bui et al., 2017; Moreno-Muñoz et al., 2019). Global objective. The construction considered is as follows. We ﬁrst denote y as all the output targets {yi}N i=1in the dataset D and f∞ as the augmented inﬁnite-dimensional GP. Notice that f∞ contains all the function values taken by f (·), including that ones at {xi}N k=1 for all partitions. The augmented log-marginal expression is therefore"
2010.02554,"data, dataset",350,2022-05-13,0,"For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm. The setup of the VEM algorithm during the ensemble ﬁtting was {VE = 30, VM = 10, ηm = 10−3, ηL = 10−6, ηψ = 10−8, ηZ = 10−8}. As in the previous experiment with toy data, we set M = 35 inducing-inputs. iii) Recyclable ensembles: For simulating potential scenarios with at least N = 106 input-output data points, we used the setting of the previous experiment, but with K = 5 · 103 tasks of Nk = 800 instead. However, as explained in the paper, its performance was hard to evaluate in the baseline methods, due to the problem of combining bad-ﬁtted GP models. Then, based on the experiments of Deisenroth and Ng (2015) and the idea of building ensembles of ensembles, we set a pyramidal way for joining the distributed local GPs. It was formed by two layers, that is, we joined ensembles twice as shown in the Figure 5 of this appendix."
2010.02554,"data, dataset, data https, data available",327,2022-05-13,1,"The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g. K(cid:29)103, it is problematic for baseline methods, due to partitions must be revisited for building predictions and if one-of-many GP fails, performance decreases. Then, we repeated the experiment in a pyramidal way. That is, building ensembles of recyclable ensembles, inspired in Deisenroth and Ng (2015). Our method obtained {NLPD=4.15, RMSE=2.71, MAE=2.27}. The results in Table 3 indicate that our model is more robust under the concatenation of approximations rather than overlapping them in the input space. (iv) Solar physics dataset: We tested the framework on solar data (available at https://solarscience.msfc.nasa.gov/), which consists of more than N =103 monthly average estimates of the sunspot counting numbers from 1700 to 1995. We applied the mapping log(1 + yi) to the output targets for performing Gaussian regression. Metrics are provided in Table 2, where std. values were small, so we do not include them. The perfomance with 50 tasks is close to the baseline solutions, but without storing all distributed subsets of data."
2010.02554,"data, dataset, data https, data available",332,2022-05-13,0,"4.1 Regression. In our ﬁrst experiments for variational GP regression with distributed models, we provide both qualitative and quantitative results about the performance of recyclable ensembles. (i) Toy concatenation: In Figure 2, we show three of ﬁve tasks united in a new GP model. Tasks are GPs ﬁtted independently with Nk=500 synthetic data points and Mk=15 inducing variables per distributed task. The ensemble ﬁts a global variational solution of dimension M =35. Notice that the global variational GP tends to match the uncertainty of the local approximations. (ii) Distributed GPs: We provide error metrics for the recyclable GP framework compared with the state-of-the-art models in Table 3. The training data is synthetic and generated as a combination of sin(·) functions (in the appendix). For the case with 10K observations, we used K=50 tasks with Nk=200 data-points and Mk=3 inducing variables in the sparse GP. The scenario for 100K is similar but divided into K=250 tasks with Nk=400. Our method obtains better results than the exact distributed solutions due to the ensemble bound searches the average solution among all recyclable GPs. The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g."
2010.02554,"data, dataset, data https, used dataset",206,2022-05-13,1,"iv) Solar physics dataset: We used the solar physics dataset (https://solarscience.msfc.nasa.gov/) which consists of N = 3196 samples. Each input-output data point corresponds to the monthly average estimate of the sunspot counting numbers from 1700 to 1995. The output targets y were transformed to the real-valued domain via the mapping log(yi + 1) to use a normal likelihood distribution. We also scaled the input area to the range x ∈ [0, 100] and normalized the outputs to be zero-mean. The number of tasks was K = 50 and 20% of the data observations were reserved for test. The initial values of kernel and likelihood hyperparameters was {(cid:96) = 0.2, σ2 n is the initial likelihood variance, that we also learn. In this case, the setup of the VEM algorithm was {VE = 20, VM = 20, ηm = 10−5, ηL = 10−8, ηψ = 10−10, ηZ = 10−10}. The number of global inducing-inputs used for the ensemble was M = 90, whilst we used Mk = 6 for each distributed approximation."
2010.02554,"data, dataset, dataset provided, data available",109,2022-05-13,0,"In practice, it might not be necessary to distribute the whole dataset D in parallel Recyclable GPs and new data. tasks, with some subsets Dk available at the global ensemble. It is possible to combine the samples in Dk with the dictionary of local GP variational distributions. In such cases, we would only approximate the likelihood terms in (3) related to the distributed subsets of samples. The resulting combined bound would be equivalent to (6) with an additional expectation term on the new data. We provide the derivation of this combined bound in the supplementary material."
2010.02554,dataset,156,2022-05-13,0,"We consider a supervised learning problem, where we have an input-output training dataset D = {xi, yi}N i=1 with x ∈ Rp. We assume i.i.d. outputs yi, that can be either continuous or discrete variables. For convenience, we will refer to the likelihood term p(y|θ) as p(y|f ) where the generative parameters are linked via θ = f (x), being f (·) a non-linear function drawn from a zero-mean GP prior f ∼ GP(0, k(·, ·)), and k(·, ·) is the covariance function or kernel. Importantly, when non-Gaussian outputs are considered, the GP output function f (·) might need an extra deterministic mapping Φ(·) that transforms it to the appropriate parametric domain of θ."
2010.02554,dataset,20,2022-05-13,0,Table 2: Performance metrics for distributed GP regression with the solar physics dataset. (std. ×102)
2010.02554,dataset,205,2022-05-13,1,"vii) Banana dataset: The banana experiment is perhaps one of the most used datasets for testing GP classiﬁcation models. We followed a similar strategy as the one used in the MNIST experiment. After removing the 33% of samples for testing, we partitioned the input-area in four quadrants, i.e. as is shown in Figure 4. For each partition we set a grid of Mk = 9 inducing-inputs and later, the maximum complexity of the global sparse model was set to M = 25. The baseline GP classiﬁcation method also used M = 25 inducing-inputs and obtained an NLPD value of 7.29 ± 7.85 × 10−4 after ten trials with different initializations. Our method obtained a test NLPD of 7.21 ± 0.04. As we mentioned in the main manuscript, the difference is understandable as the recyclable GP framework used a total amount of 4 × 16 inducing-inputs, that capture more uncertainty than the 16 of the baseline method. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,dataset,97,2022-05-13,0,"The construction of ensemble variational bounds from recyclable GP models is based on the idea of augmenting the marginal likelihood to be conditioned on the inﬁnite-dimensional GP function f∞. Notice that f∞ contains all the function values taken by f (·) over the input-space Rp, including the input targets {xi}N k=1 and the global ones Z∗. Having K partitions of the dataset D with their corresponding outputs y = {y1, y2, . . . , yK }, we begin by augmenting the marginal log-likelihood as"
2010.02554,"dataset, used dataset",218,2022-05-13,1,"of Van der Wilk et al. (2017), we threshold images of zeros and ones to black and white pixels. Then, to simulate a pixel-wise learning scenario, we used each pixel as an input-output datum whose input xi contains the two coordinates (x1 and x2 axes). Plots in Figure 4 illustrate that a predictive ensemble can be built from smaller pieces of GP models, four corners in the case of the number zero and two for the number one. (v) Compositional number: As an illustration of potential applications of the recyclable GP approach, we build a number eight predictor using exclusively two subsets of the approximations learned in the previous experiment with the image of a zero. The trick is to shift Zk to place the local approximations in the desired position. (vi) Banana dataset: We used the popular dataset in sparse GP classiﬁcation for testing our method with M =25. We obtained a test NLPD= 7.21 ± 0.04, while the baseline variational GP test NLPD was 7.29 ± 7.85×10−4. The improvement is understandable as the total number of inducing points, including the local ones, is higher in the recyclable GP scenario."
2010.09647,code,123,2022-05-13,0,"ing unconstrained. The base measure problem occurs, in a sense, but the remedy is both local and internal to the implementation of Stan. The latter Stan pushes on the user: if a user writes a Stan model with a parameter x and a transformed parameter f (x), and wishes to code a density on f (x) that corresponds to the pushforward of some known density p(x), then that user must think about base measures and partial derivatives themselves. Stan does not help; but then again, it also does not claim it would help, so Stan doesn’t compute anything visibly “incorrect”."
2010.09647,code,89,2022-05-13,0,"We have named the Base Measure Problem and provided a solution to it. Implementing the solution in probabilistic programming systems should cause negligible loss of performance for cases that were already correctly handled, and expand the set of models in which the system can compute correct probability densities. Implementation does carry a code complexity cost, but that cost is minimized by using two-argument dispatch, or emulating it with a Visitor pattern. Despite correctly accounting for measures, no non-local information is required."
2010.09647,data,80,2022-05-13,0,"The root of the base measure problem is that we didn’t want to compute with measures directly, but lost the base measure when representing probability distributions with densities. It’s not actually possible to infer the correct base measure from the data type representing the sample: a point on the unit circle in R2 is represented with two ﬂoating-point numbers, but using Lebesgue measure on R2 as the base is not helpful."
2010.09647,package,88,2022-05-13,0,"The Jacobian-determinant correction 1/| det(Jfx)| accounts for the possibility that f changes the volume of an inﬁnitesimal volume element near x. The Jacobian determinant can be computed by forming the Jacobian of f , for example with automatic diﬀerentiation; but for many functions f , it’s available more eﬃciently. Thus a conventional choice is to package such f , together with their inverse f −1, in a Bijector class with a method for computing said Jacobian determinant."
2011.00242,code,100,2022-05-13,0,"Therefore, in all applications, there are indications, in the form of source code, comments, and issues, that developers demand an effort to understand and improve cache keys, reasoning about alternative better ways to organize and identify content. We identiﬁed common content properties used to build cache keys, which are: (i) content ids, (ii) method signatures, and (iii) a tag-based identiﬁcation, in which the type of content, the class, the module or hierarchy are used (Evidence 19)."
2011.00242,code,11,2022-05-13,0,Category (Acronym) Code Scattering and Tangling (CST)
2011.00242,code,12,2022-05-13,0,Complex Naming Conventions (CNC) Additional Caching Code (ACC)
2011.00242,code,120,2022-05-13,0,"Given that implementing cache is challenging, we noticed that in six applications developers made use of supporting libraries and frameworks. This was done to prevent adding much cache-related code to the base code, because such components raise the abstraction level of caching, providing some ready-to-use features (Evidence 22). Examples of such external components are distributed cache systems, e.g. Redis [27] and Memcached [26], and libraries that can act locally, e.g. Spring Caching [51], EhCache [52], Inﬁnispan [53], Rails low-level caching [54], Google Guava [55] and Caffeine [56]."
2011.00242,code,152,2022-05-13,0,"First, we assigned concepts to pieces of extracted text (open coding), each representing application-level caching characteristics. Figure 2 exempliﬁes different codes identiﬁed during the analysis of an application. For instance, Code Tangling is created from the observation of cache-related implementation spread all over the application base code (underlined). Then, for each new concept, we veriﬁed whether they are connected somehow with existing ones, in order to generate categories (selective coding). Thus, the name assigned to a particular category aims at representing, at a higher abstraction level, all concepts related to it. Regarding the Code Tangling example, if Code Scattered were found afterwards, we could establish a relationship between the former and the latter to create a category, given that both are related to lack of separation of concerns."
2011.00242,code,161,2022-05-13,0,"[7] M. P. Robillard, W. Coelho, and G. C. Murphy, “How effective developers investigate source code: An exploratory study,” IEEE Transactions on Software Engineering, vol. 30, no. 12, pp. 889–903, 2004. [Online]. Available: http://dx.doi.org/10.1109/TSE.2016. 2532873 J. Sillito, G. C. Murphy, “Asking and answering questions during a programming change task,” in IEEE Transactions on Software Engineering, vol. 34, [Online]. Available: no. 4. http://dx.doi.org/10.1109/TSE.2008.26 S. Nadi, T. Berger, C. K¨astner, and K. Czarnecki, “Where do conﬁguration constraints stem from? An extraction approach and an empirical study,” IEEE Transactions on Software Engineering, vol. 41, no. 8, pp. 820–841, aug 2015. [Online]. Available: http://dx.doi.org/10.1109/TSE.2015.2415793"
2011.00242,code,17,2022-05-13,0,"to implementation issues of application-level caching, by providing solutions and guidance at the code level."
2011.00242,code,19,2022-05-13,0,A code comment before an expensive operation: TODO cache the global properties to speed this up??
2011.00242,code,21,2022-05-13,0,Labels of Evidence Sources: SC-Source Code (without comments); COM-Code Comments; IS-Issues; DOC-Documentation; DEV-Developers.
2011.00242,code,23,2022-05-13,0,A code comment: Is hibernate taking care of caching and not hitting the db every time? (hopefully it is)
2011.00242,code,236,2022-05-13,0,"As can be seen in Figure 3, caching implementation and associated design decisions are much more discussed and revised by developers than maintenance decisions. Caching implementation, which is spread in the code and involves the choice for appropriate locations to add and remove elements from the cache, is error-prone and can compromise code legibility. Consequently, many issues are associated with bug ﬁxes, technological details and code refactorings. Moreover, despite being less frequent, caching design is time-consuming and challenging, given that it requires understanding of the application behavior, as well as limitations, conditions and restrictions of content being cached. In applications analyzed, the mean (M) and standard deviation (SD) values of cache-related implementation issues are M = 47.45% and SD = 5.76%, while design issues achieve M = 37.92% and SD = 7.11%. Finally, because ﬁnegrained conﬁgurations require empirical analysis such as cache proﬁling, and there is little evidence that this was performed in investigated applications, maintenance decisions often result in the choice for default settings. Consequently, a lower number of issues is associated with such decisions, speciﬁcally M = 14.61% and SD = 3.44%. This issue analysis allowed us to understand the aspects of caching that require more effort from developers."
2011.00242,code,30,2022-05-13,0,"Code comments. Given that caching is an orthogonal concern in the application, unrelated to the business logic, but interleaved with its code, code comments are often"
2011.00242,code,36,2022-05-13,0,"reasoning, time, and modiﬁcations to the code (Evidence 14). We discussed our ﬁndings regarding cache design decisions and we next discuss observations made associated with how to implement design decisions."
2011.00242,code,38,2022-05-13,0,"We performed an analysis of the issues available in issue platforms to investigate the primary sources of cacherelated problems, typically bugs, in the applications. Based on user messages, code reviews and commit messages that"
2011.00242,code,43,2022-05-13,0,"A code snippet exposing a test of caching logic: it ”can set and get false values when return cache nil” do @store.set :test, false expect(@store.get(:test)).to be false end"
2011.00242,code,46,2022-05-13,0,"Before addressing each of our research questions, we show an objective analysis of the impact of caching in the investigated applications by identifying all code and issues related to them. This gives a broad sense of how caching is implemented in target systems."
2011.00242,code,48,2022-05-13,0,"Source code. Application source code is our core source of information. Since we focus on application-level caching, our analysis is concentrated in the core of the application (i.e. the business logic), which is where the caching logic is typically implemented."
2011.00242,code,49,2022-05-13,0,A code snippet: cache id = ’objectmodel ’ . $entity defs[’classname’] . ’ ’ . (int)$id . ’ ’ . (int)$id shop . ’ ’ . (int)$id lang;
2011.00242,code,56,2022-05-13,0,"Keep the cache API simple. (Evidence 15), (Evidence 17), (Evidence 20) and (Evidence 22) Caching logic tends to be spread all over the application, and a good solution should be employed to avoid writing messy code at the cost of high maintenance efforts."
2011.00242,code,59,2022-05-13,0,"Furthermore, in cases where updates in the base code are not an option (due to time or technical restrictions), a transparent and automatic caching component can provide fast results. These solutions address layers before and after application boundaries, and require only a few adaptations to the application needs (Evidence 24)."
2011.00242,code,60,2022-05-13,0,"All these caching design options may become complex and difﬁcult to understand. Indeed, identifying caching opportunities and ensuring consistency can add much code and may not be trivial to implement and understand. Due to the nature of application-level caching, such logic is spread all over the system. We noticed in 90% of the applications"
2011.00242,code,60,2022-05-13,0,"Such supporting libraries and frameworks not only provide partial ready-to-use features but also reduce the amount of additional effort required to guarantee that the cache is working. We observed in all applications that the cache includes code dedicated to test, debug and conﬁgure cache components, which can be expensive in some scenarios (Evidence 23)."
2011.00242,code,63,2022-05-13,0,"A developer quote: At ﬁrst glance, the cache code hinders the understanding of the business logic. Also, the cache logic itself it is not easy to get. A bug report on an issue platform: When you import categories with a parent category which does not exist, it prevents from duplicate it because of the cache."
2011.00242,code,87,2022-05-13,0,"Although this problem is present in the code, there are indications that developers know about it and express they are willing to improve the provided solution. In order to reduce the impact of an infrastructure component to the system business logic, we identiﬁed cases where there are suggestions to design more extensible classes and modules, refactoring and reducing cache-related code, and reusing components (Evidence 17). This acknowledgment of technical debt was observed in 90% of the applications."
2011.00242,code,91,2022-05-13,0,"such as design patterns, third-party libraries or aspects? 22 How is the caching logic mixed with the application code? 23 Is this extra cache logic tested? 24 What is the required format to cache? 25 How are objects translated to the cache? 26 How are names (keys) deﬁned for cached objects? 27 Do developers use another caching layer besides application-level? 28 Is any transparent or automatic caching component being used? 29 Do developers rely on automatic caching components? 30"
2011.00242,code,96,2022-05-13,0,"Presence of complex constructs such as batch processing and asynchronous communication, which require extra effort and reasoning from developers to be implemented. Indication of use of third-party caching solutions to help the implementation, raising the level of abstraction of some caching aspects. Choice for complex keys of caching content, causing developers to spend time and effort to elaborate and understand such keys. the Code implemented to support caching logic, such as implemented caching tests, logic to monitor cache statistics and additional interfaces to support available caching providers."
2011.00242,"code, github",72,2022-05-13,0,"Issues. An issue can represent a software bug, a project task, a help-desk ticket, a leave request form, or even user messages about the project in general. Usually, changes in the code are due to registered issues. Thus, implementation and design decisions are better explained by associated issues in issue platforms, such as GitHub Issue Tracker, JIRA, and Bugzilla."
2011.00242,"code, open-source",68,2022-05-13,0,"To identify patterns of application-level caching adopted by developers and understand what kinds of caching implementations and decisions can be automatically inferred, characterize and evaluate application-level caching-related design and implementation from a perspective of the researcher as they are implemented in the source code and described in issues of web-based applications in the context of 10 software projects, obtained from open-source repositories and software companies."
2011.00242,data,102,2022-05-13,0,"The high number of occurrences related to ensuring consistency refers to the expiration process of cached content, which requires extra reasoning from developers, because they should track which changes cause data content to become outdated, and be aware for how long the cache can provide stale data, in case the data source has been updated. In fact, consistency approaches have been widely investigated [4], [6], and the typical way of dealing with it is to analyze data dependency, from which conditions and constraints for consistency can be derived."
2011.00242,data,106,2022-05-13,0,"Avoid caching per-user data. (Evidence 4) and (Evidence 26) It is recommended to avoid caching per-user data unless the user base is small and the total size of the cached data does not require an excessive amount of memory; otherwise, it can cause a memory bottleneck. However, if users tend to be active for a while and then go away again, caching per-user data for short-time periods may be an appropriate approach. For instance, a search engine that caches query results by each user, so that it can page through results efﬁciently."
2011.00242,data,107,2022-05-13,0,"Regarding implementation choices, we observed common practices. The ﬁrst is associated with how to name cached data. In order to use in-memory caching solutions, there is no prescribed way to organize data. Typically, unique names are assigned to each cached content, thus leading to a key-value model—and this was the case in all investigated applications. Given that cache stores lots of data, the set of possible names must be large; otherwise, two names (keys) can conﬂict with each other and, thus stale (or even entirely wrong) data can be retrieved from"
2011.00242,data,11,2022-05-13,0,RQ1. What and when is data cached at the application
2011.00242,data,110,2022-05-13,0,"Despite choosing where and what to cache, cached values are valid only as long as the sources do not change, and when sources change, a consistency policy should be employed to ensure that the application is not serving stale data. Therefore, in nine analyzed applications, there are indications that developers demand an effort to design consistency approaches, reasoning about the lifetime of cached data, as well as eviction conditions and constraints. We identiﬁed common approaches to keep consistency, which are: (i) a less efﬁcient and straightforward approach is to invalidate cached values based on mapping actions that"
2011.00242,data,127,2022-05-13,0,"The theoretical categories are described in Table 5, which presents their description, an original piece of content (example of evidence) and sources of data classiﬁed in each category. Moreover, categories are also shown in Figure 5a with the associated number of occurrences in the applications analyzed and classiﬁed according to each research question. Figure 5b shows the percentage of contribution of each application to the categories emerged from the study. These ﬁgures also present categories identiﬁed in RQ2 (described in Table 6) and RQ3 (described in Table 7), which are discussed in the following sections. Acronyms used in Figures 5a and 5b are introduced in Tables 5, 6, and 7."
2011.00242,data,13,2022-05-13,0,Is there a relationship between the data cached and the application domain?
2011.00242,data,13,2022-05-13,0,RQ1. What and when is data cached at the application level?
2011.00242,data,131,2022-05-13,0,"Do not discard small improvements. (Evidence 3), (Evidence 8) and (Evidence 9) The user perceived latency is reduced by any caching solution employed. This means that even not obvious scenarios should be target of caching, i.e. it is not true that solely data that is frequently used and expensive to retrieve or create should considered for caching. Furthermore, data that is expensive to retrieve and is modiﬁed on a periodic basis can still improve performance and scalability when properly managed. Caching data even for a few seconds can make a large difference in high volume sites. If the data is handled more often than it is updated, it is also a candidate for caching."
2011.00242,data,136,2022-05-13,0,"Following the introduced phases of data analysis, we performed mainly a subjective analysis of the data, collecting: (i) typical caching design, implementation and maintenance strategies; (ii) motivations, challenges and problems behind caching, and (iii) characteristics of caching decisions. These collected data were evaluated to conceptualize how the open codes were related to each other as a set of hypotheses in accounting for resolving the primary concern. Furthermore, we also made a broad analysis of the target systems in order to investigate how application-level caching was conceived in them. All the research phases were performed manually, as the collected data (most of them expressed in natural language) analysis is associated with the interpretation of caching approaches."
2011.00242,data,14,2022-05-13,0,4.2 RQ1: What and when is data cached at the application level?
2011.00242,data,141,2022-05-13,0,"7 DISCUSSION AND CONCLUSION Application-level caching has been increasingly used in the development of web applications, in order to improve their response time given that they are becoming more complex and dealing with larger amounts of data over time. Caching has been used in different locations, such as proxy servers, often as seamless components. Applicationlevel caching allows caching additional content taking into account application speciﬁcities not captured by off-theshelf components. However, there is limited guidance to design, implement and manage application-level caching, which is often implemented in an ad-hoc way. In this paper, we presented a qualitative study performed to understand how developers approach application-level caching. The study consisted of the selection ten web applications and investigation of caching-related aspects, namely design, implementation and maintenance practices."
2011.00242,data,141,2022-05-13,0,"performance via adaptive content the Proceedings of Computing and Applications. [Online]. Available: http://dx.doi.org/10.1109/NCA.2011.55 [20] G. Soundararajan and C. Amza, “Using semantic information to improve transparent query caching for dynamic content web sites,” in Proceedings of the International Workshop on Data Engineering Issues in E-Commerce, vol. 2005. IEEE, 2005, pp. 132– 138. [Online]. Available: http://dx.doi.org/10.1109/DEEC.2005.25 [21] C. Amza, G. Soundararajan, and E. Cecchet, “Transparent caching with strong consistency in dynamic content web sites,” in Proceedings of international conference on Supercomputing. New York, New York, USA: ACM Press, jun 2005, p. 264. [Online]. Available: http://dx.doi.org/10.1145/ 1088149.1088185"
2011.00242,data,150,2022-05-13,0,"From all analyzed applications, we observed that none of them use a proactive approach to cache content. Content is always cached after it requested (i.e. reactive approach) and, as a consequence, the ﬁrst request always results in a cache miss. Due to this, prefetching techniques can be used in order to populate the cache and prevent misses by predicting and caching data that will potentially be requested in the future. It can be based on heuristics, usage observations or even with the use of complex prediction algorithms [15], [32], [58], [59]. However, the design and implementation of a reactive cache component already requires signiﬁcant effort and reasoning to be properly done, and a proactive approach increases the complexity of the caching solution even more."
2011.00242,data,16,2022-05-13,0,Deﬁne naming conventions Perform cache actions asynchronously Do not use cache as data storage Perform measurements
2011.00242,data,18,2022-05-13,0,"Design of some kind of consistency approach such as expiration policies and invalidation, preventing stale data."
2011.00242,data,183,2022-05-13,0,"the content being cached should be considered when using size-limited caches. In this case, an adequate trade-off between popularity (hits) and size of the items must be achieved. Keeping small popular items in the cache tends to optimize hit-ratio; however, a hit in a large item may be more beneﬁcial for an application than many hits on small items. At the same time, ﬁlling the cache with few large items may turn the cache performance dependent on a good replacement policy. Evaluate staleness and lifetime of cached data. (Evidence 2), (Evidence 3) and (Evidence 12) Every piece of cached data is already potentially stale, it is important to rethink the degree of integrity and potential staleness that the application can compromise for increased performance and scalability. Many cache implementations adopted an expiration policy to invalidate cached data based on a timeout since weak consistency is easier than deﬁning a hard-to-maintain, but more robust, invalidation process. In short, developers must"
2011.00242,data,186,2022-05-13,0,"Even though these approaches focus on providing an adaptive behavior to application-level cache, none of them takes application speciﬁcities into account to autonomously manage their target. They attempt only to optimize cache decisions based on cache statistics like hit-ratio and access patterns at a higher level, thus, ignoring cache meta-data expressed by application-speciﬁc characteristics, which are closely related to application computations and could help to reach an optimal performance of the caching service on time. Addressing this issues, content-aware approaches have been proposed [45], [46] for admitting and replacing items in the cache by exploring descriptive caching hits in the application model or spatial locality (relationships among web objects). Though adaptive caching is in general new and innovative, it is far from being adopted as standard practice in software industry; many improvements must be achieved before this can happen, such as reducing the overhead in terms of resource consumption and processing time of the learning process, and providing easy ways to integrate application and caching method."
2011.00242,data,189,2022-05-13,0,"The last issue is related to the maintenance of the cache system, which involves several aspects such as determining replacement policies and the size of the cache. According to Radhakrishnan [31], a traditional approach to maintain cache is to set up strategies based on initial assumptions or well-accepted characteristics of workload and access patterns. Thus, cache statistics such as hit ratio, the number of objects cached, and average object size can be collected by observing the application and cache at run-time. These data, compared in the context of desired values of these parameters, can be used to decide whether to change the ﬁrst choices. Usually, this process is repeated until an acceptable trade-off between conﬁguration and performance is reached, and then cache strategies are ﬁxed for the lifetime of the product or at least for several release cycles. However, its performance may decay over time due to changes in the workload characteristics and access patterns. Therefore, achieving acceptable application performance requires constantly tuning cache decisions, which implies extra time"
2011.00242,data,19,2022-05-13,0,Fig. 6. Cacheability ﬂowchart: intuitive process to decide whether to cache or not particular data.
2011.00242,data,19,2022-05-13,0,Guideline Evaluate different abstraction levels to cache Stack caching layers Separate dynamic static data Evaluate boundaries Specify selection criteria
2011.00242,data,198,2022-05-13,0,"issues about whether to cache some speciﬁc data or not, showing that the connection between observed bottlenecks in the application and opportunities for caching is not straightforward and requires a deeper analysis (Evidence 1). Therefore, selection criteria based on common sense or past experiences are initial assumptions to ease such decisions. Despite these criteria are usually unjustiﬁed, they guide developers while selecting content. We observed in 90% of the applications the deﬁnition of selection criteria to make the distinction of cacheable from uncacheable content easier. These criteria were observed in explanations of cache design choices in comments, issues, and documentation. We identiﬁed common criteria used to determine whether to cache or not a speciﬁc content, which are: (i) content change frequency (Evidence 2), (ii) content usage frequency (Evidence 3), (iii) content shareability (Evidence 4), (iv) content retrieval complexity (Evidence 5), (v) content size (Evidence 6), and (vi) size of the cache (Evidence 7)."
2011.00242,data,2,2022-05-13,0,Data Expiration
2011.00242,data,21,2022-05-13,0,A sentence in documentation: We use Redis [a thirdparty solution] as a cache and for transient data.
2011.00242,data,221,2022-05-13,0,"Abstract—Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users’ demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on speciﬁc details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications."
2011.00242,data,257,2022-05-13,0,"Specify selection criteria. (Evidence 1), (Evidence 2), (Evidence 3), (Evidence 5), (Evidence 6) and (Evidence 7) Selecting the right data to cache involves a great reasoning effort given that data manipulated by web applications range in dynamicity, from being completely static to changing constantly. To optimize this selection process, there are four primary selection criteria used by developers while detecting cacheable content, which should be used in decisions regarding whether to cache. These criteria are described below, ordered according to their importance; i.e. the higher the inﬂuence level, the earlier it is presented. Data change frequency. Developers should seek for data that have some degree of stability, i.e. those that are more used than changed. Even if data are volatile and change in time intervals, caching still brings a beneﬁt. This is the ﬁrst factor to be considered since caching volatile data implies the implementation of consistency mechanisms, which is not trivial and requires an extra effort and reasoning from developers. In short, the cost of consistency approaches cannot be higher than the beneﬁt of caching. Besides, when stale data is not a critical issue, an approach of weak consistency can be employed, such as time-to-live (TTL) eviction, where data is expired after a time in cache, regardless of possible changes."
2011.00242,data,29,2022-05-13,0,"Data retrieval complexity. Data that is expensive to retrieve, compute, or render, regardless of its dynamicity, is always considered a good caching opportunity."
2011.00242,data,35,2022-05-13,0,"order to avoid this, we followed a systematic analysis of our data, and conclusions are all founded on these data. Moreover, cross-checks were performed using our different sources of evidence."
2011.00242,data,39,2022-05-13,0,"ensure that the expiration policy matches the pattern of access to applications that use the data, which is based on determining how often the cached information is allowed to be outdated, and relaxing freshness when possible."
2011.00242,data,44,2022-05-13,0,7 What kinds of bottlenecks are addressed by developers? 8 What motivated the need for explicit caching manipulation? 9 What is the granularity of the cached objects? 10 What is the importance of the cached data to the application? 11
2011.00242,data,44,2022-05-13,0,"Separate dynamic from static data. (Evidence 3) and (Evidence 2) Content can be distinguished in static, dynamic, and user-speciﬁc. By partitioning the content, it is easier to select portions of the data to cache."
2011.00242,data,46,2022-05-13,0,Evaluate staleness and lifetime of cached data Avoid caching per-user data (Evidence 4) and (Evidence 26) DG-07 Avoid caching volatile data (Evidence 2) and (Evidence 3) DG-08 DG-09 Do not discard small provements Keep the cache API simple
2011.00242,data,5,2022-04-21,0,Data usage frequency. Frequent
2011.00242,"data, data https, data available",48,2022-04-21,0,"[38] Q. Yang and H. H. Zhang, “Web-log mining for predictive web caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 1050–1053, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2003.1209022"
2011.00242,python,2,2022-04-21,0,PHP Python
2011.05411,data,135,2022-04-21,0,"3. An assessment of the data security and privacy risks that might be induced by each operation, along with the technical measures implemented to mitigate and manage the risks. For instance, in an FL system, the operation of sending local ML model parameters to a coordination server for global ML model update might be the target of inference attacks, thus, inducing privacy leakage. The measures called Secure Aggregation and Homomorphic Encryption mechanisms are implemented along with the technical report. Even though such privacy-preserving methods are implemented to strengthen FL systems, there exist some risks that can be exploited for illegitimate purposes such as model poisoning with back-door sub-tasks. These possible attacks, which lead to non-compliance with the GDPR, should be addressed."
2011.05411,data,142,2022-04-21,0,"sonal data, any previous infringement, and the nature, gravity, and duration of the current infringement. For instance, Facebook and Google were hit with a collective $8.8 billion lawsuit (Facebook, 3.9 billion euro; Google, 3.7 billion euro) by Austrian privacy campaigner, Max Schrems, alleging violations of GDPR as it pertains to the opt-in/opt-out clauses. Speciﬁcally, the complaint alleges that the way these companies obtain user consent for privacy policies is an ""all-ornothing"" choice, asking users to check a small box allowing them to access services. It is a clear violation of the GDPR’s provisions per privacy experts and the EU/UK. A list of ﬁnes and notices (with non-compliance reasons) issued under the GDPR can be found on Wikipedia12"
2011.05411,data,181,2022-04-21,0,"4. Privacy-Preservation in Centralised Federated Learning Framework As an ML model can be cooperatively trained while retaining training data and computation on-device, FL naturally oﬀers privacy-guarantee advantages compared to the traditional ML approaches. Unfortunately, although personal data is not directly sent to a coordination server in its original form, the local ML model parameters still contain sensitive information because some features of the training data samples are inherently encoded into such models [5, 81, 4, 96, 86]. For example, authors in [5] have shown that during the training process, correlations implied in the training data are concealed inside the trained models, and personal information can be subsequently extracted. Melis et al. have also pointed out that modern deep-learning models conceal internal representations of all kinds of features, and some of them are not related to the task being learned. Such unintended features can be exploited to infer some information about the training data samples. FL systems, consequently,"
2011.05411,data,210,2022-04-21,0,"number of data samples and data distributions among personal mobile devices. Training over non-IID data has been shown to be much less accurate as well as slower convergence than IID data in federated settings [128]. Konečn`y with his colleagues at Google went further on improving the eﬃciency of the FSVRG algorithms in distributed settings by minimising the information in parameter update to be sent to an orchestration server [71]. Two types of updates are considered called structured updates and sketched updates in which the number of variables used in an ML model is minimised as many as possible, along with the compression of the information in the full model updates. Another ambitious federated optimisation approach is that local nodes are independently trained diﬀerent ML models as a task in a multi-learning objective simultaneously [113]. Generally, local nodes generate data under diﬀerent distributions which naturally ﬁt separate learning models; however, these models are structurally similar resulting in the ability to model the similarity using a multi-tasking learning (MTL) framework. Therefore, this approach improves performance when dealing with non-IID data as well as guarantees the learning convergence [113]."
2011.05411,data,234,2022-04-21,0,"A Data Subject is assumed to have the right ""not to be subject to a decision based solely on automated processing, including proﬁling"" - Article 22(1), the GDPR. Therefore, an FL client, as a Data Subject, has the right to receive meaningful information and explanation about whether the result of the processing (i.e., a global ML model) used in an automated decision-making system will produce legal eﬀects concerning the client or similarly signiﬁcantly aﬀects the client. Unfortunately, due to the black-box operation model and the limitation of the transparency in ML, including FL, training process, results (e.g., a global ML model in FL) are generally generated without any proper explanation [119]. Thus, it is infeasible to predict whether outcomes of an ML model might aﬀect the legal status or legal rights of the Data Subject, or negatively impact on its circumstances, behaviour or choices. Consequently, any FL system fails to comply with the GDPR requirements of the data subject’s right in control of automated decision making. Fortunately, this requirement can be neglected if a Data Controller explicitly mentions the lack of automated decision making and proﬁling right when asking for Data Subject’s consent to process personal data."
2011.05411,data,3,2022-04-21,0,11https://ico.org.uk/for-organisations/guide-to-data-protection/guideto-the-general-data-protection-regulation-gdpr/individual-rights/right-tobe-informed
2011.05411,data,4,2022-04-21,0,10https://ico.org.uk/for-organisations/guide-to-data-protection/guide to-the-general-data-protection-regulation-gdpr/lawful-basis-forprocessing/
2011.05411,data,4,2022-04-21,0,2.2.1. Data Anonymisation
2011.05411,data,4,2022-04-21,0,5.2.3. Data Minimisation
2011.05411,data,54,2022-04-21,0,"1. A systematic description of data processing operations, associated purposes, along with clariﬁcation and justiﬁcation of the operations. For instance, the operation of asking Data Subject’s consent for local ML training and sending the ML model parameters to a coordination server should be documented in detail."
2011.05411,data,66,2022-04-21,0,The GDPR deﬁnes 6-core principles as rational guidelines for service providers to manage personal data as illustrated in Fig. 7 (The GDPR Articles 5-11). These principles are broadly similar to the principles in the Data Protection Act 1998 with the accountability that obligates Data Controllers to take responsibility for complying with the principles and implementing appropriate measures to demonstrate the compliance.
2011.05411,data,77,2022-04-21,0,"3. Federated Learning: A Distributed Collaborative Learning Approach In many scenarios, the traditional cloud-centric ML approaches are no longer suitable due to the challenges of complying with strict data protection regulations on vast aggregation and processing personal data. By nature, most personal data is generated at the edge by end-users’ devices (e.g., smart phones, tablets, and wearable devices) which are equipped with increasingly powerful computing capability"
2011.05411,"data, dataset provided",6,2022-04-21,0,5.3. Rights of Data Subject
2011.05411,dataset,89,2022-04-21,0,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-users’ devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.10563,"data, dataset",62,2022-04-21,0,"At a glance, we observe that 5Gophers error show an ≈ 100-fold increase when compared to NUY-METS.This observation is in line with our expectations, considering the higher data rates that users experience in a 5G network environment (i.e., up to 1500Mbps, see Figure 6). Next, we follow up the results discussion per dataset."
2012.01288,data,50,2022-04-21,0,"17. Vulic, I., Moens, M.: Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014. (2014) 349–362"
2101.03069,"data, data available",134,2022-04-21,0,"dominant Twitter accounts to be found in the data. One would expect to find highly active Twitter users in cases where the platform is used to amplify messaging. In the data, only two accounts (both from Spain) were found to have tweeted more than 100 times during the eight-month period. One of the accounts belongs to a paediatrician while the other to the Spanish Society for Paediatric Infectious Diseases. In the absence of an ideologically-motivated group, movement or collective, and some evidence that scientific rather than political activity is the driver of social media activity, Twitter is not in this case being used as a communication platform to amplify messaging about the risks or benefits of children attending school during the COVID-19 pandemic."
2102.03237,data,30,2022-04-21,0,"Kim, J., & Owen-Smith, J. (In print). ORCID-linked labeled data for evaluating author name disambiguation at scale. Scientometrics. Doi: 10.1007/s11192-020-03826-6"
2103.02044,database,3,2022-04-21,0,"Science, Database"
2104.00863,data,9,2022-04-21,0,"encrypted data. CoRR, abs/1711.05189, 2017."
2104.14114,data,18,2022-04-21,0,"Following data processing, we obtained the time series of the number of publications by each author,"
2105.00775,"code, github, code available",50,2022-04-21,2,"Copyright © 2021 O. Mesnard and L.A. Barba, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Lorena A. Barba (labarba@gwu.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/barbagroup/petibm-rollingpitching.."
2105.00775,github,10,2022-04-21,2,2PetIBM: github.com/barbagroup/petibm 3PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 4DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,10,2022-04-21,2,6PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 7Repro-packs: doi.org/10.5281/zenodo.4732946 8DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.09869,"code, github, code available",11,2022-04-21,2,1The MATLAB code is available at https://github.com/amasoumi60/Robust Dynamic-Mode-Decomposition.
2105.10037,"code, github, code available",73,2022-04-21,2,"In this section, we analyze the efﬁcacy of our proposed method on the xDIO task. We adopt MuJoCo (Todorov et al., 2012) as the experimental test-bed and evaluate on several cross-domain tasks, along with a thorough ablation study of different modules in our overall framework. Implementation details are presented in Appendix B. Code and videos are available at: https://driptarc.github. io/xdio.html."
2105.12309,"code, github, code available",55,2022-04-21,2,the EKF Algorithm for RexROV becomes as shown in Algorithm (2). The algorithm has been applied to several test courses from [23] in the next section and their results are discussed. The source code for implementation of the EKF algorithm shown below is available in https:// gitlab.engr.illinois.edu/auvsl/submarine
2105.15074,github,14,2022-04-21,2,1The algorithms and functions used are available in the GitHub repository https://github.com/vjduarte/ANN_FASD
2106.03907,"data, data available",245,2022-04-21,0,"One common assumption to cope with confounding bias is to assume no unobserved confounders exist [8], or more generally, the ignorable treatment assignment assumption [28], which states that the treatment assignment is independent of the potential outcomes caused by the treatment, given the background data available. Although a number of methods are proposed based on this assumption [7, 9, 36], it can be too restrictive, since it is often diﬃcult to determine how the confounder aﬀects treatment assignments and outcomes. A less restrictive assumption is that we have access to proxy variables, which contain relevant side information on the confounder. In the ﬂight tickets example, we can use the number of views of the ticket reservation page as a proxy variable, which reﬂects peoples’ desire for ﬂights. Note that if we can completely recover the confounder from proxy variables, the ignorable treatment assignment assumption can be satisﬁed. Motivated by this, Lee et al. [14] and Louizos et al. [17] aim to recover the distribution of confounders from proxy variables using modern machine learning techniques such as generative adversarial networks [5] or variational auto-encoders (VAE) [11]. Although these methods exhibit powerful empirical performance, there is little theory that guarantees the correct recovery of the causal eﬀects."
2106.06064,data,114,2022-04-21,0,"In this work, we model multivariate time-series as random realizations from a nonlinear state-space model, and target Bayesian inference of the hidden states for probabilistic forecasting. The general framework we propose can be applied to univariate or multivariate forecasting problems, can incorporate additional covariates, can process an observed graph, and can be combined with data-adaptive graph learning procedures. For the concrete example algorithm deployed in experiments, we build the dynamics of the state-space model using graph convolutional recurrent architectures. We develop an inference procedure that employs particle ﬂow, an alternative to particle ﬁlters, that can conduct more effective inference for high-dimensional states."
2106.06064,dataset,42,2022-04-21,0,"3) we show that the proposed method provides a superior characterization of the prediction uncertainty compared to existing probabilistic multivariate time-series forecasting methods, both for datasets where a graph is available and for settings where no graph is available."
2111.13023,data,43,2022-04-21,0,"[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017. 2"
2111.14671,"code, github, code available",16,2022-04-21,2,"∗Equal contribution 2Download instructions, baselines, and code are available at: https://github.com/RolnickLab/"
2111.14671,dataset,29,2022-04-21,0,"In our experiments we focus on pristine shortwave radiation. Our dataset, however, allows the user to choose the desired target variables based on their needs."
2111.14915,data,15,2022-04-21,0,is trained on data from 2000-2010 to predict gentrifying neighborhoods from 2010-2020. The present
2201.02733,"dataset, github",89,2022-04-21,2,"3.2 Training HDFS Once the train sets have been generated, we deploy HDSF available in HDSF’s authors’ Github repository [8]. We trained HDSF with each of the five training sets (one original and four noisy dataset) over 20 steps, and generated models, on which we performed our tests using a separate test set (Figure 2 (b)). Thereafter we collected loss and accuracy scores for each model obtained for the same test set."
2201.02733,github,15,2022-04-21,0,[8] Hamid Karimi. 2020. HDSF Source Repository. https://github.com/hamidkarimi/
2201.08452,github,15,2022-04-21,0,-- repo_link_a nd _S HA https :// github . com / streamich / memfs
2201.08452,github,21,2022-04-21,0,"5.2 Basic usage This tool can either take JavaScript packages speciﬁed as GitHub repository links, or as npm packages."
2201.08452,github,25,2022-04-21,0,""" repo_link "" : "" https :// github . com / streamich / memfs "" , "" repo_commit_S HA "" : REDACTED FOR LENGTH"
2201.08452,"github, repo",11,2022-04-21,0,More examples are included in the npm-ﬁlter GitHub repo Readme.
2201.08452,"github, repo",12,2022-04-21,0,"To run npm-ﬁlter over GitHub repo links, use the following:"
2201.08452,"github, repo",24,2022-04-21,0,• repo_link: a link to a single GitHub repo to be analyzed • repo_link_and_SHA: link to a GitHub repo followed by a
2201.08452,"github, repo",42,2022-04-21,0,"• repo_list_file: a ﬁle containing a list of GitHub repo links to be analyzed. Each line of the input ﬁle must specify one repo link, with an optional whitespace delimited commit SHA to check the repo out at."
