id,pattern,token_count,update_date,label,para
1710.02907,"data, dataset",280,2022-04-21,0,"Experiment 2: In this set of experiments, we evaluated the ability of the proposed method to compress high dimensional data. For these experiments, the proposed compression method was deployed on each dimension and the performance metrics are averaged over all the dimensions. In this regard, Dataset 2 ( comprising of three medical images and the color version of lena.jpg image) was used to benchmark ZT-based algorithm with those of DCT and FWHT. It can be observed in Figs. 11a and 12a that ZT-based transform have competitive performance on Scan1 data for block sizes 4 128, performance of ZT-based 8, 32 algorithm plummeted in comparison with DCT and FWHT-based approaches. Experimentation with Scan2 image shows that ZT consistently compresses the MRI data better and faster than its counterpart for most of the block sizes considered as shown in Figs. 11b and 12b. With block size of 128 128, the performance of FWHT improves and became comparable with ZT. Better performance in compression and speed of implementation is again observed on the average for ZT-based method using the Scan3 image, and the performance is more pronounced when the block size is 128 128 as shown in Figs. 11c and 12c. Again the proposed algorithm using color version of Lena image was compared with those of DCT and FWHT. It was observed that for most of the block sizes, ZT consistently compresses better and faster than its counterparts as shown in Figs. 11d and 12d. For block size 4 and 8, zipper transform has slightly better compression capability and"
1811.11012,data,195,2022-04-21,0,"This section of the technical report is focused on two similar desktop applications, which will here be referred to as Intersection Viewer (abbreviated IV) Version 1 and Intersection Viewer Version 2. IV v1 (developed by Nick Hodge and J.T. Blevins, with maintenance by Noah Carter) parses SPaT data and displays it in an intuitive graphical user interface. IV v2 (developed by Jacob Hoyos, Matthew Dale, and Noah Carter) goes a step further by not only displaying the GUI but also acting as a server and relaying the SPaT data to a single speciﬁed client machine over a local network. In IV v2, said client then displays the GUI based on the received SPaT data. The intended use of these applications was to serve as a learning experience and a prototype for the application described in section 3 of this report, IV v3. In IV v3 (itself yet another prototype), the client-server relationship was transitioned to the cloud, and made accessible from all personal devices over LTE (and the Internet generally)."
1811.11012,"data, dataset",70,2022-04-21,0,"volunteers’ vehicles were mounted with BSM-broadcasting devices. (See the ﬁnal section of this technical report for a basic analysis of that particular dataset.) One aspect of such data analysis is visualization and simulation. Allowing an analyst to see the data in a helpful and intuitive format enables the analyst to make more informed and strategic decisions [2], [26]."
1912.09582,dataset,13,2022-04-21,0,for small datasets–a case with Dutch book reviews. arXiv preprint 1910.00896.
1912.09582,dataset,15,2022-04-21,1,Table 4: Sentiment Analysis accuracy scores on the 110k Dutch Book Reviews Dataset.
1912.09582,github,26,2022-04-21,0,"2https://deepset.ai/german-bert 3https://github.com/cl-tohoku/bert-japanese 4A monolingual Dutch model has also been made available at http://textdata.nl, but this this model was consistently"
2005.07667,"data, database, dataset",180,2022-04-21,0,"For that purpose, in recent years, with the extensive development of deep learning techniques, especially convolutional and recurrent neural networks, the results are drastically improving. There have been quite a few researches attempting to generate data processing results by directly linking records in the tables to the semantic meaning of the natural language input, such as [6] and [7]. However, these attempts are not scalable to big tables and are not reusable when the database schema is changed. More recent approaches use only the natural language input and the database schema and metadata to generate the queries. We review the most recent approaches in our research. Furthermore, the release of large annotated datasets containing questions and the corresponding database queries has additionally enhanced the ability to use deep learning or supervised techniques to tackle this problem. This has enabled the problem to evolve into a more complex task where the approaches should be domain independent and involving multiple tables with complex queries."
2005.07667,dataset,45,2022-04-21,0,"Dataset ATIS [8] GeoQuery [9] Restaurants [10], [19] Academic [11] Scholar [12] Yelp [13] IMDB [13] WikiSQL [15] Advising [14] Spider [16]"
2005.08622,dataset,90,2022-04-21,0,"In literature, multi-label classiﬁcation is an important ﬁeld in machine learning and it is strongly related to many realworld applications for example, in biomedical images annotation, document categorization and whatever problem which the instances inside the classes are not disjoint but they keep a hierarchical structure. In this work, we have conducted four empirical studies on different datasets to prove by experimental results the effectiveness and robustness of our proposed model, that can be applied as an extension to any Convolutional Neural Network."
2007.12442,"data available, data",74,2022-04-21,0,"Third, edge-based pub/sub middleware usually lacks any mechanism of access control, e.g., the topics are public. An attacker with knowledge of the publish/subscribe topics could inject carefully crafted information while receiving potentially sensitive data sent by the clients (AV-3). By leveraging access control mechanisms, we can also revoke access to byzantine nodes, hence providing a simple defense mechanism against replay attacks towards the broker."
2007.12442,open-source,14,2022-04-21,0,[54] The Eclipse Foundation. Eclipse Mosquitto - An open source MQTT
2008.01391,code,155,2022-04-21,0,"An SMT system with a code-switched parallel corpus was studied by Menacer et al. (2019) and Fadaee and Monz (2018) for Arabic-English language pair. The authors have manually translated or used back translation method to translate foreign words. The identiﬁcation of the language of the word is based on the orthography. Chakravarthi et al. (2018) used the same approach for Dravidian languages; they used the improved MT for creating WordNet, showing improvement in the results. For English-Hindi, Dhar et al. (2018) manually translated the code-switched component and shown improvements. Machine translation of social media was studied by Rijhwani et al. (2016) where they tackle the code-mixing for Hindi-English and Spanish-English. The same approach translated the main language of the sentence using Bing Translate API (Niu et al., 2018)."
2011.09069,data repos,121,2022-04-21,0,"In case of Facebook platform, the coverage levels of the two aggregators are found to be quite similar, though PlumX has an edge over Altmetric.com. Ortega (2020a) has also found that in collecting mentions from Facebook and Mendeley, Altmetric.com has performed poorly as compared to PlumX. One possible reason for Altmetric.com recording slightly lesser Facebook mentions is that Altmetric.com captures only public pages19 and excludes likes, shares, and comments, whereas PlumX includes shares, likes, and comments along with the interactions in user’s closed network. A similar argument is also given by Zahedi & Costas (2018b) behind the low Facebook mentions captured by Altmetric.com."
2011.09069,data repos,4,2022-04-21,0,(c) Mendeley
2101.00522,dataset,104,2022-04-21,0,"Table 3: The percentage of shift in pixel labels during adaptation for the cardiac dataset. A cell (i, j) in the table has three values. The ﬁrst value represents the percentage of pixels labeled i that are labeled j after adaptation. The second value represents the percentage of switching pixels whose true label is i - lower is better. The third value represents the percentage of switching pixels whose true label is j - higher is better. Bolded cells denote label shift where more than 1% of pixels migrate from i to j."
2103.00265,dataset,169,2022-04-21,0,"λ1 . The metric tensor is considered to be poorly conditioned if the condition number has a high value. (Odena et al., 2018) proposed to eschew the issue of computing the complete spectrum, which could be quite challenging, in favor of sampling random directions (essentially sampling small random values for εvk and empirically computing 1.45, and then adding a penalty to encourage these values to fall within a speciﬁc range. In practice they achieved good results by setting λmin to 1.0 and λmax to 20.0. Making λmax too small could have the eﬀect of making it too hard for the model to be responsive to the latent variables and setting λmin to be too large could have the eﬀect of making it impossible for the model to learn to give large regions in the space relatively constant density. In practice these hyperparameters would likely need to be tuned depending on the dataset in accordance with these concerns."
2103.15335,dataset,2,2022-04-21,0,3.1 Datasets
2103.16349,dataset,148,2022-04-21,0,"The very ﬁrst observation is that MLP itself is a also a strong baseline, which outperforms HI and state-of-the-art models across almost all datasets and prediction lengths. Regardless of this point, we take MLP as a basic model, and evaluate the the ensemble of MLP and HI. We operate weighted summation over MLP’s and HI’s outputs to get the ﬁnal prediction. The weights of two models are set as 0.5/0.5. From Table 4 and Table 5, it could be concluded that this hybrid model can obtain better results in many cases, which is especially evidential for the task of univariate forecasting. MLP + HI brings up to 32% relative improvement over HI and 45% relative improvement over MLP on MSE, and 20%, 27% relative improvement on MAE."
2104.02307,dataset,4,2022-04-21,0,Target (dataset)
2104.09994,"data, dataset",170,2022-04-21,0,"sources (network data but also sensor readings, operating system logs and telemetry data) about a network containing several IoT/IIoT devices. In [27], the authors propose a dataset collecting benign and volumetric attacks traﬃc traces for 27 IoT devices. The main purpose of this dataset is to evaluate volumetric attacks perpetrated against a network containing real commercial IoT devices. The dataset proposed in [28] was generated with the traﬃc of 2 home IoT devices under multiple attack scenarios. It also includes simulated Mirai traﬃc appearing to come from the IoT devices. Finally, IoT23 [29] is a dataset consisting of 20 captures that include malware activity as well as 3 captures of benign IoT traﬃc. To conclude, it is worthy to mention that there is a lack of dataset suitable for FL approaches detecting malware in IoT devices. Existing FL-based solutions must consider split centralized datasets in order to apply federated techniques."
2104.09994,database,42,2022-04-21,0,"be used as a decentralized database where each client would share its local model and retrieve the models of other clients when performing the aggregation. Thus, the framework would be totally decentralized without an entity coordinating the generated models."
2104.09994,"publicly available, dataset",7,2022-04-21,0,Table 1 Public IoT network datasets.
2105.04903,dataset,145,2022-04-21,0,"REFERENCES [1] Omar Adjali, Romaric Besançon, Olivier Ferret, Hervé Le Borgne, and Brigitte Grau. 2020. Building a Multimodal Entity Linking Dataset From Tweets. In Proceedings of the 12th Language Resources and Evaluation Conference. 4285–4292. [2] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2020. Open-Domain Question Answering Goes Conversational via Question Rewriting. arXiv preprint arXiv:2010.04898 (2020). [3] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 344–354."
2105.04903,dataset,177,2022-04-21,1,"Generating annotation candidates. We employ a pooling approach to generate an extended set of candidate mentions and entities. Three EL tools were used to annotate the dialogues: TagMe [28], WAT [46], and REL [57]. Each tool was employed in two ways: (i) the turn method, which annotates a single turn, irrespective of the conversation history, and (ii) the history method, which annotates each turn given the conversation history up to that turn. For the CAsT dataset, only user utterances were given to the EL tool, while for other datasets both system and user utterances were considered as conversation history. This is due to relatively long system utterances in the CAsT dataset, which makes infeasible for the EL tools to annotate the whole conversation history. To further improve the recall of our pool, we included the top-10 Wikipedia search results, using mentions as queries sent to the MediaWiki API.4"
2105.04903,dataset,35,2022-04-21,1,"The resources provided in this paper allow for further investigation of entity linking in conversational settings, can be used for evaluation or training of conversational EL systems, and complement existing conversational datasets."
2105.10148,dataset,12,2022-04-21,0,"valid dataset with a ratio of 9:1, train each algorithm on"
2105.10148,dataset,56,2022-04-21,0,"We then evaluate all the algorithms on the hard dataset in both BSuite and DM Control environments. The episodes are generated from a mixture of partially trained policies from a diﬀerent run, and the distribution of states is likely to have a quite diﬀerent coverage from the distribution generated by the target distribution."
2106.07691,"data, dataset",107,2022-04-21,1,"only 6.09% of T5base’s attempts were AP T . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5base would generate better paraphrases, as measured by a higher chance of passing AP T on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table 2, which shows that T5base was able to generate an AP T passing paraphrase for 84.8% of the sentences in TwitterPPDB."
2106.13764,"data, data https",51,2022-04-21,0,"[35] Sybu Data. Sybu javascript blocker – google chrome extension. https: //sybu.co.za/wp/projects/js-blocker/, 2016. Accessed: 2020-05-02. [36] Houssein Djirdeh. Javascript | 2019 | the web almanac by http archive. https://almanac.httparchive.org/en/2019/javascript, 2019. Accessed: 2020-01-2."
2106.13764,database,138,2022-04-21,0,"SlimWeb’s JS classiﬁcation service crawls popular web pages to identify JS elements used in these pages, and then employs the classiﬁer to label these elements and store their categories in a database. It periodically updates and shares the labels with the users’ browser plugins. On the other hand, SlimWeb’s browser plugin is responsible for blocking noncritical JS elements. These elements are identiﬁed based on the labels received from the service. When a web page is requested by the user, the plugin ﬁrst checks if a label is locally available for each JS element, such that non-critical elements are immediately blocked. In the case of a label absence, the plugin considers the corresponding JS element as critical and requests it from the web."
2107.10483,"data, dataset provided",135,2022-04-21,0,"Assumption 4 ENCO relies on neural networks to determine the conditional data distributions ...). Hence, for providing a guarantee, we assume that in the graph learning step the neural p(Xi| networks have been sufﬁciently trained such that they accurately model all possible conditional ...). In practice, the neural networks might have a slight error. However, as long as distribution p(Xi| enough data, network complexity, and training time is provided, it is fair to assume that the difference between the modeled distribution and the true conditional is smaller than an arbitrary constant (cid:15), based on the universal approximation theorem (Hornik et al., 1989). For the limited data setting, see Appendix B.2.3."
2108.02756,dataset,140,2022-04-21,0,"Inspired by the defense mechanism presented in [20], we could use our method to generate boundary samples, which are examples that fall near the decision boundary of a pre-trained classiﬁer. Such examples can be used for adversarial training to enhance robustness or for knowledge distillation (e.g., see [20, 21]). Here, we focus on synthesizing such examples regardless of the application of interest. Unlike [20], we achieve this without the need to train a CGAN with a full dataset, and with the additional ﬂexibility of choosing a desired soft output pd (e.g., uniform over only a subset of the classes). The ﬁrst four synthesized samples in the bottom row of Figure 1 are all examples of near-decision-boundary samples."
2108.02756,dataset,89,2022-04-21,0,"We augment a repeated version of vector z to create a small training dataset and utilize the back-propagation algorithm [14]. Given the two objectives of BOSS, and the utilization of the adjustable parameters of network h, φ, we introduce the surrogate losses Lh(p(g(z ; φ) ; θ), pd) and Lg(g(z ; φ), xd), and use the back-propagation algorithm to optimize φ based on the minimization"
2109.08237,"data, dataset",225,2022-04-21,0,"In the ﬁrst experiment, the DL algorithm was trained on the different datasets. Figure 6 displays an example from the test set, which shows the gold standard images and the DL reconstructions for data undersampled with R = 4. Generally the visual quality of all the images (both gold standard and reconstructed ones) reduces with increased JPEG compression level (left-to-right in Figure 6); this is expected from compressed data. However, the NRMSE metric shows an unexpected effect: it improves with the compression, i.e. the reconstruction error reduces although the image visual quality degrades. The reason for this phenomenon is that in retrospective experiments the reconstruction quality is measured w.r.t. to a ""gold standard"" image that is also processed (see the pipeline in Figure 1c); the error metric is therefore blind to data processing. Strikingly, the NRMSE could show a misleading improvement even when the human eye cannot see any difference, as demonstrated in the left two columns of Figure 6: although the reconstructions from NC and QF = 75 are visually similar, the NRMSE of the latter is lower by 30%. This reﬂects the subtle bias induced by the pipeline of subtle data crime II."
2109.08237,dataset,81,2022-04-21,0,"[21] A. D. Desai, A. M. Schmidt, E. B. Rubin, C. M. Sandino, M. S. Black, V. Mazzoli, K. J. Stevens, R. Boutin, C. Re, G. E. Gold, et al., “SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation,” in Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021."
2109.12907,dataset,10,2022-04-21,0,Table 3: Usage of relations in the dataset.
2110.05428,"data, dataset",151,2022-04-21,0,"Evaluation Metrics To measure the identiﬁability of latent causal variables, we compute Mean Correlation Coefﬁcient (MCC) on the validation dataset, a standard metric in the ICA literature for continuous variables. MCC reaches 1 when latent variables are perfectly identiﬁable up to permutation and componentwise invertible transformation in the noiseless case (we use Pearson correlation and rank correlation for linearly and nonlinearly related latent processes, respectively). To evaluate the recovery performance on causal relations, we use different approaches for (1) linear and (2) nonlinear transitions: (1) the entries of estimated state transition matrices are compared with the true ones after permutation, signs, and scaling are adjusted, and (2) the estimated causal skeleton is compared with the true data structure, and Structural Hamming Distance (SHD) is computed."
2110.05428,dataset,64,2022-04-21,1,"KiTTiMask The KiTTiMask dataset consists of pedestrian segmentation masks sampled from the autonomous driving vision benchmark KiTTi-MOTS. For each given frame, the position (vertical and horizontal) and the scale of the pedestrian masks are set using measured values. The difference in the sample time (e.g., ∆t = 0.15s) generates the sparse Laplacian innovations between frames."
2110.11575,"github, repo",38,2022-04-21,0,"Make sure the target repo (the repo to be analyzed, not the repo of this tool) is on your machine. In this demo, the target repo is downloaded from a GitHub repo:"
2110.11575,repo,67,2022-04-21,0,# make sure [ repo path ] is the target repo path # the [ output path ] can be anywhere you desire git_stats generate -p [ repo path ] -o [ output path ] # e . g . git_stats generate -p / home / user / git - stats / Weasis -o # / home / user / git - stats / Weasis - analytics
2110.15667,database,45,2022-04-21,0,"[50] L. Deng, “The mnist database of handwritten digit images for machine learning research [best of the web],” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012."
2110.15667,dataset,1,2022-04-21,0,Dataset
2110.15667,dataset,33,2022-04-21,0,"[51] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,” arXiv preprint arXiv:1708.07747, 2017."
2111.03516,"data, dataset",34,2022-04-21,0,"Figure 2 shows the F1 comparisons for each of the data augmentation methods (SMOTE, B SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE, SMOTE-RSB, and CFA) on 25 datasets using"
2111.03516,dataset,16,2022-04-21,1,classifier on the ‘PIMA’ dataset we can see that SMOTE-based methods had better performance
2111.03516,dataset,16,2022-04-21,1,§ Pima Indians Diabetes dataset: A binary-class dataset used to predict whether or not a
2111.03516,dataset,18,2022-04-21,0,"next best with 3 datasets. Baseline, B-SMOTE, and ADASYN had the highest AUC-ROC in 2"
2111.03516,dataset,20,2022-04-21,1,"§ Ecoli dataset: This dataset is a multi-class dataset, with 8 classes. The problem is to classify"
2111.03516,dataset,31,2022-04-21,0,"and Multilayer Perceptron (MLP) models. Several alternative ML models were used because dif ferent models find different decision boundaries for a given dataset, differences that could impact"
2111.04287,python,39,2022-04-21,2,"We ﬁll the gap between the rapid development of algorithms on paper and the lack of practice in real world with the introduction of BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms."
2111.13172,"data, data repository",5,2022-04-21,0,UDM: Unified Data Management
2112.03471,dataset,2,2022-04-21,0,4.1.1 Datasets
2201.00732,database,26,2022-04-21,0,"[49] Luca Biferale, Fabio Bonaccorso, Michele Buzzicotti, and Patricio Clark di Leoni, “TURB-Rot. A large database of 3d"
2201.00732,dataset,106,2022-04-21,0,"FIG. 11. (Main panel) Mean deviation between the predicted and true value of Ω for each of the ten classes considered, the errorbars show the root mean square of the diﬀerence between Ωpred and Ωtrue. (Inset) Mean squared diﬀerence between prediction and true Ω normalized to the square of the correct rotation value for each of the ten classes. The average is taken over the validation dataset, while the predictions are obtained as in Fig. 10, by the neural network, DCNN, or as the mean, ΩBay and most likely, Ω∗"
2201.00732,dataset,3,2022-04-21,0,B. Dataset extraction
2202.12674,github,4,2022-04-21,0,3https://github.com/SC-SGS/PLSSVM/blob/v1.0.1/utility scripts/generate
2202.12674,"github, data, data https, python",59,2022-04-21,0,"[18] H. E. L. Cagnin et al., “A portable OpenCL-based IEEE, 2015. [19] S. Raschka et al., “Machine learning in python: Main developments and technology trends in data science, machine learning, and artiﬁcial intelligence,” MDPI, 2020. https://github.com/ “svm-gpu,”"
10.3390_en10040547,"data available, data",161,,0,"The diesel engine data used in the performance estimations of the WHR systems were the exhaust gas temperature, scavenge air temperature, air mass ﬂow rate and jacket water cooling heat rate. The exhaust gas temperature is deﬁned as the temperature after mixing of the turbocharger and power turbine exhaust gas streams, and the scavenge air temperature is deﬁned as the air temperature after the turbocharger compressor. The exhaust gas temperature, scavenge air temperature and air mass ﬂow rate were based on measured values from the actual engine installed on the container vessel, while the jacket water heat amount was computed from the online CEAS engine calculation tool provided by MAN Diesel & Turbo [31]. Figure 3 depicts the variation of the four engine parameters mentioned above. The markers represent data points that were available from measurements or CEAS data. The values were interpolated for the remaining engine loads."
10.3390_en10040547,"data available, dataset, data",10,,0,Table 3. Relevant data available in the datasets.
10.3390_en10040547,dataset,101,,0,"For Dataset 3, the electrical power generation was estimated with discrepancies of 1.0%, 3.1% and 9.8% at main engine loads of 90%, 75% and 50%, respectively; see Figure 4i. The deviations for the HP and LP mass ﬂow rates in Figure 4j were below 4% and 18%, respectively. The HP and IP pressure predictions (Figure 4k) were within 7% and 10% of the measured values, respectively, while the temperature predictions (Figure 4l) were within 4%."
10.3390_en10040547,dataset,108,,0,"The input parameters to the design model, which were not mentioned in the above, were given the assumed values listed in Table 1, and the service steam production was assumed to be zero. The design model was then used to estimate the power output at 90% engine load for Dataset 1 and to calculate the design point parameters used in Equations (2)–(7). For the remaining measured engine load points, the off-design model was used to estimate the variations in steam turbine generator output, steam pressures, steam mass ﬂow rates and steam temperatures."
10.3390_en10040547,dataset,120,,0,"Figure 4. SRC model validation with three datasets (DS); values normalized with measured value at 90% main engine load. (a) DS 1, power; (b) DS 1, ﬂow; (c) DS 1, pressure; (d) DS 1, temperature; (e) DS 2, power; (f) DS 2, ﬂow; (g) DS 2, pressure; (h) DS 2, temperature; (i) DS 3, power; (j) DS 3, ﬂow; (k) DS 3, pressure; (l) DS 3, temperature"
10.3390_en10040547,dataset,242,,0,"The comparison with the measurements in Dataset 2 indicates that the model overestimates the power (see Figure 4e) and HP mass ﬂow rate (see Figure 4f) up to 14.3% and 51.2%, respectively. Note that the high percentage value for the HP mass ﬂow rate prediction was due to the values being relatively small at 50% main engine load. The overestimation of HP pressure was up to 8.2%; see Figure 4g. Accurate estimations were achieved for the LP steam ﬂow (maximum difference: 0.21 kg/s), the pressure between the HP and LP turbines (maximum difference: 0.12 bar) and the temperatures (maximum difference: 6.2 ◦C). This indicated that the assumption of zero service steam production might not be correct for Dataset 2. By taking out saturated service steam from the HP steam drum, the mass ﬂow rate to the HP turbine would drop. As a consequence, the HP steam pressure would drop according to the Stodola constant calculated for the HP turbine; see Equation (6). The drops in HP steam ﬂow and pressure would result in a drop in the SRC turbine power production, thus resulting in a better match between the estimated and measured values. The relative deviations of the temperature predictions in Figure 4h were below 3%."
10.3390_en10040547,"dataset, data",129,,0,"The model of the SRC system was validated by comparing the model outputs with measured data from the dual pressure SRC system on a Maersk Line container vessel. The data were measured during the sea trials. The data contain measurements from three different days collected during a time period of 10–15 min. The values used in the validation were averaged over these time intervals for each of the three days. All three datasets contain measurements at 90%, 75% and 50% main engine load, while Dataset 1 contains an additional measurement at 40% main engine load. The measured data (proprietary information of Maersk), which were used in the validation, are listed in Table 3."
10.3390_en10040547,"dataset, data",238,,0,"Figure 4 shows the comparison of the SRC model output and the measured values, and Table 3 lists the details on the location of mass ﬂow rate, pressure and temperature measurements. In Figure 4a are compared estimated and measured values of the electric power generated in the steam turbine generator for Dataset 1. An exact match between the model and the measured data was obtained at 90% main engine load, since this point was selected as the design point for the SRC model validation. At 75%, 50% and 40% main engine load, the model estimations were within 2.3%, 2.4% and 11.9% of the measured values, respectively. The larger deviation at 40% main engine load could be due to the zero ﬂow of LP steam to the turbine at this load point, which might result in inaccuracies in the turbine efﬁciency function in Equation (5). The maximum relative deviations for the HP and LP mass ﬂow rates (see Figure 4b) were below 4% and 15%, respectively. The relative deviations for the pressure predictions in Figure 4c were below 2% for the HP pressure and below 22% for the IP pressure, while the relative deviations for the temperature predictions displayed in Figure 4d were below 2%."
10.3390_en10040547,open-source,72,,0,"32. Bell, I.H.; Wronski, J.; Quoilin, S.; Lemort, V. Pure and Pseudo-pure Fluid Thermophysical Property Evaluation and the Open-Source Thermophysical Property Library CoolProp. Ind. Eng. Chem. Res. 2014, 53, 2498–2508. International Maritime Organization. Sulfur Oxides (SOx) Regulation 14; Technical Report; International Maritime Organization: London, UK, 2015."
10.3390_en10040547,open-source,83,,0,"The heat inputs to the SRC and ORC units were delivered from the exhaust gases, scavenge air and jacket water, while heat was rejected to the sea water. Constant values of speciﬁc heat capacity were assumed for the exhaust gases and scavenge air, and the jacket water and sea water were modeled using the properties of water. The thermodynamic properties of water and the organic ﬂuids were computed using the open source software Coolprop [32]."
10.3390_en10040547,"used dataset, dataset, data",249,,0,"The ﬁrst step in the validation was to select the 90% main engine load point in Dataset 1 as the design point for the SRC unit. This data point was then used in the design model to deﬁne the design of the SRC unit. Values for the temperature of the exhaust gases and scavenge air were used directly from the dataset along with HP and LP boiler pressures and mass ﬂow rates and the intermediate pressure (IP) between the HP and LP turbines. The exhaust gas mass ﬂow rate was calculated based on ﬂow characteristic curves for the turbochargers and power turbine. The scavenge air mass ﬂow rate was assumed to be equal to that of the exhaust gases, neglecting the mass ﬂow rate of fuel, which is <5% of the exhaust gas mass ﬂow rate. The approach temperature difference of the HP and LP drums was extracted based on the measured drum feed temperatures and pressures. Similarly, the degree of superheating was extracted from the measured HP steam (HP turbine inlet) temperature and pressure. It was assumed that the HP and LP turbines have the same isentropic efﬁciency. The isentropic efﬁciency of the turbines were tuned in order to match the estimated steam turbine generator output (ηg · ˙Wt) with the measured value. In the validation, the constraints listed in Table 1 were not employed."
10.3390_en10070859,"database, data",142,,0,"Production data, such as production rate, wellhead pressure, watercut, and other measurable information on the surface, is regularly observed and collected in a database. This database is used for history-matching to calibrate a more reliable reservoir model. Downhole data can improve results of history-matching, but needs additional investments due to the need for expensive tools and the operational difﬁculties encountered. Nevertheless, to identify each fracture property in unconventional reservoirs with multi-stage fracturing, it is essential to allocate the total production rate to each fracture based on downhole measurements. A PLT is an effective tool when investments have not been made in permanent downhole tools. However, PLT surveys are charged according to the number of surveys conducted; therefore, a minimum number of surveys should be conducted."
10.3390_en10091352,"data available, data",285,,0,"(i.e., the integral form of air pollutants discharged from a wide range of sources), the consumption of fossil fuel energy, and the meteorological variables in Macao on an annual basis. The number of hours with haze increased continuously from 0 to 766 h a year between 1990 and 2007 while the consumption of primary energy (including oil products, natural gas, and aviation kerosene) had increased by 130 percent. Multiple regression analyses have shown that gas oil/diesel, fuel oil, and natural gas consumed locally and the burning of aviation kerosene were signiﬁcantly associated with the number of hours per year with haze in Macao. A correlation analysis between monthly haze data and monthly records of meteorological variables has indicated that the number of hours with haze was strongly, signiﬁcantly associated with high atmospheric pressure that leads to the lower mixed layer height in the past decade. The combustion of oil products produces particulate matter, sulfur oxides, nitrogen oxides, and volatile organic compounds including non-methane hydrocarbons. All of these air pollutants are well-known sources of haze. Besides, around 150 commercial aircrafts, which emit a signiﬁcant amount of particulate matter in low troposphere, take off and land at the Macao International Airport each day [32]. It is possible that there is a signiﬁcant correlation between the number of humid hours (the hours with a relative humidity between 75 and 80 percent) each year/month and the number of haze hours per year/month. However, the hourly relative humidity data were not available from the Bureau’s publications."
10.3390_en10091352,dataset,18,,0,"where x and y are values of the variables, and n is the size of dataset."
10.3390_en10091352,"dataset, data",317,,0,"Energies 2017, 10, 1352 4 of 12 2.3. Association between Variables The association between two variables was characterized by Pearson’s correlation coefficient. The coefficient, r, was calculated as ()()()()===−−−−=niiniiniiiyyxxyyxxr12121 (5) where x and y are values of the variables, and n is the size of dataset. 2.4. Regression Analysis Multiple regression analysis was employed to assess the relationships between the independent variables and the dependent variable. The accuracy of the identified model was determined by the coefficient of determination, r2. The coefficient provides a measure of how well the collected data are predicted by the model [25]. It was calculated by: SSTSSESSTSSRr−==12 (6) where SSR is the sum of squared regression, SST is the sum of squared total, and SSE is the sum of squared error. 3. Results and Analysis 3.1. Haze and Meteorological Conditions Visibility is an integrative parameter that refers to the ability to see a distant object [25]. When air pollutants are emitted from anthropogenic sources, haze is formed under a stable atmospheric inversion layer, adversely affecting the visibility over the city. Haze has been a common phenomenon in winter in China’s cities [3,8,9,12,18,26]. In Macao, haze is defined as the condition under which visibility is equal to, or less than 5 km and relative humidity is less than 80 percent. The condition is recorded by trained meteorologists and confirmed by a forward scatter visibility meter. The Macao Meteorological and Geophysical Bureau keeps daily records of the number of hours with haze. Figure 1a shows the records for the period 1986 to 2016. (a) (b)Figure 1."
10.3390_en10091352,"dataset, data",339,,0,"2.4. Regression Analysis Multiple regression analysis was employed to assess the relationships between the independent variables and the dependent variable. The accuracy of the identified model was determined by the coefficient of determination, r2. The coefficient provides a measure of how well the collected data are predicted by the model [25]. It was calculated by: SSTSSESSTSSRr−==12 (6) where SSR is the sum of squared regression, SST is the sum of squared total, and SSE is the sum of squared error. 3. Results and Analysis 3.1. Haze and Meteorological Conditions Visibility is an integrative parameter that refers to the ability to see a distant object [25]. When air pollutants are emitted from anthropogenic sources, haze is formed under a stable atmospheric inversion layer, adversely affecting the visibility over the city. Haze has been a common phenomenon in winter in China’s cities [3,8,9,12,18,26]. In Macao, haze is defined as the condition under which visibility is equal to, or less than 5 km and relative humidity is less than 80 percent. The condition is recorded by trained meteorologists and confirmed by a forward scatter visibility meter. The Macao Meteorological and Geophysical Bureau keeps daily records of the number of hours with haze. Figure 1a shows the records for the period 1986 to 2016. (a) (b)Figure 1. (a) The number of hours with haze per year for the period 1986 to 2016; (b) The number of hours with haze per month for the period 1986 to 2016. Figure 1a shows that the number of hours with haze increased steadily from 0 in 1990 to 297 in 2002. However, it sharply increased to 694 in 2003. Between 2003 and 2008, the number of hours with 02004006008001986199119962001200620112016Haze (hours/year)YearEnergies 2017, 10, 1352"
10.3390_en10111898,code,69,,0,"19. Nanou, S.I.; Papathanassiou, S.A. Grid Code Compatibility of VSC-HVDC Connected Offshore Wind Turbines Employing Power Synchronization Control. IEEE Trans. Power Syst. 2016, 31, 5042–5050. [CrossRef] 20. Chaudhary, S.K.; Teodorescu, R.; Rodriguez, P.; Kjaer, P.C.; Gole, A.M. Negative sequence current control in"
10.3390_en10122158,"code package, package",215,,0,"the ESP-r simulation program for an analysis of the PV and building energy consumption, and ANSYS Fluent for a computational ﬂuid dynamics (CFD) analysis of the performance of the wind power conversion system [8,45]. ANSYS Fluent is one of the most-used CFD software offering various turbulence models based on the Reynolds-Averaged Navier Stokes (RANS) model. The power production of the wind turbines is estimated by an examination of the wind speed distribution around the applied system through CFD analyses. The ESP-r software package is recognized and used widely in more than 70 countries as an industry standard for the simulations. The authors employed ESP-r 11.1, which considers the energy use of heating, cooling, lighting, and PV energy generation. Thus, ESP-r has been used extensively to assess building energy applications, particularly as a simulation tool to compare various cities [14,29]. In addition, the energy performance of the PV module has been analyzed based on information, such as the open circuit voltage (Voc), short circuit current (Isc), and maximum power point voltage (Vmpp) in the simulation [46]. The information on the solar PV"
10.3390_en10122158,"dataset provided, code package, package, data",310,,0,"Energies 2017, 10, 2158 3 of 20 • Classify the climate data in global cities • Plot an energy potential chart and diagram by the variation of solar irradiation and wind power • Set the building module and BIPvWt system for energy generation and consumption output • Analyze the energy balance as an application of the BIPvWt in specific areas in terms of energy consumption and generation 2.1. Climate Data The climate data was selected from the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) and Energy Efficiency & Renewable Energy (EERE), which have information on more than 2100 city locations [44]. The paper selects 143 representative cities as variables to compare the solar and wind energy potential in the ASHRAE climate data. They are the major cities in Europe and Asia, and in each state of the U.S. Some cities are capitals of each country or state, and the others are selected based on the population and population density. In addition, the U.S. has the most cases compared to other areas because the climate data is well distributed in terms of climate classification and the weather data is relatively convincing. Each zone was classified according to the ASHRAE standard, which ranges from zone 1 (very hot) to 8 (subarctic), and the zone was analyzed using the thermal and humidity criteria. The population and density data in selected cities were well defined and informed (Demographia 2015). Figure 1 presents a histogram of the city lists according to the ASHRAE climate classification. Figure 1. Histogram of 143 cities according to the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) classification. 2.2."
10.3390_en10122158,"dataset provided, code package, package, data",332,,0,"The population and density data in selected cities were well defined and informed (Demographia 2015). Figure 1 presents a histogram of the city lists according to the ASHRAE climate classification. Figure 1. Histogram of 143 cities according to the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) classification. 2.2. Energy Simulation and Input Data Two simulation programs were used to evaluate the energy potential, BIPvWt: the ESP-r simulation program for an analysis of the PV and building energy consumption, and ANSYS Fluent for a computational fluid dynamics (CFD) analysis of the performance of the wind power conversion system [8,45]. ANSYS Fluent is one of the most-used CFD software offering various turbulence models based on the Reynolds-Averaged Navier Stokes (RANS) model. The power production of the wind turbines is estimated by an examination of the wind speed distribution around the applied system through CFD analyses. The ESP-r software package is recognized and used widely in more than 70 countries as an industry standard for the simulations. The authors employed ESP-r 11.1, which considers the energy use of heating, cooling, lighting, and PV energy generation. Thus, ESP-r has been used extensively to assess building energy applications, particularly as a simulation tool to compare various cities [14,29]. In addition, the energy performance of the PV module has been analyzed based on information, such as the open circuit voltage (Voc), short circuit current (Isc), and maximum power point voltage (Vmpp) in the simulation [46]. The information on the solar PV for the energy simulation is based on the data provided by the manufacturer for a silicon-based PV system [47]. 051015202530354012345678Number of CitiesASHRAE ClassificationA: moistB: dryC: marineEnergies 2017, 10, 2158"
10.3390_en10122158,"dataset provided, data",22,,0,for the energy simulation is based on the data provided by the manufacturer for a silicon-based PV system [47].
10.3390_en10122158,"dataset provided, dataset, data",170,,0,"Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin."
10.3390_en10122158,"dataset provided, dataset, data",340,,0,"Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin. Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs.Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.)."
10.3390_en10122158,"dataset provided, dataset, data",350,,0,"Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs.Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin. Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs. Energies 2017, 10, 2158"
10.3390_en10122158,"dataset, data",265,,0,"The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classiﬁcation to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classiﬁcation compared to the wind case. These results can be explained by the characteristics of the ASHRAE standard, which originate from the division of the thermal and humidity criteria [44]. In the wind energy cases, however, the variation is dispersed irregularly in a similar climate or adjacent cities."
10.3390_en10122158,"dataset, data",328,,0,"(cid:2873)°+Power(cid:3101),(cid:2872)(cid:2873)° (1) 3. Results and Discussion The results are divided into two parts: energy potential analysis in multiple urban areas and energy balance evaluation in selected cities. In the first part, as shown in the Appendix A, the energy potential can be compared according to the variation of global cities, which have their own climate patterns. Second, a feasibility test was performed by analyzing the energy consumption and generation in an office module in a specific building. 3.1. Solar and Wind Energy Potential in Urban Area The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classification to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classification compared to the wind case."
10.3390_en10122158,"dataset, data",332,,0,"Energies 2017, 10, 2158 6 of 20 different angles of incidence (0°, 22.5°, and 45°) were considered to reduce the number of analysis cases [56]. The power production of the wind turbines of a BIPvWt system applied to a building envelop was estimated by the annual wind data from selected cities. The wind rose data were fitted using the Weibull distribution, which is a good fit to the measured wind speed data [57], for each wind direction [58,59]. In addition, the applied system was assumed to generate electric power from the approaching wind within an angle of 90°. For example, the system installed in a northward direction is affected by wind from the northeast, north-northeast, north, north-northwest, and northwest azimuths. Finally, the total power production of the applied system installed towards the (cid:2030) direction can be written as the sum of the power converted from five different approaching wind directions as follows: Power(cid:3101),(cid:2930)(cid:2925)(cid:2930)(cid:2911)(cid:2922)=Power(cid:3101),(cid:2879)(cid:2872)(cid:2873)°+Power(cid:3101),(cid:2879)(cid:2870)(cid:2870).(cid:2873)°+Power(cid:3101),(cid:2868)°+Power(cid:3101),(cid:2870)(cid:2870). (cid:2873)°+Power(cid:3101),(cid:2872)(cid:2873)° (1) 3. Results and Discussion The results are divided into two parts: energy potential analysis in multiple urban areas and energy balance evaluation in selected cities. In the first part, as shown in the Appendix A, the energy potential can be compared according to the variation of global cities, which have their own climate patterns."
10.3390_en10122158,"dataset, data",350,,0,"Second, a feasibility test was performed by analyzing the energy consumption and generation in an office module in a specific building. 3.1. Solar and Wind Energy Potential in Urban Area The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classification to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classification compared to the wind case. These results can be explained by the characteristics of the ASHRAE standard, which originate from the division of the thermal and humidity criteria [44]. In the wind energy cases, however, the variation is dispersed irregularly in a similar climate or adjacent cities. (A) (B) Figure 4. Energy potential according to the ASHRAE classification. (A) Solar energy potential; (B) wind energy potential. 024681012012345678Relative Ratio of Solar Energy Potential ASHRAE Classification024681012012345678Relative Ratio of Wind Energy Potential ASHRAE ClassificationEnergies 2017, 10, 2158"
10.3390_en11020419,"dataset provided, data",109,,0,"The Sec SS 3 has installed 4 dry transformers of 1000 kVA each. In their current use, three remain always connected, the forth being in reserve, to be able to serve the hospital in case of failure of one of the other three. According to the data supplied by the manufacturer [20], its short-circuit and no-load power values are: Psc = 9.79 kW and P0 = 2.3 kW. Considering these data, and according to Equation (6), the optimum load index and the maximum efﬁciency will be: Copt = 0.4847 and ηmax = 0.99060."
10.3390_en11020419,"dataset provided, data",321,,0,"Energies 2018, 11, x FOR PEER REVIEW 7 of 13 2.3.2. Secondary Substation Studied The study proposed does not make sense when demand is constant, since in that case there are always a number of connected transformers that optimizes efficiency. Analogously, it also makes no sense if the Sec SS has a single transformer, since in that case there is only one connection option. Therefore, this type of study offers greater possibilities of energy saving with: (1) more variations, both daily and seasonal, in the demand of the Sec SS; and (2) a greater number of transformers that can enter into service. In the studied hospital complex, this occurs in the Sec SS 3. The magnitude of the changes in demand is basically based on the fact that this Sec SS is connected to the central air conditioning services of the hospital, which implies that there are large variations in both hourly and seasonal demand. The variation in hourly demand is also associated with the fact that this Sec SS also provides the consumption of the hot water pumping service of the hospital, and lighting, elevators, medical equipment, etc. of nearby buildings. The differences of hourly consumption, in a same day, can cause that the demand in the hour of greater consumption multiplies by 2.5 the demand of the hour of lower consumption. Regarding the differences in monthly consumption, the demand corresponding to the month of greatest consumption reaches 175% of the energy supplied in the month of least consumption. Analyzing the demand of the Sec SS 3, it has been observed that there is a great difference in the demand between the one that takes place in working days and the one on holidays."
10.3390_en11020419,"dataset provided, data",322,,0,"of nearby buildings. The differences of hourly consumption, in a same day, can cause that the demand in the hour of greater consumption multiplies by 2.5 the demand of the hour of lower consumption. Regarding the differences in monthly consumption, the demand corresponding to the month of greatest consumption reaches 175% of the energy supplied in the month of least consumption. Analyzing the demand of the Sec SS 3, it has been observed that there is a great difference in the demand between the one that takes place in working days and the one on holidays. This is basically due to the existence of consultations and diagnostic services in the hospital, which only operate on weekdays and therefore do not require space heating or air conditioning (supplied from Sec SS 3). Figure 3 shows the monthly and daily energy demands corresponding to the Sec SS 3. This has been carried out by differentiating between working and non-working days. Figure 3. Monthly and daily demands performed on the Sec SS 3. The total demand for the last spring month, June, is higher than total demand of the first spring month, April. However, there are not significant differences in the time distribution in which this demand is performed. Considering the daily demand, it has been observed that there are three annual periods in which the hourly percentages of daily demand are similar. These periods relate to the different climatology, and correspond to: winter (16 December to 15 March), spring-autumn (16 March to 15 June and 16 September to 15 December) and summer (16 June to 15 September). Figure 4 shows the values of the hourly demand percentages with respect to the daily demand in the Sec SS 3."
10.3390_en11020419,"dataset provided, data",324,,0,"Figure 3 shows the monthly and daily energy demands corresponding to the Sec SS 3. This has been carried out by differentiating between working and non-working days. Figure 3. Monthly and daily demands performed on the Sec SS 3. The total demand for the last spring month, June, is higher than total demand of the first spring month, April. However, there are not significant differences in the time distribution in which this demand is performed. Considering the daily demand, it has been observed that there are three annual periods in which the hourly percentages of daily demand are similar. These periods relate to the different climatology, and correspond to: winter (16 December to 15 March), spring-autumn (16 March to 15 June and 16 September to 15 December) and summer (16 June to 15 September). Figure 4 shows the values of the hourly demand percentages with respect to the daily demand in the Sec SS 3. This has been carried out for all three periods of time, considering whether it is a working or non-working day. It is observed that the demand in the non-working days is much more constant than in the working days; this is mainly due to the operation of consultation and diagnostic services. The Sec SS 3 has installed 4 dry transformers of 1000 kVA each. In their current use, three remain always connected, the forth being in reserve, to be able to serve the hospital in case of failure of one of the other three. According to the data supplied by the manufacturer [20], its short-circuit and no-load power values are: Psc = 9.79 kW and P0 = 2.3 kW. Considering these data, and according to Energies 2018, 11, 419"
10.3390_en11020419,download,52,,0,"19. American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE). Advanced Energy Design Guide for Large Hospitals; ASHRAE: Atlanta, GA, USA, 2012; ISBN 978-1-936504-23-7. Available online: https://www.ashrae.org/standards-research--technology/advanced-energy-design-guides/50percent-aedg-free-download (accessed on 27 December 2017)."
10.3390_en11030500,"dataset, data",167,,0,"Currently, taxis in many cities are equipped with vehicle information collection devices such as GPS systems. There were approximately 65,000 taxis in Beijing in 2014, and the proportions of different car models of the current taxi are shown in Figure 1 [27]. The data used in this study come from GPS devices installed in taxis, which send information such as taxi identiﬁcation, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations. An example of the dataset is shown in Table 2."
10.3390_en11030500,"dataset, data",167,,0,"With improved information and communication technologies, as well as location-based services (LBS) such as mobile phone communications, social software, vehicle-carried GPS (Global Position System) positioning terminals, etc., large-scale, high-quality, and consecutive spatiotemporal trajectory data on urban mobility has become an increasingly popular dataset and principal resource. Many researchers employ advanced data mining techniques and big geospatial data, among which taxi GPS data is one of the prevailing resources, to analyze individual travel patterns, the organization and planning of urban public spaces, construction of smart cities, and so forth. At present, studies using taxi GPS data include, but are not limited to, the following aspects: route planning and path-ﬁnding, trafﬁc operational state identiﬁcation, identiﬁcation of origin-destination (OD) and the clustering method, and taxi fuel consumption and emissions estimation. The typical and important current researches are summarized in Table 1."
10.3390_en11030500,"dataset, data",335,,0,"Energies 2018, 11, x 3 of 22 the dynamic spatiotemporal distribution as maps. To verify the accuracy of the results, we creatively convert carbon emissions and fuel consumption into CEPK and FCPOK, respectively, which are more global, standard, and intuitive factors. The dynamic spatiotemporal distribution of carbon emissions and taxi travel patterns on weekdays and weekends are then highlighted. Finally, the limitations of this research and potential future research areas are proposed. 2. Methodology In this section, we formulate the methodological framework used in this study. First, we process the taxi GPS data to enable its direct use in the next step. Second, using the carbon emission calculation model based on a taxi fuel consumption algorithm and emission factors, we calculate the carbon emissions over the whole network. Finally, a visualization method called kernel density analysis is applied to obtain the spatiotemporal dynamic distribution of carbon emissions. 2.1. Description and Data Cleaning of Taxi GPS Data Currently, taxis in many cities are equipped with vehicle information collection devices such as GPS systems. There were approximately 65,000 taxis in Beijing in 2014, and the proportions of different car models of the current taxi are shown in Figure 1 [27]. The data used in this study come from GPS devices installed in taxis, which send information such as taxi identification, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations."
10.3390_en11030500,"dataset, data",364,,0,"The data used in this study come from GPS devices installed in taxis, which send information such as taxi identification, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations. An example of the dataset is shown in Table 2 The data are stored in Oracle and, thus, are easily dealt with using the connection between Oracle (Oracle 11g, Oracle, Redwood City, CA, USA) and ArcGIS (ArcGIS 10.3, Esri, Redlands, CA, USA) software. For example, we clean records with invalid locations, those whose coordinates deviate significantly within a few seconds, and those whose speed is always at zero. We delete repetitive records and convert some fields to enable its direct use in the next step, thus reducing the data scale, as well as improving result accuracy and computing efficiency. We then match data points to spatial coordinates using the geographic map in ArcGIS, after which the “Track intervals to line” tool is applied to obtain taxi trajectories with an average speed, which are used later to obtain the fuel consumption, the distance, and driving duration between two adjacent points of the same taxi. Through the reconstructed taxi trajectories shown in Figure 2, we obtain the structure of the road network in Beijing (Figure 3), which corresponds closely to the actual road network. 3%5%11%31%50% Elantra Jetta Sonata Elysee OthersProportion of the car models Figure 1. Proportion of different car models of the current taxi in Beijing. Energies 2018, 11, 500"
10.3390_en11030500,"used dataset, data available, dataset, data",216,,0,"Furthermore, because of their low cost, wide coverage, easy data access, accurate allocation, high continuity, and, most importantly, their feasibility, big taxi GPS data can also identify the trafﬁc state [7,8]. This includes exhaustive analyses of spatiotemporal congestion patterns on urban roads [9] and measurements of trafﬁc jam indicators [10], allowing for a better understanding of the operational states of road networks. Moreover, by proposing different and effective anomaly detection methods, some studies have used the taxi dataset to detect anomalous trafﬁc events, which occur when the corresponding indicators deviate signiﬁcantly from the expected values [11], and to detect anomalous routes such as fraudulent taxi travel patterns or trafﬁc accidents, as well as identifying the parts of the trajectories responsible for the anomalies and ranking them with an ongoing anomaly score [12]. Other studies have monitored unexpected behaviors, such as vehicle breakdowns, or one-time events like large sporting events, fairs, and conventions, which exhibit the largest statistically signiﬁcant departure from expected behavior [13]. Additional research has estimated the average relationship between travel time and driving speed [14–16]."
10.3390_en11040947,"data https, data available, data",14,,0,Data. Available online: www.csir.co.za (accessed on 22 July 2016).
10.3390_en11071817,benchmark,91,,0,"A real option model of renewable energy generation investment under the current benchmark electricity price system was constructed. The numerical simulation and sensitivity test of the model were carried out by the Monte-Carlo method. This paper discussed the feasibility of investing in wind power projects under the current net electricity price level in China, analyzed the investment opportunity of the investment in the renewable energy project of wind power generation and revealed the change law of the value of the carbon price gap with the related parameters."
10.3390_en11071817,case study data,4,,0,4. Case Study
10.3390_en11071817,database,16,,0,27. Polaris Power Network Database. 2017. Available online: http://news.bjx.com.cn/html/20170420/821314.
10.3390_en11071817,database,18,,0,28. Carbon Emissions Trading Database. 2017. Available online: http://www.tanpaifang.com/ (accessed on
10.3390_en11071832,"data https, data available, data",18,,0,16. Ensol. Technical Data of the Flat Solar Collectors. Available online: http://ensol.pl/new_ensol/wp-content/
10.3390_en11082032,"dataset provided, data",45,,0,Author Contributions: Y.Y. conceived this study. Y.L. designed the methodology and performed the experiments. M.C. carried out the data analysis. X.W. provided guidance for this study. All authors contributed to the writing of this paper.
10.3390_en11113125,benchmark,71,,0,"Cost minimization is the main objective for which the hybrid heuristic technique is designed to optimize the DSM using the RTP tariff. Figure 7a elucidates EC of all OTIs. The ﬁgures clearly demonstrate that benchmark schemes outperformed in terms of EC; however, the proposed hybrid algorithm outperformed by sacriﬁcing UC with affordable WT. EC values for single homes using RTP are shown in Table 6."
10.3390_en11113125,benchmark,89,,0,"The performance of the proposed hybrid algorithm is evaluated using a CPP price tariff. Our proposed hybrid algorithm outperformed as compared to benchmark schemes. Algorithm is envisioned to evade peak formation in any obvious slots of working hours. Therefore, price reduction happens. Our proposed and implemented technique performed fabulously in the case of different power consumption patterns. Figure 4 shows the behavior of load using CPP with four different OTIs. However, total load should be equal before and after scheduling."
10.3390_en12030483,benchmark,46,,0,"IEEE Trans. Power Deliv. 2014, 29, 2018–2031. [CrossRef] Strunz, K. Benchmark Systems for Network Integration of Renewable and Distributed Energy Resources; Technical Report 575, CIGRÉ TF C6.04.02; CIGRÉ: Paris, France, 2014."
10.3390_en12030483,retrieve,56,,0,"The operating principle of such a method is the following. At every time instant tn power and ROCOF measurements are retrieved, and the inertia constant is computed using (17). Then, the residual of N consecutive inertia estimates with respect to the current inertia estimation H(tn) is computed"
10.3390_en12050875,"database, data",105,,1,"The case study used for testing the methodology was chosen from the commercial reference buildings database [26] of the US Department of Energy (DOE). A secondary school located in San Francisco (California) and constructed after the year of 1980 was selected. Data about the energy load demands (whose hourly values are shown in Figure 4) were calculated by means of EnergyPlus simulation software [27] and then imported and processed in MATLAB. Hourly temperatures of the typical meteorological year of San Francisco, which are shown in Figure 5, were considered."
10.3390_en12050902,dataset,154,,0,"The input dataset was generated and normalized between −1 and 1 with the Min-Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration. If this error presented an increasing behavior in a pre-determined number of veriﬁcations (20 iterations), the training was ﬁnalized. The intent of this technique was to ﬁnd the exact moment when the FFNN started to lose the ability to generalize."
10.3390_en12050902,dataset,345,,0,"SOC Imc bat < E(t) < 0 E(t) < Imc bat SC Li‐ion SC Li‐ion SOCbat > SOCbat max & SOCsc = SOCsc max Safety condition SOCbat < SOCbat max & SOCsc = SOCsc max 0 −E(t) × Imax 0 Imc bat × Imax SOCbat < SOCbat max & SOCsc ≠ SOCsc max 0 −E(t) × Imax −(E(t) − Imc bat) × Imax Imc bat × Imax SOCbat > SOCbat max & SOCsc ≠ SOCsc max −E(t) × Imax 0 −E(t) × Imax 0 The input dataset was generated and normalized between −1 and 1 with the Min‐Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration. If this error presented an increasing behavior in a pre‐determined number of verifications (20 iterations), the training was finalized. The intent of this technique was to find the exact moment when the FFNN started to lose the ability to generalize. Figure 8. Multi‐Layer Feed‐Forward Neural Network architecture. 3.2. Control Scheme The implemented control scheme can operate in two different modes, i.e., with closed‐loop current control or closed‐loop voltage control, as shown in Figure 9. The operating modes were selected before starting the system. Output LayerInput LayerHidden Layerb11b12b13b1JW11,1W1i,jSOC(t)batSOC(t)scf1f1f1f1f2f2W21,1W2j,2b22b21E(t) I(t)batI(t)SCEnergies 2019, 12, 902"
10.3390_en12050902,dataset,355,,0,"Energies 2018, 11, x FOR PEER REVIEW  11 of 26  Table 2. Rules for ESS discharge mode. SOC 0 < E(t) < Imd bat E(t) > Imd bat SC Li‐ion SC Li‐ion SOCbat < SOCbat min & SOCsc < SOCsc min Safety condition SOCbat < SOCbat min & SOCsc > SOCsc min E(t) × Imax 0 E(t) × Imax 0 SOCbat > SOCbat min & SOCsc > SOCsc min 0 E(t) × Imax (E(t) − Imax bat) × Imd bat Imd bat × Imax SOCbat > SOCbat min & SOCsc < SOCsc min 0 E(t) × Imax 0 Imd bat × Imax Table 3. Rules for ESS charge mode. SOC Imc bat < E(t) < 0 E(t) < Imc bat SC Li‐ion SC Li‐ion SOCbat > SOCbat max & SOCsc = SOCsc max Safety condition SOCbat < SOCbat max & SOCsc = SOCsc max 0 −E(t) × Imax 0 Imc bat × Imax SOCbat < SOCbat max & SOCsc ≠ SOCsc max 0 −E(t) × Imax −(E(t) − Imc bat) × Imax Imc bat × Imax SOCbat > SOCbat max & SOCsc ≠ SOCsc max −E(t) × Imax 0 −E(t) × Imax 0 The input dataset was generated and normalized between −1 and 1 with the Min‐Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration."
10.3390_en12050902,"dataset, data",255,,0,"where i represents the number of inputs; xi the input data vector; j represents the number of neurons in the hidden layer (in this case 7 neurons); f 1 and f 2 represent the activation functions; Iw represents the weights of the connection between the input layer and the hidden layer; Lw represents the weights of the connection between the hidden layer and the output layer; and b1 and b2 represents the bias of the neurons of the respective layers. The activation function, which works as a threshold, must be chosen appropriately for each layer from the several commonly used functions, such as: the Heaviside function, the symmetric saturating linear function, the sigmoid function, the Gaussian function, the hyperbolic tangent function, and the spline function. In this study was used the symmetric saturating linear function in both layers (output and hidden layers). One of the most important features of an ANN is the training method, which can be divided in two fundamental classes: supervised training method and unsupervised training methods [49]. The implemented training approach was an ofﬂine supervised method, wherein the Levenberg-Marquardt backpropagation algorithm was used to optimize the FFNN internal states (weights and bias). In order to increase ANN learning accuracy and improve their capacity to generalize, input datasets were created with artiﬁcial data, taking into account several rules."
10.3390_en12061114,"data available, data, dataset provided, publicly available, data https",22,,1,20. Building data public open system. Available online: http://open.eais.go.kr (accessed on 23 January 2019).
10.3390_en12061114,"database, publicly available, data",78,,1,"The Korean government provides two separate databases through the publicly available building data system: a building energy database and an architectural database [20]. The building energy database records building addresses and monthly electric and gas energy consumption. Meanwhile, the architectural database records building addresses, building names, building usage, total ﬂoor area, number of ﬂoors, number of elevators, number of parking lots, and year built."
10.3390_en12071342,code,89,,0,"where Tf is the ﬁlter time constant and T represents the sampling period. A high Tf results in higher smoothing proﬁles, whereas a low one will allow higher order frequencies to pass. The ﬁlter is tuned based on the irradiance pattern, which may not ensure an RR of 10% all the time, as the grid code demand. In [18], the output power of a 150 kW PV system is smoothed by an LPF with time constant of 120-seconds."
10.3390_en12071342,code,96,,0,"Different methods for PV generation ramp-rate calculation have been reported in the literature. The two most common are the difference between two end points or the difference between the maximum and minimum points of the same considered interval [5]. In addition, depending on the grid code, the RR calculation may be presented on instantaneous values or per minute values. In this work, the RR calculation is considered to be the difference between two end points of a given 60-second interval as presented in Equation (6):"
10.3390_en12071342,code,281,,0,"It is estimated that PV energy has surpassed the 400 GWp worldwide capacity at the end of 2017 [1]. This represents less than two percent of the worldwide electricity demand, but when compared to the ambition of China alone, of 1300 GW of solar capacity by the year of 2055 [2], illustrates what is yet to come for PV energy systems. However, the increased penetration of solar energy brings new challenges for grid operators, one of which concerns the short-term variability of solar irradiance [3]. This causes high variations in the injected power that can cause serious grid stability issues. To mitigate this problem, power ramp-rate limitation measures have been included in the electrical grid codes of many countries [4]. Generally, these RR limitations are deﬁned on a second or minute time frame or even in both. In addition, the maximum allowable RR can be deﬁned as a percentage of the plant capacity or as a deﬁned set of power. Some examples are the grid code of Ireland (EirGrid), which states a positive ramp up to 30 MW/minute and Hawaii (HECO), ± 2 MW/minute [5]. For Germany [6] and Puerto Rico (PREPA) [5], a maximum ramp-rate of 10%/minute of the rated PV power is considered. Other grid codes also quantify the maximum allowable ramp-rate in the order of seconds. For example, in Denmark [7], a maximum power ramp-rate of 100 kW/s is required."
10.3390_en12071342,code,346,,0,"20 January 2018. Available online: https://press.trendforce.com/press/20170914-2962.html (accessed on 8 April 2019). Yang, X.J. ; Hu, H.; Tan, T.; Li, J. China’s renewable energy goals by 2050. Environ. Dev. 2016, 20, 83–90. [CrossRef] Yang, Y.; Enjeti, P.; Blaabjerg, F.; Wang, H. Wide-scale adoption of photovoltaic energy: Grid code modiﬁcations are explored in the distribution grid. IEEE Ind. Appl. Mag. 2015, 21, 21–31. [CrossRef] Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants with a Power Output Above 11 kW; Energinet.dk: Denmark, 2015; pp. 1–96. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/ Regulations-for-grid-connection (accessed on 8 April 2019). Booth, S.; Gevorgian, V. Review of PREPA Technical Requirements for Interconnecting Wind and Solar Generation; National Renewable Energy Laboratory: Golden, CO, USA, 2013. BDEW Bundesverband der Energie- und Wasserwirtschaft e.V. Technische Richtlinie, Erzeugungsanlagen Am Mittelspannungsnetz; BDEW: Germany, June 2008; p. 138. Available online: https://www.bonn-netz.de/ Einspeisung/Vertraege/Stromeinspeisevertraege/Anlage-3-Technische-Richtlinien-Erzeugungsanlagenam-Mittelspannungsnetz.pdf (accessed on 8 April 2019). Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants Above 11 kW; Energinet.dk: Denmark, 2016; pp. 1–108. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/Regulations-forgrid-connection (accessed on 8 April 2019). Sangwongwanich, A.; Yang, Y.; Blaabjerg, F. A cost-effective power ramp-rate control strategy for single-phase two-stage grid-connected photovoltaic systems. In Proceedings of the 2016 IEEE Energy Conversion Congress and Exposition (ECCE), Milwaukee, WI, USA, 18–22 September 2016. Omran, W.A. ; Kazerani, M.; Salama, M.M.A. Investigation of Methods for Reduction of Power Fluctuations Generated From Large Grid-Connected Photovoltaic Systems. IEEE Trans. Energy Convers. 2011, 26, 318–327. [CrossRef]"
10.3390_en12071342,code,349,,0,"Tsao, R. Strong Chinese Market to Push Annual Global Photovoltaic Demand Above 100 Gigawatts for 2017. 20 January 2018. Available online: https://press.trendforce.com/press/20170914-2962.html (accessed on 8 April 2019). Yang, X.J. ; Hu, H.; Tan, T.; Li, J. China’s renewable energy goals by 2050. Environ. Dev. 2016, 20, 83–90. [CrossRef] Yang, Y.; Enjeti, P.; Blaabjerg, F.; Wang, H. Wide-scale adoption of photovoltaic energy: Grid code modiﬁcations are explored in the distribution grid. IEEE Ind. Appl. Mag. 2015, 21, 21–31. [CrossRef] Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants with a Power Output Above 11 kW; Energinet.dk: Denmark, 2015; pp. 1–96. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/ Regulations-for-grid-connection (accessed on 8 April 2019). Booth, S.; Gevorgian, V. Review of PREPA Technical Requirements for Interconnecting Wind and Solar Generation; National Renewable Energy Laboratory: Golden, CO, USA, 2013. BDEW Bundesverband der Energie- und Wasserwirtschaft e.V. Technische Richtlinie, Erzeugungsanlagen Am Mittelspannungsnetz; BDEW: Germany, June 2008; p. 138. Available online: https://www.bonn-netz.de/ Einspeisung/Vertraege/Stromeinspeisevertraege/Anlage-3-Technische-Richtlinien-Erzeugungsanlagenam-Mittelspannungsnetz.pdf (accessed on 8 April 2019). Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants Above 11 kW; Energinet.dk: Denmark, 2016; pp. 1–108. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/Regulations-forgrid-connection (accessed on 8 April 2019). Sangwongwanich, A.; Yang, Y.; Blaabjerg, F. A cost-effective power ramp-rate control strategy for single-phase two-stage grid-connected photovoltaic systems. In Proceedings of the 2016 IEEE Energy Conversion Congress and Exposition (ECCE), Milwaukee, WI, USA, 18–22 September 2016. Omran, W.A. ; Kazerani, M.; Salama, M.M.A. Investigation of Methods for Reduction of Power Fluctuations Generated From Large Grid-Connected Photovoltaic Systems."
10.3390_en12081458,benchmark,19,,0,54. CIGRE. CIGRE Task Force C6.04: Benchmark Systems for Network Integration of Renewable and Distributed Energy
10.3390_en12081458,code,108,,0,"Diﬀerent reactive power control methods including PF(P), constant PF and constant Q control are tested. The conﬁgurations of the methods are based on Technical Regulation 3.2.1 (TR 3.2.1) and Technical Regulation 3.2.2 (TR 3.2.2), the Danish grid code for power plants up to and including 11 kW and PV power plants above 11 kW, respectively. The demonstration of the methods is shown in Figure 2 [34,35]. The PV inverters usually operate with two types of PF, i.e., lagging and leading PF. From the generator convention, they are deﬁned as"
10.3390_en12081458,code,188,,0,"Diﬀerent reactive power control methods have diﬀerent eﬀects on these problems. With the PF(P) control, the power loss and the overload problem in the transformers are the lowest among all the three cases. From this aspect, the PF(P) control shows the best performance in all three case studies. With the constant PF control, the lowest power loss of the entire grid occurs if the PV systems operate with a PF of 0.9 (leading), which is not required by the current Danish grid code for small scale residential PV plants, i.e., TR 3.2.1. Judging from the yearly average power loss condition, it can be beneﬁcial to operate PV systems with leading PF, especially in the grid with many cables. In terms of the worst overvoltage situation, constant PF control results in the lowest maximum bus voltage. With a constant Q control applied to large-scale PV power plants, the grid has the highest loss and experiences the most severe overvoltage problem among all three cases."
10.3390_en12081458,code,230,,0,"Abstract: Driven by the Energy Strategy 2050 of Denmark, renewable energy sources (RESs) are increasingly integrated into the Danish power grid. Solar photovoltaic (PV) plants play an important role in this process. This paper conducted a study to investigate the impacts of residential solar PV integration in the distribution grid on voltage security and grid loss based on the 10 kV distribution grid in Bornholm. Three case studies are performed to test three diﬀerent reactive power control methods, i.e., PF(P), constant PF and constant Q, at diﬀerent penetration levels. The assessment of the impacts of PV integration and diﬀerent control methods are done in the DIgSILENT PowerFactory. It was found that PV integration can contribute to reducing the loss of the system, increased overvoltage in buses and overload in transformers, and 40% penetration at the low voltage is considered to be an optimal level based on the result. PF(P) control gives the best performance among all three methods under the current grid codes. With constant PF control, it was found that the system loss can be signiﬁcantly reduced if the PV systems operate with a power factor of 0.9 leading, which is not the norm of the current Danish grid code."
10.3390_en12081458,dataset,130,,0,"The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reﬂect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figures 6 and 9 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand."
10.3390_en12081458,dataset,335,,0,"By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9."
10.3390_en12081458,dataset,337,,0,"Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with"
10.3390_en12081458,dataset,341,,0,"In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants."
10.3390_en12081458,dataset,342,,0,"Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor."
10.3390_en12081458,dataset,345,,0,"The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, 1458"
10.3390_en12081458,dataset,346,,0,"Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case."
10.3390_en12091599,"data available, data",83,,0,"The proposed method employs the RANSAC algorithm to estimate the parameters for the ﬁtting function. In contrast, the classic parameter estimation algorithm least-squares method is based on the smoothness assumption and cannot detect and eliminate the abnormal data. However, the smoothness assumption is not available in most cases, including 3D point cloud data with noise that cannot be compensated. Thus, the RANSAC algorithm is a key component in the process of axis acquisition."
10.3390_en12091599,"database, data",329,,0,"Riveiro, B.; DeJong, M.J.; Conde, B. Automated processing of large point clouds for structural health monitoring of masonry arch bridges. Autom. Constr. 2016, 72, 258–268. [CrossRef] Díaz-Vilariño, L.; González-Jorge, H.; Bueno, M. Automatic classiﬁcation of urban pavements using mobile LiDAR data and roughness descriptors. Constr. Build. Mater. 2016, 102, 208–215. [CrossRef] Vazaios, I.; Vlachopoulos, N.; Diederichs, M.S. Integration of Lidar-based structural input and discrete fracture network generation for underground applications. Geotech. Geol. Eng. 2017, 35, 2227–2251. [CrossRef] Palmer, D.; Koumpli, E.; Cole, I. A GIS-Based Method for Identiﬁcation of Wide Area Rooftop Suitability for Minimum Size PV Systems Using LiDAR Data and Photogrammetry. Energies 2018, 12, 3506. [CrossRef] Le Clainche, S.; Lorente, L.; Vega, J.; Vega Jose, M. Wind Predictions Upstream Wind Turbines from a LiDAR Database. Energies 2018, 3, 543. [CrossRef] Yan, Y.; Tan, Z.; Su, N. Building Extraction Based on an Optimized Stacked Sparse Autoencoder of Structure and Training Samples Using LIDAR DSM and Optical Images. Sensors 2017, 17, 1957. [CrossRef] [PubMed] Bosché, F.; Ahmed, M.; Turkan, Y. The value of integrating Scan-to-BIM and Scan-vs-BIM techniques for construction monitoring using laser scanning and BIM: The case of cylindrical MEP components. Autom. Constr. 2015, 49, 201–213. [CrossRef] Argüelles-Fraga, R.; Ordóñez, C.; García-Cortés, S. Measurement planning for circular cross-section tunnels using terrestrial laser scanning. Autom. Constr. 2013, 31, 1–9. [CrossRef]"
10.3390_en12091599,dataset,161,,0,"25. Chen, S.; Walske, M.L.; Davies, I.J. Rapid mapping and analysing rock mass discontinuities with 3D terrestrial laser scanning in the underground excavation. Int. J. Rock Mech. Min. Sci. 2018, 110, 28–35. [CrossRef] Sánchez-Rodríguez, A.; Riveiro, B.; Soilán, M. Automated detection and decomposition of railway tunnels from Mobile Laser Scanning Datasets. Autom. Constr. 2018, 96, 171–179. [CrossRef] Fumarola, M.; Poelman, R. Generating virtual environments of real world facilities: Discussing four different approaches. Autom. Constr. 2011, 20, 263–269. [CrossRef] Fischler, M.A.; Bolles, R.C. Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and automated cartography. Commun. ACM 1981, 24, 381–395. [CrossRef]"
10.3390_en12091616,benchmark,141,,0,"The abovementioned control strategies have been numerically simulated by MATLAB/Simulink tool. The main system parameters used in the simulation are listed in Table 1. The sampling frequency remains the same as 20 kHz. For simplicity, the conventional DPC method and conventional MPC-based DPC methods without one-step-delay compensation are denoted as “CDPC” and “CMPC-I”, which are used as a benchmark for comparison. The conventional MPC with only one-step-delay compensation and stability enhancement is denoted as “CMPC-II”, the conventional MPC with only proposed mutual inﬂuence elimination is denoted as “MMPC-I”, and the proposed multi-functional MPC with both steady-state and dynamic performance improvement as “MMPC-II”. For convenience, the power ﬂow direction from the AC side to the DC load is supposed as positive."
10.3390_en12091616,benchmark,338,,0,"Energies 2019, 12, x FOR PEER REVIEW 8 of 17 The abovementioned control strategies have been numerically simulated by MATLAB/Simulink tool. The main system parameters used in the simulation are listed in Table 1. The sampling frequency remains the same as 20 kHz. For simplicity, the conventional DPC method and conventional MPC-based DPC methods without one-step-delay compensation are denoted as “CDPC” and “CMPC-I”, which are used as a benchmark for comparison. The conventional MPC with only one-step-delay compensation and stability enhancement is denoted as “CMPC-II”, the conventional MPC with only proposed mutual influence elimination is denoted as “MMPC-I”, and the proposed multi-functional MPC with both steady-state and dynamic performance improvement as “MMPC-II”. For convenience, the power flow direction from the AC side to the DC load is supposed as positive. To analyze both the steady and dynamic-state performances for each of the control strategies, the P*steps up from 0 kW to 4 kW at 0 s while the Q* remains at 0 kVar. After that, the active power decreases to −5 kW at 0.02 s, the reactive power boosts to 3 kVar at 0.04 s. At 0.06 s, the active power boosts from −5 kW to 7 kW, while the reactive power reduces to −4 kVar at 0.08 s. At 0.1 s, the active power decreases from 7 kW to 0 kW. 5.1. Steady-State Performance Comparison To compare the steady-state performance, the AC side input current and reactive power in the simulation are presented from 0 s to 0.04 s to indicate the detailed power ripples. As we can see from Figure 5, both the active and reactive powers track their reference values with high accuracy. From Figure 5a, we can observe that the power ripple reduces and the line currents are more sinusoidal with CMPC-I method."
10.3390_en12091616,benchmark,340,,0,"For convenience, the power flow direction from the AC side to the DC load is supposed as positive. To analyze both the steady and dynamic-state performances for each of the control strategies, the P*steps up from 0 kW to 4 kW at 0 s while the Q* remains at 0 kVar. After that, the active power decreases to −5 kW at 0.02 s, the reactive power boosts to 3 kVar at 0.04 s. At 0.06 s, the active power boosts from −5 kW to 7 kW, while the reactive power reduces to −4 kVar at 0.08 s. At 0.1 s, the active power decreases from 7 kW to 0 kW. 5.1. Steady-State Performance Comparison To compare the steady-state performance, the AC side input current and reactive power in the simulation are presented from 0 s to 0.04 s to indicate the detailed power ripples. As we can see from Figure 5, both the active and reactive powers track their reference values with high accuracy. From Figure 5a, we can observe that the power ripple reduces and the line currents are more sinusoidal with CMPC-I method. By introducing the delay compensation, the performance is further improved with CMPC-II method. (a) (b) (c) Figure 5. From top to bottom. (a) CDPC three-phase currents, CDPC reactive powers, CMPC-I three-phase currents, CMPC-I reactive powers; (b) CMPC-II three-phase currents, CMPC-II reactive powers, MMPC-I three-phase currents, MMPC-I reactive powers; (c) MMPC-II three-phase currents, MMPC-II reactive powers. While with MMPC-II control, the ripples of active and reactive powers in the steady-state are improved in comparison with CMPC-I and MMPC-I method, also it is almost the same as CMPC-II control, as shown in Figure 5c, which aligns well with theoretical analyses. Energies 2019, 12, 1616"
10.3390_en12091616,code,256,,0,"By selecting the voltage space vector which achieves the lowest cost function value of (17) after evaluation of all the voltage vectors, four separate optimization problems are solved comprehensively. In (17), each term has a corresponding weighting factor. In actual application, ﬁrstly, the selection of these weighting factors is through trial and error by simulation. Then, it could be implemented in the experiment and make adjustment accordingly. Finally, the selected weighting factors would be implemented in practical application. Each weighting factor value would have an inﬂuence on the weighting factor selection of the others, and the weighting factor values are in relation with system conﬁgurations, thus it is quite a complex task for the mathematical derivation and veriﬁcation about the calculation of each weighting factor, which is out of scope of the main target of this paper, it will be researched in the future work. By selecting proper weighting factors, good dynamic and steady-state performance can be balanced in a systematic way. It is obvious that the additional terms would increase the computational burden compared with the conventional methods due to more complex cost function, especially in less capable hardware system. However, it could be solved by using general methods like machine code optimization once the control strategy has been programmed. In actual application with 20k sampling frequency, the increased computational burden does not aﬀect the control implementation."
10.3390_en12091789,python,191,,0,"In this study, the test MG system has a PV, a WT, a CDG, a BESS, and load demand, as shown in Figure 1. The MG is interconnected with a CBESS and the utility grid. The system can be operated in both grid-connected and islanded modes. The analysis is conducted for a 24-hour scheduling horizon (T = 24 h) and each time interval is set to be 1 hour. The MILP-based model for MG is implemented in Python integrated with CPLEX 12.6 [26]. The Q-learning-based model for CBESS is also implemented in Python. The market price signals, load proﬁle, and the total output of RDGs are shown in Figure 5a,b, respectively. The information of the CDG unit, BESS, and CBESS are tabulated in Table 1. The operation bounds of BESS and CBESS were chosen as [0%, 100%], same as [27]. The detailed numerical results are shown in the following sections for both grid-connected and islanded modes."
10.3390_en12091789,"python, data",341,,0,"Energies 2019, 12, 1789 9 of 17 In this paper, a Q-learning-based operation strategy for CBESS is proposed for the optimal operation of CBESS. To show the effectiveness of the Q-learning-based operation, the resultsof Q-learning-based operation methods are comparedwith the results of the centralized operationmethod. The detailed numerical results are presented in the following section. 3. Numerical Results 3.1. Input Data In this study, the test MG system has a PV, a WT, a CDG, a BESS, and load demand, as shown in Figure 1. The MG is interconnected with a CBESS and the utility grid. The system can be operated in both grid-connected and islanded modes. The analysis is conducted for a24-hour scheduling horizon (T = 24 h) and each time interval is set to be 1 hour. The MILP-based model for MG isimplemented in Python integrated withCPLEX 12.6[26]. The Q-learning-basedmodel for CBESS is also implemented in Python. The market price signals, load profile, and the total output of RDGs are shown in Figures 5a,b, respectively. The information of the CDG unit, BESS, and CBESS are tabulated in Table 1. The operation bounds of BESS and CBESS were chosenas [0%, 100%], sameas [27]. The detailed numerical resultsare shown in the following sections for both grid-connected and islanded modes. (a)(b)Figure 5. Input data: (a) market price signals and load profile; (b) output power of the renewable distributed generator (RDG). Table 1. The detail parameters of BESS, CBESS, and controllable distributed generator (CDG). Parameters BESS CBESSParametersCDGMax. CapP(kWh)200 300 Max. maxP(kWh)500 InitialCapiniPSoC⋅(kWh)50 150 Min. minP(kWh)0 Min."
10.3390_en12091789,"python, data",350,,0,"The detailed numerical resultsare shown in the following sections for both grid-connected and islanded modes. (a)(b)Figure 5. Input data: (a) market price signals and load profile; (b) output power of the renewable distributed generator (RDG). Table 1. The detail parameters of BESS, CBESS, and controllable distributed generator (CDG). Parameters BESS CBESSParametersCDGMax. CapP(kWh)200 300 Max. maxP(kWh)500 InitialCapiniPSoC⋅(kWh)50 150 Min. minP(kWh)0 Min. minCapPSoC⋅(kWh)0 0 Cost/kWh CDGC(KRW)136 Char. Loss L+(%) 5 3 Start-up costSUC(KRW) 200 Dis. LossL−(%) 5 3 Shut-down cost SDC(KRW)100 3.2. Operation of the System in Grid-Connected Mode This section presents the operation of the MG and CBESS in grid-connected mode. The MG-EMS performs optimization to minimize the total operation cost of the MG. The amount of buying/selling power is determined based on the amount of surplus/shortage power in the MG system, as shown in Figure 6a. The buying/selling power of the MG is traded with two external systems, i.e., CBESS and the utility grid. It can be observed from Figure 6b that the CBESS always decides to importpower from cheaper resources. During intervals 3, 4, 6, the generation cost of CDG is less than the buying prices from the utility grid. Therefore, CBESS decides to charge surplus power from MG instead ofbuying from the utility grid. Figure 6c shows the buying power ofthe MG system. The MG decides to import power from the external systems for minimizing the total operation cost. At intervals 2 and 5, MGimports power from the utility grid to fulfill load amount with cheaper price compared with 1201401601802004006008001471013161922kWhInterval (hour)LoadPrbuyPrsellwon/kWh05101520251357911131517192123kWhInterval (hour)PVWTEnergies 2019, 12, 1789"
10.3390_en12132496,"database, data available, open-source, data",115,,1,"upon the type of ecosystem, as measured by the Köppen climate classiﬁcation, which has been recently updated [30]. At present, climatic data is available for most of the climates; according to the present analysis, the test reference year came from several open source databases [31], with the only exception of Zurich, whose TRY belongs of the Meteonorm database [20]. Among the diﬀerent ways to deﬁne the weather conditions, the Köppen–Geiger scale and the degree days (DDs) have been used [28]. The European map of the Köppen–Geiger climate classiﬁcation is shown in Figure 1."
10.3390_en12132522,database,5,,0,3.1. Establish Feature Database
10.3390_en12132522,database,5,,0,4.2. Establish Feature Database
10.3390_en12132522,database,38,,0,"(b) Establish a feature database. Extract features by Spearman correlation coefﬁcient and GBDT relative importance, and choose the feature set that minimizes the average error of the model as the ﬁnal feature database."
10.3390_en12132522,database,75,,0,"So far, we can jointly determine the contribution of each feature through the analysis of GBDT relative importance and Spearman correlation coefﬁcient. Next, we need to further ﬁlter features according to the prediction performance of different feature combinations on GBDT. Here, we use mean squared error (MSE) to evaluate. The ultimate feature database is the feature set that minimizes MSE. The formula is as follows:"
10.3390_en12132522,"database, data",45,,0,"Line loss prediction modelPrediction and analysisEstabish GBDT line loss prediction modelValidate and evaluate the modelPredict line loss rate Select electrical featuresEstablish feature databaseCompare algorithms and analyze the error Data PreprocessingFeature database in LV distribution network Intergrate and standardize data Energies 2019, 12, 2522"
10.3390_en12132522,"database, dataset, data",162,,0,"The aims of this paper are to distinguish key features that signiﬁcantly inﬂuence line loss rate, and predict line loss rate under the condition that outliers exist. Consequently, we propose a gradient boosting decision tree (GBDT)-based approach to calculate line loss rate in the LV distribution network. First, we select the features by correlation analysis and construct the corresponding database. Second, considering the great difference in grid structure and the numerical dispersion of line loss rate, we use density-based spatial clustering of applications with noise (DBSCAN) to classify the LV distribution network. Finally, we establish GBDT prediction models for each area, assess the prediction results, and revise the outliers. Rationality and effectiveness have been veriﬁed through the analysis of the data set in a city and the comparison among other algorithms. What is more, the prediction accuracy can be signiﬁcantly improved."
10.3390_en12132522,"dataset, data",23,,0,(a) Input data set. Set the scan radius (cid:101) and the minimum number of samples in the neighborhood
10.3390_en12132522,"dataset, data",27,,0,"(c) Form a data set of core object. For Xi, ﬁnd all neighbor points within (cid:101) distance. Points with a"
10.3390_en12132522,"dataset, data",62,,0,"(d) For those core points that are not already assigned to a cluster, create a new cluster. Recursively ﬁnd all its density connected points and assign them to the same cluster as the core point. (e) Iterate through the unvisited points in data set. Points that do not belong to any cluster will be"
10.3390_en12132522,"dataset, data",114,,0,"Since GBDT is a serial computing model, it is challenging to carry out parallel computing. Therefore, in this paper, we normalize GBDT with a subsample, which ranges from 0 to 1. This approach is called stochastic gradient boosting tree (SGBT) [46]. At each round of iteration, a subsample of training data is drawn at random from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to ﬁt the base learner. In this way, the GBDT model can be partially paralleled, thereby reducing model overﬁtting to a certain extent."
10.3390_en12132522,"dataset, data",130,,0,"In this paper, we propose a GBDT-based approach to predict line loss rate in the LV distribution network. Effectiveness has been demonstrated by the analysis and veriﬁcation in the data set of the LV distribution network. First of all, the strength of our approach lies in its high accuracy of predicting line loss rate. In addition, paralleling in a subsample fashion can reduce overﬁtting to some extent, which overcomes the shortcoming of the original GBDT. Moreover, its good robustness to the outliers and missing values, as well as its partially paralleled design makes it possible to perform better in the process of predicting line loss rate, compared with SVR and RF. We believe that GBDT is of high"
10.3390_en12132522,"dataset, data",156,,0,"Although these algorithms are good at handling various nonlinear problems, in many cases SVR is sensitive to outliers and missing values, while GBDT and RF are not. Both GBDT and RF are ensemble learning methods and predict by combining the outputs from individual trees, yet on our dataset, GBDT proved to be more effective than RF. The reason is that, for data including categorical variables with different numbers of levels (e.g., the total length of the line ranges from hundreds to thousands), random forests are more biased in favor of those attributes with more levels, compared with GBDT. Therefore, the variable importance scores from random forest are not reliable for this type of data, hence the prediction accuracy of RF would be severely affected. In summary, the overall prediction results of GBDT are better than SVR and RF."
10.3390_en12132522,"dataset, data",170,,0,"Classiﬁcation and regression trees (CART) is one of the most well-established machine learning techniques, ﬁrst introduced by Breiman in 1984 [21]. CART is a typical binary tree, its essence is to divide the feature space into two parts and split the scalar attribute and the continuous attribute [22–26]. The CART algorithm consists of the following two steps: (1) Generate decision tree. This is done via training data set, build nodes from top to bottom. In order to make the resulting child nodes as pure as possible, split each node according to the best attribute. For the classiﬁcation problem, use GINI value as the basis for splitting node; for the regression problem, use the smallest variance instead. (2) Pruning. A technique that improves predictive accuracy by reducing overﬁtting and includes pre-pruning and post-pruning. Pre-pruning is to terminate the growth of the decision tree in advance"
10.3390_en12132522,"dataset, data",194,,0,"Abstract: Line loss rate plays an essential role in evaluating the economic operation of power systems. However, in a low voltage (LV) distribution network, calculating line loss rate has become more cumbersome due to poor conﬁguration of the measuring and detecting device, the difﬁculty in collecting operational data, and the excessive number of components and nodes. Most previous studies mainly focused on the approaches to calculate or predict line loss rate, but rarely involve the evaluation of the prediction results. In this paper, we propose an approach based on a gradient boosting decision tree (GBDT), to predict line loss rate. GBDT inherits the advantages of both statistical models and AI approaches, and can identify the complex and nonlinear relationship while computing the relative importance among variables. An empirical study on a data set in a city demonstrates that our proposed approach performs well in predicting line loss rate, given a large number of unlabeled examples. Experiments and analysis also conﬁrmed the effectiveness of our proposed approach in anomaly detection and practical project management."
10.3390_en12142675,"data https, data available, data",52,,0,"Environmental Commissioner of Ontario. Surplus Baseload Electricity Generation in Ontario. 2017. Available online: https://eco.on.ca/blog/surplus-baseload-electricity-generation-in-ontario (accessed on 6 April 2018). Power Imports and Exports Data. IESO, 2017. Available online: http://www.ieso.ca/en/power-data/supplyoverview/imports-and-exports (accessed on 5 April 2018)."
10.3390_en12142675,"data https, data available, data",71,,0,"for Ontario. Int. J. Hydrogen Energy 2012, 37, 7343–7354. [CrossRef] IESO. 2017 Electricity Data, Independent Electricity System Operator. 2018. Available online: http: //www.ieso.ca/corporate-ieso/media/year-end-data (accessed on 9 May 2018). Independent Electricity System Operator, Hourly Ontario Energy Price Ieso. 2012. Available online: https://www.ieso.ca/imoweb/marketdata/hoep.asp (accessed on 16 May 2018)."
10.3390_en12173287,"data available, data",45,,0,"from the oﬃcial annual reports of the Consorzio Italiano Biogas (CIB), the Gestore Servizi Energetici (GSE) and the data available through the European Project ISAAC (Increasing Social Awareness and ACceptance of biogas and biomethane) [23–25]."
10.3390_en13010140,"case study data, data",185,,0,"To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS. This case is used as a reference while comparing the total power losses reduction in all considered cases. Then Case-I considers only PVs in place. After that, Case-II includes solar PVs and an aggregated BESS. Finally, Case-III comprises solar PVs and distributed BESSs. A detailed description of each case is presented as per the following subsections:"
10.3390_en13010140,"case study data, data",337,,0,"Energies 2020, 13, x FOR PEER REVIEW 8 of 16 operation is the operation that will do this task as shown in (13) and (14), where γ and σ are random numbers between 0 and 1. ()mdmmnXXXXγ=+− (11) ()ndnnmXXXXγ=+− (12) maxmin()mdmdXXXXσ=+− (13) maxmin()ndndXXXXσ=+−. (14) Several GA approaches have been utilized in recent studies such as [7,15,29]. It indicates and highlights the high effectiveness of the GA compared to other stochastic approaches that are based on the error percentage of the solution and the processing duration. Figure 5 illustrates the utilization of GA to find the optimal allocation of BESS. DN DataBESS DataGA and Load FlowOptimal Allocation of BESS Figure 5. Illustration of GA utilization to find BESS optimal allocation. In this study, the DN data and the BESSs data are fed as inputs to the GA. The input data has been processed through many iterations and GA internal operations. The resultant of the GA processing is the optimal locations of the BESSs in the DN which assure the minimum average power losses. 4. Results and Discussions To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS."
10.3390_en13010140,"case study data, data",341,,0,"DN DataBESS DataGA and Load FlowOptimal Allocation of BESS Figure 5. Illustration of GA utilization to find BESS optimal allocation. In this study, the DN data and the BESSs data are fed as inputs to the GA. The input data has been processed through many iterations and GA internal operations. The resultant of the GA processing is the optimal locations of the BESSs in the DN which assure the minimum average power losses. 4. Results and Discussions To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS. This case is used as a reference while comparing the total power losses reduction in all considered cases. Then Case-I considers only PVs in place. After that, Case-II includes solar PVs and an aggregated BESS. Finally, Case-III comprises solar PVs and distributed BESSs. A detailed description of each case is presented as per the following subsections: 4.1. Distribution System with Solar PVs: Case-I Six solar PVs, each with a size of 1.6 kW, are added arbitrary in the distribution network, as shown in Figure 6. This case demonstrates DN with a high level of PV penetration. The PVs are installed on buses 3, 8, 14, 25, 30, and 31. Energies 2020, 13, 140"
10.3390_en13010197,code,21,,0,"32. DL/T 596—1996 Prevenive Test Code for Electric Power Equipment; China Electric Power Press: Beijing, China,"
10.3390_en13010197,database,12,,0,3. Weight Determination Based on Fuzzy Iteration and Expert Weighted Database
10.3390_en13010197,"database, data",112,,0,"After the key state quantity of the power distribution switch is selected, it is necessary to perform reasonable weight allocation for each state quantity to perform comprehensive evaluation of the state of the distribution network equipment. In this paper, we use the eclectic fuzzy decision-making and multi-level fuzzy comprehensive evaluation model to analyze the previous data of the distribution transformer; continuously update the weight ratio of the evaluation set through the weight inverse operation; reduce the inﬂuence of subjective factors brought by the expert review opinions; and improve the data, the reliability of the analysis and ultimately the establishment of a weight expert database."
10.3390_en13010197,"database, data",156,,0,"In this paper, based on the large amount of operational information generated during the operation of the distribution network equipment, a state evaluation model of the distribution network equipment integrating multi-source information is established. The model comprehensively considers the critical state quantities of the distribution network equipment, and based on the fuzzy iterative method of big data and the establishment of the weight expert database, weights the multi-source information and reasonably evaluates the equipment status. Finally, taking the distribution transformer as an example, the evaluation results of the fusion of multi-source information proposed in this paper are proved to be more comprehensive. The method proposed in this paper can accurately judge the running status of the power distribution equipment based on various types of information, and provide a reference for the subsequent power marketing evaluation of the user equipment state, which is more instructive."
10.3390_en13010197,"database, data",267,,0,"The fuzzy positive ideal and the fuzzy negative ideal are determined by analyzing a large number of transformers of the same type. This paper adopts a combination of eclectic fuzzy decision-making and multilevel fuzzy comprehensive evaluation, and the weights of quantitative indicators and qualitative indicators can be obtained by performing repeated fuzzy iterations. The weight ratio of the evaluation set is constantly updated through the weight inverse operation, reducing the inﬂuence of subjective factors brought by the expert review opinions and avoiding errors caused by data redundancy or errors or omissions, which improving the reliability of data analysis and promoting the establishment of the weight expert database, ﬁnally. The distances between the evaluated object and the fuzzy positive and negative ideals are determined, and the membership degree belonging to the fuzzy positive ideal is calculated. The greater the membership degree, the better the state of the transformer. The smaller the membership degree, the worse the state of the transformer, so it is necessary for the operation and maintenance personnel to pay attention to it and arrange the maintenance work in good time. The comparison and analysis of the relevant data in Tables 6 and 8 show that the ﬁnal score of each transformer in Table 8 can truly reﬂect the actual operation status of the transformer in Table 6, which provides quantitative parameters for the evaluation of distribution equipment, and which is beneﬁcial to the further focus and ﬁeld evaluation of the equipment, providing help for maintenance and operation."
10.3390_en13010197,"database, data",305,,0,"Energies 2020, 13, x FOR PEER REVIEW 7 of 15 improve the data, the reliability of the analysis and ultimately the establishment of a weight expert database. 3.1. Compromising Fuzzy Decision Weight Solving Process The flow chart of the compromise fuzzy decision [34] is shown in Figure 1. The basic principle is the virtual fuzzy positive ideal and the fuzzy negative ideal. Then, the Euclidean distance method is used to determine the distance between the candidate object and the fuzzy positive and negative ideals, and the membership degree belonging to the fuzzy positive ideal is calculated to determine the selection scheme. The greater the degree of membership, the better the solution and the priority. Triangle fuzzy number expression of indicator dataNormalization of fuzzy indicator matrixConstruct a fuzzy decision matrix, that is to perform weightingDetermine fuzzy positive ideals and fuzzy negative idealsMake fuzzy preference decision to obtain the membership degree of the evaluation objectInversely calculate counterweights by frequency statistics, or determine counterweights by fuzzy analytic hierarchy processCompare the inverse weight with the fuzzy weight and calculate the approximate rateApproximation rate is greater than expectedOutput the membership of the evaluation objectInversely calculate counterweights by frequency statistics, or determine counterweights by fuzzy analytic hierarchy processMake the inverse weights the construction weightsReconstruct fuzzy decision matrixendApproximation rate is smaller than expected Figure 1. A flow chart of eclectic fuzzy decision-making model. The basic solution steps for the compromising fuzzy decision are as follows: Step 1: The indicator data is transformed into a triangle fuzzy number representation. Let()FR be the overall fuzzy set on R, set ()MFR∈. The membership function Mμ of M is expressed as Energies 2020, 13, 197"
10.3390_en13020429,code,105,,0,"Simulink 1N). This is mostly due to the lower running time of the plant (9.42% lower). On the other hand, the ORC unit works most of the time at operating conditions having higher electrical eﬃciencies and as a result the mean electric eﬃciency is 3% higher (6.65% compared to 6.44% of the Simulink 1N). The CPU time in Table 3 is measured on a workstation equipped with 32 GB of RAM and the Intel Xeon E5530 @ 2.4 Ghz processor whilst the code is able to use a single thread only."
10.3390_en13020429,code,137,,0,". where the source term Qloss included in the advection Equation (1) can be positive in the case of heat losses from the internal ﬂuid to the ambient and vice versa. Considering that the pipelines under investigation are referred to a microscale CHP plant, a detailed solution of the equation as for DH networks is not necessary whilst the robustness and the velocity of the solving code are preferred. Hence, all the terms of the second member of Equation (1) can be deleted and the source term excluded, with good approximation. Indeed, Van der Heijde et al. [5] have shown that the diﬀusivity term can be neglected, while the pressure diﬀerence and the wall friction are not relevant in comparison with the"
10.3390_en13020505,code,156,,0,"Battery companies have the code of quality assurance of their products to ensure that the battery can operates normally for a certain number of cycles. In this paper, throughput capacity of energy is used as a life standard to evaluate whether the ESS should be retired. According to the battery energy, the number of life cycles, the cut-oﬀ capacity and the depth of discharge (DOD) speciﬁed in the technical agreement, the total throughput capacity of energy of the LTO battery can be calculated by Equation (18). When the cumulative energy throughput of the battery in actual use reaches this value, it can be judged that the battery should be out of service, as shown in Equation (19). At the same time, the decline of the battery life can also be roughly judged based on the used energy throughput."
10.3390_en13040898,code,190,,0,"Supplementary Materials: The following are available online at http://www.mdpi.com/1996-1073/13/4/898/s1, Figure S1: Analysis of Desmodesmus sp. nl3 S516 intron. (a) Phylogenetic analysis of S516 group I introns of IE and IC1 classes amongst microalgae based on the P3–P8 conserved structures, (b) pairwise analysis of group I introns of the Scenedesmaceae family. Figure S2: Amino acid sequences of homing endonucleases (HE) of Naegleria jamiesoni (Njam, AAB71747.1), Porphyra umbilicalis (Pumb, AAV35433), Allovahlkampﬁa spelaea (Aspe, ABD62811), Coemansia mojavensis (Cmoj, BAB87243), Naegleria philippinensis (Nphil, CAJ44447), Desmodesmus opoliensis GS2j (DopS2j, AB917110), Sclerotinia sclerotiorium (Sscl, XP_001587714). Color code (according Clustal X color scheme): blue: hydrophobic residues; red: positively charged residues; magenta: negatively charged residues; green: polar residues; cyan: aromatic residues, orange: glycine residue, yellow: proline residue. See [21] for detailed analysis of HE."
10.3390_en13040898,database,49,,0,The overlapping partial sequences between two consecutive sequences were assembled using NCBI Blast Tool (Standard nucleotide blast) to obtain a complete 18S rDNA–ITS1-5.8S-ITS2 sequence which was compared with sequences on NCBI database for hits. The sequence of nl3 is deposited in GenBank (MN746324).
10.3390_en13040898,database,116,,0,"For identiﬁcation, sequence of 18S rDNA excluding the introns and complete ITS1-5.8S-ITS2 region of isolate nl3 was searched against non-redundant nucleotide database for homologous sequences using online blast program (BLASTN) (http://www.ncbi.nlm.nih.gov/BLAST/). Clustal X2.1 software was then employed for the automatic multiple alignments of homologous sequences and studied sequence. A preliminary analysis with Paup was carried out [22]. Phylogenetic trees were generated using maximum likelihood (ML, GTR, G + I:4 model) with TREEFINDER [23], distance (neighbour-joining, K2 model) and maximum parsimony (MP) with MEGA7 [24]), with 1000 bootstraps."
10.3390_en13040898,database,315,,0,"A PCR fragment was obtained using the primers NS1 and ITS4 (Table 1) for both strains named nl3 and nl6. These primers, originally described to amplify the 18S-ITS1-5.8S-ITS2 rDNA region in fungi [25], proved also to work well for land plants [26]. The PCR fragment has a size of around 3500 bp for nl3 and 2600 bp for nl6. PCR products were sequenced using the primers listed in Table 1 in both forward and reverse orientations and blasted against the sequences deposited on NCBI database. The BLAST search results showed for nl6 a 100% identity with N. salina (D12, accession number JX185299.1), while nl3 proved to be closer although not identical to Desmodesmus sp. GM4i (AB917136.1). The sequence of Desmodesmus sp. nl3 is deposited in GenBank (MN746324). Desmodesmus sp. nl3 and Desmodesmus sp. GM4i, both have an intron inserted at the same position in the 18S rDNA, S516 (referring to the insertion in the rDNA of E. coli), although the nl3 is longer in size (754 pb versus 404 bp). The presence of S516 intron in Desmodesmus GM4i has already been reported [27]. Therefore, we analyzed the secondary structure of the nl3 S516 intron. It comprises the 9 typical stem-loop structures of group I introns, with an additional loop of 347 bp at the level of the P9.3 branch (Figure 1). Overall, the deduced structure and sequence variations strongly indicate that the intron belongs to the E class [28], probably of the E2 type. Another group I intron, S1046, class C, is also present in the 18S rDNA of the strain nl3."
10.3390_en13040898,dataset,17,,0,"Datasets. Mol. Biol. Evol. 2016, 33, 1870–1874. [CrossRef]"
10.3390_en13040937,"data available, data",135,,0,"Considering the areas obtained for each created scenario with the suggested zones for the cultivation of energy crops and the implementation of microalgae crops, and bearing in mind the realistic yield data found in the literature, it is possible to estimate the production values theoretically for each species and scenario. From this starting point, Table 6 presents the value of the areas obtained (according to ArcGIS software), as well as the percentage of these areas out of the whole Portugal mainland area of 89,015 km2, the minimum and maximum productivity (according to Table 1) and the estimated minimum and maximum production for each crop. It is important to specify that the calculated productivity values are overestimated, considering a productivity of 100%."
10.3390_en13040937,"data available, data",174,,0,"In order to obtain the appropriate areas for the crops implementation, it was necessary to compile as many data or factors as possible, according to information available from various sources, mainly on official websites of Portuguese and European institutions. The administrative maps of the territory and those related to land use and occupation provided by DGT have been considered; environmental aspects such as temperature, precipitation, sunshine and frost provided by APA, including the map created with CO2 production in each municipality; various ecological factors of soil and subsoil and the edapho-morphological aptitude for agriculture and forestry of the Ecological Planning, Investigation and Cartography - EPIC WebGIS platform (ISA-UL); protected areas and soils susceptible to desertification from ICNF; contaminated soils based on the mining areas managed by EDM and the capacity and treatment applied in the wastewater treatment plants (WWTPs) in mainland Portugal, according to the EEA platform, each factor being considered a spatial thematic layer."
10.3390_en13040937,"data available, data",204,,0,"Based on data found in the literature, the saline soils are considered marginal, therefore, areas with moderate and high concentrations of saline elements are inadequate for the implementation of food crops [7]. The term salinization refers to areas with low precipitation and high evapotranspiration that causes salt accumulation making it impossible to wash on the soil surface. These areas can be found in the coastal part of the territory [8]. Much of the marginal land could be used for agriculture due to the quality and type of soil, however, many of them, are found in high zones, with high slopes, hard-to-reach areas or abandoned land, that are no longer used for this purpose [8] and now are considered suitable for other purposes such as the implementation of energy crops. For these reasons, in this study, we consider as marginal lands the pasture areas such as natural herbaceous vegetation, areas with dense, light dense undergrowth, dense and dense sclerophyte vegetation, other woody formations and, lastly, areas related to uncovered spaces or with sparse vegetation [44]."
10.3390_en13040937,"data available, download, data",337,,0,"To select the energy crops to implement in the mainland Portugal; To search which types of soils are of interest and present a low ILUC (indirect land-use change) risk; To search and download all colletected maps finded in shapefile or raster format from official websites of Portuguese Institutions like Agência Portuguesa do Ambiente (APA), Instituto Superior de Agronomia da Universidade de Lisboa (ISA-UL), Instituto da Conservação da Natureza e das Florestas (ICNF), Empresa de Desenvolvimento Mineiro (EDM) and European Institutions too as the European Environment Agency (EEA); To create the georeferenced databases on ArcGIS, an ArcMAP document (tool of ArcGIS software) has to be created for each chosen culture, introducing only the selected maps for specif properties and/or attributes of interest such as temperature, precipitation, frost, land steepness, soil texture, soil pH, soil thickness, presence of physical obstacles, ecological soil value, current permeability, natural and semi-natural vegetation with conservation value, soil-morphological aptitude to irrigated agriculture and silviculture, soil susceptibility to desertification, protected areas, land use and land cover (COS 2010 and COS 2015), corine land cover (CLC 2012), contaminated soils, wastewater treatment plant capacity, CO2 production in the energy and industrial sectors in mainland Portugal, among others. Bearing in mind the characteristics of growth and adaptation of each culture combined with the intersection of all maps, output data have been obtained suggesting available and appropriate areas for the cultivation of each culture. The productivity forecasting and predicted bioenergy generation are presented and critically discussed; Lastly, the publication on Laboratório Nacional de Energia e Geologia–LNEG´s spatial data infrastructure, i.e., institutional geoportal of energy and geology, to access all the created maps and related information. Figure 1 represents a summary of the applied methodology."
10.3390_en13040937,database,51,,0,"The database for georeferenced mapping of the mainland territory, to evaluate areas of potential interest for the cultivation of energy crops, microalgae, as well as to map the cultivated agricultural/silvicultural species (including their residues), was created with ArcGIS software, a tool for GIS."
10.3390_en13040937,"database, data",232,,0,"Abstract: The main objective of the Portuguese project “CONVERTE-Biomass Potential for Energy” is to support the transition to a low-carbon economy, identifying biomass typologies in mainland Portugal, namely agri-forest waste, energy crops and microalgae. Therefore, the aim was to design and construct a georeferenced (mapping) database for mainland Portugal, to identify land availability for the implementation of energy crops and microalgae cultures, and to locate agricultural and forestry production areas (including their residues) with potential for sustainable exploitation for energy. The ArcGIS software was used as a Geographic Information System (GIS) tool, introducing the data corresponding to the type of soil, water needs and edaphoclimatic conditions in shapefile and raster data type, to assess the areas for the implantation of the biomass of interest. After analysing the data of interest in each map in ArcGIS, the intersection of all maps is presented, suggesting adequate areas and predicting biomass productions for the implementation of each culture in mainland Portugal. Under the conditions of the study, cardoon (72 kha, 1085 kt), paulownia (81 kha, 26 kt) and microalgae (29 kha, 1616 kt) presented the greater viability to be exploited as biomass to energy in degraded and marginal soils."
10.3390_en13051203,dataset,244,,0,"high-quality scenarios. Step 0: initialization. Randomly select S H, and set the estimated social welfare as ˆΨs = 0, ∀s ∈ S H. Initialize the training dataset as empty. Step 1: social welfare calculation. Solve the GEP problem using Algorithm 1, and use its optimal solution (x∗, p∗) and Equation (18) to calculate the actual social welfare for the set of high-quality scenarios S H. Checkpoint: The algorithm ﬁnishes if the error between actual and estimated social welfare is small enough and proceeds to Step 2 otherwise. Step 2: social welfare estimation. Add new results from Step 1 to the training dataset; obtain updated regression parameters for Model in Equation (20), then use the updated model to estimate social welfare values for the whole set of scenarios S W. Step 3: scenario selection. Update the set of high-quality scenarios S H by minimizing the Kantorovich distance from S W, DKan(S H, S W), which was deﬁned in Equation (19). Various heuristic algorithms, such as the golden section search method [49], can be used in this step. As proven by Dupaˇcová et al. [17], the probabilities of high-quality scenarios are given by ps = ∑ pi, ∀s ∈ S H, where"
10.3390_en13051203,"dataset provided, data",155,,0,"We use a linear regression model to provide a computationally efﬁcient estimation of social welfare. Conceptually, high-quality scenarios could be selected by ﬁrst calculating the social welfare for all scenarios and then selecting a subset to minimize the probabilistic distance between the distributions of social welfare resulting from the high-quality and whole set of scenarios. However, this approach requires solving the GEP problem millions of times using the time consuming Algorithm 1. Alternatively, our strategy is to train a regression model to estimate social welfare and select the high-quality scenarios based on the estimated rather than actual social welfare values. If trained efﬁciently, the regression model requires only a small set of training data to provide reasonably accurate estimation; thus, we only need to use Algorithm 1 to calculate the actual social welfare for a small number of scenarios to produce the training data."
10.3390_en13051203,"used dataset, dataset",208,,0,"We used the same dataset for the U.S. Eastern and Western Interconnections as used in [50] with some modiﬁcations. The dataset contained 169 buses, 730 transmission lines, 1640 existing generators, and 1568 candidate generators, representing the transmission infrastructures of the North American power grid. The locations of the 169 buses are shown in Figure 3. Demand for each year was divided into 19 load blocks. There were 60 generation technologies and fuel types, including coal, gas, oil, nuclear, hydro, geothermal, biomass, wind, and solar. Approximately 30% of existing generation capacity was renewable, and this ratio was required to increase by 1% each year, so that it would reach 45% by the end of year 15. One million demand and fuel cost elements of the scenarios were randomly generated with an average 1% annual growth rate for both. All algorithms were implemented in MATLAB [51], and to solve the models, we used MATLAB and the TOMLAB interface [52] to call CPLEX V.12 [53], used as the mixed integer linear programming solver."
10.3390_en13051246,"code, data",231,,0,"Abstract: Rooftop gardens ona building have proved to be a good way to improve its storm water management, but many other beneﬁts can be obtained from the installation of these systems, such as reduction of energy consumption, decrease of the heat stress, abatement on CO2 emissions, etc. In this paper, the eﬀect from the presence of these rooftop gardens on abuilding’s energy consumption has been investigated by experimental campaigns using a green roof ona public building in a Mediterranean location in Spain. The obtained results demonstrate a substantial improvement by the installation of the green roof onthe building’s cooling energy demand for a standard summer day, in the order of 30%, and a reduction, about 15%, in the heating energy demand for a winter day. Thus, given the longer duration of the summer conditions along the year, a noticeable reduction on energy demand could be obtained. Simulation analysis, using commercial software TRNSYS code, previously calibrated using experimental data for typical summer and winter days, allows for the extrapolation to the entire year of these results deducing noticeable improvement in energy eﬃciency, in the order of 19%, but with an increase of 6% in the peak power during the winter period."
10.3390_en13051246,"dataset provided, data",57,,0,"A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at diﬀerent layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools."
10.3390_en13051246,"dataset provided, data",154,,0,"Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, 1246"
10.3390_en13051246,"dataset provided, data",326,,0,"The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, 1246"
10.3390_en13051246,"dataset provided, data",345,,0,"Energies 2020, 13, x FOR PEER REVIEW 4 of 15  Figure 3. Initial roof structure. The green roof was built over the present “inverted roof”. It was decided to remove the layer of gravel and a water retention layer was added below the growing medium (separated with a filter fabric layer). This storage layer increases the capacity of the roof for retaining water after a rain episode and significantly reduces the amount of runoff generated. Figure 4 displays the green roof structure, which includes the following layers: growth medium (80 mm thickness), permeable textile layer (2 mm), drainage layer (water storage layer, 30 mm), geotextile layer/root barrier layer (3 mm), XPS insulation (40 mm), waterproofing membrane (5 mm) and a concrete hollow block (300 mm). Figure 4. Green roof structure. The growth medium is a mixture of conventional gardening organic substrate (40%), volcanic lava rocks (40%) and silica sand (20%). In the upper part of the green roof, there are plants covering almost the entire area with a height in the range of 50 to 150 mm. These plants are genus sedum (a mixture of sedum album AH, sedum floriferum, sedum sediform, sedum reflexum, sedum spurium, sedum moranense and sedum acre). Figure 5 displays the plan view of the building, denoting the roof area where the green roof was installed by the dotted line. The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof."
10.3390_en13051246,"dataset provided, data",350,,0,"Figure 4. Green roof structure. The growth medium is a mixture of conventional gardening organic substrate (40%), volcanic lava rocks (40%) and silica sand (20%). In the upper part of the green roof, there are plants covering almost the entire area with a height in the range of 50 to 150 mm. These plants are genus sedum (a mixture of sedum album AH, sedum floriferum, sedum sediform, sedum reflexum, sedum spurium, sedum moranense and sedum acre). Figure 5 displays the plan view of the building, denoting the roof area where the green roof was installed by the dotted line. The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7."
10.3390_en13081967,"dataset, data",41,,0,"48. Borradaile, G.J. Statistics of Earth Science Data; Springer: Berlin/Heidelberg, Germany, 2003. 49. Chuchro, M.; Danek, M. Selection of optimal gridded dataset for application in Polish Sudetes Mountains."
10.3390_en13092250,"data available, data",343,,0,"Therefore, instead of assuming the concept of future data, it will be useful to implement an algorithm that recognizes the distribution of the features of arriving data. Besides, in some applications of prosumers, the robustness of the methods is essential to differentiate outliers from concept drifts. However, in this study, the agent trusts the external information he receives from measurement systems and weather information services. In the data stream, there could be different kinds of drifts mixed and outliers data samples. Another important concern with the concept drift in the speciﬁc context of the prosumer agent is the concurrence of the drift types. Thus the agent could be facing sudden drifts, gradual drifts, and incremental drifts within one timeframe [27]. For that reason, adaptation algorithms that were made to solve problems related to speciﬁc cases of drift are not the best option for the agent. Here, we test some of those algorithms to validate this afﬁrmation. It is impractical for a prosumer agent to have different models trained with different data sets and ensemble the forecasts. Therefore, considering the available mechanisms to update the agents knowledge, when using a single model, the only strategy is to adapt the parameters. Nevertheless, in addition to parameters’ adaptation, it is also possible to combine the models by weighting them as in ensemble learning [28]. The choice of the method should be made by taking into account the residential agent’s restrictions related to the processing time and hardware limitation. Normally, the models of three main groups are used to provide information for other processing systems, such as a Home Energy Management Systems (HEMS), with a practical objective, like either minimizing energy cost or maximizing comfort. These systems usually provide results in ﬁve to ﬁfteen minutes intervals, thus limiting convenient exploitation of ensemble learning."
10.3390_en13092250,"data available, data",347,,0,"The appearance of gradual drifts makes it impractical to assume that the concept of future data is always closer to the latest data. Therefore, instead of assuming the concept of future data, it will be useful to implement an algorithm that recognizes the distribution of the features of arriving data. Besides, in some applications of prosumers, the robustness of the methods is essential to differentiate outliers from concept drifts. However, in this study, the agent trusts the external information he receives from measurement systems and weather information services. In the data stream, there could be different kinds of drifts mixed and outliers data samples. Another important concern with the concept drift in the speciﬁc context of the prosumer agent is the concurrence of the drift types. Thus the agent could be facing sudden drifts, gradual drifts, and incremental drifts within one timeframe [27]. For that reason, adaptation algorithms that were made to solve problems related to speciﬁc cases of drift are not the best option for the agent. Here, we test some of those algorithms to validate this afﬁrmation. It is impractical for a prosumer agent to have different models trained with different data sets and ensemble the forecasts. Therefore, considering the available mechanisms to update the agents knowledge, when using a single model, the only strategy is to adapt the parameters. Nevertheless, in addition to parameters’ adaptation, it is also possible to combine the models by weighting them as in ensemble learning [28]. The choice of the method should be made by taking into account the residential agent’s restrictions related to the processing time and hardware limitation. Normally, the models of three main groups are used to provide information for other processing systems, such as a Home Energy Management Systems (HEMS), with a practical objective, like either minimizing energy cost or maximizing comfort."
10.3390_en13092250,"database, data",144,,0,"Afterwards, the models were adapted during the selected period with the techniques presented earlier. The average RMSE (and NRMS) for each case is presented in Table 1. The results obtained with the proposed method are systematically better than the most straightforward adaptation by sliding window, which means that implementing the method gives more reliable information to the prosumer than not doing so, even though the error reduction may be small. Other algorithms will occasionally perform better for adapting speciﬁc models. For example, the golden ratio method reduces the error when adapting the power generation model, but it is not suitable to adapt the thermal load model. Furthermore, the proposed method forgets less data, thus making it possible for the prosumer to use a single database for all three models."
10.3390_en13092250,"database, data",196,,0,"Generally, for a residential prosumer agent, it is possible to distinguish two environments that are labeled as local and external. The former refers to the behind-the-meter resources [7], while the latter describes situations where the prosumer agent can interact with other agents and information services. In most cases, the external environment only collects data of either weather variables or their forecast, but in a decentralized management scheme, it is possible to consider the external environment as a multi-agent system (MAS) [8]. The agent is able to perceive the local environment by observing the power consumption data of different appliances. In fact, it constructs a time-series database by accumulating new information from a data stream [9]. However, this process is problematic since agents have limitations on memory and processing time [10]. In addition, the data stream can drift over time, thus causing previously trained data models of appliances to lose accuracy [11]. Therefore, model adaptation on the basis of recent data is essential [12]."
10.3390_en13092250,dataset,27,,0,"24. Moreno-Torres, J.G.; Raeder, T.; Alaiz-Rodríguez, R.; Chawla, N.V.; Herrera, F. A unifying view on dataset"
10.3390_en13092250,"dataset, data",206,,0,"method [33]. The measurement of ﬁt, in this case, is the RMSE because it gives more weight to bigger deviations; thus, it is better to identify the appearance of concept drifts [36]. It is relevant to mention that the threshold to accept the results of the cross-validation depends on the nature of the target variable of each model [37]. The test data set will be the closest batch to future features. Now, to identify that batch, the distance will be measured as in the FISH method [34] as a combined distance in time and space of the samples. If the result of the cross-validation test is not good enough, then the model will be retrained only with the closest N batches. The parameter N needs to be tuned according to the model to avoid convergence problems in training but knowing that, when a concept drift appears, it is safer to train with a small amount of data to ensure that all samples correspond to the new concept. Figure 3 summarizes the proposed algorithm with the procedure for when new data arrives."
10.3390_en13092250,open-source,15,,0,"J. Open Source Softw. 2018, 3, 884. [CrossRef]"
10.3390_en13092250,"open-source, data",176,,0,"Germany, 2001. Odell, J.; Giorgini, P.; Müller, J. Agent-Oriented Software Engineering V; Springer: Berlin/Heidelberg, Germany, 2004. Damisa, U.; Nwulu, N.I.; Sun, Y. Microgrid energy and reserve management incorporating prosumer behind-the-meter resources. IET Renew. Power Gener. 2018, 12, 910–919. [CrossRef] Zhang, Y.; Huang, T.; Bompard, E.F. Big data analytics in smart grids: A review. Energy Inform. 2018, 1, 1–24. [CrossRef] Khamphanchai, W.; Saha, A.; Rathinavel, K.; Kuzlu, M.; Pipattanasomporn, M.; Rahman, S.; Akyol, B.; Haack, J. Conceptual Architecture of Building Energy Management Open Source Software (BEMOSS). In Proceedings of the 2014 5th IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe), Istanbul, Turkey, 12–15 October 2014; pp. 1–6. [CrossRef]"
10.3390_en13092250,python,65,,0,"38. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Müller, A.; Nothman, J.; Louppe, G.; et al. Scikit-learn: Machine Learning in Python. Psychol. Sci. 2012, 25, 1682–1690. [CrossRef]"
10.3390_en13092250,"python, package",25,,0,"39. Holmgren, W.; Hansen, C.; Mikofski, M. pvlib python: A python package for modeling solar energy systems."
10.3390_en13092375,"code package, package",79,,0,"The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power ﬂow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China."
10.3390_en13092375,"code package, package",290,,0,"Energies 2020, 13, x FOR PEER REVIEW 11 of 19 5. Case Analysis The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power flow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China. 5.1. Test System Description The simplified diagram of the large-scale wind power base sending-side system in Gansu Province is shown in Figure 8. The receiving-side system is equivalent to an infinity bus system. Hexi Substation is the border node in the sending-side that the transmission channel connects to. Four thermal generation plants are involved; the capacities of the units in each plant are shown in Table 1; and the relative parameters of each kind of unit are shown in Table 2. The electrical distance parameters that describe the grid topology are shown in Table 3 (the resistances of the lines are small and ignored). The base power is 𝑆(cid:3003)=100 𝑀𝑉𝐴 in the system. While calculating the transient, stability-constrained TTC of the transmission channel, the transient models of the involved electronic components are used, and the relative parameters and the TTC calculation process are shown in the Appendix. Figure 8. Simplified diagram of the large-scale wind power base sending-side system in Gansu Province. Table 1. Capacities of units in each thermal generation plant."
10.3390_en13092375,"code package, package",339,,0,"Case Analysis The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power flow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China. 5.1. Test System Description The simplified diagram of the large-scale wind power base sending-side system in Gansu Province is shown in Figure 8. The receiving-side system is equivalent to an infinity bus system. Hexi Substation is the border node in the sending-side that the transmission channel connects to. Four thermal generation plants are involved; the capacities of the units in each plant are shown in Table 1; and the relative parameters of each kind of unit are shown in Table 2. The electrical distance parameters that describe the grid topology are shown in Table 3 (the resistances of the lines are small and ignored). The base power is 𝑆(cid:3003)=100 𝑀𝑉𝐴 in the system. While calculating the transient, stability-constrained TTC of the transmission channel, the transient models of the involved electronic components are used, and the relative parameters and the TTC calculation process are shown in the Appendix. Figure 8. Simplified diagram of the large-scale wind power base sending-side system in Gansu Province. Table 1. Capacities of units in each thermal generation plant. Thermal Generation Plant Unit Number Unit Capacity /(MW) JC Unit 1 500 Unit 2 500 Unit 3 500 Unit 4 300 Unit 5 200 ZY Unit 1 200 Unit 2 100 Unit 3 100 JQ Unit 1 500 Unit 2 300 Unit 3 200 BLS Unit 1 200 Unit 2 100 Unit 3 100 Energies 2020, 13, 2375"
10.3390_en13112825,"data https, data",162,,0,"A preliminary achievement regarding the universality objective is reported in Reference [52] where a probabilistic data association ﬁlter [56,57] was employed to associate the online measurements from batteries to their model parameters, thus, resulting in a chemistry-adaptive BFG. Further research needs to be done on this topic so that reliable algorithms can be developed to extend adaptivity for load-range, size, temperature, nominal voltage and age as well. This would require large computing power that the traditional battery management systems are not allocated for, for example, portable electronics. Cloud computing [58] allows one to outsource intense computing to external sources; that is, by combining information fusion with cloud computing, a greater deal of universality can be achieved in battery management systems, paving the way for optimal battery reuse (see Section 2.5) and reduced electronic clutter in households and work places."
10.3390_en13112825,"data https, data available, data",29,,0,"57. Bar-Shalom, Y.; Willett, P.K.; Tian, X. Tracking and Data Fusion; YBS Publishing: 2011. Available online: http:"
10.3390_en13143528,"data available, data",97,,0,"In this study, initially, to remove the eﬀect of bad data (e.g., null values, bad log measurements) available data were processed. Afterward, data from wells 2 and 3 were selected as the training (25596 samples), test (5484 samples), and validation set (5484 samples). Then, a three-layer feed forward neural network (Figure 2) with sigmoid hidden neurons and linear output neurons was used to build the model. The network was trained with the Levenberg–Marquardt backpropagation algorithm."
10.3390_en13143528,"data available, data",138,,0,"Geomechanics has shown its potential to cover a broad range of work during the lifespan of a ﬁeld from exploration to production and then abandonment [1,2]. By examining the available data and comprehending the key issues observed while drilling previous wells, we can predict the optimum way of drilling and ﬁnding proper casing shoe locations in the development phase. By coupling ﬂuid-ﬂow with the stress–strain regime in the ﬁeld, compaction and subsidence analyses can be performed [3,4]. The basic approach to geomechanics analysis is to process available data for predicting rock elastic properties, in-situ stresses, and pore pressures. Shear velocity (Vs) is a kind of data that is not always available for wells, especially old wells, and needs to be predicted."
10.3390_en13143528,"data available, data",342,,0,"Energies 2020, 13, 3528 4 of 16  some weight functions and then are linked to the output layer that corresponds to the results we are looking for. In this study, initially, to remove the effect of bad data (e.g., null values, bad log measurements) available data were processed. Afterward, data from wells 2 and 3 were selected as the training (25596 samples), test (5484 samples), and validation set (5484 samples). Then, a three‐layer feed forward neural network (Figure 2) with sigmoid hidden neurons and linear output neurons was used to build the model. The network was trained with the Levenberg–Marquardt backpropagation algorithm. Figure 2. Neural network model including 3 hidden layers. 3 inputs are DTC, GR, RHOZ, and the single output is DTS. After gathering all required data, by assuming elastic isotropy and using the following equations, elastic moduli such as dynamic Young’s modulus and Poisson’s ratio were calculated [40], 𝜈(cid:3404)12(cid:4666)𝐷𝑇𝑆DTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (7) 𝐸(cid:3005)(cid:3404)𝑅𝐻𝑂𝐵(cid:4666)1DTS(cid:4667)(cid:2870)3(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)4(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (8) where ν and ED represent Poisson’s ratio and dynamic Young’s modulus. However, Equation (8) will provide a dynamic elastic modulus which needs to be converted to static modulus using available correlations such as the equation presented by Eissa and Kazi [41]."
10.3390_en13143528,"data available, data",349,,0,"Figure 2. Neural network model including 3 hidden layers. 3 inputs are DTC, GR, RHOZ, and the single output is DTS. After gathering all required data, by assuming elastic isotropy and using the following equations, elastic moduli such as dynamic Young’s modulus and Poisson’s ratio were calculated [40], 𝜈(cid:3404)12(cid:4666)𝐷𝑇𝑆DTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (7) 𝐸(cid:3005)(cid:3404)𝑅𝐻𝑂𝐵(cid:4666)1DTS(cid:4667)(cid:2870)3(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)4(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (8) where ν and ED represent Poisson’s ratio and dynamic Young’s modulus. However, Equation (8) will provide a dynamic elastic modulus which needs to be converted to static modulus using available correlations such as the equation presented by Eissa and Kazi [41]. The reason for this conversion is due to the difference in the measurement condition [42,43]. Uniaxial compressive strength (UCS) and friction angle (φ) were also estimated based on Plumb’s correlation [44]. Calculated rock properties for Well‐1 are presented in Figure 3. The next step in analyzing wellbore stability is predicting pore pressure. In this study, MDT (modular formation dynamics tester) data in the reservoir section, and Eaton method in the clay‐rich intervals were used to predict pore pressure and interpolated for other sections as well [45]. In order to find the clay‐rich intervals, the cross plot of the Gamma ray versus density log, as in Figure 4, was used."
10.3390_en13143528,"data available, data",361,,0,"In this study, MDT (modular formation dynamics tester) data in the reservoir section, and Eaton method in the clay‐rich intervals were used to predict pore pressure and interpolated for other sections as well [45]. In order to find the clay‐rich intervals, the cross plot of the Gamma ray versus density log, as in Figure 4, was used. The pore pressure profile was then calibrated by used mud weight and observed kicks while drilling. The Eaton equation is presented below [46], 𝑃(cid:3043)(cid:3034)(cid:3404)𝑂𝐵𝐺(cid:3398)(cid:4666)𝑂𝐵𝐺(cid:3398)𝑃(cid:3043)(cid:3041)(cid:4667)(cid:4666)(cid:3005)(cid:3021)(cid:3004)(cid:3041)(cid:2888)(cid:2904)(cid:2887) (cid:4667)(cid:2871), (9) where Ppg is pore pressure gradient, OBG is overburden gradient, Ppn is normal pore pressure gradient, and DTCn is the transient time of compressional wave in the normally pressured zone. Vertical stress (σv) was then calculated based on the weight of overburden [47], 𝜎(cid:3049)(cid:3404)𝑔(cid:1516)RHOB 𝑑𝑧(cid:3053)(cid:2880)(cid:3269)(cid:3271)(cid:3253)(cid:2868), (10) where g is the gravitational acceleration, RHOB is bulk density log, and z refers to depth. Principal horizontal stresses should be predicted using indirect methods such as poroelastic horizontal strain method [48], by considering the tectonic strains: 𝜎(cid:3035)(cid:3404)𝜐1(cid:3398)𝜐𝜎(cid:3049)(cid:3397)1(cid:3398)2𝜐1(cid:3398)𝜐𝛼𝑃(cid:3043)(cid:3397)𝐸1(cid:3398)𝜐(cid:2870)𝜀(cid:3051)(cid:3397)𝜐𝐸1(cid:3398)𝜐(cid:2870)𝜀(cid:3052), (11) Energies 2020, 13, 3528"
10.3390_en13143600,"dataset provided, data",338,,0,"The energy saving potential was analyzed from climate characteristics, natural lighting, and natural ventilation [11]. It was indicated that more than 40% of the total energy consumption was expended by air conditioning systems for airport terminals in China [12]. Energy consumption data of 29 airport terminals in Greece were obtained; comparison of energy consumption levels was conducted between airport terminals and other types of public buildings. The energy conservation potential was analyzed on three airport terminals, among them, in detail, around the premise that indoor environmental quality was met [13]. The energy saving potential of the public buildings were mainly studied on the HVAC system [14,15] and lighting system [16,17]. Energy saving eﬀect under diﬀerent forms of air conditioning systems, lighting power, and heat transfer coeﬃcient of the envelope was studied by simulation, based on energy consumption data of public buildings in Tianjin [18]. According to the benchmarking index system established by H. Li, et al. [19], the energy saving measures were developed mainly from reasonable ventilation cooling load and improvement of air conditioning system energy performance. The vehicle exhaust and the building operation are the main sources of CO2 emissions in coach stations. Research of the vehicle emissions in transportation terminals was most concentrated on pollutant emissions of vehicle exhaust and its negative eﬀects on the indoor environment of the terminals [20,21]. According to research on a coach station with 30,000 daily passengers in cold region of China, annual pollutant emissions (CO and PM2.5) from vehicle exhaust were 0.43t/a in the coach station [22]. In a coach station, in a hot summer and cold winter region in China, the CO2 emissions from building operations accounted for 47.25% of the total CO2 emissions [23]. It is indicated that"
10.3390_en13143600,"dataset provided, data",339,,0,"Most research on the energy consumption of public buildings was based on detailed ﬁeld investigations on energy consumption of each energy-using sector. The annual total energy consumption of airport terminals and railway stations was 206.3 and 115.7 kWh/(m2·a), respectively [8]. The sub-item annual power consumption of eight railway stations in diﬀerent climate regions in China were obtained, results showed that the HVAC system was the largest energy-using sector. The energy saving potential was analyzed from climate characteristics, natural lighting, and natural ventilation [11]. It was indicated that more than 40% of the total energy consumption was expended by air conditioning systems for airport terminals in China [12]. Energy consumption data of 29 airport terminals in Greece were obtained; comparison of energy consumption levels was conducted between airport terminals and other types of public buildings. The energy conservation potential was analyzed on three airport terminals, among them, in detail, around the premise that indoor environmental quality was met [13]. The energy saving potential of the public buildings were mainly studied on the HVAC system [14,15] and lighting system [16,17]. Energy saving eﬀect under diﬀerent forms of air conditioning systems, lighting power, and heat transfer coeﬃcient of the envelope was studied by simulation, based on energy consumption data of public buildings in Tianjin [18]. According to the benchmarking index system established by H. Li, et al. [19], the energy saving measures were developed mainly from reasonable ventilation cooling load and improvement of air conditioning system energy performance. The vehicle exhaust and the building operation are the main sources of CO2 emissions in coach stations. Research of the vehicle emissions in transportation terminals was most concentrated on pollutant emissions of vehicle exhaust and its negative eﬀects on the indoor environment of the terminals [20,21]."
10.3390_en13143695,"dataset, data",43,,0,where x1 and x2 represent the data set of temperature and irradiance. While the expression for reference value of peak power voltage given by ANN is represented as general expression in Equation (3) which can also be written as:
10.3390_en13143695,"dataset, data",117,,0,"After training, ANN is tested by giving different inputs and then the model predictive output of an ANN is obtained which is compared with an already available model that was obtained by multiple linear regression of the data set. In this comparison, as shown in Figure 6, irradiance is kept constant at 1000 W/m2 and only temperature is varied for testing the trained ANN model with the one obtained by multiple linear regression algorithm, therefore in Figure 6, only x1 is represented on the x-axis while x2, which is irradiance, is taken as constant for computation of peak power voltage which is the output y of both algorithms."
10.3390_en13164055,code,63,,0,"In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental eﬀects of the vortex on the energy absorption capability of the hydro-turbine. This multi-purpose code solves three-dimensional RANS equations for steady and turbulent ﬂuid ﬂow, especially in the ﬁeld of turbomachinery simulations and analyses."
10.3390_en13164055,code,324,,0,"In this study, four values of tip-clearance size, namely, δ = 0%, 0.25%, 0.5%, and 0.75% (corresponding to actual sizes of 0.0, 0.95, 1.9, and 2.85 mm, respectively) were considered to analyze the tip-clearance effects. For each TCS case, a series of monitoring points were set along the blade tip, as shown in Figure 2. The points, PS1–PS11, were located on the pressure side of the tip region, from the leading to the trailing edge of the runner blade. In the same way, there were corresponding points named SS1–SS11 at the blade-tip suction side. Similar monitoring points were also placed at blade spans 0.8 and 0.6 (80% and 60% of blade height) to collect pressure statistic at these positions to examine pressure fluctuations. Figure 2. Monitoring points at blade tip. 3. Numerical Method 3.1. Turbulence Model and Boundary Conditions In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental effects of the vortex on the energy absorption capability of the hydro-turbine. This multi-purpose code solves three-dimensional RANS equations for steady and turbulent fluid flow, especially in the field of turbomachinery simulations and analyses. The shear-stress transport k–ω turbulence model was developed by Menter to solve the complex flows by combining the advantages of the k–ε model in the far-field and of the k–ω model in viscous sub-layer at near-wall regions [21]. It was chosen as the turbulence model for simulations because of its good capability in estimating vortex appearance and flow separation with adverse pressure gradients near the complex geometry surfaces. The validation of the SST turbulence model can be Energies 2020, 13, 4055"
10.3390_en13164055,code,343,,0,"Energies 2020, 13, x FOR PEER REVIEW 4 of 18 The pressure coefficient is a dimensionless number describing the relative pressure throughout a flow field, and is defined as 𝐶𝑃=𝑃𝑙𝑜𝑐𝑎𝑙0.5𝜌𝑈𝑡𝑖𝑝2 (3) where Plocal, local static pressure; and Utip, blade tip velocity in the design condition. Hydraulic power, output power, and hydraulic efficiency are calculated using the following equations: 𝑃ℎ=𝜌𝑔𝑄𝐻 (4) 𝑃𝑚𝑒𝑐=𝑇𝜔 (5) 𝜂ℎ=𝑃𝑚𝑒𝑐𝑃ℎ (6) where Ph, hydraulic power; Pmec, mechanical power generated in turbine shaft; ρ, water density; ηh, hydraulic efficiency; Q, flow rate; H, effective head; T, torque on runner blade; and ω, rotational speed. In this study, four values of tip-clearance size, namely, δ = 0%, 0.25%, 0.5%, and 0.75% (corresponding to actual sizes of 0.0, 0.95, 1.9, and 2.85 mm, respectively) were considered to analyze the tip-clearance effects. For each TCS case, a series of monitoring points were set along the blade tip, as shown in Figure 2. The points, PS1–PS11, were located on the pressure side of the tip region, from the leading to the trailing edge of the runner blade. In the same way, there were corresponding points named SS1–SS11 at the blade-tip suction side. Similar monitoring points were also placed at blade spans 0.8 and 0.6 (80% and 60% of blade height) to collect pressure statistic at these positions to examine pressure fluctuations. Figure 2. Monitoring points at blade tip. 3. Numerical Method 3.1. Turbulence Model and Boundary Conditions In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental effects of the vortex on the energy absorption capability of the hydro-turbine."
10.3390_en13164055,"dataset provided, data",107,,0,"Nevertheless, the aforementioned literature mainly focused on the research objects of axial pumps, mixed-ﬂow pumps, and gas turbines, while tip clearance in hydro-turbines had not been considered. In this study, numerical simulations applying the shear-stress transport (SST) k–ω turbulence model were conducted to demonstrate TLV formation under design and oﬀ-design conditions, and evaluate the inﬂuence of diﬀerent tip-clearance sizes on the energy performance and stability of a propeller turbine. Computational results were compared and validated by experimental data provided in the references to ensure the precision of the numerical method. The distribution of velocity vectors and"
10.3390_en13164055,package,86,,0,"The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The reﬁned mesh was implemented at the blade’s hub and shroud to achieve higher mesh density. Meanwhile, all wall surfaces (including blades and pipes) adopted 10-layer inﬂation to ensure accurate predictions of the"
10.3390_en13164055,package,336,,0,"Figure 3 shows three components of the computational domains in simulations, namely, the suction, runner, and discharge domains. Since the four runner blades were the same as each other in all aspects, one-fourth of the simulation domain (90° periodicity) was utilized instead of the full-size domain in order to reduce calculation time and computational resources. At the inlet of the suction domain, total pressure corresponding to the designed gross head was applied with a turbulence intensity of 5%. Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains. At first, the steady-state simulations were carried out to assess the impact of tip-clearance size on turbine performance; then, results were taken as initial suggestions for the transient simulations to monitor pressure fluctuations at the tip-gap region of the turbine. The time step for transient simulations was 1.1 × 10−3 s corresponding to 3o when the turbine rotates at the design speed. The convergence criterion was set as 10–5 for the root-mean-square residual value. Figure 3. Computational domains. 3.2. Mesh Strategy and Grid Independent Analysis The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The refined mesh was implemented at the blade’s hub and shroud to achieve higher mesh density."
10.3390_en13164055,package,341,,0,"Energies 2020, 13, x FOR PEER REVIEW 5 of 18 found in research performed by Zhang et al. [10], Bardina et al. [22], and Chen et al. [23]. Moreover, a number of studies involving the flow in pump-jet propulsors, mixed-flow pumps, and propeller turbines have confirmed the reliability of the SST k-ω turbulence model with favorable results [13,14,16]. In this turbulence model, the eddy viscosity is defined as the function of the turbulence kinetic energy-k and turbulence frequency-ω: 𝜇𝑡=𝜌𝛼1𝑘𝑚𝑎𝑥⁡(𝑆𝐹2,𝛼1𝜔) (7) 𝐹2=𝑡𝑎𝑛ℎ[{𝑚𝑎𝑥(2√𝑘𝛽′𝜔𝑦;500𝜇𝜌𝑦2𝜔)}2] (8) where α1 and β’, model constants: α1 = 5/9, β’ = 0.09 [24]; F2, blending function in boundary layer flow; and S, an invariant measure of the strain rate. Figure 3 shows three components of the computational domains in simulations, namely, the suction, runner, and discharge domains. Since the four runner blades were the same as each other in all aspects, one-fourth of the simulation domain (90° periodicity) was utilized instead of the full-size domain in order to reduce calculation time and computational resources. At the inlet of the suction domain, total pressure corresponding to the designed gross head was applied with a turbulence intensity of 5%. Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains."
10.3390_en13164055,package,347,,0,"Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains. At first, the steady-state simulations were carried out to assess the impact of tip-clearance size on turbine performance; then, results were taken as initial suggestions for the transient simulations to monitor pressure fluctuations at the tip-gap region of the turbine. The time step for transient simulations was 1.1 × 10−3 s corresponding to 3o when the turbine rotates at the design speed. The convergence criterion was set as 10–5 for the root-mean-square residual value. Figure 3. Computational domains. 3.2. Mesh Strategy and Grid Independent Analysis The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The refined mesh was implemented at the blade’s hub and shroud to achieve higher mesh density. Meanwhile, all wall surfaces (including blades and pipes) adopted 10-layer inflation to ensure accurate predictions of the flow field at near-wall regions. The local grid at the tip-gap region was also generated with 30 layers due to the complicated flow pattern in this area, giving rise to better observation of the vortex structure. The Y+ value for the wall grid of the turbine blade was around 1, satisfying the demand of SST k–ω turbulence model to precisely predict flow behavior in the viscous sub-layer [21]. Energies 2020, 13, 4055"
10.3390_en13184709,"data available, data",336,,0,"Energies 2020, 13, x FOR PEER REVIEW 10 of 14 Moreover, the effects of the initial maximum capillary pressure and the viscosity of the non-wetting phase on water imbibition are studied in this work, and the corresponding results are shown in Figures 11 and 12. According to the figures, when the initial capillary pressure increases and the non-wetting phase viscosity decreases, the corresponding imbibition rates increase. However, the final imbibition recovery factor tends to be similar, which is because the water imbibition process is a capillary-dominated flow and the recovery factor is controlled by capillarity, which is affected by pore shape, aspect ratio, contact angle, etc. Therefore, the viscous forces in the reservoir conditions increase the imbibition rates but not the final recovery factor. Figure 11. Spontaneous imbibition with different initial maximum capillary pressures. Figure 12. Spontaneous imbibition with different non-wetting phase viscosities. To further investigate the capillarity effect on the water imbibition of Barnett shale, the following sensitivity studies are conducted: the determination of the pore throat aspect ratio, contact angle and shape factor of the cross-area. The corresponding results for the water imbibition recovery factor with respect to dimensionless time are shown in Figures 13–15, respectively. A higher aspect ratio tends to increase the percentage of the non-wetting phase trapped by the snap-off effect, which increases the residual non-wetting phase saturation and hence the final recovery factor (c.f. Figure 13). When the contact angle and cross-area shape factor decrease, leading to an increase of capillary pressure, the imbibition rate increases at the beginning. However, the final recovery factor decreases slightly (c.f. Figures 13 and 14) because snap-off tends to occur more frequently with a lower contact angle and shape factor, as shown in Equation (13). Energies 2020, 13, 4709"
10.3390_en13184709,"data available, data",341,,0,"Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures."
10.3390_en13184709,"data available, data",341,,0,"Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]."
10.3390_en13184709,"data available, data",344,,0,"Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8."
10.3390_en13184709,"data available, data",347,,0,"Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, 4709"
10.3390_en13184709,"data available, data",347,,0,"th in cm [49]; we follow the same units here. According to the figure, with the increasing proportion of oil-wet pores, the imbibition rate and recovery factor decrease dramatically. The results indicate that wettability dominates the water imbibition characteristics. Figure 6. Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process."
10.3390_en13184709,"data available, data",348,,0,"Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10."
10.3390_en13184709,"data available, data",349,,0,"Energies 2020, 13, x FOR PEER REVIEW 8 of 14  Figure 5. Water saturation profiles of the water imbibition process using the proposed dynamic pore network model with an average water saturation equal to 0.2239. Generally, a mixed wettability is assumed for shale and tight formations. Here, we considered mixed wet conditions for Barnett shale. We assigned three types of mixed wet conditions—30% oil-wet, 50% oil-wet and 70% oil-wet—as shown in Figure 6. The corresponding spontaneous water imbibition-induced oil recoveries are shown in Figure 7 for dimensionless time 𝑡(cid:3005)=𝐶𝑡(cid:3495)(cid:3038)(cid:3109)(cid:3097)(cid:3091)(cid:3298)(cid:2869)(cid:3013)(cid:3118) , where 𝐶 is the unit conversion factor, which is equal to 0.018849 if 𝑡 is in minutes, 𝑘 in md, 𝜙 in decimal, 𝜎 in 𝑚𝑁/𝑚, 𝜇(cid:3050) in cp, and L is a characteristic length in cm [49]; we follow the same units here. According to the figure, with the increasing proportion of oil-wet pores, the imbibition rate and recovery factor decrease dramatically. The results indicate that wettability dominates the water imbibition characteristics. Figure 6. Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability."
10.3390_en13184709,"data available, data",350,,0,"Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 10 of 14 Moreover, the effects of the initial maximum capillary pressure and the viscosity of the non-wetting phase on water imbibition are studied in this work, and the corresponding results are shown in Figures 11 and 12. According to the figures, when the initial capillary pressure increases and the non-wetting phase viscosity decreases, the corresponding imbibition rates increase. However, the final imbibition recovery factor tends to be similar, which is because the water imbibition process is a capillary-dominated flow and the recovery factor is controlled by capillarity, which is affected by pore shape, aspect ratio, contact angle, etc. Therefore, the viscous forces in the reservoir conditions increase the imbibition rates but not the final recovery factor. Figure 11. Spontaneous imbibition with different initial maximum capillary pressures. Figure 12. Spontaneous imbibition with different non-wetting phase viscosities. To further investigate the capillarity effect on the water imbibition of Barnett shale, the following sensitivity studies are conducted: the determination of the pore throat aspect ratio, contact angle and shape factor of the cross-area."
10.3390_en13195050,"data available, data",128,,0,"Behavioral validation aims to compare the model-generated behavior to that of a real system. During the construction of the baseline model, our model used a speciﬁc case: China’s data obtained from available references or interviews about the real system [41]. Submodels were adopted in the LCT model, such as environmental sensitivity and feedback between supply and demand. The adopted submodels of the existing models can serve as structural validation for ABM, and Denmark can be used as one of representatives of the LCT model where the environmental sensitivity and feedback between supply and demand has been performed [1]. Therefore, we compared our simulation data with the real change that occurred in Denmark."
10.3390_en13195065,"dataset, data",293,,0,"particles therein. Wang et al. [4], Liu, et al. [5], Oda et al. [13], Mitchell et al. [14] analyzed changes to the permeability during crack propagation in granite, where seepage only occurs after the onset of cracking. Alkan [15] explored the corresponding relationship between change of permeability in salt rock and acoustic emission information, ﬁnding that the acoustic emission event concentration occurred in the area of variation of permeability. Nara et al. [16] investigated changes in wave velocity during permeability evolution in basalt. Moreover, Pereira et al. [17] researched the eﬀect of fracture distribution on permeability. They all found that the permeability of intact rocks was very low and increased only when cracks appeared. Liu et al. [18] studied the correspondence between creep deformation of mudstone and pore pressure and found the permeability evolution could be determined by the microstructural evolution of rock. Li et al. [19] found that axial strain (in a rock specimen) has a functional relationship with permeability. Pradip et al. [20] and Legrand et al. [21] investigated the rate of convergence under stable seepage in crushed rock and pressure-drop data during seepage and used a capillary model to predict the experimental data expressed in terms of a pore friction factor. In the test results of rock failure under hydraulic coupling, there are few complete datasets relating to volumetric strain and circumferential strain and the correlations of coeﬃcient of permeability with hydraulic gradient and volumetric strain remain unclear."
10.3390_en13215563,"data available, data",87,,0,"This paper pays attention to the moving target surveillance by a group of UAVs. A practical application of the considered scenario is that, in wireless sensor networks, the sensor nodes collect data from the environment. UAVs function as data sinks to collect the sensory data from sensor nodes [8]. In general, the number of available UAVs is smaller than that of the sensor nodes. Thus, the UAVs carry out a periodical surveillance of the sensor nodes."
10.3390_en13225874,"case study data, data",295,,0,"Abstract: Annual mean wind speed distribution models for power generation based on regional wind resource maps are limited by spatial and temporal resolutions. These models, in general, do not consider the impact of local terrain and atmospheric circulations. In this study, long-term ﬁve-year wind data at three sites on the North, East, and West of the Baltimore metropolitan area, Maryland, USA are statistically analyzed. The Weibull probability density function was deﬁned based on the observatory data. Despite seasonal and spatial variability in the wind resource, the annual mean wind speed for all sites is around 3 m/s, suggesting the region is not suitable for large-scale power generation. However, it does display a wind power capacity that might allow for non-grid connected small-scale wind turbine applications. Technical and economic performance evaluations of more than 150 conventional small-scale wind turbines showed that an annual capacity factor and electricity production of 11% and 1990 kWh, respectively, are achievable. It results in a payback period of 13 years. Government incentives can improve the economic feasibility and attractiveness of investments in small wind turbines. To reduce the payback period lower than 10 years, modern/unconventional wind harvesting technologies are found to be an appealing option in this region. Key contributions of this work are (1) highlighting the need for studying the urban physics rather than just the regional wind resource maps for wind development projects in the build-environment, (2) illustrating the implementation of this approach in a real case study of Maryland, and (3) utilizing techno-economic data to determine suitable wind harnessing solutions for the studied sites."
10.3390_en13225874,"dataset, data",323,,0,"US Department of Energy (DOE); Energy Information Administration (EIA). Annual Energy Outlook 2050. 2020. Available online: https://www.eia.gov/outlooks/aeo/pdf/AEO2020%20Full%20Report.pdf (accessed on 15 July 2020). US Department of Energy (DOE). Wind Vision: A New Era for Wind Power in the United States. 2020. Available online: http://energy.gov/eere/wind/wind-vision (accessed on 15 July 2020). Goudarzi, N.; Zhu, W.D. A review on the development of the wind turbine generators across the world. Int. J. Dyn. Control 2013, 1, 192–202. [CrossRef] Lee, J.A.; Doubrawa, P.; Xue, L.; Newman, A.J.; Daxl, C.; Scott, G. Wind resource assessment for Alaska’s oﬀshore region: Validation of a 14-year high-resolution WRF data set. Energies 2019, 12, 2780. [CrossRef] Dorrell, J.; Lee, K. The cost of wind: Negative economic eﬀects of global wind energy development. Energies 2020, 13, 3667. [CrossRef] Poore, R.; Lettenmaier, T. Alternative Design Study Report: WindPACT Advanced Wind Turbine Drive Train Designs Study; NREL/SR-500-33196; National Renewable Energy Laboratory (NREL): Golden, CO, USA, 2003. Polinder, H.; Van Der Pijl, F.F.A.; De Vilder, G.-J.; Tavner, P.J. Comparison of Direct-Drive and Geared Generator Concepts for Wind Turbines. IEEE Trans. Energy Convers. 2006, 21, 725–733. [CrossRef] Goudarzi, N.; Zhu, W.D. Oﬀshore and onshore wind energy conversion: The potential of a novel multiple-generator drivetrain. Key Eng. Mater. 2013, 569–570, 644–651. [CrossRef]"
10.3390_en13236216,"dataset provided, data",268,,0,"In order to get information about possible modiﬁcations of the hard carbon layers arrangement induced by the diﬀerent binders used for electrodes formulation, Raman spectra was performed on the CGDHC-based electrodes prepared with CMC, Alg, PAA, and PVDF binders, respectively labeled as CGDHC-CMC, CGDHC-Alg, CGDHC-PAA, and CGDHC-PVDF. This information can be obtained by calculating the ratio between the ID and IG bands. The comparison of Raman spectra of electrodes with diﬀerent binders for LIBs and NIBs is shown in Figure S3, while the calculated ID /IG and La are presented in Table S1. As expected, all the Raman spectra present the same characteristic peaks −1 (G-band) [8], together with already observed for the powder at ~1345 cm −1, correlated to 2D and D+G bands [31]. Commonly, two broad peaks in the range of 2650–2950 cm the graphitization degree of hard carbon is bound to conductivity [23], while porosity and defects enable surface storage processes (thus relevant for NIBs) [33]. However, in the present case the data provide evidence, for all electrodes, of similar peak shapes and only small variations of the intensity ratio between D and G bands. This behavior may be attributed to only minor interactions of the active CGDHC and of the conductive additive with the functional groups. Therefore, we may exclude a relevant role of the binder in modifying structural or conduction properties of the active materials."
10.3390_en14010108,code,28,,0,Funding: This work was supported by the funding program PIACERI 2020-22 of the University of Catania (project code 22722132140; principal investigator M.V.).
10.3390_en14010123,"dataset, data",46,,0,"Step 2. Identiﬁcation of the SARIMA model, all its seasonal and non-seasonal parameters. In this step autocorrelation (ACF) and partial autocorrelation (PACF) are examined to determine the best combinational order of the SARIMA model for each data set."
10.3390_en14010123,"dataset, data",183,,0,"The high frequency components are identified from IMF 1 to IMF 4, and the low frequency components are given from IMF 5 to IMF 9. The last component is the residual showing the trend of the original time series. The first model implemented in this paper is a SARIMA without autoregressive coefficients, p and P both equal 0 (no significant positive spikes in ACF and PACF plots). The ACF and PACF plots are used as a starting point to determine the best SARIMA parameters. According to the most used notation the model belongs to the SARIMA (0,1,1) × (0,1,1)12 type. Since the time series data of AQI have seasonal and non-seasonal trends, a non-seasonal and seasonal first order differencing are employed. In this model D of 1 calculate a first order seasonal difference and a Q = 1 use a first order errors in the model. Also, the value m = 12 means that the data is monthly and suggests a yearly seasonal cycle."
10.3390_en14010123,"dataset, data",333,,0,"Energies 2020, 13, x FOR PEER REVIEW 13 of 28  (a) (b) Figure 6. (a) AQI from 2015 to 2019, (b) AQI for 2020 for months from January to October. In general, there are two steps to AQI forecast used in this paper: Step 1. Decompose the original time series of the AQI into components (IMFs) and residual (residual), from 2015 to 2019. Step 2. Identification of the SARIMA model, all its seasonal and non-seasonal parameters. In this step autocorrelation (ACF) and partial autocorrelation (PACF) are examined to determine the best combinational order of the SARIMA model for each data set. In the first step, nine IMFs are obtained, and one residue, shown in Figure 7. The high frequency components are identified from IMF 1 to IMF 4, and the low frequency components are given from IMF 5 to IMF 9. The last component is the residual showing the trend of the original time series. The first model implemented in this paper is a SARIMA without autoregressive coefficients, p and P both equal 0 (no significant positive spikes in ACF and PACF plots). The ACF and PACF plots are used as a starting point to determine the best SARIMA parameters. According to the most used notation the model belongs to the SARIMA (0,1,1) × (0,1,1)12 type. Since the time series data of AQI have seasonal and non-seasonal trends, a non-seasonal and seasonal first order differencing are employed. In this model D of 1 calculate a first order seasonal difference and a Q = 1 use a first order errors in the model. Also, the value m = 12 means that the data is monthly and suggests a yearly seasonal cycle."
10.3390_en14010123,"dataset, data",428,,0,"The model can be analytically formulated as follows: (1−(cid:1828))(1−(cid:1828)(cid:2869)(cid:2870))(cid:1850)(cid:3047)=(cid:1853)+(1+(cid:2004)(cid:1828))(1+(cid:1990)(cid:1828)(cid:2869)(cid:2870))(cid:2013)(cid:3047) ⋮ (cid:1850)(cid:3047)=(cid:1853)+(cid:1850)(cid:3047)(cid:2879)(cid:2869)+(cid:1850)(cid:3047)(cid:2879)(cid:2869)(cid:2870)−(cid:1850)(cid:3047)(cid:2879)(cid:2869)(cid:2871)+(cid:2013)(cid:3047)+(cid:2004)(cid:2013)(cid:3047)(cid:2879)(cid:2869)+(cid:1990)(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2870)+(cid:2004)(cid:1990)(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2871) (17)It was found that the model fitted the data well and the stochastic seasonal fluctuation was successfully modelled. The AIC and BIC values of all selected models are shown in Table 6. Obviously from Table 6 can be seen that the model SARIMA (0,1,1) × (0,1,1)12 has the smallest value of AIC and BIC. Once the model is established, the parameters can be estimated using statistical techniques, such as least square estimation method. According to specific case in this paper and the estimation of the model parameters the equation is: (cid:1850)(cid:3047)=(cid:2013)(cid:3047)+0.8509(cid:2013)(cid:3047)(cid:2879)(cid:2869)−0.9321(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2870)−0.8323(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2871). Table 6. AIC and BIC values for optimal model selection. Models AIC BIC SARIMA(0,1,1) × (0,1,1)12 4.90 3.92 SARIMA(0,0,1) × (0,1,1)12 4.91 3.95 SARIMA(1,0,1) × (0,1,1)12 5.01 4.0 SARIMA(1,1,1) × (0,1,1)12 5.09 4.02 SARIMA(0,0,0) × (0,1,1)12 4.92 3.98 Energies 2021, 14, 123"
10.3390_en14010123,"publicly available, data",134,,0,"From the public service PVGIS the data on how many kWh in a given month at the target location with the given orientation and slope of the module can produce 1 kWp of photovoltaic modules are obtained. The required number of modules is determined by dividing the obtained required power by the power of one module. Also, the area of a photovoltaic system is simply obtained by multiplying the number of modules by the area of one module. The capacity of a solar prosumers is designed in such a way that its own production satisﬁes the customer’s needs for electricity in times of more expensive tariffs. The solar prosumer is connected to the installation of the building and is in parallel with the public distribution network."
10.3390_en14020484,package,165,,0,"The main application of the driver board is to drive the SM switching devices. In this context, an isolated driver with two complementary channels in a single package is used (SI824x from Silicon Labs [42]). This driver is speciﬁcally targeted to drive complementary switching devices (as in the case of half-bridge power converter developed). In addition, its main feature is the integrated deadtime generator between the high-side/low-side drivers that allow highly precise control for achieving optimal total harmonics distortion (THD). Another driver, HCPL-3120, is employed to drive an additional IGBT switch for the purpose of overvoltage protection, as explained in Section 3.3.2. Figure 8 shows the driver circuit board hardware. The pulse width modulation (PWM) input signals are supplied from the main central control unit and the outputs of this board are connected to the gate-emitter of each IGBT that composes the SM."
10.3390_en14020484,"publicly available, data",113,,0,"41. Barros, L.A.M.; Tanta, M.; Martins, A.P.; Afonso, J.L.; Pinto, J.G. STATCOM Evaluation in Electriﬁed Railway Using V/V and Scott Power Transformers. In Proceedings of the Sustainable Energy for Smart Cities, SESC 2019, Braga, Portugal, 4–6 December 2019; Afonso, J.L., Monteiro, V., Pinto, J.G., Eds.; Springer International Publishing: Cham, Switzerland, 2020; pp. 18–32. Silicon Labs Si824x—Class D Audio Driver with Precision Dead-Time Generator. Available online: https://www.silabs.com/ documents/public/data-sheets/Si824x.pdf (accessed on 9 October 2018)."
10.3390_en14030737,code,23,,0,11. European Commission. Commission Regulation (EU) 2016/1388 of 17 August 2016 Establishing a Network Code on Demand Connection;
10.3390_en14030737,code,52,,0,"10. European Commission. Commission Regulation (EU) 2016/1447 of 26 August 2016 Establishing a Network Code on Requirements for Grid Connection of High Voltage Direct Current Systems and Direct Current-Connected Power Park Modules (Text with EEA Relevance); European Commission: Brussels, Belgium, 2016."
10.3390_en14030737,code,74,,0,"In this paper, the deﬁnition used is the one from Network Code on Requirements for Grid Connection of Generators (NC RfG), which is in line with the deﬁnition from [18]: “synthetic inertia means the facility provided by a power park module or HVDC system to replace the effect of inertia of a synchronous power-generating module to a prescribed level of performance” [9]."
10.3390_en14030737,code,77,,0,"ISBN 978-83-01-20006-0. ENTSO-E. Need for Synthetic Inertia (SI) for Frequency Regulation: ENTSO-E Guidance Document for National Implementation for Network Codes on Grid Connection; ENTSO-E: Brussels, Belgium, 2018. European Commission. Commission Regulation (EU) 2016/631 of 14 April 2016 Establishing a Network Code on Requirements for Grid Connection of Generators (Text with EEA Relevance); European Commission: Brussels, Belgium, 2016."
10.3390_en14030737,"data available, data",229,,0,"In the presented case, time series data for 24 h period in 15 min interval is available. These data represent forecasted system load and generation. The results of a set of load ﬂow calculations are presented in Figure 7. Left-hand side plot shows a scenario in which only synchronous generation is present. The plot shows generation (black line) and system load (blue dashed line) patterns and calculated system kinetic energy Esys (red thick line). Note the reduced value of Esys at night indicating temporary tripping of generators due to low demand night period. Expected worst-case scenario RoCoF can also be noticed (red line) and should it exceed the threshold level necessary actions could be taken, as described in Step 4. Right-hand side plot of Figure 7 is constructed for a scenario in which a part of synchronous generation is superseded by wind generation, whose total power output is marked with green line. Calculated system inertia is, thus, lower and resulting RoCoF is lower. In this scenario, generation is also switched off during the night because of wind generation supplying low system demand. Note that there are many intervals for which RoCoF is below the threshold of −1 Hz/s, thus synthetic inertia is introduced."
10.3390_en14030737,"dataset provided, data",305,,0,"A modiﬁed version of the IEEE 14-bus system shown in Figure 4 is used for the purpose of providing a proof of concept. The modiﬁcation regarded ratings of the machines and their type are presented in Table 1. The example is based on data from Scenario 2, in which the largest generation trip was 117 MW (G3). Data for this power ﬂow are provided in Table 1, whereas the calculations are conﬁrmed by the plots in Figure 5. Based on Equation (6), the resulting RoCoFmax for this ∆P is equal to 1.92 Hz/s, whereas the goal is to limit the RoCoF to RoCoFlim = 1.0 Hz/s. According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic inertia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13). By using this relationship, the main time constant TSI can be adjusted to satisfy the requirement of the speed of response of the SI controller. Finally, based on Equations (13) and (14), gain KSI can be calculated to match the required amount of energy ∆Eτ and, in this example, is equal to 61.49 MWs/Hz."
10.3390_en14030737,"dataset provided, data",340,,0,"The results also con-firm that for the time period of interest, i.e., up to 1 s after the disturbance, the frequency response can be considered linear, which is the main assumption for parameterisation of the controller. The last plot in Figure 5 shows energy delivered to the power system by BESS during this disturbance. One second after the outage, it reached the value of 56.2, which fulfils the control objective expressed by Equation (14). Figure 4. Single line diagram of the modified 14-bus IEEE system with BESS added to bus 12; the BESS model is a generic current injection model with WECC BESS Control System for RMS simulation [33] (left) and equivalent block diagram for this model in a lumped form (right). Energies 2020, 13, x FOR PEER REVIEW 8 of 17 Table 1. Operating point of generators in the modified 14-bus IEEE system. Gen 1 Gen 2 Gen 3 Gen 6 Gen 8 Type Synchronous Synchronous Synchronous Wind Wind Rating [MVA]/Load [MW] 200/50 220/30 160/117 72/49 100/77 H [s] 3.2 4.0 2.0 0 0 Figure 5. Response to generation trip of 1. purely inertial system (grey line); 2. inertial system with synthetic inertia (SI support exhibiting linear decline of frequency in the first 0.5 s (red line); 3. same as no. 2 but with governor action (dashed line). 4. System Operation Planning Methodology with Focus on RoCoF As explained in Section 1, the assessment of inertia adequacy to support the required RoCoF throughout the whole planning period can be based on Equation (1) only if syn-chronous generators are the only type of generation supplying the system with power. Then, it is a matter of summing their respective kinetic energy and identifying the largest possible trip in the power system."
10.3390_en14030737,"dataset provided, data",343,,0,"Energies 2020, 13, x FOR PEER REVIEW 7 of 17 size of the disturbance but shortly after is influenced by instantaneous power delivered by the SI (which is the controller’s output). Thus, the required energy E500ms can be deliv-ered only if RoCoF is equal to RoCoFlim, which can happen only if Eτ is equal to E500ms. In practice, it is impossible to keep this equilibrium during the dynamic process, thus small deviations should be expected. 3.3. Synthetic Inertia Concept Verification Based on Simple Theoretical Model of a Power System A modified version of the IEEE 14-bus system shown in Figure 4 is used for the pur-pose of providing a proof of concept. The modification regarded ratings of the machines and their type are presented in Table 1. The example is based on data from Scenario 2, in which the largest generation trip was 117 MW (G3). Data for this power flow are provided in Table 1, whereas the calculations are confirmed by the plots in Figure 5. Based on Equation (6), the resulting RoCoFmax for this ΔP is equal to 1.92 Hz/s, whereas the goal is to limit the RoCoF to RoCoFlim = 1.0 Hz/s. According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic iner-tia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13)."
10.3390_en14030737,"dataset provided, data",345,,0,"Figure 4. Single line diagram of the modified 14-bus IEEE system with BESS added to bus 12; the BESS model is a generic current injection model with WECC BESS Control System for RMS simulation [33] (left) and equivalent block diagram for this model in a lumped form (right). Energies 2020, 13, x FOR PEER REVIEW 8 of 17 Table 1. Operating point of generators in the modified 14-bus IEEE system. Gen 1 Gen 2 Gen 3 Gen 6 Gen 8 Type Synchronous Synchronous Synchronous Wind Wind Rating [MVA]/Load [MW] 200/50 220/30 160/117 72/49 100/77 H [s] 3.2 4.0 2.0 0 0 Figure 5. Response to generation trip of 1. purely inertial system (grey line); 2. inertial system with synthetic inertia (SI support exhibiting linear decline of frequency in the first 0.5 s (red line); 3. same as no. 2 but with governor action (dashed line). 4. System Operation Planning Methodology with Focus on RoCoF As explained in Section 1, the assessment of inertia adequacy to support the required RoCoF throughout the whole planning period can be based on Equation (1) only if syn-chronous generators are the only type of generation supplying the system with power. Then, it is a matter of summing their respective kinetic energy and identifying the largest possible trip in the power system. However, this simple methodology does not enable us to include other inertia-providing resources in the calculation. For instance, virtual inertia provided by wind turbines or synthetic inertia from BESS with different energy to power ratios and control principles would be very difficult to be accurately represented in this equation. Therefore, there is a need for frequency stability assessment methodology that takes into account non-synchronous contributors to system inertia and frequency support. This methodology is explained below. It consists of five steps depicted in"
10.3390_en14030737,"dataset provided, data",352,,0,"According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic iner-tia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13). By using this relationship, the main time constant TSI can be adjusted to satisfy the requirement of the speed of response of the SI controller. Finally, based on Equations (13) and (14), gain KSI can be calculated to match the required amount of energy ∆𝐸𝜏 and, in this example, is equal to 61.49 MWs/Hz. Figure 5 shows the result of G3 outage for three configurations of frequency control in the power system. The theoretical case with only inertial response is shown in red; the case with added SI contribution tuned according to the rules described above is shown in thick grey, whereas the same case with primary frequency control active is marked with a dashed line. Synthetic inertia brings considerable improvement to the frequency decline process: the frequency does not drop under the assumed level of 49.0 Hz during the first second, indicating that the average RoCoF is not lower than −1 Hz/s. The results also con-firm that for the time period of interest, i.e., up to 1 s after the disturbance, the frequency response can be considered linear, which is the main assumption for parameterisation of the controller. The last plot in Figure 5 shows energy delivered to the power system by BESS during this disturbance."
10.3390_en14030774,"code, data available, code available, data",332,,0,"The PSO engine de-termined DER PF settings by evaluating circuit performance to minimize the risk of volt-age or protection violations while also maximizing economic value. The formulation op-timization problem was designed to acquire the voltage regulation values of operating DERs off the unity PF value, as shown in Equation (5). min 𝑃𝐹[𝜔0∙𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)+𝜔1∙𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒)+𝜔2∙𝐶(𝑷𝑭)] (5) where: 𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)=1 if any |V | > 𝑉𝑙𝑖𝑚 (6) 𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒) is the standard deviation of 𝑽−𝑽𝑏𝑎𝑠𝑒 (7) 𝐶(𝑃𝐹)=∑1−|𝑷𝑭| (8) where, the variable 𝑽 represents the vector of bus voltages of the distribution system, while the variable 𝑽𝑏𝑎𝑠𝑒 is a vector the nominal voltages at each one of the buses. The variable PF is a vector representing the PFs of each one of the DERs in the system. For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values. The varia-ble 𝑉𝑙𝑖𝑚 represents the maximum and minimum voltage limit. These values were selected to comply with voltage range A of ANSI C84.1, which specifies voltage limits of ±0.05 pu. During the execution of the optimization algorithm, calculated solutions that deviate from the specified 𝑉𝑙𝑖𝑚 would be discouraged. Equation (8) is implemented to prevent calcu-lated values that deviate from unity PF, due to active power curtailment during high irra-diance condition. For the experiments performed in this paper, the weights used for vari-ables 𝜔0, 𝜔1 and 𝜔2 were 1.0, 2.0 and 0.05, respectively. The PSO algorithm was config-ured so that it would not execute if all the bus voltages of the distribution feeder were within the selected nominal voltage threshold of 0.20%."
10.3390_en14030774,"code, data available, code available, data",333,,0,"For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values. The varia-ble 𝑉𝑙𝑖𝑚 represents the maximum and minimum voltage limit. These values were selected to comply with voltage range A of ANSI C84.1, which specifies voltage limits of ±0.05 pu. During the execution of the optimization algorithm, calculated solutions that deviate from the specified 𝑉𝑙𝑖𝑚 would be discouraged. Equation (8) is implemented to prevent calcu-lated values that deviate from unity PF, due to active power curtailment during high irra-diance condition. For the experiments performed in this paper, the weights used for vari-ables 𝜔0, 𝜔1 and 𝜔2 were 1.0, 2.0 and 0.05, respectively. The PSO algorithm was config-ured so that it would not execute if all the bus voltages of the distribution feeder were within the selected nominal voltage threshold of 0.20%. Alternatively, bus voltage values outside the ANSI C84.1 Range A would execute the PSO algorithm. To reduce DER com-munication traffic, if the new calculated PF did not have an effect on the objective function, specified by a threshold value of 1×10−7, the DERs PF value would not be adjusted. Connected Energy Software, Cloud ApplicationParticle Swarm OptimizationWinIGS State EstimatorWinIGS Section 2 PDC Data CaptureWinIGS Section 1 PDC Data Capture PMU IEDs (metered locations)PMU IEDs (metered locations)State Estimation Solution (C37.118)Connected Energy DER Communication ModuleIEEE C37.118PV Production ForecastsCalculate P and Q for Loads for OpenDSSInstantiate OpenDSSInitial set of DER reactive power settingsRun OpenDSS over time horizonOptimal DER reactive power settingsUpdate DER power factors using PSOCalculate objective functionPV Production DatabaseWinIGS Section 3 PDC Data CapturePMU IEDs (metered locations)DERsIEDs Figure 2. Block Diagram of the Information Flow in the PSO Optimization Method. Energies 2021, 14, 774"
10.3390_en14030774,"code, data available, code available, data",350,,0,"Communications to/from the Connected Energy system used the DNP3 Application Note AN2013-001 information model to change the grid-support functions [92,93]. PF functions were used to change the active and reactive power behaviors of the PV systems. Data Bus (DBus) is a TCP/IP protocol developed by EPRI used to enable commu Energies 2021, 14, x FOR PEER REVIEW 7 of 20  nute over a 15-min horizon using 3 periods with a 5-min step size. The forecast PV pro-duction for each of the epochs was calculated using the forecasting code. In cases where there was no PV production data available, scaled surrogate PV system forecasts were used. The p_mult and q_mult values persisted for the entire time-domain simulation. PSO was selected to locate the optimal DER PF settings because the fitness landscape was non-convex due to the voltage regulators and other binary components. The PSO engine de-termined DER PF settings by evaluating circuit performance to minimize the risk of volt-age or protection violations while also maximizing economic value. The formulation op-timization problem was designed to acquire the voltage regulation values of operating DERs off the unity PF value, as shown in Equation (5). min 𝑃𝐹[𝜔0∙𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)+𝜔1∙𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒)+𝜔2∙𝐶(𝑷𝑭)] (5) where: 𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)=1 if any |V | > 𝑉𝑙𝑖𝑚 (6) 𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒) is the standard deviation of 𝑽−𝑽𝑏𝑎𝑠𝑒 (7) 𝐶(𝑃𝐹)=∑1−|𝑷𝑭| (8) where, the variable 𝑽 represents the vector of bus voltages of the distribution system, while the variable 𝑽𝑏𝑎𝑠𝑒 is a vector the nominal voltages at each one of the buses. The variable PF is a vector representing the PFs of each one of the DERs in the system. For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values."
10.3390_en14030774,"data available, data, python, code, code available",251,,0,"The optimization engine used the PSO method to determine the optimal PFs of controllable DER systems. This method determined the optimal PV PF setpoints and associated optimal power ﬂow (OPF) by wrapping an OpenDSS time series simulation of the reduced-order feeder model inside a PSO. The active and reactive components of the loads in the OpenDSS model were populated using live state estimation results. PV forecasts for each of the PV systems were populated in the OpenDSS model to optimize operations over a future time horizon. The OpenDSS load data was changed by the WinIGS state estimation solution and the PV production was populated by the PV forecasts. A simpliﬁed representation of the PSO approach is shown in Figure 2. A Python interface was created to capture the state estimation IEEE C37.118 data streams from WinIGS. These phasor data for each of the buses and PV systems were used to calculate the active and reactive power levels for the dynamic loads in the OpenDSS model. Then using the communication interface to OpenDSS, the active and reactive multipliers, p_mult and q_mult, were updated in the OpenDSS environment. The optimization was completed every minute over a 15-min horizon using 3 periods with a 5-min step size. The forecast PV production for each of the epochs was calculated using the forecasting code. In cases where there was no PV production data available, scaled surrogate PV system forecasts"
10.3390_en14030774,github,44,,2,"Supplementary Materials: The following are available online at https://ieeexplore.ieee.org/document/ 8478426. The anonymized, reduced-order OpenDSS and Opal-RT feeder model, and all portions of the non-proprietary ProDROMOS codebases are included in the project GitHub repository: https://github.com/sunspec/prodromos."
10.3390_en14041018,code,22,,0,"code for the simulation of ﬁxed-bed burners. Energy Convers. Manag. 2015, 105, 30–44. [CrossRef]"
10.3390_en14041018,code,29,,0,"Figure 4. Reaction rate as a function of initial particle radius for particles simulated with the semi-2D code with ρ = 700 kg/m3, AR = 2."
10.3390_en14041018,"data available, data",17,,0,Data Availability Statement: The data presented in this study are available in the Appendix A.
10.3390_en14041018,database,23,,0,"27. Chase, M.W., Jr. “Thermophysical Properties of Fluid Systems” in NIST Chemistry WebBook, NIST Standard Reference Database"
10.3390_en14041018,"dataset, data",176,,0,"The cross validation is made to ensure that the presented model is robust. It can be done in many ways; here, the random subset method [36] is used with 6 splits and 20 iterations, thus on average 17% of the data set is removed in each iteration. All models have been made with two PLS components. When doing the cross validation, preferably the obtained RMSECV values should be low, and ExpVar values high. The values in Tables 3 and 4 are the basis for choosing the relevant preprocessings for a model. Due to the lower RMSECV(A) and RMSECV(E) and higher ExpVar(X) (compared to the number of input data in X) and ExpVar(Y) values, model 2 has been chosen both for A and E. If models appear equally good or almost equally good based on RMSECV and ExpVar data, the simpler model is usually preferred within PLS."
10.3390_en14051297,"data https, data",46,,0,"a CAMEO Chemicals (https://cameochemicals.noaa.gov, accessed on 5 February 2021). b ChemAxon (http://www.chemicalize.org, accessed on 2 January 2021). c Hazardous Substances Data Bank (http://pubchem.ncbi.nih.gov, accessed on 2 January 2021)."
10.3390_en14051321,"data available, data",19,,0,Data Availability Statement: The data presented in this study are available on request from the corresponding author.
10.3390_en14051387,"benchmark, data",197,,0,"MPC simulations were carried out over 10 days, with a focus on the last 48-hour period, with outdoor temperature conditions ranging from −20 ◦C to 3 ◦C (Figure 9). A business as usual (BAU) indoor temperature setpoint proﬁle, with a nighttime setpoint of 18 ◦C and a daytime setpoint of 22 ◦C, was created as a benchmark, which is shown in the top graph of Figure 10 as a dashed black line. For the MPC studies, the zone temperature setpoint was constrained by a lower bound TSP,min (17 ◦C at night and 19 ◦C during the day) and an upper bound TSP,max (24 ◦C at all times) in order to maintain a level of thermal comfort for the zone occupants. To demonstrate the methodology, typical known occupancy schedules for the warehouse (7 a.m.–6 p.m.) and weather data were used for this MPC study; however, an existing weather forecast tool such as CanMETEO [45] could be incorporated into eventual implementation within the building automation system in a real building."
10.3390_en14051387,code,18,,0,order. Easy and quick modiﬁcations of the model are thus possible without rewriting the simulation code.
10.3390_en14051387,code,73,,0,The simulation code of the models were developed using MATLAB and can be conveniently modiﬁed to adjust the model order. The model order was automatically adjusted from very simple (1 node) to more detailed by deﬁning the two-dimensional grid of rows and columns of brick nodes. The number of rows multiplied by the number of columns was thus the total number of brick capacitance nodes for that given model
10.3390_en14051387,"data available, data",214,,0,"Figure 2 depicts how heat is delivered to the warehouse zone by the HVAC system [39]. Sensors measuring air temperatures are located throughout the HVAC system, and four thermocouples are embedded in the ETS, which measure brick temperatures. During operation of the ETS, a portion of supply air is directed to the thermal storage device to provide the zone with increased heat from stored heat energy. The zone is modeled using a 1R1C explicit ﬁnite difference modeling approach (Figure 3), with a heating coil model with proportional-integral control. The zone model was originally developed for a previous study [39] and was calibrated using several days of measured data from the real building. Statistical indices were found for a Root-Mean-Square Error (RMSE) of 0.75 ◦C and a Mean Absolute Error (MAE) of 0.66 ◦C. This simple model was used for demonstration of the overall methodology presented in this study and was sufﬁcient for shorter control horizons, such as the ones evaluated here (1 or 2 days). C is the equivalent thermal capacitance of the zone, Tair is the effective operative temperature of the zone, R is the equivalent resistance"
10.3390_en14051387,"data available, data",269,,0,"Typically, in MPC, the optimal control problem is solved at each deﬁned control step by looking ahead at forecast weather and occupancy schedules over the prediction horizon, PH. The prediction horizon is a time period where we have reasonably reliable information, ranging from a few hours to a couple of days. Using data available from the prediction horizon period, an optimization routine is solved and an optimal sequence of control moves is identiﬁed through the implementation of MPC. The identiﬁed schedules and control moves are applied to the building over a “control horizon”, which can be the same length or be shorter than the prediction horizon. Once the current control horizon has ended, the optimization exercise is performed again for the following prediction horizon. This process is repeated until the end of the simulation time (e.g., one day or one year). For the sake of simplicity, in this particular case, the prediction and control horizon are the same length of time and MPC was only initiated with a notiﬁcation signal: either 30 h for a 12-hour ahead notiﬁcation at 6 p.m. or 22 h for a 4-hour ahead notiﬁcation at 2 a.m. (for a DR event at 6 a.m.). Further studies could be carried out on performance related to the control horizon length; however, Date et al. [44] found that longer control horizons (12 h or more) were favourable for a similar building in a similar climate."
10.3390_en14051470,code,86,,0,"A Bosch, solenoid, common rail type injector is installed in the combustion chamber. The injector nozzle originally had 5 holes, with an included angle of 124◦, hole diameter of 0.21 mm (nozzle code DSLA124P 1659). To avoid spray-spray interaction and spray window interaction and to allow fuel injection spray studies (not conducted in this work), the fuel injector is modiﬁed so only one hole was left open, with the remainder laser welded closed."
10.3390_en14071925,"data available, data",10,,0,Data Availability Statement: Data available in this manuscript.
10.3390_en14092648,"data available, data, download, publicly available, data https, dataset",69,,1,Data Availability Statement: Publicly available datasets were analyzed in this study. This data can be found here: https://projects.worldbank.org (accessed on 31 August 2020); https://data. worldbank.org/indicator/NY.GDP.DEFL.ZS (accessed on 23 September 2020); https://irena.org/ Statistics/Download-Data (accessed on 3 April 2021); https://edgar.jrc.ec.europa.eu/overview.php? v=booklet2020 (accessed on 3 April 2021).
10.3390_en14092709,benchmark,24,,0,"32. Lottersberger, F.; Hafner, N.; Jodin, D. Efﬁciency indicators for benchmark and improvement of energy efﬁciency on automated"
10.3390_en14092709,"data available, data",110,,0,"The technical parameters of devices are gathered in Table 3. These are average values characteristic not of speciﬁc producers but of broader classes of devices for initial selection in most practical applications. The values result from the technical speciﬁcations of devices in their standard conﬁguration, obtained by a broad overview of the equipment manufacturers’ offerings. The prices can differ according to the conﬁguration of the equipment and the purchase contract conditions. The prices were subjectively determined on the basis of practice and equipment offers and can be considered reliable for Central Europe in the year 2020, but no ofﬁcial data are available."
10.3390_en14092709,"data available, data",222,,0,"n = annual cost of human labor in variant n (EUR); n = annual cost of transport equipment maintenance in variant n (EUR); n = annual cost of energy consumption of equipment and MHSs in variant n (EUR); n = annual cost of warehouse control system maintenance in variant n (EUR); n = annual cost of rack system maintenance in variant n (EUR). Table 10 presents the expenditure on warehouse equipment and infrastructure. Expenditure for transport devices is a product of the number of devices in variants (Table A7) and the unit price (Table 3). The expenditure on buildings is a function of the assumed cost of erecting 1 m2 of an industrial hall of a given height and the areas of functional zones different in subsequent variants. Construction costs obtained from warehouse realizations in central Poland (suburban industrial areas) were averaged. The average cost of erecting 1 m2 of an industrial hall of up to 10 m in height is EUR 347.83; for between 10 to 20 m in height, it is EUR 543.48; and for above 20 m, it is EUR 739.13. No ofﬁcial data are available, however."
10.3390_en14092709,database,138,,0,"43–53. [CrossRef] Jacyna, M.; Wasiak, M.; Bobi ´nski, A. SIMMAG3D as a tool for designing of storage facilities in 3D. Arch. Transp. 2017, 42, 25–38. [CrossRef] Jachimowski, R.; Goł˛ebiowski, P.; Izdebski, M.; Pyza, D.; Szczepa ´nski, E. Designing and efﬁciency of database for simulation of processes in systems. Case study for the simulation of warehouse processes. Arch. Transp. 2017, 41, 31–42. [CrossRef] Szczepa ´nski, E.; Jachimowski, R.; Izdebski, M.; Jacyna-Gołda, I. Warehouse location problem in supply chain designing: A simulation analysis. Arch. Transp. 2019, 50, 101–110. [CrossRef]"
10.3390_en14102800,benchmark,28,,0,"49. Udomsilp, D.; Lenser, C.; Guillon, O.; Menzler, N.H. Performance Benchmark of Planar Solid Oxide Cells Based on Material"
10.3390_en14102800,open-source,193,,0,"The simulation is conducted in MFiX 19.2.2 (Multiphase Flow with Interphase eXchange, MFiX), a free open-source multiphase ﬂow resolver, which is a powerful tool to simulate the gasiﬁcation of solid fuel and allows users to develop model conveniently [43]. TFM is the most mature model in MFiX and can calculate dense, reactive multiphase ﬂow by treating both solids and gases as interpenetrating continua. TFM is adopted to describe the gas–solid ﬂow and the governing equations are solved using ﬁnite volume method. The implicit Euler scheme with the ﬁrst-order accurate is employed for temporal discretization and Superbee, a second-order accurate scheme, is used for spatial discretization. The automatic time-step adjustment is utilized to accelerate calculation. A grid size smaller than 8dp (dp stands for the diameter of solid particle) is thought to be ﬁne enough [42,44]; about 4dp is adopted here to achieve a satisﬁed accuracy and grid independency. The electrochemical reactions occur in SOFC are treated as reactive boundary conditions in MFiX and the SOFCs are operating under ﬁxed current densities."
10.3390_en14123501,"data https, data",6,,0,"Martinopoulos, G. Bin Weather Data"
10.3390_en14123501,database,96,,0,"26. Kneifel, J.; O’Rear, E.; Webb, D.; O’Fallon, C. An exploration of the relationship between improvements in energy efﬁciency and life-cycle energy and carbon emissions using the BIRDS low-energy residential database. Energy Build. 2018, 160, 19–33. [CrossRef] [PubMed] Said, S.A.M.; Habib, M.A.; Iqbal, M.O. Database for building energy prediction in Saudi Arabia. Energy Convers. Manag. 2003, 44, 191–201. [CrossRef]"
10.3390_en14123501,"dataset, data",84,,0,"The results are presented in diagrams and tables. Due to space limitations, only parts of them are presented in this manuscript; more analytical results are included elsewhere [14]. Scope of this work is to provide a data set, based on recent historical climatic records, which is useful for performing energy analysis of HVAC systems with simpliﬁed multiple measure methods such as the classical bin method [15,16], the modiﬁed bin method [17]"
10.3390_en14123501,"dataset, data",103,,0,"Table 2 contains the data for the cooling period, Table 3 contains the data for the heating period and Table 4 the data for the transient months. The period of the day was divided into six 4-h shifts for a better part load energy analysis. The results are derived from monthly data [14]. These datasets can be used for the estimation of energy requirements and fuel consumption according to the bin and modiﬁed bin method [15,17], and may serve for updating the bin data of the Warmer zone of EN14825 [16]."
10.3390_en14123501,"dataset, data",113,,0,"In the present work, a 30-year long (1983–2012) time series of hourly outdoor air temperature and relative humidity records from the meteorological station of NOA in Athens, Greece was used to produce speciﬁc data for the application of bin methods. To that end, the outdoor air temperature and relative humidity hourly measurements were analyzed and the frequency of outdoor air dry-bulb temperature at intervals of 2 ◦C, as well as the corresponding mean coincident humidity ratio or wet-bulb temperature values, was calculated. The main goal was to provide an up-to-date bin data set for the city of Athens, based on recent records."
10.3390_en14123501,"dataset, data",301,,0,"Energies 2021, 14, x FOR PEER REVIEW 17 of 21  Table 6. Heat transfer coefficients U (W/m2K) and shading coefficients SC of windows. Window Types (Layer/Frame) Room/Space U (W/m2K) (Summer) U (W/m2K) (Win-ter) Shading Co-Efficient SC Double, metal Floors’ offices/corri-dors 3.3 3 0.85 Single, metal Ground floor of-fices/rooms 4.6 4.7 1 Single, metal Staircases 6 6.4 1 The operating hours of the building are from 9:00 a.m. to 20:00 p.m., with an internal temperature of 20 °C in the heating season and 26 °C in the cooling season. Temperatures of 15 and 30 °C were assumed for the rest of the day. For the heating and cooling periods, the temperature of the stairwells and the entrance area was set at 15 and 30 °C in each case. Throughout the year, the ventilation rate was set at 1 and 0 ACH in operation and non-operation periods, respectively. The office building was simulated using the 3 dec-ades (1983–1992, 1993–2002 and 2003–2012) temperature bin data of the city of Athens. Resulting Impact of Climate Change on the Energy Demands for Heating and Cooling It is obvious that there is a noticeable increase in the total energy requirements of the office building for cooling and dehumidification during the period from May to October, as highlighted in Figure 10 and in Table 5. There is an increase of approximately 10% per decade (11.4% in the second decade and 8.2% in the third), while overall cooling loads increased by 20.6%. Figure 10. Total (sensible and latent) monthly energy requirements for cooling."
10.3390_en14123501,"dataset, data",331,,0,"The total increase in the energy requirements of the building for both heating and cooling from the first to the last decade is 14.5% (Table 7). Table 7. Latent and sensible loads for the 1983–2013 period. Time Period Cooling Loads Heating Loads Sensible Latent Total (kWh) Sensible Latent Total (kWh) 1983–1992 94.8% 5.2% 413,009 86.7% 13.3% 60,804 1993–2002 95.0% 5.0% 460,184 87.0% 13.8% 50,403 2003–2012 91.8% 8.2% 497,963 83.7% 16.3% 44,665 The trend of cooling loads is increasing in all summer months, especially in June, July and August, and is decreasing in all winter months (Figure 10). The decrease in the heat-ing loads cannot offset the increase in cooling loads during the specified period, so overall energy requirements are increased. This is also reflected in the building’s specific energy requirements for heating and cooling. 5. Conclusions In the present work, a 30-year long (1983–2012) time series of hourly outdoor air tem-perature and relative humidity records from the meteorological station of NOA in Athens, Greece was used to produce specific data for the application of bin methods. To that end, the outdoor air temperature and relative humidity hourly measurements were analyzed and the frequency of outdoor air dry-bulb temperature at intervals of 2 °C, as well as the corresponding mean coincident humidity ratio or wet-bulb temperature values, was cal-culated. The main goal was to provide an up-to-date bin data set for the city of Athens, based on recent records. Some conclusions concerning the behavior of the calculated climate parameters dur-ing the selected time period and the impact of changes observed on local climate are dis-cussed as well. From the results it is obvious that the annual average dry-bulb temperature Energies 2021, 14, 3501"
10.3390_en14123501,"dataset, data",335,,0,"Figure 10. Total (sensible and latent) monthly energy requirements for cooling. On the other hand, there is a noticeable decrease in the total energy requirements of the office building for heating and humidification during winter (from November to April), with a decrease of approximately 14.2% per decade (17.1% in the second decade and 11.4% in the third) and an overall heating load decrease of 26.5%, as evidenced in Figure 11 and in Table 5. It should be pointed out that Figures 10 and 11 present the sensible and latent energy requirement of the building, the performance coefficients of the primary equipment not being accounted for. Energies 2021, 14, x FOR PEER REVIEW 18 of 21  Figure 11. Total (sensible and latent) monthly energy requirements for heating. In particular, the increase in sensible loads accounts for 11.3% over the period con-sidered, while in contrast, the increase in latent loads totals 62.6% in the three decades. The total increase in the energy requirements of the building for both heating and cooling from the first to the last decade is 14.5% (Table 7). Table 7. Latent and sensible loads for the 1983–2013 period. Time Period Cooling Loads Heating Loads Sensible Latent Total (kWh) Sensible Latent Total (kWh) 1983–1992 94.8% 5.2% 413,009 86.7% 13.3% 60,804 1993–2002 95.0% 5.0% 460,184 87.0% 13.8% 50,403 2003–2012 91.8% 8.2% 497,963 83.7% 16.3% 44,665 The trend of cooling loads is increasing in all summer months, especially in June, July and August, and is decreasing in all winter months (Figure 10). The decrease in the heat-ing loads cannot offset the increase in cooling loads during the specified period, so overall energy requirements are increased."
10.3390_en14123501,publicly available,16,,0,"publicly available sensors. Urban Clim. 2019, 28, 100464. [CrossRef]"
10.3390_en14123548,benchmark,172,,0,"This paper proposes a novel approach for solving the problems in allocating the DG and capacitor simultaneously in RDS with various load models. The objective of the system is to reduce the power loss and to enhance the VSI of the system. The Bat algorithm is used for sizing and locating both capacitor and DG. Linear changing of feeder loads is performed ranging from 0.5 (light load) to 1.6 (peak load) with a step size of value 0.01, where at each step, sizing is performed using CFT whose formulation is performed in the form of a simple quadrature equation. The present approach is supportive in the selection of a particular size of capacitor and DG based on distribution network operators (DNOs) and their load steps. The standard IEEE 33-bus test system and 69-bus benchmark test system are used to test the effectiveness and feasibility. Comparison of the simulated results is performed with that of other heuristic-based algorithms."
10.3390_en14133765,"data, database, retrieve, data https, dataset",23,,0,Data Availability Statement: All data retrieved from Eurostat datasets: https://ec.europa.eu/ eurostat/data/database (accessed on 5 January 2021).
10.3390_en14133765,"database, data",76,,0,"15. Eurostat Database. Available online: https://ec.europa.eu/eurostat/data/database (accessed on 10 May 2021). 16. Hache, E. Do renewable energies improve energy security in the long run? Int. Econ. 2018, 156, 127–135. [CrossRef] 17. Valentine, S.V. Emerging symbiosis: Renewable energy and energy security. Renew. Sustain. Energy Rev. 2011, 15, 4572–4578."
10.3390_en14133765,"dataset, data",91,,0,"The determined eigenvalues of the correlation matrix (Table 8) reﬂect the signiﬁcance of the principal components in describing the information resources of the input variables (percentage share in the variability of the data set). The Kaiser criterion was used to determine the optimal number of the principal components. Based on this criterion, such number of principal components was adopted to explain as much variation as at least one original diagnostic variable. Thus, for the indicators from 2008, these are two principal"
10.3390_en14133765,"dataset, data",183,,0,"In multivariate analyses of the energy market, including renewable energy, the research subject is oftentimes characterized by a large number of mutually correlated variables (factors). Such a large number of these variables makes it possible to describe the studied phenomena in a very accurate, and at the same time credible, manner. However, this often causes difﬁculties in interpretation. Therefore, in order to ﬁnd signiﬁcant relationships between the variables describing the research object, it is necessary to use methods reducing the dimension of the data space while maintaining as much of its variance as possible [53]. The PCA is one of such methods. It is a statistical measure in which a data set is reduced by creating a new space where the initial factors account for the most variability. The reduction of the number of dimensions is obtained by transforming variables into uncorrelated principal components, which are ordered in a descending manner by the size of the described variance of the community [54]:"
10.3390_en14133765,package,335,,0,"In addition, by the end of 2020, at least 10% of ﬁnal energy consumption in transport was to come from renewable energy sources. At the end of 2014, the European Council adopted and then, in December 2018, revised targets under which it undertook to achieve by 2030 a reduction of at least 40% in greenhouse gas emissions versus 1990 levels and an increase to 32% of the share of renewable energy in all sources of energy consumed throughout the European community [11]. Other breakthrough solutions were proposed by the European Commission and presented at the UN COP25 climate summit in Madrid (Spain) [12]. They include a new, pioneering European climate strategy called the European Green Deal [13]. According to this strategy, the EU’s economy is to become a ""zero-emission"", i.e., climate-neutral economy by 2050. In turn, by 2030, carbon dioxide (CO2) emissions are to be reduced by 50% (plans assume even a 55% decrease) versus its emissions in 1990 [13]. These assumptions are more ambitious than those adopted at the UN climate summit COP24, which took place in 2018 in Katowice (Poland) [14]. Currently, the European Commission has announced that it will push for a higher target of reducing CO2 emissions by 2030 versus 1990. When compared to the previously adopted target, it will be raised from 40% to 55%. This target will enable an increase in the production and consumption of energy from RES. The latest main objectives of the EU’s energy policy until 2050 (the European Green Deal) are aimed at achieving a situation in which the EU economy becomes environmentally neutral. It will result in the elimination of fossil fuels as energy sources."
10.3390_en14133765,package,344,,0,"In general, when considering the volume of energy production from RES in total (as Mtoe of production), Germany is the undisputed leader in this respect among all the EU countries. Germany is responsible for nearly 20% of this production in the entire EU (in 2008, it was about 19%, and in 2018—nearly 20%). However, in the presented research, the authors used, among others, indicators that are related to the implementation of the EU climate and energy package until 2020 in terms of RES, as well as indicators showing the growth of RES production capacity or the volume of energy production from RES in relation to the wealth of a given country. This approach to the analysis showed that the situation in Germany in terms of RES is not as good as compared to the leading countries. Over the last 10 years, Germany has not changed its position in the RES development ranking (Table 10) and still ranks 12th. The level of its development is described as average low (Figure 6). Within 10 years, between 2008 and 2010, the share of RES in total energy consumption increased from 10.01% to 16.5%, but Germany has not yet achieved its 2020 target. In fact, many less prosperous countries have already achieved this goal (e.g., Bulgaria, Estonia, Cyprus, Czech Republic). In order to switch to RES, Germany must pay at least €160 billion over ﬁve years [71], which Kay Scheller says is extremely disproportionate to the outcomes. Additionally, Germany has introduced a number of incentive mechanisms to promote RES: a tariff and low-interest loans for investments in new power plants, as well as several programs for the development of the heating and cooling sector. In transport, subsidy systems and ﬁscal regulation are used [61]."
10.3390_en14133765,package,348,,0,"The current state of RES development in Germany needs to be looked at a bit more critically. In general, when considering the volume of energy production from RES in total (as Mtoe of production), Germany is the undisputed leader in this respect among all the EU countries. Germany is responsible for nearly 20% of this production in the entire EU (in 2008, it was about 19%, and in 2018—nearly 20%). However, in the presented research, the authors used, among others, indicators that are related to the implementation of the EU climate and energy package until 2020 in terms of RES, as well as indicators showing the growth of RES production capacity or the volume of energy production from RES in relation to the wealth of a given country. This approach to the analysis showed that the situation in Germany in terms of RES is not as good as compared to the leading countries. Over the last 10 years, Germany has not changed its position in the RES development ranking (Table 10) and still ranks 12th. The level of its development is described as average low (Figure 6). Within 10 years, between 2008 and 2010, the share of RES in total energy consumption increased from 10.01% to 16.5%, but Germany has not yet achieved its 2020 target. In fact, many less prosperous countries have already achieved this goal (e.g., Bulgaria, Estonia, Cyprus, Czech Republic). In order to switch to RES, Germany must pay at least €160 billion over ﬁve years [71], which Kay Scheller says is extremely disproportionate to the outcomes. Additionally, Germany has introduced a number of incentive mechanisms to promote RES: a tariff and low-interest loans for investments in new power plants, as well as several programs for the development of the heating and cooling sector."
10.3390_en14133765,package,350,,0,"The unquestionable leader in the energy transition process is the European Union (EU). Here, the contribution of RES to savings related to the import of fossil fuels in 2015 amounted to approximately EUR 15 billion. It is estimated that in 2030, it should be as much as EUR 58 billion [7]. This is due to the energy policy pursued by the EU. As early as in 2001, the EU adopted Directive 2001/77/EC [8], which requires each Member State to take appropriate measures to achieve a speciﬁc indicative target of RES-generated electricity. At that time, the target was to achieve renewable energy consumption at the level of 12% with an indicative 22% share of electricity produced from RES by 2010. The next measure was Directive 2003/30/EC [9], aimed at promoting the use of biofuels in transport and other renewable fuels (5.75% share of biofuels in transport fuel consumption). Pursuant to the next Directive 2009/28/EC [10], the EU countries were supposed to increase the share of energy obtained from RES in their total energy consumption. The European climate and energy package contained objectives that were to be achieved by 2020. These targets included a 20% reduction in EU greenhouse gas emissions below levels reported in 1990, increasing the share of energy produced from RES to 20% and improving energy efﬁciency by 20%. In addition, by the end of 2020, at least 10% of ﬁnal energy consumption in transport was to come from renewable energy sources. At the end of 2014, the European Council adopted and then, in December 2018, revised targets under which it undertook to achieve by 2030 a reduction of at least 40% in greenhouse gas emissions versus 1990 levels and an increase to 32% of the share of renewable energy in all sources of energy consumed throughout the European community [11]."
10.3390_en14133765,"package, data",326,,0,"2. Mohsin, M.; Abbas, Q.; Zhang, J.; Ikram, M.; Iqbal, N. Integrated effect of energy consumption, economic development, and population growth on CO2 based environmental degradation: A case of transport sector. Environ. Sci. Pollut. Res. 2019, 26, 32824–32835. [CrossRef] Kanagawa, M.; Nakata, T. Assessment of access to electricity and the socio-economic impacts in rural areas of developing countries. Energy Policy 2008, 36, 2016–2029. [CrossRef] Gonzalez-Salazar, M.A. ; Venturin, M.; Poganietz, W.R.; Finkenrath, M.; Leal, M.R.L.V. Combining an accelerated deployment of bioenergy and land use strategies: Review and insights for a post-conﬂict scenario in Colombia. Renew. Sustain. Energ. Rev. 2017, 73, 159–177. [CrossRef] EU Energy in Figures. Statistical Pocketbook. 2018. Available online: https://op.europa.eu/en/publication-detail/-/publication/ 99fc30eb-c06d-11e8-9893-01aa75ed71a1 (accessed on 13 October 2020). Sorin, G.A. ; Anca, E. The effect of ﬁnancial development on renewable energy consumption. A panel data approach. Renew. Energy 2020, 147, 330–338. Study on Technical Assistance in Realisation of the 2016 Report on Renewable Energy. In Preparation of the Renewable Energy Package for the Period 2020–2030 in the European Union, Freiburg, Germany. 22 February 2017. Available online: https://ec.europa.eu/energy/sites/ener/ﬁles/documents/res-study_ﬁnal_report_170227.pdf (accessed on 13 October 2020). Directive 2001/77/EC of the European Parliament and of the Council of 27 September 2001 on the Promotion of Electricity Produced from Renewable Energy Sources in the Internal Electricity Market; Ofﬁcial Journal of the European Union location L 283. 27 October 2001. Available online: https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A32001L0077 (accessed on 13 October 2020)."
10.3390_en14133765,"package, data",342,,0,"Environ. Sci. Pollut. Res. 2019, 26, 32824–32835. [CrossRef] Kanagawa, M.; Nakata, T. Assessment of access to electricity and the socio-economic impacts in rural areas of developing countries. Energy Policy 2008, 36, 2016–2029. [CrossRef] Gonzalez-Salazar, M.A. ; Venturin, M.; Poganietz, W.R.; Finkenrath, M.; Leal, M.R.L.V. Combining an accelerated deployment of bioenergy and land use strategies: Review and insights for a post-conﬂict scenario in Colombia. Renew. Sustain. Energ. Rev. 2017, 73, 159–177. [CrossRef] EU Energy in Figures. Statistical Pocketbook. 2018. Available online: https://op.europa.eu/en/publication-detail/-/publication/ 99fc30eb-c06d-11e8-9893-01aa75ed71a1 (accessed on 13 October 2020). Sorin, G.A. ; Anca, E. The effect of ﬁnancial development on renewable energy consumption. A panel data approach. Renew. Energy 2020, 147, 330–338. Study on Technical Assistance in Realisation of the 2016 Report on Renewable Energy. In Preparation of the Renewable Energy Package for the Period 2020–2030 in the European Union, Freiburg, Germany. 22 February 2017. Available online: https://ec.europa.eu/energy/sites/ener/ﬁles/documents/res-study_ﬁnal_report_170227.pdf (accessed on 13 October 2020). Directive 2001/77/EC of the European Parliament and of the Council of 27 September 2001 on the Promotion of Electricity Produced from Renewable Energy Sources in the Internal Electricity Market; Ofﬁcial Journal of the European Union location L 283. 27 October 2001. Available online: https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A32001L0077 (accessed on 13 October 2020). Directive 2003/30/EC of the European Parliament and of the Council of 8 May 2003 on the Promotion of the Use of Biofuels or Other Renewable Fuels for Transport; Ofﬁcial Journal of the European Union location L 283. 17 May 2003. Available online: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32003L0030&from=en (accessed on 13 October 2020)."
10.3390_en14133970,"code, code available, github",41,,2,"The code and documentation of TESS are available at https://github.com/slacgismo/ TESS, accessed on 29 June 2021. The current setup reﬂects the speciﬁcation of TESS for our ﬁeld deployment in Colorado, as described in Section 4."
10.3390_en14133970,"code, code available, github",224,,2,"Residential distribution systems are a potential source of system ﬂexibility, particularly in demand. This potential can be realized by an approach called transactive systems (TS), which coordinates residential distributed energy resources (DER) through a market-based mechanism. In this work, we present Transactive Energy Service System (TESS), a modularized platform for the implementation of TS, which enables the deployment of adjusted market mechanisms, economic bidding, and the potential entry of third parties. (The code and documentation of TESS are available at https://github.com/slacgismo/TESS, accessed on 29 June 2021). TESS thereby opens up current integrated closed-system TS, allows for the better adaptation of TS to power systems with high shares of renewable energies, and lays the foundations for a smart grid with a variety of stakeholders. Furthermore, we describe TESS as we have modiﬁed it for a ﬁeld implementation within the service territory of Holy Cross Energy, an electric cooperative serving Eagle, Pitkin, Garﬁeld, Mesa, and Gunnison counties in Colorado. Importantly, our speciﬁcation addresses challenges of implementing TS in existing electric retail systems, for instance, the design of bidding strategies when a (non-transactive) tariff system is already in place."
10.3390_en14144178,"data available, data, database, publicly available, data https",98,,1,"Data Availability Statement: The software used (MERGER), available on the IS-EPOS platform from https://tcs.ah-epos.eu/ (accessed on 9 July 2021), provides graphic outputs that coincide with those shown in the article. Extreme Weather parameters can be requested from the Met Ofﬁce: UK Climate Projections (UKCP) (http://datapoint.metofﬁce.gov.uk/public/data/, accessed on 9 July 2021). Data used for the PGA Probability simulation are available online at British Geological Survey earthquake database search (http://quakes.bgs.ac.uk/earthquakes/dataSearch.html, accessed on 9 July 2021)."
10.3390_en14144357,provide implementation,152,,0,"Energy access has increased signiﬁcantly in Maputo in recent decades as a result of the government’s effort to provide energy to all citizens through implementation of energy policies, an expansion of the electricity grid, and an increase of generation capacity. The trade of power generation technologies has found a good market in Maputo and, thus, solar PV system components are of easy access. However, even with this market opening, there is still a lack of knowledge regarding the potential of solar systems and its installation and operation procedure that could spread the application of solar technology. The Maputo province in Mozambique has high global irradiation when compared with other good locations in Europe and Asia. Maputo’s global horizontal irradiation is about 1785 kWh/m2/year and its solar power potential is estimated to be 400 MW [38]."
10.3390_en14154436,code,86,,0,"Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor between ﬁnal electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical building code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]."
10.3390_en14154436,code,177,,0,"estimated parameter bypass damper compressor condenser technical building code desiccant regenerative indirect evaporative cooler direct expansion desiccant wheel exhaust air electric energy consumption (cooling period) (kWh m−2 year−1) evaporator factor fan heat transfer coefﬁcient (W m−2 K−1) heating coil heating, ventilating and air conditioning indirect evaporative cooler insulation (m2 K W−1) number of parameters metabolic rate (W m−2) mixing box mass ﬂow (kg s−1) nearly zero energy buildings outdoor air static pressure (Pa) predicted percentage dissatisﬁed (%) predicted mean vote (-) capacity (kW) return air regenerative indirect evaporative cooler area (m2) supply air dry bulb temperature (◦C) heat transfer coefﬁcient (W m−2 K−1) air velocity (m s−1) expansion valve; valve volumetric air ﬂow rate (m3 h−1) effective mechanical power (W m−2) electric power consumption (kW) thermal comfort indicator (%) air quality indicator (%) input variable estimated variable"
10.3390_en14154436,code,334,,0,"Energies 2021, 14, 4436 13 of 27  III <1350 IV >1350 For this case, the weighting factor values of air quality, wfAQ, for each category were obtained with the ΔCO2 value in real time and the limit ΔCO2 value of each category, see Equation (10). wfAQ = ΔCO2,actual/ΔCO2,limit (10)The sum of the product of this factor and the step time was performed to each cooling period of the climate zones. The categories I and II were considered favorable, as well as for the favorable thermal comfort conditions. 2.5.3. Energy Consumption and CO2 Emission The energy consumption of the air-cooling systems was obtained as the sum of the electric consumption of each HVAC element, i.e., compressor, fans and pumps. The time period used to integrate this consumption was the cooling period, see Table 9. EEC = ⅀ Electric Energy Consumptionelement × Time step/Sclassroom (11)In Equation (11), the energy-consuming elements of the DX system were the com-pressor, the exhaust fan, the condenser fan and the process fan. Regarding the RIEC and DRIEC systems, the elements that consume energy were the pumps, the exhaust fan and the process fan. The regeneration fan was also considered for the DRIEC system. Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor be-tween final electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical build-ing code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]. Table 13. CO2 emission factor for each climate zone."
10.3390_en14154436,code,336,,0,"Regarding the RIEC and DRIEC systems, the elements that consume energy were the pumps, the exhaust fan and the process fan. The regeneration fan was also considered for the DRIEC system. Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor be-tween final electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical build-ing code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]. Table 13. CO2 emission factor for each climate zone. Climate Zone Country Factor (kgCO2 kWh−1) Lampedusa Italy 0.466 Seville Spain 0.331 Thessaloniki Greece 0.577 Zagreb Croatia 0.273 The CO2 emissions results [kgCO2 m−2 year−1], for each system and each climate zone throughout cooling period, were calculated as the product of the total energy consump-tion [kWh m−2 year−1] and the respective CO2 emission factor [kgCO2 kWh−1]. 3. Results and Analysis Daily and annual analysis were carried out for the three air-cooling systems. The daily analysis was performed for the climate zone of Lampedusa. Two summer days were selected: a typical summer day and a severe summer day. Then, the influence of climatic severity in the thermal comfort, air quality and energy consumption criteria were studied to understand the annual results in each climate zone. 3.1. Thermal Comfort 3.1.1. Daily Analysis of the Air-Cooling Systems The daily results of the air-cooling systems DX, RIEC and DRIEC are represented in Figures 5–7, respectively. For each air-cooling system, air temperatures, air humidity ratio and PPD values over typical and severe summer days are shown. Energies 2021, 14, 4436"
10.3390_en14154436,"data https, data available, data",49,,0,"40. Weather Data. Trnsys 17, vol. 8. Available online: http://www.trnsys.com/ (accessed on 5 December 2020). 41. Comité técnico ISO/TC 159 UNE-EN ISO 7730. Ergonomía del ambiente térmico. 2006. Available online: https://www.une.org/"
10.3390_en14154436,"database, data",54,,0,"The three air-cooling systems were simulated for the representative cities of the hotdry, warm-humid, warm-dry and mixed-humid climate zones. The energy simulations were carried out using the Meteonorm software database [40]. The average values of climate data of the four climate zones are shown in Table 10."
10.3390_en14154507,database,67,,0,"27. Mazzeo, D.; Matera, N.; De Luca, P.; Baglivo, C.; Congedo, P.M.; Oliveti, G. A literature review and statistical analysis of photovoltaic-wind hybrid renewable system research by considering the most relevant 550 articles: An upgradable matrix literature database. J. Clean. Prod. 2021, 295, 126070. [CrossRef]"
10.3390_en14154507,database,260,,0,"the pipes are buried, it should be noted that a greater depth allows the system to be less affected by external temperature ﬂuctuations. In horizontal systems, to avoid interaction between the pipes, it is recommended that the pipes be spaced at least 1.5 m and buried at a depth of 1.2 m to 1.8 m [33]. There are clear advantages in using the earth-to-air heat exchangers in buildings located in hot climates during the summer season, already with short pipes, while in winter the beneﬁts of the system are just for few hours during the day. In the paper [34], it was investigated how geometric conﬁguration, soil thermal conductivity, heat transfer ﬂuid velocity, and installation depth affect the behavior of earthto-air heat exchangers, using CFD simulations. In addition, the paper [35] shows the impact of the unsaturated ground on the performance of the ground heat exchanger as the depth varies through numerical simulation. The results showed that the performance under unsaturated soil conditions decreased up to 40% compared to that under fully saturated ground conditions. In this regard, the study [36] has given a complete database on transient ground temperatures, which was acquired by surveying a case study located in southern Brazil. Regarding outdoor climate conditions, the study [37] considers different climate zones in China, showing higher energy savings of the system application in warm climates."
10.3390_en8031685,"data available, data",167,,0,"This study first imported wind speed data measured by a tidal station (in Miaoli, Taiwan) and a buoy (in Tainan, Taiwan) into the WAsP simulation software to estimate the high altitude wind speeds for the two areas. Then, a Lidar system was set up near the tidal stations and buoys, and high-altitude wind speeds measured by the Lidar system were compared with the WAsP-estimated high altitude wind speed. The two data sources were found to be rather consistent, and regression analysis R-squared values are in the range of 0.8–1. Long-term wind speed data observed by buoys and tidal stations in other areas were also imported into WAsP to create a forecast of wind speed at 50–200 m on the west cost of Taiwan (Hsinchu, Miaoli, Taichung, ChangHua, Yunlin, Chiayi and Tainan).  At the same time, WAsP Engineering 3.0 was used to analyze extreme wind speed."
10.3390_en8031685,dataset,22,,0,"on buildings in urbanized area using 3-D GIS and Lidar datasets. Build. Environ. 2013, 59,  528–535."
10.3390_en8031685,download,53,,0,"Copyright of Energies (19961073) is the property of MDPI Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use."
10.3390_en8053640,download,53,,0,"Copyright of Energies (19961073) is the property of MDPI Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use."
10.3390_en9100838,database,17,,0,56. Phyllis2—Database for Biomass and Waste. Available online: https://www.ecn.nl/phyllis2/ (accessed on
10.3390_en9100838,database,29,,0,"Table 2. Elemental analysis of the lipid extracted algae feedstock for the developed study cases, from the Energy Centre of Netherlands Phyllis2 database [56]."
10.3390_en9100838,"database, code package, package",151,,0,"The complete plant layouts were simulated on the commercial software Aspen Plus™. The thermodynamic property package Peng Robinson Wong Sandler (PRWS) was selected for units operating near to and beyond supercritical processing, namely within blocks A2 and A3. The PRWS was selected due to its reported higher accuracy for phase equilibria estimations for asymmetric and/or polar-nonpolar and/or supercritical mixtures as is the case with biomass SCWR [41]. While the Predictive Soave Redlich Kwong (PSRK) was used for subcritical to ambient processing units. The thermo-physical properties of solid content in LEA were deﬁned as a non-conventional compound and estimated based on software built-in coal database correlations HCOALGEN and DCOALIGT in Aspen Plus™. The gross and net heating values of 22.95 and 21.23 MJ/kg on dry ash free basis are similar to those reported in literature elsewhere [20,56]."
10.3390_en9100838,"database, code package, package",305,,0,"Energies 2016, 9, 10 of 26 Table 2. Elemental analysis of the lipid extracted algae feedstock for the developed study cases, from the Energy Centre of Netherlands Phyllis2 database [56]. Proximate Analysis, wt. % (as Received Basis) Ultimate Analysis, wt. % (Dry Ash Free Basis) Ash content 4.59 Carbon 50.99 Moisture 5.62 Hydrogen 7.44 Volatiles 75.34 Oxygen 33.61 Fixed carbon 14.45 Sulfur 0.48  Nitrogen 7.48 3. Developed Conceptual Plant Designs A simplified block representation of the envisaged BioSNG and hydrogen production pathways is shown in Figure 1. From the left, at the LEA feedstock entry point, the block diagram illustrates the sequential common upstream processing steps for both production pathways, which principally convert the solid organic content into dry sweet syngas. At first, solid valorization takes place under SCWR conditions within blocks A1–3, shown in light green. This is followed by the extraction of the SCWG product water content and acid gases (CO2 and H2S) removal, in blocks B1 and B2 respectively, shown in shades of blue. The directly produced SCWG product is a mixture of H2, CH4 and COx, excluding water, whose concentrations depend on the operating conditions of the SCWG reactor. A parametric analysis for a similar LEA feedstock was reported in our earlier publication [20]; where the theoretical maximum or equilibrium limit for CH4 purity in dry syngas at supercritical processing of 400 °C, 250 bar and 15 wt. % solid content was only 51.5 mol%. Meanwhile for hydrogen, the upper limit at 600 °C, 5 wt. % solid content and a similar pressure was 59.3 mol%."
10.3390_en9100838,"database, code package, package",346,,0,"From the left, at the LEA feedstock entry point, the block diagram illustrates the sequential common upstream processing steps for both production pathways, which principally convert the solid organic content into dry sweet syngas. At first, solid valorization takes place under SCWR conditions within blocks A1–3, shown in light green. This is followed by the extraction of the SCWG product water content and acid gases (CO2 and H2S) removal, in blocks B1 and B2 respectively, shown in shades of blue. The directly produced SCWG product is a mixture of H2, CH4 and COx, excluding water, whose concentrations depend on the operating conditions of the SCWG reactor. A parametric analysis for a similar LEA feedstock was reported in our earlier publication [20]; where the theoretical maximum or equilibrium limit for CH4 purity in dry syngas at supercritical processing of 400 °C, 250 bar and 15 wt. % solid content was only 51.5 mol%. Meanwhile for hydrogen, the upper limit at 600 °C, 5 wt. % solid content and a similar pressure was 59.3 mol%. As a result, downstream blocks, C1–2 and D1–2, were designed for further syngas upgrading to meet the desirable properties for the two final products; Finnish grid quality BioSNG injection and 99.99% purity hydrogen as a minimum requirement for chemical industrial purposes or fuel cell power generation systems [35,37]. Figure 1. Simplified block diagram for the envisaged lipid extracted algae hydrothermal treatment to Bio- synthetic natural gas and hydrogen production pathways. The complete plant layouts were simulated on the commercial software Aspen Plus™. The thermodynamic property package Peng Robinson Wong Sandler (PRWS) was selected for units operating near to and beyond supercritical processing, namely within blocks A2 and A3. The PRWS was selected due to its reported higher accuracy for phase equilibria estimations for asymmetric Energies 2016, 9, 838"
10.3390_en9100838,"database, data",126,,0,"In this present study, we examine the conversion of lipid extracted algae (LEA), post biodiesel production, for an integrated onsite CHP conﬁguration along with synthetic gas, either BioSNG or hydrogen, for offsite power and chemicals production. The conceptual bio-reﬁnery, envisages a poly-generative energy structure, with a lower carbon footprint than a whole algae stand-alone hydrothermal plant as reported in literature [54]. The elemental analysis for the plant’s solid feedstock used in this study is provided in Table 2, and is based on averaged data reported in the Phyllis2 database of the Energy Centre of Netherlands (ECN) for a broad range of lipid extraction methods [56]."
10.3390_en9100838,dataset,230,,0,"As expected for the lower temperature processing conditions, the model CH4 molar prediction showed better conformity with the higher carbon conversion datasets D-1 and D-2 than that of D-3 and D-4. The predicted H2 concentration was clearly distorted by the CH4 favorable catalytic activity of ruthenium, a similar conclusion was found when model results were compared to the work of Haiduc et al. [43]. Another reason behind the model distortion compared to D-3 and D-4 was the detected molar composition of heavier hydrocarbons C+ in the experimental runs, which indicates incomplete conversion into the expected equilibria gases. A limited carbon gasiﬁcation rate was reported, as low as 45% in D-3, something that is expected at such lower temperature conditions and shorter residence times [6]. When analyzing D-3 and D-4, both cases with increased solid throughput, a condition needed to favor CH4 production, longer residence times that enable complete decomposition of solid organics into equilibrium gases is needed. To illustrate further, a look at the total carbon gas yields (not shown in the table), the model (~90% CGE) with a 0.29 g/gfeed compared favorable to D-1 (93% CGE) with 0.26 g/gfeed than D-3 (45% CGE) with 0.09 g/gfeed."
10.3390_en9100838,"dataset, data",297,,0,"For the higher temperature conditions, Chakinala et al. [52] performed a parametric analysis with a capillary tube reactor setup. The model validation for H2 production across varied processing conditions was shown in our earlier publication [20]. Chakinala et al. [52] investigated the effect of catalyst nature on the product as shown in the variation of CGE and product gas composition within data sets D-5 to D-8. The higher reported CGE values lead to better conformity by the model data set-M. D-7 with excess loading of a ruthenium based catalysts 2 g/gfeed, recorded complete carbon partitioning into gas phase and had an almost identical representation of H2 and CH4 molar compositions to that of the model. The reported C+ (heavier hydrocarbon) composition, despite minimal compared to that of D-5, D-6 and D-8, showed that gas reforming equilibrium was still not achieved. Hydrocarbon reforming and gas phase reactions such as steam reforming and water gas shift activity are reported to take place after a much longer residence time than that reportedly set in the current experimental setup [4,5,7,11]. As such, it could be concluded that the modelling approach is capable of illustrating reasonably accurate predictions to the product gas nature as well as reactor yields from an algal slurry, for appropriate reactor setup conditions, that enable complete carbon conversion and gas formation equilibria. On a more conservative interpretation of the model ﬁndings, the thermo-chemical processing limit for chemical fuel production could be estimated. Similar approaches have been adapted in literature and were also validated with other available experimental data sets for a wide variety of biomass [31,35,36,46]."
10.3390_en9100838,package,2,,0,Ideal package
10.3390_en9100838,package,189,,0,"The principal challenge to ensure the development of a reliable reactor or process model remains the computation of the speciﬁc thermodynamic properties of the highly asymmetric and multi-dimensional slurry mixtures. The parametrization of the SCWR phase equilibria interactions for the various processing components of solid and ﬂuid phases, polar and non-polar in nature, within super- and sub-critical conditions remains an area under development [41]. As such, predictive empirical equations of states (EOS) have garnered signiﬁcant attention over recent years for SCWG assessment studies. The advantage of empirical EOS in general compared to their activity coefﬁcient counterparts is the ability to predict phase equilibria at elevated pressures where inﬁnite dilution in a single phase is experienced [41]. Some of the EOS adapted in literature are the Peng-Robinson (PR) [24,26,28,31,32,35], Soave Redlich Kwong (SRK) [36,38], Duan [27,40], Statistical Association Fluids Theory (SAFT) [25], Virial EOS [30] and the original ideal package [33]. Some authors employed"
1401.8008,"code, package, code available, github, code package",56,2022-04-27,2,"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github."
1401.8008,data,103,2022-05-13,0,"Comparison data also results when considering subjective human evaluations of pairs of items. For example, if each item is a movie, a person might say that Les Misérables is better than Star Wars, and The Empire Strikes Back is as good as Star Wars. Another example is rating food items such as wine, in which a person may prefer one wine to another, but not be able to perceive a diﬀerence between two other wines. In this context, it is important to use a model which can predict no diﬀerence between two items."
1401.8008,data,11,2022-05-13,0,4. Comparison to SVMrank in sushi and simulated data sets
1401.8008,data,112,2022-05-13,0,"Fig. 6. Test AUC for each model used after training on the ﬁrst 4 months of match data in the 8 different 12-month periods. All SVM model AUCs are shown in addition to AUC of the ELO, Glicko scores and trivial model. The plots in black show the trivial AUC calculated by predicting the most common positive label. The plots in dark blue are AUC values obtained from using Glicko or ELO scores only. The plots in light blue show the AUC distribution from models using only ELO and Glicko features and plots in white are AUC values from models using all computed features."
1401.8008,data,113,2022-05-13,0,"The goal of learning to compare is to accurately predict a test set of labeled pairs (2), which includes equality yi = 0 pairs. We test the SVMcompare algorithm alongside two baseline models that use SVMrank (Joachims 2002). We chose SVMrank as a baseline because of its similar large-margin learning formulation, to demonstrate the importance of directly modeling the equality yi = 0 pairs. SVMrank does not directly model the equality yi = 0 pairs, so we expect that the proposed SVMcompare algorithm makes better predictions when these data are present. The diﬀerences between the algorithms are summarized in Table 2:"
1401.8008,data,115,2022-05-13,0,"Lemma 3.1 establishes the fact that one can learn a ranking function r and a corresponding comparison function c1, deﬁned in (5), by solving either the LP (7) or the QP (12). To make corresponding learning problems for non linearly-separable data as deﬁned by the linear separability test in this section, one can add slack variables to either the QP or the LP. In the next subsection, we pursue only the QP, since it leads to a dual problem with a sparse solution that can be solved by any standard SVM solver such as libsvm (Chang & Lin 2011)."
1401.8008,data,138,2022-05-13,0,"In this subsection, we assume the data are not linearly separable, and want to learn a nonlinear ranking function. We deﬁne a positive deﬁnite kernel κ : Rp × Rp → R, which implicitly deﬁnes an enlarged set of features Φ(x) (middle panel of Figure 1). As in (9), we learn a function f (x) = β + u⊺Φ(x) which is aﬃne in the feature space. Let α, α′ ∈ Rm be coeﬃcients such that u = i), and so we have m i=1(αiκ(˜xi, x) + α′ f (x) = β + i, x)). We then use Lemma 3.1 to deﬁne the ranking function"
1401.8008,data,15,2022-05-13,0,"Overall from the sushi data, it is clear that the proposed SVMcompare model performs"
1401.8008,data,15,2022-05-13,0,"generalizes to a test set of data, as measured by the zero-one loss:"
1401.8008,data,161,2022-05-13,0,"In Figure 5 we ﬁxed the number of training pairs n = 400 and varied the proportion ρ of equality pairs for the three simulated squared norm ranking functions r. We select the model with maximum area under the validation set ROC curve, then use test set AUC to evaluate the learned models. All methods perform close to the optimal true ranking function when r(x) = ||x||2 2. For the other patterns, it is clear that all the methods perform similarly when there are mostly inequality pairs (ρ = 0.1), since SVMrank was designed for this type of training data. In contrast, when there are mostly equality pairs (ρ = 0.9), the compare and rank2 methods clearly outperform the rank method, which ignores the equality pairs. It is also clear that the rank2 and compare methods perform similarly in terms of test AUC."
1401.8008,data,19,2022-05-13,0,"Joachims, T. (2002), Optimizing search engines using clickthrough data, in ‘KDD’."
1401.8008,data,199,2022-05-13,0,"In the supervised learning to rank problem (Li 2011), we are given labeled pairs of items x, x′, where the label y ∈ {−1, 1} indicates which item in the pair should be ranked higher. The goal is to learn a ranking function r(x) ∈ R which outputs a real-valued rank for each item. In this paper we consider a related problem in which the expanded label space y ∈ {−1, 0, 1} includes the y = 0 label which indicates that there should be no rank diﬀerence. In this context the goal is to learn a comparison function c(x, x′) ∈ {−1, 0, 1}. Comparison data naturally arise from competitive two-player games in which the space of possible outcomes includes a draw (neither player wins). In games such as chess, draws are a common result between highly skilled players (Elo 1978). To accurately predict the outcome of such games, it is thus important to learn a model that can predict a draw."
1401.8008,data,2,2022-05-13,0,training data
1401.8008,data,23,2022-05-13,0,"predicted (Shashua & Levin 2003). In this article we propose SVMcompare, a support vector algorithm for these data."
1401.8008,data,29,2022-05-13,0,"Overall, our analysis of the chess match data suggests that the proposed SVMcompare model performs with a higher AUC than the existing state-of-the-art ELO and Glicko results."
1401.8008,data,31,2022-05-13,0,"Overall from the simulations, it is clear that when the data contain equality pairs, it is advantageous to use a model such as the proposed SVMcompare method which learns"
1401.8008,data,32,2022-05-13,0,"Taken together, these imply ˆµ = −1/β. Now, by deﬁnition of the ﬂipped data (8), we can re-write the max margin QP (9) as"
1401.8008,data,51,2022-05-13,0,"These data are geometrically represented in the top panel of Figure 1. Pairs with equality labels yi = 0 are represented as line segments, and pairs with inequality labels yi = {−1, 1} are represented as arrows pointing to the item with the higher rank."
1401.8008,data,57,2022-05-13,0,"For future work, it will be interesting to see if the same results are observed in learning to rank data from search engines. For scaling to these very large data sets, we would like to try algorithms based on smooth discriminative loss functions, such as stochastic gradient descent with a logistic loss."
1401.8008,data,67,2022-05-13,0,"The optimal decision boundaries r(x) ∈ {−1, 1} and margin boundaries r(x) ∈ {−1 ± µ, 1 ± µ} are drawn in Figure 2. Note that ﬁnding a feasible point for this LP is a test of linear separability. If there are no feasible points then the data are not linearly separable."
1401.8008,data,7,2022-05-13,0,3.2. Kernelized QP for non-separable data
1401.8008,data,77,2022-05-13,0,"Fig. 2. The separable LP and QP comparison problems. Left: the difference vectors x′ − x of the original data and the optimal solution to the LP (7). Middle: for the unscaled ﬂipped data ˜x′ − ˜x (8), the LP is not the same as the QP (9). Right: in these scaled data, the QP is equivalent to the LP."
1401.8008,data,89,2022-05-13,0,"1 where x ∈ R2. Left: the training data Fig. 3. Application to a simulated pattern r(x) = ||x||2 are n = 100 pairs, half equality (segments indicate two points of equal rank), and half inequality (arrows point to the higher rank). Others: level curves of the learned ranking functions. The rank model does not directly model the equality pairs, so the rank2 and compare models recover the true pattern better."
1401.8008,data,89,2022-05-13,0,"Our experimental results on simulated and real data clearly showed the importance of directly modeling the equality pairs, when they are present. We showed in Figure 5 that when there are few equality pairs, as is the usual setup in learning to rank problems, the baseline SVMrank algorithm performs as well as our proposed SVMcompare algorithm. However, when there are many equality pairs, it is clearly advantageous to use a model such as SVMcompare which directly learns from the equality pairs."
1401.8008,data,94,2022-05-13,0,"3.1. LP and QP for separable data In our learning setup, the best comparison function is the one with maximum margin. We will deﬁne the margin in two diﬀerent ways, which correspond to the linear program (LP) and quadratic program (QP) discussed below. To illustrate the diﬀerences between these max-margin comparison problems, in this subsection we assume that the training data are linearly separable. Later in Section 3.2, we propose an algorithm for learning a nonlinear function from non linearly-separable data."
1401.8008,"data, database",181,2022-05-13,0,"Ranking data sets are often described not in terms of labeled pairs of inputs (xi, x′ i, yi) but instead single inputs xi with ordinal labels yi ∈ {1, . . . , k}, where k is the number of integral output values. Support Vector Ordinal Regression (Chu & Keerthi 2005) has a large-margin learning formulation speciﬁcally designed for these data. Another approach is to ﬁrst convert the inputs to a database of labeled pairs, and then learn a ranking model such as the SVMcompare model we propose in this paper. Van Belle et al. (2011) observed that directly using a regression model gives better performance than ranking models for survival data. However, in this paper we limit our study to models for labeled pairs of inputs, and we focus on answering the question, “how can we adapt the Support Vector Machine to exploit the structure of the equality yi = 0 pairs when they are present?”"
1401.8008,"data, dataset",101,2022-05-13,0,"The notation and organization of this article is as follows. We use bold uppercase letters for matrices such as X, K, and bold lowercase letters for their row vectors xi, ki. In Section 2 we discuss links with related work on classiﬁcation and ranking, then in Section 3 we propose a new algorithm: SVMcompare. We show results on 3 illustrative simulated data sets and 2 real by learning to rank a sushi data set and a chess dataset in Section 4 and 5. We then discuss future work in Section 6."
1401.8008,"data, dataset",159,2022-05-13,0,"In ranking problems, the goal is to learn a ranking function r(x) ∈ R from Summary. labeled pairs x, x′ of input points. In this paper, we consider the related comparison problem, where the label y ∈ {−1, 0, 1} indicates which element of the pair is better (y = −1 or 1), or if there is no signiﬁcant difference (y = 0). We cast the learning problem as a margin maximization, and show that it can be solved by converting it to a standard SVM. We use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed SVMcompare algorithm outperforms SVMrank when there are equality y = 0 pairs. In addition, we show that SVMcompare outperforms the ELO rating system when predicting the outcome of chess matches."
1401.8008,"data, dataset",68,2022-05-13,0,"Fig. 5. Area under the ROC curve (AUC) for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. For each data set we picked n = 400 pairs, varying the proportion ρ of equality pairs. We plot mean and standard deviation of AUC over 4 test sets."
1401.8008,"data, dataset",78,2022-05-13,0,"Another way to formulate the comparison problem is by ﬁrst performing a change of variables, and then solving a binary SVM QP. The idea is to maximize the margin between signiﬁcant diﬀerences yi ∈ {−1, 1} and equality pairs yi = 0. Let Xy, X′ y be the |Iy| × p matrices formed by all the pairs i ∈ Iy. We deﬁne a new “ﬂipped” data set with"
1401.8008,"data, dataset",86,2022-05-13,0,"2.2. SVMrank for comparing In this subsection we explain how to apply the existing SVMrank algorithm to a comparison data set. The goal of SVMrank is to learn a ranking function r : Rp → R. When r(x) = w⊺x (where ⊤ denotes the transpose) is linear, the primal problem for some cost parameter C ∈ R+ (where R+ is a set of all non-negative real numbers) is the following quadratic program (QP):"
1401.8008,"data, dataset",87,2022-05-13,0,"Fig. 4. Test error for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. We randomly generated data sets with ρ = 1/2 equality and 1/2 inequality pairs, then plotted test error as a function of data set size n (a vertical line shows the data set which was used in Figure 3). Lines show mean and shaded bands show standard deviation over 4 test sets."
1401.8008,"data, dataset",95,2022-05-13,0,"For each experiment, there are train, validation, and test sets each drawn from the same data set of examples. We ﬁt a 10 × 10 grid of models to the training set (cost parameter C = 10−3, . . . , 103, Gaussian kernel width 2−7, . . . , 24), and select the model using the validation set. We use two evaluation metrics to judge the performance of the models: zero-one loss and area under the ROC curve (AUC)."
1401.8008,"data, dataset, data https",164,2022-05-13,1,"5.1. Data source and processing We downloaded the chess match dataset from Chessmetrics (http://www.chessmetrics.com/cm/), containing 1.8 million games played over the 11year period from 1999–2009 by 54205 chess players. For each of the years 1999–2006, we consider the ﬁrst four months (Jan–Apr) as a train set, and the last eight months as a test set (May–Dec). We removed all matches containing a player who had less than 10 matches against other players in the train set, to prevent our data set from containing players with very little information. We also removed all matches that contained a player’s ﬁrst match from the train set as we would have no information about this player. Before pre-processing, 30.9% of matches were a draw and 69.1% of matches resulted in a win or loss. After pre-processing, the median percentage of draws and win-loss results"
1401.8008,"data, dataset, data https",233,2022-05-13,1,"4.2. Learning to rank sushi data We downloaded the sushi data set of Kamishima et al. (2010) from kamishima (http://www.kamishima.net/sushi). We used the sushi3b.5000.10.score from kamishima, which consist of 100 diﬀerent sushis rated by 5000 diﬀerent people. Each person rated 10 sushis on a 5 point scale, which we convert to 5 preference pairs, for a total of 17,832 equality yi = 0 and 7,168 inequality yi ∈ {−1, 1} pairs. For each pair i we have features i ∈ R14 consisting of 7 features of the sushi and 7 features of the person. Sushi xi, x′ features are style, major, minor, oily, eating frequency, price, and selling frequency. Person features are gender, age, time, birthplace and current home (we converted Japanese prefecture codes to latitude/longitude coordinates). As in the simulations of Section 4.1, we picked train, validation, and test sets, each with the same number of pairs n and the same proportion ρ of equality pairs. We ﬁt a grid of models to the training set, select the model with minimal zero-one loss on the validation set, and then use the test set to estimate the generalization ability of the selected model."
1401.8008,"dataset, package",127,2022-05-13,0,"was 44.7% and 55.3% respectively over each of the 8 datasets. For each match i, we i ∈ R16 consisting of ELO scores, Glicko scores, if the player had computed features xi, x′ the initial move, the percentage of instances where a player either lost to a lower ranked player, or won against a higher ranked player, the average score diﬀerence of opponents, win/loss/draw/games played raw values and percentages in addition to various other statistics. ELO scores were initially set at 1200 for all players and FIDE rules were applied to score calculations. ELO and Glicko scores were updated after every match using the PlayerRatings R package (Stephenson & Sonas 2016)."
1401.8008,github,3,2022-05-13,0,https://github.com/tdhock/compare-paper
1401.8008,package,13,2022-05-13,0,"Player Ratings Estimation (R package version 1.0-1)’, CRAN ."
1401.8008,package,26,2022-05-13,0,"Karatzoglou, A., Smola, A., Hornik, K. & Zeileis, A. (2004), ‘kernlab – an S4 package"
1605.07495,code,14,2022-05-13,0,Algorithm 1: Pseudo code of selection method based on non-dominated relative crowding distance
1605.07495,code,16,2022-05-13,0,Algorithm 3: Pseudo code of calculation methods of CR and LR in (15)
1605.07495,code,233,2022-05-13,0,"(i.e., ξcd) and the relative crowding distance (i.e., ξrcd) of all the 6 solutions are given out in TABLE II. Then, with nondominated CDV, the Θ2, Θ3 and Θ4 are qualiﬁed to be the candidates. Thus, the global best solution will be selected out according to their ξrcd (i.e., the selected ones will be marked with circle). For instance, since with the largest ξrcd, the Θ4 has the highest priority to be selected as global best in any case, while the Θ2 is with the lowest priority (only when Y = 3) for its smallest ξrcd. Finally, we assume there are 100 particles in total and we give out the result of particle dividing when Y = 1, 2, 3 (i.e., the numbers of particles in the group assigned to each selected solution are given out after each circle). We notice that the Θ5 is not be selected for any case though its relative crowding distance higher than that of the Θ2, it is because that the CDV of the Θ5 is dominated by the Θ3, which excluding its possibility to be selected as global best. We also give out the pseudo code of this selection method in Algorithm 1."
1605.07495,code,65,2022-05-13,0,"more non-dominated solutions. Meanwhile, each sub-swarm aims to ﬁnd better values for its assigned objective function and all sub-swarms work together in order to get a bigger PF . The pseudo code of MOPSO-NRCD is given at Algorithm 2 (the calculation method of objective functions is given in Algorithm 3, whose detail will be given in the next section)."
1605.07495,code,7,2022-05-13,0,Algorithm 2: Pseudo code of MOPSO-NRCD
1606.01039,data,10,2022-05-13,0,(b) Signal used for ﬁlling missing-data gaps.
1606.01039,data,106,2022-05-13,0,Results using (16) are presented in Figures 5(e)-5(f). We see that in Figure 5(f) the posterior mean describes properly the periodic behaviour and amplitude envelope smooth evolution of the modelled signal. We observe that prediction on the decay gap using (16) is closer to the actual data (red dots) than the results obtained with (15) as well as (14). This is reﬂected in the smallest RMS error in table 2. This is because (16) allows to describe periodic functions that
1606.01039,data,119,2022-05-13,0,"To face the issue of modelling time dynamics we modiﬁed the previous covariance function (15), by multiplying it with an exponentiated quadratic kernel (14). This allows to “smooth” the strictly periodic behaviour of (15). The resulting kernel corresponds to (16). From Figure 4(b) we see that although the posterior mean of the predictive distribution does not exactly ﬁt the data, the model is able to learn the pitch of each of the three sound events with a smaller RMS error (Table 1), as well as the time dynamics or variations in the amplitude envelope of the signal."
1606.01039,data,131,2022-05-13,0,"Experiments were done over real audio. We evaluated diﬀerent kernel conﬁgurations on a pitch estimation task, and on a missing data imputation task. All experiments assume we previously know the number of change-windows and its locations. In the pitch estimation task all the parameters of the covariance function are known, except those related with the fundamental frequency of each sound event, i.e. the value of ωm in (15) and (16) when using these kernels in the general model (10). Thus, we focus on optimizing only these model hyperparameters from the data. In the missing data imputation task the score of the modelled piece of music audio is used for tuning manually the model hyperparameters."
1606.01039,data,132,2022-05-13,0,"In music information research, the aim of audio content analysis is to estimate musical concepts which are present but hidden in the audio data [17]. With this purpose, diﬀerent signal processing techniques are applied to music signals for extracting useful information and descriptors related to the musical concepts. Here, musical concepts refers to parameters related to written music, such as pitch, melody, chords, onset, beat, tempo and rhythm. Then, perhaps the most general application is one which involves the prediction of several musical dimensions, that of recovering the score of a music track given only the audio signal [10]. This is known as automatic music transcription (AMT) [5]."
1606.01039,data,132,2022-05-13,0,"We performed regression on the signal shown in Figure 3(a) using the kernel (15). Figure 4(a) shows the posterior mean of the predictive distribution after training (blue continuous line). The black circle points correspond to observed data. We see the trained model is able to estimate the pitch for each sound event with a RMS error of 0.6282 semitones (Table 1). On the other hand, the amplitudeenvelope evolution of the signal is beyond the scope of the structure that this kernel can model. This is because this covariance function can only describe constant amplitude-envelope, periodic signals, with a fundamental frequency and several harmonics (see Figure 2(f))."
1606.01039,data,142,2022-05-13,0,"In this study we used two short audio excerpts, in order to explore the method, so that we can eﬃciently ﬁt models and search in the hyperparameter space. The excerpt used for pitch estimation experiments corresponds to 0.7 seconds of the song Black Chicken 37 by Buena Vista Social Club. This segment of audio contains three notes of a bass melody (Figure 3(a)). In the missing data imputation task we used polyphonic audio corresponding to 1.14 seconds of Chopin’s Nocturne Op. 15 No. 1, where more than one note occur at the same time. The segments of signal in red in Figure 3(b) represent gaps of missing data. We reduced the sample frequency of both audio excerpts from 44.1KHz to 8KHz."
1606.01039,data,144,2022-05-13,0,"In this article we discussed a Gaussian processes regression framework for modelling music audio. We compared diﬀerent models in pitch estimation as well as in prediction of missing data. We showed which kernels were more appropriate for describing properties of music signals, speciﬁcally: nonstationarity, dynamics, and spectral harmonic content. The advantage of this approach is that by designing a proper kernel we can introduce prior knowledge and beliefs about the properties of music signals, and use all that prior information to improve prediction. The presented work could be extended using eﬃcient representations of GPs in order to model larger audio signals. Other kernels could be studied, as the spectral mixture for modelling harmonic content [2], and Latent Force models [1] for describing mechanistic characteristics of the signal."
1606.01039,data,153,2022-05-13,0,"We compared three diﬀerent models predicting missing-data gaps. We studied kernels (14), (15), and (16). In Figure 3(b) ﬁrst gap (red segment) contains the transient (onset and attack [3]) of a sound event, whereas the second gap is located in a more stable segment of the data (smooth decay). Figures 5(a)5(b) depict the prediction using (14). These ﬁgures correspond to zoom in small sections of the signal where the gaps occur (Figure 3(b)). We see that the model using this kernel overﬁts the data, i.e. the posterior mean (blue line) ﬁts all the observed data (black dots) with high conﬁdence (grey shaded area), but the"
1606.01039,data,183,2022-05-13,0,"Figures 5(c)-5(d) show the prediction using covariance function (15). In the transient gap (Figure 5(c)) the posterior mean (blue line) does not follows the data, this is because transients are short intervals during which the signal evolves in a nonstationary, nontrivial and unpredictable way [3]. opposite to this, the model using kernel (15) can only describe the behaviour of constant amplitudeenvelope periodic stochastic functions. In the second gap (Figure 5(d)) the posterior mean describes properly the periodic behaviour of the data, but it does not follow the amplitude-envelope of the observations. This is because this covariance function is able to describe periodic functions that have several harmonic components. The drawback of this kernel is that it assumes constant the amplitude of the periodic stochastic functions that describes. These diﬀerent performance on the prediction is reﬂected on the RMS error obtained for each gap (Table 2)."
1606.01039,data,2,2022-05-13,0,3.1 Data
1606.01039,data,219,2022-05-13,0,"R where the time input variable t ∈ R, we model the whole function f (t) as a GP. That is, instead of putting a prior over the function parameters η, we introduce a prior over the function f (t) itself [16]. Learning in GP regression corresponds to computing the posterior distribution over the function f (t) conditioned on the observed data y = [y1, · · · , yN ] [15, 13]. The underlying idea in GP regression is that the correlation function introduces dependences between function f (t) values at diﬀerent inputs. Thus, the function values at the observed points give information also of the unobserved points [14]. The structure of the kernel (1) captures high-level properties of the unknown function f (t), which in turn determines how the model generalizes or extrapolates to new test time instants [9]. This is quite useful because we can introduce prior knowledge about what we believe the proprieties of music signals are, by choosing a proper kernel that reﬂects those characteristics. In section 2.2 we study in more detail the design of kernels."
1606.01039,data,264,2022-05-13,0,"In [20] GPs are used for time-frequency analysis as probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct timefrequency components, with the analytic goal being to infer these components by applying Bayes’ rule [20]. GPs have also been used for underdetermined audio source separation. In [8] the mixture signal is modelled as a linear combination of independent convolved versions of latent GPs or sources. The model splits the mixture signal in frames also considered independent, by using weightfunctions. Thus each source is modelled as a series of concatenated locally stationary frames, each one with its corresponding covariance function. With this assumption the resulting signal is supposed to be non-stationary [8]. On the other hand, despite the approach we present also assumes the latent GPs fm in (10) as non-correlated, the observed signal is not framed into independent segments. Instead of using weight-functions that act over the observed data, we introduce change-windows φm inﬂuencing each latent GP ending up with latent processes representing speciﬁc sound events that happen at certain segments of time. Therefore the proposed model keeps the correlation between the observations throughout all the signal. That is what allows to make prediction in gaps of missing data (section 4.2). GPs have been used also for estimating spectral envelope and fundamental frequency of singing voice [21], and for time-domain audio source separation [22]."
1606.01039,data,42,2022-05-13,0,"Figure 3: (a) analysed audio (blue line), change-windows (dashed lines). (b) observed data (blue line), missing-data gaps (red line), change-windows (dashed lines)."
1606.01039,data,45,2022-05-13,0,"Figure 5: Zoom in a portion of missing-data gaps. In each ﬁgure the continuous blue line represent the posterior mean, grey shaded areas correspond to the posterior variance, red dots are missing data, whereas black dots are observed data."
1606.01039,data,8,2022-05-13,0,4.2 Filling gaps of missing data in audio
1606.01039,data,8,2022-05-13,0,"and Data Analysis. Wiley, 1988."
1606.01039,data,91,2022-05-13,0,"The covariance function (1) used for computing the prior distribution (5) allows us to introduce in the model all the knowledge and beliefs we have about the properties of the data. We are trying to model music signals, and some of the broad properties of audio signals are non-stationarity, rich spectral content, dynamics (locally periodic, non constant amplitude envelope), mechanistic behaviour, and music structure. Therefore we seek covariance functions that can describe or reﬂect these properties."
1606.01039,"data, data available",57,2022-05-13,0,"conﬁdence decreases and the prediction is quite poor in the input space zones where the data is not available (red dots). Also, we see that the model using (14) does not expect any periodic behaviour in the gaps. The RMS error for both gaps is presented in Table 2."
1606.01039,"data, dataset",117,2022-05-13,0,"The regression problem concerns the prediction of a continuous quantity [12], here a function f (t), given a data set D = {(ti, yi)}N i=1, where yi are assumed as noisy measurements of f (t) at typically regularly-spaced time instants ti (though GP regression framework allows for irregular sampling or missing data), i.e. yi = f (ti)+ǫi, where ǫi ∼ N (0, σ2 noise). In GP regression for mono channel audio signals, instead of estimating parameters η of ﬁxed-form functions f (t, η) : R 7→"
1608.04885,code,13,2022-05-13,0,in the synthesis of code which mimics proposed service behaviour replacing the real
1608.04885,code,13,2022-05-13,0,"– First line speciﬁes HTTP version, three-digit code and a text string"
1608.04885,code,14,2022-05-13,0,Listing A.2 contains the Java code for the symmetric ﬁeld identiﬁcation class which can
1608.04885,code,14,2022-05-13,0,indicates the operation code that the client requests to or to which the server
1608.04885,code,14,2022-05-13,0,"interact with distributed software systems, while it interacts with local code in lieu"
1608.04885,code,15,2022-05-13,0,The java source code of the implementation of modifying the centroid response is listed in
1608.04885,code,15,2022-05-13,0,the code of mock objects are typically coupled with the code of the software under
1608.04885,code,17,2022-05-13,0,for both identiﬁed symmetric ﬁelds are shown in Table 5.3. The java source code of the
1608.04885,code,2,2022-05-13,0,Java Code
1608.04885,code,27,2022-05-13,0,Listing A.3 contains the Java code for the ﬁeld substitution method that performs sym metric ﬁeld substitution to modify a recorded response for generating a response.
1608.04885,code,3,2022-05-13,0,A Java Code
1608.04885,code,4,2022-05-13,0,APPENDIX A. JAVA CODE
1608.04885,"code, code available",14,2022-05-13,0,environment with the code under investigation is only possible if the code itself is
1608.04885,"code, code package, code available",13,2022-05-13,0,Listing A.1 contains the Java code for describing the symmetric ﬁeld information.
1608.04885,data,1,2022-05-13,0,data
1608.04885,data,10,2022-05-13,0,Table 7.8: Response Accuracy for Clusters with Noisy Data
1608.04885,data,10,2022-05-13,0,deﬁnes temporal rules expressing data dependencies among exchanged messages.
1608.04885,data,10,2022-05-13,0,signiﬁcant diﬀerence between this form and ASN.1 on-the-wire data representation
1608.04885,data,11,2022-05-13,0,EncodingDecodingApplication data representationmessageApplication data representationmessagemessageCHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,11,2022-05-13,0,attach location data to tweets and discover tweets & locations.
1608.04885,data,11,2022-05-13,0,"hierarchical data structure, for instance, a tree structure."
1608.04885,data,12,2022-05-13,0,approach. This approach adopted clustering techniques and data mining techniques to
1608.04885,data,13,2022-05-13,0,A reordered dissimilarity matrix image indicates cluster tendency in the data by dark
1608.04885,data,13,2022-05-13,0,One of the most common transformation of network data is from the representation
1608.04885,data,13,2022-05-13,0,and management of data in distributed applications. The middleware service protocol is
1608.04885,data,13,2022-05-13,0,databases. In Proceedings of the 20th International Conference on Very Large Data
1608.04885,data,13,2022-05-13,0,"twitter data, rather than changing/updating data. Although the current client program"
1608.04885,data,13,2022-05-13,0,"– Compound data types can be constructed by nesting primitive types, shown"
1608.04885,data,13,2022-05-13,0,• Built on a client-server architecture and uses separate control and data connections
1608.04885,data,14,2022-05-13,0,3. A variable-length SMB Data contain 2-byte length ﬁeld that indicates the size
1608.04885,data,14,2022-05-13,0,Textual protocols are built around the notion of message ﬁelds encoded with text data
1608.04885,data,14,2022-05-13,0,and transmitting structured data over a network connection. It is used primarily to
1608.04885,data,14,2022-05-13,0,"application integration, data integration, message oriented middleware (MOM), object"
1608.04885,data,14,2022-05-13,0,binary and textual methods. The binary encoding method targets to a standardized data
1608.04885,data,14,2022-05-13,0,"command type identiﬁer, and value represents data for the command. The most"
1608.04885,data,14,2022-05-13,0,how primitive data types and compound data structure are encoded so they can be
1608.04885,data,14,2022-05-13,0,individual machines. The same issues of complex setup and data availability exist.
1608.04885,data,14,2022-05-13,0,primitive data types and compound data structures. Both X DR and ASN.1 specify
1608.04885,data,14,2022-05-13,0,"speciﬁc data representation, we need to design speciﬁc rules to obtain the embedded"
1608.04885,data,14,2022-05-13,0,the data it wants to transmit from representation it uses internally into a message
1608.04885,data,14,2022-05-13,0,"the oﬄine processing, utilising data mining techniques. It can dramatically reduce the"
1608.04885,data,14,2022-05-13,0,"transmit data between a server and web applications, serving as an alternative of"
1608.04885,data,14,2022-05-13,0,• Uses ASN.1 to deﬁne the data types used to build an SNMP message
1608.04885,data,15,2022-05-13,0,"In this chapter, we have utilised data mining techniques in a large enterprise software"
1608.04885,data,15,2022-05-13,0,"data representations, where ASN.1 and XDR are the most popular ones. For a"
1608.04885,data,15,2022-05-13,0,library into clusters of interactions of the same type. The data mining techniques were
1608.04885,data,15,2022-05-13,0,"schema, which is a speciﬁcation for what JSON data is required for a given"
1608.04885,data,15,2022-05-13,0,• Streaming: Give developers low latency access to Tweet data and other events have
1608.04885,data,16,2022-05-13,0,External Data Representation (XDR) [22] is the network format used to transfer
1608.04885,data,16,2022-05-13,0,JavaScript Object Notation (JSON) [21] is a text-based data interchange format.
1608.04885,data,16,2022-05-13,0,Most of Twitter API operations are used to provide the twitter data for 3rd party Twitter
1608.04885,data,16,2022-05-13,0,Network Data Management Protocol (NDMP) [14] is used to transport data between
1608.04885,data,16,2022-05-13,0,"data on that directory. Once the server received an unbind request, it must unbind"
1608.04885,data,16,2022-05-13,0,"data reorganization by a clustering technique. Operations Research, 20(5):993–1009,"
1608.04885,data,16,2022-05-13,0,"than the Whole Library approach, even though the latter uses all the available data points"
1608.04885,data,16,2022-05-13,0,"• XML deﬁnes a basic syntax for mixing markup with data text, but the designer"
1608.04885,data,17,2022-05-13,0,[82] M. K. Jiawei Han. Data Mining: Concepts and Techniques. Morgan Kaufmann
1608.04885,data,17,2022-05-13,0,"name in their request, followed by a payload, containing the data the service is expected"
1608.04885,data,17,2022-05-13,0,"visual cluster analysis. In Data Mining, 2008. ICDM’08. Eighth IEEE International"
1608.04885,data,18,2022-04-21,0,"[122] A. A. Sofokleous and A. S. Andreou. Automatic, Evolutionary Test Data Generation"
1608.04885,data,18,2022-05-13,0,and semantics of the data. The model step is the ﬁnal stage where we specify how virtual
1608.04885,data,18,2022-05-13,0,"encode data, which is intended or expected to be read by a machine rather than a human"
1608.04885,data,18,2022-05-13,0,"fast as a real service can. To answer this question, we propose and implement a data"
1608.04885,data,19,2022-05-13,0,"2For each LDAP operation, we demonstrate its raw data, as well as the textual representation, which"
1608.04885,data,19,2022-05-13,0,"Data Units (PDU)), exchange of interactions (via service primitives) with service users at"
1608.04885,data,19,2022-05-13,0,"following, we ﬁrst describe how we collect data of six case study protocols in Section. 7.3.1."
1608.04885,data,19,2022-05-13,0,"that can be transmitted over the network; that is, the data is encoded into a message."
1608.04885,data,19,2022-05-13,0,"– Represents each data item with a triple of the form <tag, length, value>,"
1608.04885,data,20,2022-05-13,0,"[62] S. Elbaum, G. Rothermel, S. Karre, and M. F. II. Leveraging user-session data"
1608.04885,data,20,2022-05-13,0,work in pattern matching sequences of data. One is the n − gram approach [72] and the
1608.04885,data,21,2022-05-13,0,VAT [35][148] is a technique that exists in data mining for the visual assessment of cluster
1608.04885,data,21,2022-05-13,0,approach (cf. Section 5.4.2 in Chapter 5) to data sets of all six case study protocols. Having
1608.04885,data,22,2022-05-13,0,"Given a trace library (cf. Deﬁnition 5 in Chapter 4.2), shown as Table 6.1, a data clustering"
1608.04885,data,22,2022-05-13,0,"[147] D. Yuan, Y. Yang, X. Liu, and J. Chen. A data placement strategy in scientiﬁc"
1608.04885,data,23,2022-05-13,0,"protocols, they can be further divided based on the multi-byte order and data representa tions, which are illustrated as follows:"
1608.04885,data,24,2022-05-13,0,7.8 Response Accuracy for Clusters with Noisy Data . . . . . . . . . . . . . . . 118
1608.04885,data,25,2022-05-13,0,Abstract Syntax Notation One (ASN.1) [20] is an standard that deﬁnes a rep resentation for data sent over a network.
1608.04885,data,26,2022-05-13,0,"[97] G. J. McLachlan, K.-A. Do, and C. Ambroise. Analyzing Microarray Gene Expres sion Data. Wiley-Interscience, 2004."
1608.04885,data,27,2022-05-13,0,3.2 Encoding and decoding application data . . . . . . . . . . . . . . . . . . . . 34
1608.04885,data,28,2022-05-13,0,"Most application-level protocols deﬁne message structures containing some form of oper ation or service name in their requests, followed by a payload on what data this service"
1608.04885,data,28,2022-04-21,0,"interaction models. Moreover, by utilizing data mining techniques, the eﬃciency of re sponse generation in the emulation environment has been greatly improved. However the"
1608.04885,data,29,2022-05-13,0,"integrated with many other systems for managing and interpreting data from many busi ness activities. The other systems (called services) include a legacy mainframe program,"
1608.04885,data,3,2022-05-13,0,of data.
1608.04885,data,30,2022-05-13,0,"Binary protocols rely on speciﬁc data structure; and hence, transmitted messages usu ally resort to ﬁxed-length ﬁelds or to a special notation to indicate the length of variable"
1608.04885,data,30,2022-05-13,0,"There are two popular network data representations (i.e. External Data Represen tation (XDR) and Abstract Syntax Notation One (ASN.1)), proposed to encode"
1608.04885,data,31,2022-05-13,0,"• XML syntax provides for a nested structure of tag/value pairs, which is equiv alent to a tree structure for the represented data. This is similar to XDR and"
1608.04885,data,34,2022-05-13,0,"[104] R. T. Ng and J. Han. Eﬃcient and eﬀective clustering methods for spatial data min ing. In Proceedings of the 20th International Conference on Very Large Data Bases,"
1608.04885,data,34,2022-05-13,0,"[140] L. Wang, X. Geng, J. Bezdek, C. Leckie, and R. Kotagiri. Enhanced visual anal ysis for cluster tendency assessment and data partitioning. Knowledge and Data"
1608.04885,data,35,2022-05-13,0,"[138] L. Wang, J. C. Bezdek, C. Leckie, and R. Kotagiri. Selective sampling for approxi mate clustering of very large data sets. International Journal of Intelligent Systems,"
1608.04885,data,43,2022-05-13,0,"as AgileLoad [1] and Selenium [10]. AgileLoad [39] features functions like auto matic modeling, real time data correlation, anomaly diagnostic and recommenda tions, enabling automatic identiﬁcation of performance bottlenecks in the systems."
1608.04885,data,5,2022-05-13,0,2. Network data representations
1608.04885,data,7,2022-05-13,0,• External Data Representation (XDR)
1608.04885,data,8,2022-05-13,0,3.2.2 Network Data Management Protocol (NDMP)
1608.04885,data,8,2022-05-13,0,Figure 3.2: Encoding and decoding application data
1608.04885,data,8,2022-05-13,0,data between diﬀerent kinds of computer systems.
1608.04885,data,83,2022-05-13,0,Protocol Taxonomy      NFS NDMP SNMP LDAP SMB HTTP FTP SMTP POP3 IRC JMS SOAP Message Format Textual   √ √ √ √ √ √ Binary √ √ √ √ √   √ Multi-byte Transmission Order Big-Endian √ √ √ √     Little-Endian   √    Data Representation XDR √ √      ASN.1  √ √     Stateful Stateful v4 √ √ √ √ √ √ √  Stateless v1-v3 √  √    Layer Low √ √ √ √ √ √ √ √ √ √  High      √ √ CHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,9,2022-04-21,0,data is transformed into messages sent and received.
1608.04885,data,9,2022-05-13,0,"– Unstructured body data follows, with speciﬁed size"
1608.04885,data,9,2022-05-13,0,• All requests are four characters followed by data
1608.04885,"data, data available",12,2022-05-13,0,require physical relocation and manual reconﬁguration. Real system data may not
1608.04885,"data, data https",14,2022-05-13,0,[22] XDR: External Data Representation Standard. 2006. http://tools.ietf.org/
1608.04885,"data, data https",17,2022-05-13,0,[14] NDMP: Network Data Management Protocol. Network Working Group. 1996. http:
1608.04885,"data, dataset",16,2022-05-13,0,"As shown in Figure 5.4, we randomly partitioned the original interactions’ data set into"
1608.04885,"data, dataset",16,2022-05-13,0,a statistical analysis will generalise to an independent data set. For the purpose of our
1608.04885,"data, dataset",16,2022-05-13,0,results of a statistical analysis will generalise to an independent data set. For the purpose
1608.04885,"data, dataset",18,2022-05-13,0,"similar search requests in our data set, some of them resulting in responses with zero or one"
1608.04885,database,12,2022-05-13,0,widely utilised in distributed database systems for the vertical partition of large
1608.04885,database,13,2022-05-13,0,of relationships between proposed services and a database. This information is used
1608.04885,database,15,2022-05-13,0,which is a database term for a speciﬁcation of how to interpret a collection of
1608.04885,database,16,2022-05-13,0,[130] M. Tamer ˝Ozsu and P. Valduriez. Principles of Distributed Database Systems.
1608.04885,dataset,13,2022-05-13,0,Our experimental results using the 6 message trace datasets demonstrate that our approach
1608.04885,dataset,13,2022-05-13,0,has two additional datasets: a dataset with textual representation converted from the
1608.04885,dataset,13,2022-05-13,0,• The datasets were obtained by randomly generating client requests for services of
1608.04885,dataset,14,2022-05-13,0,accuracy overall for the datasets tested. The combined approach achieves 100% accuracy
1608.04885,dataset,14,2022-05-13,0,datasets. The Accuracy Ratio column is calculated by dividing the number of valid
1608.04885,dataset,15,2022-05-13,0,client-to-server load. They require detailed knowledge of target protocols and suit able datasets.
1608.04885,dataset,15,2022-04-21,0,"represent the average times spent generating requests, for all the requests in the datasets"
1608.04885,dataset,15,2022-05-13,0,than those of our datasets. Further testing on real system interactions are warranted.
1608.04885,dataset,15,2022-05-13,0,• Our evaluation was performed on six datasets from four protocols. Given the great
1608.04885,dataset,16,2022-05-13,0,The impact of the entropy weightings can only be observed for the LDAP binary dataset.
1608.04885,dataset,18,2022-05-13,0,higher accuracy than f = 1. For the other datasets the threshold had no impact on the
1608.04885,dataset,18,2022-05-13,0,the LDAP (binary) dataset the thresholds of f = 0.5 and f = 0.8 produced signiﬁcantly
1608.04885,dataset,19,2022-05-13,0,(i.e. 0% noise) of interaction messages by operation type for the six datasets tested.
1608.04885,dataset,19,2022-05-13,0,"for four of the datasets, and 99.95% and 99.34% for the remaining two (LDAP binary"
1608.04885,dataset,20,2022-05-13,0,"binary dataset (denoted by LDAP text (1)), and another textual dataset that was used in"
1608.04885,dataset,21,2022-05-13,0,"accuracy stays above 97% for all datasets, when the noise ratio is 5%. As the noise ratio"
1608.04885,dataset,29,2022-05-13,0,"datasets, no impact from the weightings can be observed, as the consensus sequence pro totype by itself (Consensus Only) already produces 99-100% accuracy."
1608.04885,dataset,31,2022-05-13,0,"responses, indeed much faster than the real services being emulated. The response genera tion time is comparable to the Cluster Centroid approach, being faster for some datasets,"
1608.04885,dataset,35,2022-05-13,0,"from the trace library (for three datasets Consensus+Weighting is signiﬁcantly more ac curate, for two it has the same accuracy, for one it is slightly lower). The reason for the"
1608.04885,dataset,4,2022-05-13,0,the other datasets.
1608.04885,dataset,6,2022-05-13,0,7.5 Sample protocol message trace datasets
1608.04885,dataset,8,2022-05-13,0,Table 7.5: Sample protocol message trace datasets
1608.04885,"dataset, used dataset",17,2022-05-13,0,"We have used one message trace dataset for each of these protocols. In addition, LDAP"
1608.04885,github,9,2022-05-13,0,[9] Mockery. https://github.com/padraic/mockery.
1608.04885,open-source,13,2022-05-13,0,ActiveMQ is an open source message broker which fully implements the Java Message
1608.04885,open-source,22,2022-05-13,0,"[142] T. Wang, G. Yin, X. Li, and H. Wang. Labeled topic detection of open source"
1608.04885,package,12,2022-05-13,0,1 package com . ca . calabs . bilby . substitution ;
1608.04885,publicly available,13,2022-05-13,0,"However, the disadvantage is that the communication contract is not always publicly"
1608.04885,publicly available,15,2022-05-13,0,publicly available so that we can use this knowledge to deﬁne criteria for validation.
1608.04885,python,20,2022-05-13,0,"including Java, C#, Groovy, Perl, PHP, Python and Ruby so that the tests can"
1701.07853,code,6,2022-05-13,0,"Elisa Mussumecia, Fl´avio Code¸co Coelhoa"
1701.07853,data,10,2022-05-13,0,empirical data. The adjacency matrix A is given by
1701.07853,data,133,2022-05-13,0,"In this work we decided to look at the spread of news stories over the internet characterizing the resulting spread network and the dynamics of the spread. We start by looking at an actual case of news spread, and estimate the spread network by applying ideas of temporal networks and topic Modeling, connecting similar articles within the bounds of temporal window of inﬂuence. Then we postulate that the spread dynamics approximates an epidemic process and model it using a Network SIR model[3]. The spread of ideas as an epidemic process is not a new idea[4], but here we Propose new tools to estimate the spread network from data and compare it with simulated networks produced by an SIR epidemic model."
1701.07853,data,28,2022-05-13,0,"From the simulation (ﬁgure 11) we obtain the state matrix, which we use to compare the simulated infection distribution with the original data. Then"
1701.07853,data,4,2022-05-13,0,2.1. Data sources
1701.07853,"data, database",91,2022-05-13,0,"The data used for this study was obtained from the Media Cloud Brasil project (MCB) which collects news articles from thousands of sources in the Brazilian Internet since 2013. From the MCB database we obtained 2129 articles talking about the Charlie Hebdo terrorist attack in February 2015. The articles span from the day of the attack to the end of march of 2015. The data include the full text of the article, the URL of publication and the date and time of the publication."
1701.07853,dataset,52,2022-05-13,0,"Figure 10: Total number of articles infected between 0 < λ < 0.00005. The blue area is the area where the peak of the simulation is the same as the peak of the dataset distribution, threfore is the area where the λ values were tested for our simulation."
1701.07853,dataset,63,2022-05-13,0,"where NXY is the number of times an article from publisher X (the publisher of article i), has infected an article from publisher Y (the publisher of article j) and NY Is the total number of articles from publisher Y that have been infected, regardless of publisher. These counts are derived from the empirical dataset."
1701.07853,"dataset, database",51,2022-05-13,0,"The dataset used is the result of a very speciﬁc search on a news articles database, therefore we can expect to the articles to display a great similarity among themselves. Figure 4, shows the distribution of pairwise similarities that were used to construct the empirical inﬂuence network."
1803.06456,code,61,2022-05-13,0,"Here, the baselines are the same as the TE schema. However, we are not able to compare the PAN winner methods with our model as they have not published their code and many implementation details are left unknown to us in their reports to reimplement their methods. We call our second model PRNN in our reports."
1803.06456,data,140,2022-05-13,0,"methods as long as we keep the test and training sets the same as theirs. TE methods: We perform the Transformation Encoder (TE) on a problem P = (DS, DT ) that its documents are represented under one feature set with one feature value assignment to compute the transformation error. We then leverage the error rates taking from (at most) F = 7 feature sets (Section 3.2) of TE to form the ﬁnal TE feature vector (V ). Indeed, Each of the dimensions captures the transformation loss of one feature set. We apply the TE to both training and test data. Two well-known GNB and DT classiﬁers are used for veriﬁcation. We indicate them as TE+GNB and TE+DT respectively in our experiments."
1803.06456,data,176,2022-05-13,0,"where zs ∈ Rd is the reconstructed input and must be transformed into the target (zs ≈ xt). This can be done by setting TE’s objective function as the minimization of the transformation loss. We set the TE transformation loss Er to be the cross-entropy between reconstructed input (zs) and the target input (xt) as: Er(xt, zs) = − (cid:80)d i + (1 − xt Now, we assign our authorship veriﬁcation problem into the proposed Transformation Encoder. It is intuitively expected that TE shows diﬀerent manner when it transforms the source into the target while both having many features in common compared to the case where they have less common features. Here, the goal is to utilize TE for the AV problems that suﬀer from restricted labeled data. So, we put a document expansion method on top of TE as an initial step to overcome the restriction to some extent."
1803.06456,data,236,2022-05-13,0,"In this paper we deﬁne two diﬀerent schemas to study the AV problem. Under the ﬁrst schema we address the following challenges: 1- writing samples of available authors are quite limited during the training step as the length of the given text documents is short (a few hundred to a few thousand words) and size of the training set is so small (from 10 to 200 examples). So, it is quite hard to infer the same or diﬀerent-authorship status of given pairs. 2- The test and train documents are from diﬀerent genera and/or topics which makes the learning and prediction process much harder as the word distribution might diﬀer considerably. 3- No writing samples of the future authors is speciﬁed to us during the training and we may have seen no samples by the future authors at all. Under the second schema the scale of the training data is larger compared to the ﬁrst schema. However, we address the problem of identifying the diﬀerence in documents from identical domains in two ways: 1- authorship diversity in similar contents by utilizing Amazon reviews from 300 distinct authors. 2- Scientiﬁc documents from the same area of research by diﬀerent authors who have almost identical level of expertise in the ﬁeld. It also can be considered as an application of plagiarism detection."
1803.06456,data,4,2022-05-13,0,1 http://pan.webis.de/data.html
1803.06456,data,43,2022-05-13,0,"of the two classes decreases as the transformation encoder updates its weights in each epoch. However, there is a diﬀerence between the transformation loss of the positive and negative data in both diagrams. The loss of negative data is less"
1803.06456,data,44,2022-05-13,0,"Fig. 4: t-SNE plot of two folds of output of the fusion layer for PAN2015 in 5-fold CV. +: positive training data, ×: positive test data, ◦: negative training data, •: negative test data"
1803.06456,"data, dataset",188,2022-05-13,0,"Document Expansion for Small Scale Datasets Neural networks need suﬃcient amounts of data during their learning process to avoid the over-ﬁtting problem to produce the desired output. So, we propose a document expansion method to make use of the existing labeled training data of small scale datasets such as PAN to a great extent. A sliding window with the length of l sentences moves forward through each text document by one sentence per step making a smaller document each time. More speciﬁcally, a document with n sentences will be distributed into n − l + 1 smaller documents. New line characters, as well as empty sentences, are ignored here. So, using this expansion technique each problem P = (S, T ) in the small datasets will be converted to P = (DS, DT ) j}lT where DS = {ds j=1 are the set of all shorter documents after expansion of S and T (source and target). lS and lT denote the size of DS and DT respectively."
1803.06456,"data, dataset",330,2022-05-13,0,"However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy. The best accuracy belongs to the Amazon dataset where we have the largest dataset. It can be inferred that when the scale of the underlying dataset is large enough, the network learns the relation between the two language models of its given inputs well. It should be noted that for the two PAN2013 and PAN2014E even after CV the network cannot converge and the validation loss increases after each epoch. To avoid it we increase the total number of document pairs by splitting each document into two smaller ones with an equal number of sentences and making new pairs. This technique decreases the validation loss during training. However, it still suﬀers from lack of labeled examples and causes weakest results compared to the other larger datasets. To illustrate how PRNN discriminate writing styles we provide the t-SNE plot of the output of the fusion layer in a 5-fold CV classiﬁcation for two folds of PAN2015 (Figure 4). According to Figure 4, both classes have almost similar distribution in the test and training data. But, in some rare parts, the positive and negative points are close. They are probably the portion of the data that mislead the classiﬁer during the training step or will be misclassiﬁed in predictions."
1803.06456,"data, dataset",344,2022-05-13,0,"than the positive’s. In other words, it is easier to transform one document into the other while they are a diﬀerent-authorship pair (negative pair) compared to a same-authorship pair (positive pair). It makes the results of reconstruction loss to be counterintuitive. The reason is that we represent both documents of each problem under vector space model and only based on the vocabulary of the source document. So, the exclusive features of the target document, the features that only belong to the target but no to the source document, will be ﬁltered under this document representation model. Moreover, it is expected that the documents written by diﬀerent authors have fewer features in common and have more exclusive features than the ones written by the same author. This fact makes the target document of diﬀerent-authorship pair sparser than that of the same-authorship pair. And transforming the source document into a sparse document (its vector is sparse) makes less error than to a dense document (its vector is dense). This feature diﬀerentiates the positive and negative data and exists for both training and test sets and makes the transformation loss a distinctive feature for the veriﬁcation. A more direct way to classify the documents (instead of using classiﬁers) is to simply thresholding the reconstruction error. However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy."
1803.06456,"data, dataset",46,2022-05-13,0,"Fig. 3: Transformation loss for 50 epochs: averaged over all problems in the PAN2015 dataset. (A): training data (100 problems), (B): test data (500 problems), Feature set: unigram."
1803.06456,"data, dataset",84,2022-05-13,0,"Neural Network (PRNN) for small and large scale datasets. TE transforms one document of the pair into the other and observes the transformation loss as a distinctive feature for classiﬁcation. PRNN investigates the diﬀerence between the language models of documents. Experiments show that TE can achieve stable results in all four PAN datasets with various size, genre and/or topics. Also, PRNN beats almost all baselines avoiding over-ﬁtting problem by a reasonable amount of training data."
1803.06456,"data, dataset, dataset provided",344,2022-05-13,0,"For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes. The underlying feature set is unigram. The ﬁgures show that in both training and test sets the TE loss"
1803.06456,"data, dataset, dataset provided",349,2022-05-13,0,"TE schema The classiﬁcation results are compared in Table 3. The highest accuracy is indicated in bold and the second highest is underlined. For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes."
1803.06456,dataset,10,2022-05-13,0,Experimental results on evaluation datasets show that both methods achieve
1803.06456,dataset,153,2022-05-13,0,"Let P = (S, T ) denotes a pair of documents, indicating S as the source and T as the target. Here, the task is to investigate whether S and T are written by the same author. We map this problem into a binary classiﬁcation paradigm. Accordingly, if S and T are authored by the same person, P belongs to the positive class. Nevertheless (S and T have diﬀerent authors) P belongs to the negative class. In the ﬁrst step, we explain the Transformation Encoder which is a feature extraction-based method designed for the small-scale datasets with 200 labeled samples at most. However, many AV problems might have a larger scale with much more examples. So, we introduce the Parallel Recurrent Neural Network (PRNN) for large scale datasets in the second step."
1803.06456,dataset,167,2022-05-13,0,"We analyze authorship veriﬁcation on several datasets with binary structure. To our knowledge this amount of analysis has not been done in authorship veriﬁcation on diverse types of datasets. Two models are proposed. First, a Transformation Encoder (TE) to model error feature vectors for classiﬁcation inspired by the idea of autoencoders. TE is compatible with the AV problems with small-scale training sets. Giving a pair of input documents, TE transforms one input into the other. In this process, the transformation loss is observed as a reasonable measure of closeness of the two inputs to be used by a classiﬁer. The second model is a parallel recurrent neural network (PRNN) that is inspired by the popular similarity measures in Statistical Machine Learning (ML). Being based on language models, it is mostly applicable for relatively larger datasets. PRNN compares the proximity of the language model of its two input sequences"
1803.06456,dataset,2,2022-05-13,0,3.1 Dataset
1803.06456,dataset,2,2022-05-13,0,Dataset Train
1803.06456,dataset,222,2022-05-13,1,"Abstract. We propose two models for a special case of authorship veriﬁcation problem. The task is to investigate whether the two documents of a given pair are written by the same author. We consider the authorship veriﬁcation problem for both small and large scale datasets. The underlying small-scale problem has two main challenges: First, the authors of the documents are unknown to us because no previous writing samples are available. Second, the two documents are short (a few hundred to a few thousand words) and may diﬀer considerably in the genre and/or topic. To solve it we propose transformation encoder to transform one document of the pair into the other. This document transformation generates a loss which is used as a recognizable feature to verify if the authors of the pair are identical. For the large scale problem where various authors are engaged and more examples are available with larger length, a parallel recurrent neural network is proposed. It compares the language models of the two documents. We evaluate our methods on various types of datasets including Authorship Identiﬁcation datasets of PAN competition, Amazon reviews and machine learning articles. Experiments show that both methods achieve stable and competitive performance compared to the baselines."
1803.06456,dataset,234,2022-05-13,1,"case no writing samples of a questioned author are speciﬁed and they are unknown to us. No general solution has been oﬀered for the veriﬁcation problem under this assumption till 2014 [7]. Since then, a few works can be found in the literature: Koppel and Winter [7] propose an almost unsupervised method for the blog corpus dataset using “impostors” method. Optimized Classiﬁcation Trees, the winner method of PAN2014 Essays dataset, optimizes a decision tree based on various types of features and diﬀerent comparison methods including cosine similarity, correlation coeﬃcient and euclidean distance [8]. Multi-headed RNN is a character-level RNN and contains a common recurrent state among all authors with an independent softmax output per author [9]. Fuzzy C-Means clustering, the winner of the PAN2014 competition for novels dataset, adopts C-Means clustering and lexical features for the task [10]. Recently, an approach based on the compression models has been evaluated on PAN datasets [11]. Their method achieves promising results for the two years of PAN competitions but not for the other two datasets. Our methods is similar to these methods and considers the problems with the binary structure but we examine them on all PAN small-scale datasets as well as two large scale datasets."
1803.06456,dataset,260,2022-05-13,0,"for PAN datasets. All other parameters are selected based on pilot experiments. We report accuracy, the Area Under Receiver Operating Characteristic (ROC) curve [4] (AUC) and Score=AUC× Acc in TE experiments. The higher AUC and Score indicate more eﬀective classiﬁcation. PRNN schema The plain text of each document is used as the input of PRNN. The features sets for the baselines are the same as the TE baselines. However, we did not use the original training and test sets of the PAN datasets as the size of the training set is too small to be used for PRNN. To avoid overﬁtting problem we perform 5-fold Cross Validation (CV) for the PAN2015, Amazon and MPLA* where we have suﬃcient amount of examples in training folds. And for the PAN2013, PAN2014E and PAN2014N datasets that are relatively smaller we perform 10-fold CV to increase the size of the training folds. This setting is applied for PRNN as well as the baselines. We use Theano to implement PRNN. All classiﬁer’s parameters are the same as the TE schema. The back-propagation is done using stochastic gradient descent with learning rate=0.001, batch size=1, and dropout rate=0.2. We use the Glove pre-trained vectors5 as an initial value for the embedding vectors when there is a match. Otherwise, a random vector from a continuous uniform distribution over [0, 1) is used."
1803.06456,dataset,3,2022-05-13,0,Dataset Positive Negative
1803.06456,dataset,30,2022-05-13,0,Table 4: Classiﬁcation accuracy for PRNN schema using 5 and 10-fold CV across different datasets. The input for the baselines are empowered by the proposed similarity vector.
1803.06456,dataset,334,2022-05-13,0,"Here, the comparison methods are presented in three categories: baseline, PAN winners and our TE method. The details are provided as follows. Baseline: We connect several Machine Learning reliable classiﬁers widely used in the area with the seven similarity measures to set strong baselines (Table2). Since each example in our underlying dataset structure comprises two documents, we need to adapt it to the structure of an ordinary classiﬁer input by converting them to one single entity. A simple direct way is to concatenate their feature vectors. However, our experiments show it provides weak results mostly equal to the random label assignment. So, we deﬁne the summary vector as a single unit representative of each example/problem P = (DS, DT ) by utilizing several similarity measures. The summary vector comprises a class of several metrics each measures one aspect of the closeness of the two documents (DS and DT ) of the pair for all underlying feature sets. For any two feature vector documents x, y their summary vector is sum(x, y) = [simj i (x, y)1≤i≤M,1≤j≤F computes the ith similarity metric of M metrics in Table 2 under jth of F = 7 feature sets (Section 3.2) between x, y. Then, we use a classiﬁer including SVM, Gaussian Naive Bayes (GNB), K-Nearest Neighbor (KNN), Logistic Regression (LR), Decision Tree (DT) and Multi-Layer Perception (MLP) to predict the class label. PAN winners: We compare our method with the top methods of PAN AV competition between 2013 and 2015. The results of each method for one year of the competition are available and we report them here. So, our comparisons are not impacted by diﬀerent parameter setting and implementation details of these"
1803.06456,dataset,348,2022-05-13,1,"PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random. On the other hand, the negative candidate set is made of all unique and possible combinations of review pairs having diﬀerent authors. The negative class having equal size with the positive class is created by random selection from the negative candidate set."
1803.06456,dataset,348,2022-05-13,1,"for PAN2013), and literally the second document includes one piece of writing. Two documents of a pair might be from signiﬁcantly various genres and topics. The length of a document changes from a few hundred to a few thousand words. PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random."
1803.06456,dataset,5,2022-05-13,0,Table 1: Datasets information
1803.06456,dataset,86,2022-05-13,1,TE schema To evaluate the Transformation Encoder we use all available authorship identiﬁcation datasets released by PAN 1 (Table 1). Each PAN dataset consists of a training and test corpus and each corpus has a various number of distinct problems. One problem is a pair of two documents: the ﬁrst document of a problem composed of up to ﬁve writings (even as few as one) by a single person (implicitly disjoint For PAN2014 and PAN2015 and explicitly disjoint
1803.06456,dataset,88,2022-05-13,0,"PRNN is designed to solve the AV problem for relatively large scale datasets. The structure of the problem is the same as TE’s. We model a pair of documents using a simple parallel recurrent architecture. The overall model is shown in Figure 2. In general, PRNN consists of three components: two parallel columns of identical layers, one shared fusion layer and a SoftMax layer as the output. We proceed to describe the network in the following paragraphs."
1803.06456,dataset,93,2022-05-13,0,"to investigate their authorship. We also propose the summary vector to adapt our problem to a common binary classiﬁcation style to create strong baselines as there are limited studies in authorship veriﬁcation according to the literature. Applying this adaptation we are able to employ the recognized classiﬁers as well as similarity measures that are widely used in ML to build our baselines. Besides, the two pre-existing datasets, Amazon reviews and MPLA-400, are mapped to the binary structure to be used for our large scale AV problem."
1803.06456,"dataset, github",17,2022-05-13,0,2 https://github.com/dainis-boumber/MLP-400-datasets 3 we use scikit-learn software for all linguistic features 4 http://deeplearning.net/software/theano/
1803.06456,python,106,2022-05-13,0,"3. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. Journal of Machine Learning Research 12 (2011) 2825–2830 4. Egan, J.P.: Signal detection theory and {ROC} analysis. Academic Press (1975) 5. Japkowicz, N., Myers, C., Gluck, M., et al.: A novelty detection approach to"
1807.01857,code,147,2022-05-13,0,"Fig. 1 shows the schematic diagram of our proposed approach for IDE-based web search. Once the developer selects an exception from the Error Log or Console View of Eclipse IDE, our approach collects necessary information about it such as error message, stack trace and source code context. Then, it collects the results from three reliable search engines (e.g., Google, Bing and Yahoo) and one programming Q & A site (e.g., StackOverﬂow) through API endpoints against the error message and develops the corpus. The proposed approach then considers the context of the occurred error or exception, popularity and search engine recommendation of the collected results and calculates the proposed metrics to determine their acceptability and relevance with the target exception. Once the ﬁnal scores are calculated from those metrics, the results"
1807.01857,code,148,2022-05-13,0,"A sites, forums or discussion boards in their program directly or with minor modiﬁcations. Therefore, a result link containing source code snippet similar to the surrounding code block of the selected error or exception location is likely to discuss relevant issues that the developer needs. We consider three lines before and after the affected line in the source ﬁle as the source code context of the error or exception and extract the code snippets from result links though HTML scrapping. Then, we apply SimHash Algorithm on both code contexts and generate their SimHash values. We use equation (1) to determine Source Code Context Matching Score for each result link. The score values from zero to one and it indicates the relevance of the result link with the target error in terms of the context of source code."
1807.01857,code,252,2022-05-13,0,"To summarize, we propose a novel IDE-based web search solution that (1) exploits the search and ranking capabilities of three reliable search engines and a programming Q & A site through their API endpoints, (2) considers not only the content of the search (i.e., query keywords) but also the problem context such as stack trace and source code context, link popularity and link recommendation from the search engines, and (3) provides search result within the context of IDE with web browsing capabilities. We conduct an experiment with 25 runtime errors and exceptions related to Eclipse plugin development. Our approach recommended solutions with 96% accuracy which necessarily outperforms the traditional keyword-based search. In order to validate the results, we conduct a user study involving ﬁve prospective participants which gave a response agreement of 64.28%. Given that the relevance checking of a solution against the selected error is completely a subjective process, the preliminary results are promising. However, the proposed approach needs to be further validated with more errors and exceptions followed by an extensive user study to establish itself as a complete IDE-based web search solution. We also have plans to enable multiprocessing for the application and host it as a web service API so that others can readily use it with real time experience and also can use the API in their own IDEs rather than Eclipse."
1807.01857,code,47,2022-05-13,0,"[2] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer. Two studies of opportunistic programming: interleaving web foraging, learning, and writing code. In Proc. SIGCHI, pages 1589–1598, 2009."
1807.01857,code,66,2022-05-13,0,"Title Matching Score measures the content similarity between search query and result title. Stack Trace Matching Score and Source Code Context Matching Score determine the relevance of the result link based on its contextual similarity with that of the selected error or exception; therefore, they constitute the Context Relevance Score, Scxt. We get this score using equation (5)."
1807.01857,code,81,2022-05-13,0,"[5] M. Goldman and R. C. Miller. Codetrail: Connecting source code and web resources. J. Vis. Lang. Comput., 20(4):223–235, August 2009. [6] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns. What makes a good code example?: A study of programming Q & A in Stack Overﬂow. In Proc. ICSM, pages 25–34, 2012."
1807.01857,code,86,2022-05-13,0,"4) Source Code Context Matching Score (Scc): Sometimes, stack trace may not be enough for problem ﬁxation and developers post related source code in forums and discussion boards for clariﬁcation. We are interested to check if the source code contexts of the discussed errors or exceptions in the result links are similar to that of the selected exception from IDE. The code contextual similarity is possible; because, the developers often reuse code snippets from programming Q &"
1807.01857,code,98,2022-05-13,0,"In our experiment, we select 25 runtime errors and exceptions related to Eclipse plug-in development, and collect associated information such as error or exception messages, stack traces and source code context. We then use those information (e.g., error content and context) to search for solution using our approach. We also perform extensive web search manually with different available search engines and ﬁnd out the solutions for all errors and exceptions. We should note that we choose the most appropriate solution as the accepted one for each exception or error"
1807.01857,"code, package",187,2022-05-13,0,"In this paper, we propose an Eclipse IDE-based search solution called SurfClipse to the encountered errors or exceptions which addresses the concerns identiﬁed in case of existing approaches. We package the solution as an Eclipse plug-in which (1) exploits the search and ranking algorithms of three reliable web search engines (e.g., Google, Bing and Yahoo) and a programming Q & A site (e.g., StackOverﬂow) through their API endpoints, (2) provides a content (e.g., error message), context (e.g., stack trace and surrounding source code of the subject error), popularity and search engine recommendation (of result links) based ﬁltration and ranking on the extracted results of step one, (3) facilitates the most recent solutions, accesses the complete and extensible solution set and pulls solutions from numerous forums, discussion boards, blogs, programming Q & A sites and so on, and (4) provides a real web surﬁng experiences within the IDE context using Java based browser."
1807.01857,data,102,2022-05-13,0,"discussion boards and Q & A sites with the help of search enginies, but also ensures the access to the most recent content of StackOverﬂow through API access. However, the existing approaches by Cordeiro et al. [4] and Ponzanelli et al. [7] provide results from a single and ﬁxed sized data dump of StackOverﬂow and therefore, the results do not contain the most recent posts (i.e., discussing the most recent errors or exceptions) from StackOverﬂow as well as the promising solutions from other programming Q & A sites."
1807.01857,data,103,2022-05-13,0,"8) Search Trafﬁc Rank Score (Sstr): The amount of search trafﬁc to a site can be considered as an important indicator of its popularity. In this research, we consider the relative popularity of the result links found in the corpus. We use the statistical data from two popular site trafﬁc control companiesAlexa and Compete through their provided APIs and get the average ranking for each result link. Then, based on their ranks, we provide a normalized Search Trafﬁc Rank Score between zero and one considering minimum and maximum search trafﬁc ranks found."
1807.01857,data,216,2022-05-13,0,"Abstract—Traditional web search forces the developers to leave their working environments and look for solutions in the web browsers. It often does not consider the context of their programming problems. The context-switching between the web browser and the working environment is time-consuming and distracting, and the keyword-based traditional search often does not help much in problem solving. In this paper, we propose an Eclipse IDE-based web search solution that collects the data from three web search APIs– Google, Yahoo, Bing and a programming Q & A site– StackOverﬂow. It then provides search results within IDE taking not only the content of the selected error into account but also the problem context, popularity and search engine recommendation of the result links. Experiments with 25 runtime errors and exceptions show that the proposed approach outperforms the keyword-based search approaches with a recommendation accuracy of 96%. We also validate the results with a user study involving ﬁve prospective participants where we get a result agreement of 64.28%. While the preliminary results are promising, the approach needs to be further validated with more errors and exceptions followed by a user study with more participants to establish itself as a complete IDE-based web search solution."
1807.01857,data,282,2022-05-13,0,"Traditional web search forces the developers to leave the working environment (i.e., IDE) and look for the solution in the web browsers. In contrast, if the developer chooses SurfClipse, it allows to check the search results within the context of IDE (e.g., Fig. 1-(b)). Once she selects an error message using context menu option (e.g., Fig. 1-(a)), the plugin pulls results from three reliable search engines and one programming Q & A site against that error message. Then, it calculates the proposed metrics for each result related to the error content, error context, popularity and search engine recommendation to determine its relevance with the occurred error or exception, and then sorts and displays the results. Moreover, the plug-in allows the developer to browse the solution on a Java-based web browser (e.g., Fig. 1-(c)) without leaving the context of the IDE which makes it time-efﬁcient and ﬂexible to use. The plug-in by Cordeiro et al. [4] also shows the results within the context of the IDE; however, (1) the result set is limited (i.e., only from StackOverﬂow and does not consider the whole web), (2) cannot address newly introduced issues (i.e., ﬁxed corpus and subjected to the availability of StackOverﬂow data dump), (3) only considers stack trace information as problem context, and (4) the developer cannot enjoy the web browsing experience."
1807.01857,data,318,2022-05-13,0,"1) Search Engine Weight Based Score (Ssew): According to Alexa1, one of the widely recognized web trafﬁc data providers, Google ranks second, Yahoo ranks fourth and Bing ranks sixteenth among all websites in the web this year. While these ranks indicate their popularity (e.g., site trafﬁc) and reliability (i.e., users’ trust) as information service providers, it is reasonable to think that search results from different search engines of different ranks have different levels of acceptance. We conduct an experiment with 75 programming task and exception related queries2 against those search engines and a programming Q & A site (e.g., StackOverﬂow) to determine the relative weights or acceptance. We collect the top 15 search results for each query from each search tool and get their Alexa ranks. Then, we consider the Alexa ranks of all result links provided by each search tool and calculate the average rank for a result link provided by them. The average rank for each search tool is then normalized and inversed which provides a value between 0 and 1. We get a normalized weight of 0.41 for Google, 0.30 for Bing, 0.29 for Yahoo and 1.00 for StackOverﬂow. The idea is that if a result link against a single query is found in all three search engines, it gets the search engine scores (i.e., conﬁdence) from all three of them which sum to 1.00. StackOverﬂow has drawn the attention of a vast community (1.7 million3) of programmers and software professionals, and it also has a far better average Alexa rank than that of the search engines; therefore, the results returned from StackOverﬂow are provided a search engine score (i.e., conﬁdence) of 1.00."
1807.01857,"data, code",117,2022-05-13,0,"Existing studies related to our research focus on integrating commercial-off-the-shelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying within IDE environment [4, 7], embedding web browser inside the IDE [3] for code example recommendation and so on. In this paper, we propose a novel approach that exploits result data from the state of art web search APIs and provides ﬁltered and ranked search results taking problem content, context, result link’s popularity and search engine recommendation about the result links into consideration. Our proposed approach not only collects solution posts from a large set of forums,"
1807.01857,"data, code, dataset provided",293,2022-05-13,0,"for the solution in the web browsers. The context-switching between IDE and the web browser is distracting and timeconsuming. Moreover, checking relevance from hundreds of search results is a cognitive burden on the novice developers. Existing studies focus on integrating commercial-off-theshelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying them within the IDE environment [4], embedding web browser inside the IDE [3] and so on. Cordeiro et al. [4] propose an IDE-based recommendation system for runtime exceptions. They extract the question and answer posts from StackOverﬂow data dump and suggest posts relevant to the occurred exceptions considering the context from the stack trace information generated by the IDE. They also suggest a nice solution to the context-switching issue through visualization of the solution within the IDE. However, the proposed approach suffers from several limitations. First, they consider only one source (e.g., StackOverﬂow Q & A site) rather than the whole web for information and thus, their search scope is limited. Second, the developed corpus cannot be easily updated and is subjected to the availability of the data dump. For example, they use the StackOverﬂow data dump of September 2011, that means it cannot provide help or suggestions to the recently introduced software bugs or errors after September 2011. Third, the visualization of the solutions is not efﬁcient as it uses plain text to show the post contents such as source code, stack trace and discussion. Thus the developers do not really experience the style and presentation of a web page."
1807.04488,code,115,2022-04-21,0,"Baseline Query Selection: We select the title of a bug report as the baseline query for our experiments, as was also selected by earlier studies [21, 28, 49]. However, we discard such queries that (i.e., in verbatim titles) already return their ﬁrst correct results within the Top-10 positions, they possibly do not need query reformulation [21]. Finally, we ended up with a collection of 1,675 baseline queries. We perform the same preprocessing steps as were done on the source documents (Section II-C), on the queries before using them for code search in our experiments."
1807.04488,code,117,2022-04-21,0,"Answering RQ3–Do Document Structures Matter? While most of the earlier reformulation techniques miss or ignore the structural aspect of a source document, we consider such aspect as an important paradigm of our technique. We consider a source document as a collection of structured entities (e.g., signatures, methods, ﬁelds) [38] rather than a regular text document. Thus, we make use of method signatures and ﬁeld signatures rather than the whole source code for query reformulation given that they are likely to contain more salient terms and less noise [23]. Fig. 4 demonstrates how incorpora Method signature Field signature Both signatures Both signatures"
1807.04488,code,13,2022-05-13,0,"Code Elements in Informal Documentation. ICSE, pages 832–841, 2013."
1807.04488,code,132,2022-05-13,0,"earlier [32], we apply a heuristic threshold of 0.0001 for the convergence checking. The algorithm captures importance of a source term not only by estimating its local impact but also by considering its global inﬂuence over other terms. For example, the term, “Classpath”, Fig. 1, occurs in multiple structured tokens (Listing 1), complements the semantics of ﬁve other terms, and thus is highly important within the term graph (i.e., Fig. 1). Once the iterative computation is over, each of the terms from the graph is found with a numeric score. We consider these scores as the relative weight or importance of the corresponding terms from the source code."
1807.04488,code,146,2022-04-21,0,"To summarize, we propose a novel technique–ACER–for improved query reformulation for concept location. It takes an initial query as input, identiﬁes appropriate search terms from the source code using a novel term weight, and then suggests the best reformulation to the initial query using document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight systems report that our technique can improve 71% of the baseline queries and preserve 26% of them, which are highly promising. Comparison with ﬁve closely related approaches including the state-of-the-art not only validates our empirical ﬁndings but also demonstrates the high potential of our technique. In future, we plan to apply our term weighting method, CodeRank, to other SE text retrieval tasks involving source code such as bug localization and traceability recovery."
1807.04488,code,146,2022-04-21,0,"source subject systems show that our technique can improve 71% (and preserve 26%) of the baseline queries which are highly promising according to relevant literature [13, 21, 34]. Our suggested queries return correct results for 64% of the queries in the Top-100 results. Our ﬁndings report that CodeRank is a more effective term weighting method than the traditional methods (e.g., TF, TF-IDF) for search query reformulation in the context of source code. Our ﬁndings also suggest is an important paradigm for both term weighting and query reformulation. Comparison with ﬁve closely related existing approaches [13, 21, 23, 43, 49] not only validates our empirical ﬁndings but also demonstrates the superiority of our technique. Thus, the paper makes the following contributions:"
1807.04488,code,149,2022-05-13,0,"In this paper, we propose a novel technique–ACER–for automatic query reformulation for concept location in the context of software change tasks. We (1) ﬁrst introduce a novel graph-based term weight –CodeRank– for identifying important terms from the source code, and then (2) apply that term weight and source document structures (e.g., method signatures) to our technique for automatic query reformulation. CodeRank identiﬁes important terms not only by analyzing salient structured entities (e.g., camel case tokens), but also by exploiting the co-occurrences among the terms across various entities. Our technique–ACER–accepts a natural language query as input, develops multiple candidate queries from two different important contexts, (1) method signatures and (2) ﬁeld signatures of the source documents independently using CodeRank, and then suggests the best reformulation ( based"
1807.04488,code,163,2022-05-13,0,"[14] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker. Mining Source Code to Automatically Split Identiﬁers for Software Analysis. In Proc. MSR, pages 71–80, 2009. [15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The Vocabulary Problem in Human-system Communication. Commun. ACM, 30(11):964–971, 1987. [16] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the Use of Relevance Feedback in IR-based Concept Location. In Proc. ICSM, pages 351–360, 2009. [17] S. Haiduc and A. Marcus. On the Use of Domain Terms in Source Code. In Proc. ICPC, pages 113–122, 2008. [18] S. Haiduc and A. Marcus. On the Effect of the Query In Proc. ICPC, pages"
1807.04488,code,167,2022-05-13,0,"CodeRank: PageRank [10] is one of the most popular algorithms for web link analysis which was later adapted by Mihalcea and Tarau [32] for text documents as TextRank. In this research, we adapt our term weighting method from TextRank [9, 32, 41] for source code, and we call it CodeRank. To date, only traditional term weights (e.g., TF, TFIDF [21, 43, 49]) are applied to source code which were originally proposed for regular texts [26] and are mostly based on isolated frequencies. On the contrary, CodeRank not only analyzes the connectivity (i.e., incoming links and outgoing links) of each source term, but also the relative weight of the connected terms from the graph recursively, and calculates the term weight, S(Vi), as follows (Step 6, Fig. 2):"
1807.04488,code,168,2022-05-13,0,"There exist a number of studies in the literature that reformulate a given query for concept location in the context of software change tasks. Existing studies apply relevance feedback from developers [16], pseudo-relevance feedback from IR tools [21], partial phrasal matching [23, 44], and machine learning [21, 34] to query reformulation. They also make use of context of query terms from source code [25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Hill et al. [23] consider the presence of query terms in the method or ﬁeld signatures as an indicator of their relevance, and suggest natural language phrases from them as reformulated queries. Sisman and Kak [49] choose such terms for query reformulation that frequently co-occur with query terms within"
1807.04488,code,17,2022-05-13,0,Fig. 1. An example term graph generated by CodeRank for source code of Listing 1
1807.04488,code,17,2022-05-13,0,Listing 1. Source code used for automatic query reformulation (abridged from [3])
1807.04488,code,179,2022-05-13,0,"In order to suggest meaningful reformulations to an initial query, feedback on the query is required. Gay et al. [16] ﬁrst reformulate queries based on explicit feedback from the developers. Although such feedback could be useful, gathering them is often time-consuming and sometimes infeasible. Hence, a number of recent studies [13, 21, 40, 41] apply pseudorelevance feedback as a feasible alternative. The top ranked results returned by the code search tool for an initial query are considered as the pseudo-relevance feedback for the query. We ﬁrst reﬁne an initial query by removing the punctuation marks, numbers, special symbols and stop words (Step 1, Fig. 2). Then we collect the Top-K (i.e., K = 10, best performing heuristic according to our experiments) search results returned by the query, and use them as the source for our candidate terms for query reformulation (Steps 2, 3, Fig. 2)."
1807.04488,code,185,2022-04-21,0,"Abstract—During software maintenance, developers usually deal with a signiﬁcant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique –ACER– that takes an initial query, identiﬁes appropriate search terms from the source code using a novel term weight –CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with ﬁve closely related existing techniques in query reformulation not only validates our empirical ﬁndings but also demonstrates the superiority of our technique."
1807.04488,code,190,2022-05-13,0,"Stop word and Keyword Removal: Since our structured tokens comprise of natural language terms, we discard stop words from them as a common practice (Step 4, Fig. 2). We use a standard list [6] hosted by Google for stop word removal. Programming keywords can often be considered as the equivalence of stop words in the source code which are also discarded from our analysis. Since we deal with Java source code, the keywords of Java are considered for this step. As suggested by earlier study [21], we also discard insigniﬁcant source terms (i.e., having word length< 3) from our analysis. Stemming: It extracts the root (e.g., “send”) out of a word (e.g., “sending”). Although existing studies suggest contradictory [28, 45] or conﬂicting [24] evidences for stemming with the source code, we investigate the impact of stemming with RQ4 where Snowball stemmer [24, 37] is used for stemming."
1807.04488,code,219,2022-05-13,0,"Thus, to answer RQ1, the reformulation of ACER improves the baseline queries signiﬁcantly both in terms of query effectiveness and retrieval performance. ACER improves 71% of the baseline queries with 64% Top-100 retrieval accuracy. Answering RQ2–CodeRank vs. Traditional Term Weighting Methods: Table VII shows the comparative analysis between CodeRank and two traditional term weights– TF and TF-IDF– which are widely used in the text retrieval contexts [13, 28, 43]. While TF estimates the importance of a term based on its occurrences within a document, TF-IDF additionally captures the global occurrences of the term across all the documents of the corpus [26]. On the contrary, CodeRank employs a graph-based scoring mechanism that determines the importance of a term based on its co-occurrences with other important terms within a certain context. From Table VII, we see that CodeRank performs signiﬁcantly better than both TF (i.e., paired t-test, p-value=0.005<0.05) and TF-IDF (i.e., p-value<0.001) in identifying important search terms from source code, especially from the method signatures. Considering the whole source code rather than signatures improves the performance of both TF (i.e., 56% query improvement) and"
1807.04488,code,228,2022-05-13,0,"Fig. 3 shows how CodeRank and traditional term weights perform in reformulating the baseline queries with their (a) Top-10 and (b) Top-30 terms. We see that TF reaches its peak performance pretty quickly (i.e., K = 3), and then shows a stationary or irregular behaviour. That means, TF identiﬁes frequent terms for query reformulation, and few of them (e.g., Top-3) could be highly effective. On the contrary, our method– CodeRank– demonstrates a gradual improvement in the performance up to Top-12 terms (i.e., K=12, Fig. 3-(b)), and crosses the performance peak of TF with a large margin (i.e., paired t-test, p-value=0.004<0.05, Cohen’s D=3.77>1.00 (large)), for K=10 to K=15). CodeRank emphasizes on the votes from other important terms (i.e., by leveraging co-occurrences) for determining weight of a term, and as demonstrated in Fig. 3, this weight is found to be more reliable than TF. TF-IDF is found relatively less effective according to our investigation. Thus, to answer RQ2, CodeRank performs signiﬁcantly better than traditional methods in identifying effective terms for query reformulation from the source code."
1807.04488,code,23,2022-04-21,0,• RQ3: Does employment of document structure improve ACER’s suggestion on good quality search terms from the source code?
1807.04488,code,231,2022-05-13,0,"Candidate Token Mining: Developers often express their intent behind the code and encode domain related concepts in the identiﬁer names and comments [17]. However, code comments are often inadequate or outdated [51]. All identiﬁer types also do not have the same level of importance. For example, while the signature of a method encodes the high level its body focuses on granular level implementation details and thus possibly contains more noisy terms [23]. In fact, Hill et al. [23] ﬁrst analyze method signatures and ﬁeld signatures to suggest natural language phrases as queries for code search. In the same vein, we thus also consider method signatures (msig) and ﬁeld signatures (f sig) as the source for our candidate reformulation terms. We extract structured identiﬁer names from these signatures using appropriate regular expressions [42] (Step 4, Fig. 2). Since different contexts of a source document might convey different types or levels of semantics (i.e., developers’ intent), we develop a separate candidate token set (CTsig) for each of the two signature types (sig ∈ {msig, f sig}) from the feedback documents (∀d ∈ DRF ) as follows: CTsig ="
1807.04488,code,248,2022-05-13,0,"Candidate Reformulation Selection: Algorithms 1 and 2 show the pseudo-code of our query reformulation technique– ACER–for concept location. We ﬁrst collect pseudo-relevance feedback for the initially provided query (Q) where Top-K source documents are returned (Lines 3–5, Algorithm 1). Then we collect method signatures and ﬁeld signatures from each of the documents (∀d ∈ DRF ), and extract structured tokens from them. We prepare three token sets–CTmsig, CTf sig and CTcomb from these signatures (Lines 6–12, Algorithm 1, Step 4, Fig. 2) where CTcomb combines tokens from both signatures. Then we perform limited natural language preprocessing on each token set where Samurai algorithm [14] is used for token splitting. We develop separate term graph for each of these token sets where individual terms are represented as vertices, and term co-occurrences are encoded as connecting edges (Lines 3–7, Algorithm 2, Step 5, Fig. 2). We apply CodeRank term weighting to each of the graphs which provides a ranked list of terms based on their relative importance. Then we select Top-K (e.g., K = 10) important terms from each of the three graphs, and prepare three reformulation candidates (Lines 8– 12, Algorithm 2, Steps 6, 7, 8, Fig. 2). Algorithm 1 ACER: Proposed Query Reformulation"
1807.04488,code,26,2022-05-13,0,[3] Example code snippet. URL https://goo.gl/WSZHiC. [4] Samurai preﬁx and sufﬁx list. URL https://hiper.cis.udel.
1807.04488,code,263,2022-05-13,0,"Once candidate tokens are extracted from method signatures and ﬁeld signatures, and are splitted into candidate terms, we develop source term graphs (e.g., Fig. 1) from them (Step 5, Fig. 2). Developers often encode their intent behind the code and domain vocabulary into the carefully crafted identiﬁer names where multiple terms are concatenated. For example, the method name–getChatRoomBots–looks like a natural language phrase–“get chat room bots”–when splitted properly. Please note that each of these three terms–“chat”, “room” and “bots”– co-occur with each other to convey an important concept– a robotic technology, and thus, they are semantically connected. On the other hand, the remaining term–“get”– cooccurs with them due to a temporal relationship (i.e., develops a verbal phrase). Similar phrasal representations (reﬁned with lexical matching) were directly returned by Hill et al. for query reformulation. However, their approach could be limited due to the added constraint (e.g., warrants query terms in signatures). We thus perform further analysis on such phrases, and exploit the co-occurrences among the terms for our graph based term weighting. In particular, we encode the term co-occurrences into connecting edges (E) in the term graph (G(V, E)) where the individual terms (Vi) are denoted as vertices (V )."
1807.04488,code,281,2022-05-13,0,"Studies show that about 80% of the total efforts is spent in software maintenance [36] where developers deal with a signiﬁcant number of software issues [35, 45, 52]. Software issue reports (a.k.a., change requests) discuss both unexpected (or erroneous features such as bugs) and expected but nonexistent features (e.g., new functionality). For both bug resolution and new feature implementation, a developer is required to map the concepts discussed in the issue report to appropriate source code within the project which is widely known as concept location [29, 31, 40]. Developers generally choose one or more important keywords from the report texts, and then use a search method (e.g., regular expression) to locate the source code entities (e.g., classes, methods) that need to be changed. Unfortunately, as the existing studies [28, 30] report, developers regardless of their experience perform poorly in choosing appropriate search terms for software change tasks. According to Kevic and Fritz [28], only 12.20% of the search terms chosen by the developers were able to locate relevant source code entities for the change tasks. Furnas et al. [15] also suggest that there is a little chance (i.e., 10%–15%) that developers guess the exact words used in the source code. One way to assist the developers in this regard is to automatically suggest helpful reformulations (e.g., complementary keywords) to their initially chosen queries."
1807.04488,code,29,2022-05-13,0,"• A novel term weighting method –CodeRank– for source code that identiﬁes the most important terms from a given code entity (e.g., class, method)."
1807.04488,code,29,2022-05-13,0,"• RQ2: Does CodeRank perform better than traditional term weighting methods (e.g., TF, TF-IDF) in identifying effective search terms from the source code?"
1807.04488,code,3,2022-04-21,0,Preprocessing Code search
1807.04488,code,31,2022-05-13,0,"[49] B. Sisman and A. C. Kak. Assisting Code Search with Automatic Query Reformulation for Bug Localization. In Proc. MSR, pages 309–318, 2013."
1807.04488,code,311,2022-05-13,0,"Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]. Thus, they might also fail to identify the appropriate terms from the structured source documents for query reformulation."
1807.04488,code,320,2022-05-13,0,"We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig. 5 shows how (a) Top-10 and (b) Top-30 reformulation terms improve the baseline queries. We see that"
1807.04488,code,324,2022-05-13,0,"Table I shows an example change request [2] submitted for eclipse.jdt.debug system, and it refers to “debugger source lookup” issue of Eclipse IDE. Let us assume that the developer chooses important keywords from the request title, and formulates a generic initial query–“debugger source lookup.” Unfortunately, the query does not perform well, and the 79th position of the returns the ﬁrst correct result at result list. Further extension–“debugger source lookup work variables”–also does not help, and returns the result at the 77th position. The existing technique – RSV [13]– extends the query as follows–“debugger source lookup work variables launch conﬁguration jdt java debug”–where the new terms are collected from the project source using TF-IDF based term weight. This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig."
1807.04488,code,328,2022-05-13,0,"This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig. 1) based on term co-occurrences from each signature type, apply CodeRank term weighting, and extract multiple candidate reformulations with the highly weighted terms (e.g., orange coloured, Fig. 1). Then we analyze the quality of the candidates using their quality measures [19], apply machine learning, and suggest the best reformulation to the initial query. Thus, our technique (1) ﬁrst captures salient terms from the source documents by analyzing their structural aspects (i.e., unlike bag of words approaches [46]) and an appropriate term weight–CodeRank, and (2) then suggests the best query reformulation using document structures (i.e., multiple candidates derived from various signatures), query quality analysis and machine learning [19]. Experiments using 1,675 baseline queries from eight open"
1807.04488,code,33,2022-05-13,0,"[38] M. M. Rahman and C. K. Roy. On the Use of Context in Recommending Exception Handling Code Examples. In Proc. SCAM, pages 285–294, 2014."
1807.04488,code,335,2022-05-13,0,"* = Statistically signiﬁcant difference between two measures from the same signature, MRD = Mean Rank Difference between ACER and baseline queries tion of document structures into a technique could be useful for query reformulations. We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig."
1807.04488,code,342,2022-05-13,0,"reformulation tasks. They also make use of context of query terms from source code [23, 25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]."
1807.04488,code,35,2022-04-21,0,"Fig. 2 shows the schematic diagram of our proposed technique–ACER–for automatic query reformulation. We use a novel graph-based metric of term importance–CodeRank– for source code, and apply source document structures, query"
1807.04488,code,36,2022-05-13,0,"[23] E. Hill, L. Pollock, and K. Vijay-Shanker. Automatically Capturing Source Code Context of NL-queries for Software Maintenance and Reuse. In Proc. ICSE, pages 232–242, 2009."
1807.04488,code,37,2022-04-21,0,"[25] M. J. Howard, S. Gupta, L. Pollock, and K. VijayShanker. Automatically Mining Software-based, Semantically-Similar Words from Comment-Code Mappings. In Proc. MSR, pages 377–386, 2013."
1807.04488,code,38,2022-05-13,0,"[31] A. Marcus, A. Sergeyev, V. Rajlich, and J.I. Maletic. An Information Retrieval Approach to Concept Location in Source Code. In Proc. WCRE, pages 214–223, 2004."
1807.04488,code,4,2022-05-13,0,C. Source Code Preprocessing
1807.04488,code,4,2022-05-13,0,URL https://code.google.com/p/
1807.04488,code,49,2022-05-13,0,"[22] S. Haiduc, G. De Rosa, G. Bavota, R. Oliveto, A. De Lucia, and A. Marcus. Query Quality Prediction and the Refoqus Reformulation for Source Code Search: Tool. In Proc. ICSE, pages 1307–1310, 2013."
1807.04488,code,61,2022-04-21,0,"[43] J.J. Rocchio. The SMART Retrieval System—Experiments in Automatic Document Processing. Prentice-Hall, Inc. [44] M. Roldan-Vega, G. Mallet, E. Hill, and J. A. Fails. CONQUER: A Tool for NL-based Query Reﬁnement and In Proc. ICSM, Contextualizing Code Search Results. pages 512–515, 2013."
1807.04488,code,7,2022-05-13,0,B. Corpus Indexing & Source Code Search
1807.04488,code,7,2022-05-13,0,Tasks to Source Code. 2014.
1807.04488,code,7,2022-04-21,0,that structure of a source code document
1807.04488,code,77,2022-05-13,0,"[51] C. Vassallo, S. Panichella, M. Di Penta, and G. Canfora. CODES: Mining Source Code Descriptions from Developers Discussions. In Proc. ICPC, pages 106–109, 2014. [52] I. Vessey. Expertise in Debugging Computer Programs: An Analysis of the Content of Verbal Protocols. TSMC, 16(5):621–637, 1986. [53] J. Yang and L. Tan."
1807.04488,code,87,2022-05-13,0,"Here sig(d) extracts all tokens from method signatures or ﬁeld signatures, and structured(t) determines whether the token t ∈ Tsig is structured or not. Although we deal with Java source code in this research where the developers generally use camel case tokens (e.g., MessageType) or occasionally might use same case tokens (e.g., DECIMALTYPE), our approach can be easily replicated for snake case tokens (e.g., reverse traversal) as well."
1807.04488,data,130,2022-05-13,0,"Threats to internal validity relate to experimental errors and biases [55]. Although CodeRank and document structures play a major role, the data resampling step (Section II-F, Step 9, Fig. 2) has a signiﬁcant role behind the high performance of our technique. Unfortunately, to the best of our knowledge, Refoqus [21] does not have such a step. Thus, the performance comparison might look like a bit unfair. Besides, models based on data resampling are sometimes criticized for intrinsic biases [5]. However, we apply data resampling to Refoqus as well (i.e., Refoqussampled), and demonstrate that our technique still performs better in terms of worsening ratio."
1807.04488,data,19,2022-05-13,0,"Index Terms—Query reformulation, CodeRank, term weight ing, query quality analysis, concept location, data resampling"
1807.04488,data,328,2022-05-13,0,"Haiduc et al. [19] suggest that quality of a query with respect to the corpus could be determined using four of its statistical properties– speciﬁcity, coherency, similarity and term relatedness–that comprise of 21 metrics [11]. They apply machine learning on these properties, and separate high quality queries from low quality ones. We thus also similarly apply machine learning on our reformulation candidates (and their metrics), and develop classiﬁer model(s) where Classiﬁcation And Regression Tree (CART) is used as the learning algorithm [19]. Since only the best of the four reformulation candidates (i.e., including baseline) is of our interest, the training data was inherently skewed. We thus perform bootstrapping (i.e., random resampling) [27, 50] on the data multiple times (e.g., 50) with 100% sample size and replacement (Step 9, Fig. 2), train multiple models using the sampled data, and then record their output predictions. Then, we average all the predictions for each test instance from all models, and determine their average probability of being the best candidate reformulation. Thus, we identify the best of the four candidates using our models, and suggest the best reformulation to the initial query (Lines 16–20, Algorithm 1, Steps 10, 11, Fig. 2). Bassett and Kraft [8] suggest that repetition of certain query terms might improve retrieval performance of the query. If none of the candidates is likely to improve the initial query according to the quality model (i.e., baseline itself is the best), we repeat all the terms from the initial query as the reformulation. Algorithm 2 getQRCandidate: Get a candidate reformulation (cid:46) CTsig: extracted"
1807.04488,data,33,2022-05-13,0,"[54] J. Yao, B. Cui, L. Hua, and Y. Huang. Keyword Query Reformulation on Structured Data. In Proc. ICDE, pages 953–964, 2012."
1807.04488,data,36,2022-05-13,0,"[50] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online Defect Prediction for Imbalanced Data. In Proc. ICSE, volume 2, pages 99–108, 2015."
1807.04488,data,37,2022-04-21,0,"[7] A. Bachmann and A. Bernstein. Software Process Data Quality and Characteristics: A Historical View on Open and Closed Source Projects. In Proc. IWPSE, pages 119– 128, 2009."
1807.04488,data,4,2022-04-21,0,Quality metric data resampling
1807.04488,"data, code",319,2022-05-13,0,"a ﬁxed size of window in the code. Rocchio [43] and RSV [13] determine importance of a term using TF-IDF based metrics. Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source."
1807.04488,"data, code",338,2022-05-13,0,"Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source. Thus, while earlier studies adopt various methodologies or information sources, our technique not only employs a novel and promising term weight –CodeRank, but also exploits structures of the source documents for identifying the best reformulation to a given query for improved concept location."
1807.04488,"data, data https",32,2022-04-21,0,[1] ACER experimental data. URL https://goo.gl/ZkaNvd. [2] Debbugger source lookup does not work with variables. URL https://bugs.eclipse.org/bugs/show bug.cgi? id=31110.
1807.04488,"data, data https",8,2022-04-21,0,Replication: All experimental data and relevant materials
1807.04488,"data, dataset",308,2022-04-21,0,"From Table IX, we see that RSV and Refoqus perform better than the other existing approaches. They improve about 55% and about 53% of the baseline queries respectively. Such ratios are also pretty close to the originally reported performances by Haiduc et al. on a different dataset, which possibly validates the correctness of our implementation. While 55% query improvement is the maximum performance provided by any of the existing approaches, our technique–ACER–improves about 70% of the baseline queries (i.e., 1% difference between Table V and Table IX due to rounding error) which is signiﬁcantly higher, i.e., paired t-test, p-value=6.663e-06<0.05, Cohen’s D=2.43>1.00 (large). Refoqus adopts a similar methodology like ours. Unfortunately, the approach is limited due to possibly the low performance of its candidate reformulations. One might argue about the data resampling step (i.e., Step 9, Fig. 2) of ACER for the high performance. However, we also apply data resampling to Refoqus using the same settings as ours for further investigation. We see that Refoqussampled has a similar improvement ratio like ours, but it still worsens a signiﬁcant amount of queries, 29%, compared to our 3.40%. Thus, our technique still performs better than Refoqus in the equal settings. Our quantile measures and mean ranks are more promising than those from the baseline or competing methods as reported in Table IX. Table V and RQ1 also suggest that our queries have high potential for reducing human efforts. We also experiment with an extended dataset (i.e., 1,755=1,675 + 8x10) containing 80 very good queries. As reported in Table"
1807.04488,"data, dataset, open-source",55,2022-05-13,0,"Data Collection: We collect a total of 1,675 bug reports from eight open source subject systems (i.e., ﬁve Eclipse systems and three Apache systems) for our experiments. Table III shows the experimental dataset. We ﬁrst extract resolved bug reports (i.e., marked as RESOLVED) from BugZilla and"
1807.04488,database,32,2022-05-13,0,"[55] T. Yuan, D. Lo, and J. Lawall. Automated Construction of a Software-speciﬁc Word Similarity Database. In Proc. CSMR-WCRE, pages 44–53, 2014."
1807.04488,dataset,13,2022-04-21,0,TABLE III EXPERIMENTAL DATASET System #CR #Classes ecf–279.279 log4j–1.2.18 sling–9.0 tomcat70–7.0.73
1807.04488,dataset,4,2022-05-13,0,A. Experimental Dataset
1807.04488,"dataset, code, github",133,2022-04-21,0,"JIRA repositories, and then collect corresponding bug-ﬁxing commits from GitHub version control histories of these eight systems. Such approach was regularly adopted by the relevant literature [8, 21, 41, 49], and we also follow the same. In order to ensure a fair evaluation or validation, we discard the bug reports from our dataset for which no source code ﬁles (e.g., Java classes) were changed or no relevant source ﬁles exist in the system snapshot collected for our study. We also discard such bug reports that contain stack traces using appropriate regular expressions [33]. They do not represent a typical change request (i.e., mostly containing natural language texts) from the regular software users."
1807.04488,open-source,7,2022-05-13,0,from eight open source subject systems.
1807.04488,publicly available,103,2022-04-21,0,"that their implementations are not publicly available. In the case of Refoqus, we implement 27 metrics (20 pre-retrieval [19] and 7 post-retrieval [21]) that estimate query difﬁculty. We develop a machine learning model using CART algorithm (i.e., as used by them) and 10-fold cross validation. Then, we use the model to return the best reformulation out of four candidates of Refoqus– query reduction, Dice expansion, Rocchio’s expansion and RSV expansion–for each baseline query. Table IX and Fig. 6 summarize our comparative analyses."
1810.03977,data,100,2022-05-13,0,The images are normalized and then given to the model for training.First convolution layer the kernel size used is 3×3 with input shape 32×56×56 with RELU (Rectiﬁed Linear Unit) activation function in the ﬁrst convolution layer and then with max pooling layer of size 32×27×27 we are down sampling the data to half of the original dimension and subsequent layers follow the similar pattern.The brief description of the CNN layers architecture along with the output shape is described in table 1.drop out is 0.25 which means we randoms abandon some of the weights to avoid the over ﬁtting
1810.03977,data,173,2022-05-13,0,"Support vector machines(SVM’s) are the most used machine learning algorithms for the image spam detection and because of high accuracy and robustness to misclassiﬁcations is the reason researcher prefer SVM[6]. SVM is a supervised learning algorithm used for the classiﬁcation of data.It consists of support vectors which divide and classiﬁes the data.It classiﬁes the non linear data using kernel trick in which the non linear data is projected to higher dimensions to make it linearly separable by a plane which is generally referred as hyper plane.It does this using a kernel function and their are diﬀerent types of kernel functions like linear,polynomial,radial basis function(RBF) and GausImage spam detection is a binary classian kernels. siﬁcation problem and two classes are spam and not spam.Using the training data that is collected and labeled according to respective classes model is trained and then the model is tested by giving the test data and performance of model is evaluated."
1810.03977,data,39,2022-05-13,0,"[8] R. Vinayakumar, P. Poornachandran, K. P. Soman, Scalable Framework for Cyber Threat Situational Awareness Based on Domain Name Systems Data Analysis, Springer Singapore, Singapore, 2018, pp. 113–142."
1810.03977,data,50,2022-05-13,0,"Initially, neural networks are used for image spam detection and then now research has shifted focus on applying the deep learning algorithms. Deep learning consists of neural network layers which automatically extracts the features from the data in hierarchical pattern and then predicts and classiﬁes the data."
1810.03977,data,81,2022-05-13,0,Convolutional neural networks (CNN’s) are one of the highly eﬃcient deep learning algorithms used for classifying data (particularly image data) using supervised learning technique. They consist of an Input layer and convolution layer followed by pooling layer and again convolution and pooling layers alternatively based on the size and architecture of the network. The ﬁnal layer is a fully connected layer. Fully connected layer converts the ﬁnal scalar outputs of individual classes
1810.03977,"data, dataset",10,2022-05-13,0,Table 2: Results metrics evaluated on test data set
1810.03977,"data, dataset",144,2022-05-13,0,In this research we have used the convolutional neural network(CNN) which is a deep learning network architecture for image spam detection.The deep learning approach gives better accuracy when compared with the machine learning and other conventional image processing based methods and also avoids the manual feature extraction task by automatically identifying the features by itself reducing the time and eﬀort.Binary classiﬁcation of image is performed the model is trained with existing labelled data set and then tested with the test data then metrics are evaluated.Further research can be carried out by exploring other deep learning algorithms like RNN and LSTM and tuning the architecture and hyper parameters may provide interesting insights.Capsule networks can also be tested on the data set which are giving promising results recently when compared with the convolutional neural networks for image related techniques[10].
1810.03977,"data, dataset",24,2022-05-13,0,"features[5]. Features like sender,meta data,message header are extracted and training dataset is prepared and labeled."
1810.03977,"data, dataset",4,2022-05-13,0,4. Data set
1810.03977,"data, dataset",47,2022-05-13,0,"Total images are split in ratio of 80 percent training data and 20 percent testing data.The model is evaluated after the convolutional neural network is trained on training dataset.It is then tested on the testing data set and result metrics accuracy,precision,recall and f1score"
1810.03977,dataset,32,2022-05-13,0,Dataset is subdivided into both training and testing datasets. Training dataset consists of 742 spam images and 648 normal images.Testing dataset consists of 186 spam images and 162 normal images.
1810.03977,dataset,47,2022-05-13,1,The dataset used in the experiment consist of 928 spam images and 810 normal images which collected from diﬀerent sources[9] and all are RGB images in various dimensions which in preprocessing are reshaped to 56×56 images.The sample images are shown in ﬁgure.1 and ﬁgure.2
1810.03977,dataset,8,2022-05-13,0,Figure 1: Sample spam image from dataset
1810.03977,dataset,8,2022-05-13,0,Figure 2: Sample non-spam image from dataset
1810.03977,dataset,96,2022-05-13,0,Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classiﬁcation achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques.
1812.05067,code,133,2022-05-13,0,"Experimental evaluation We have used our implementation to typecheck a variety of examples, including all the examples from the RelCost paper. Some of the examples, such as the relational analysis of merge sort (msort), have rather complex paper proofs. However, in all cases, the total typechecking time (including existential elimination and SMT solving) is less than 1s, suggesting that the approach is practical. Table 1 shows the experimental results over a subset of our example programs (our appendix lists all our example programs, including their code and experimental results). A “-” indicates a negligible value. Our experiments were performed on a 3.20GHz 4-core Intel Core i5-6500 processor with 16 GB of RAM."
1812.05067,code,239,2022-05-13,0,"Heuristics illustrated with merge sort We explain how our implementation types one example—the standard merge sort function— relationally. The goal of this exercise is primarily to illustrate some of our heuristics. The merge sort function, msort, splits a list into two nearly equal-sized sublists using an auxiliary function we call bsplit, recursively sorts each sublist and then merges the two sorted sublists using another function merge. In their paper on RelCost, Çiçek et al. [14] show that the relative cost of two runs of msort with two input lists of length n that diﬀer in at most α positions is upper-bounded by Q(n, α) = ) · min(α, 2H −i ), where H = ⌈log2(n)⌉ and h is a speciﬁc linear function. This open-form expression lies in O(n · (1 + log2(α))).1 Next, we explain at a high-level how this relative cost is typechecked bidirectionally. We show below the code of the top-level merge sort function msort. ﬁx msort(_).Λ.Λ.λl .case l of nil → nil | h1 :: tl1 → case tl1 of nil → cons(h1, nil) | _ :: _ → let r = bsplit ()[ ] [ ] l in"
1812.05067,code,36,2022-05-13,0,"We do not show the code of the helper functions bsplit and merge, but they have the following types (these types are also checked with BiRelCost; we omit those details here):"
1812.05067,data,154,2022-05-13,0,"Next, we extend relSTLC in two steps inspired by the features of previously proposed relational type systems. Our ﬁrst step, named RelRef, adds relational reﬁnement types over lists (as an example of an inductive data type), and a comonadic type that represents syntactic equality of two values. Our second step, named RelRefU, adds to RelRef the possibility to relate arbitrary programs of possibly dissimilar syntactic structure, thanks to the possibility to switch to a complementary unary type system. Both these extensions add intrinsic nondeterminism to the type system to allow a programmer ﬂexibility in writing programs. The source of nondeterminism in both these systems is non-syntax-directed typing and subtyping rules. RelRef has such rules for relational reﬁnement types and for subtyping, while RelRefU has such a rule for switching to unary typing and more such rules for subtyping."
1812.05067,data,98,2022-05-13,0,"Relational eﬀects [8, 14, 16, 28, 39] are often of a quantitative nature and measure some quantitative diﬀerence between two executions of the two expressions. These relational eﬀects are similar in spirit to their standard unary counterpart [13, 31, 32, 34] but their interpretation is a relation between the eﬀects of the two executions. For example, in diﬀerential privacy, a relational eﬀect is used to measure the level of indistinguishability between the observable outputs on two inputs diﬀering in one data element."
1904.11228,data,112,2022-05-13,0,"The feature matrix of data in the vth view is denoted as Xv = [xv N ]T ∈ RN ×dv , xv 1 ∈ Rdv×1, dv is the dimension of feature in the vth view, N is the number of data samples. We pack the feature matrices in V views {Xv}V v=1 and the overall feature matrix of data can be represented as X = [X1, X2, ..., XV ] ∈ RN ×d, (cid:80)V v=1 dv = d. The objective of unsupervised multi-view feature selection is to identify l most valuable features with only X."
1904.11228,data,145,2022-05-13,0,"The selected features should preserve the dynamically learned similarity structure. Conventional approaches separate the similarity structure construction and feature selection into two independent processes, which will potentially lead to sub-optimal performance. In this paper, we learn the collaborative similarity structure dynamically and further integrate it with feature selection into a uniﬁed framework. Specifically, based on the collaborative similarity structure learning in Eq.(3), we employ sparse regression model to learn a projection matrix P ∈ Rd×k, so that the projected lowdimensional data XP can approximate the relaxed cluster indicator F. To select the features, we impose l2,1 norm penalty on P to force it with row sparsity. The importance of features can be measured by the l2 norm of each row feature in P. The overall optimization formulation can be derived as"
1904.11228,data,15,2022-05-13,0,"ture selection for big data analytics. 32(2):9–15, 2017."
1904.11228,data,173,2022-05-13,0,"N Sj = 1, Sj ≥ 0, WT where Sj ∈ RN ×1 characterizes the similarities between any data points with j, it should be subjected to the constraint that 1TSj = 1, Sj ≥ 0, Wj = [w1 j ]T ∈ RV ×1 is comj , w2 prised of view weights for the jth column of similarities, it is constrained with WT j 1V = 1, W = [W1, W2, ..., WN ] ∈ RV ×N is view weight matrix for all columns in the similarity structures. As indicated in recent work [Nie et al., 2014], a theoretically ideal similarity structure for clustering should have the property that the number of connected components is equal to the number of clusters. The similarity structure with such neighbor assignment could beneﬁt the subsequent feature selection. Unfortunately, the similarity structure learned from Eq.(1) does not have such desirable property."
1904.11228,data,176,2022-05-13,0,"hand, with multi-view features, the data could be characterized more precisely and comprehensively from different perspectives. On the other hand, high-dimensional multiview features will inevitably generate expensive computation cost and cause massive storage cost. Moreover, they may contain adverse noises, outlying entries, irrelevant and correlated features, which may be detrimental to the subsequent learning process [Zhu et al., 2016b; Zhu et al., 2016a; Zhu et al., 2017a]. Unsupervised multi-view feature selection [Wang et al., 2016; Li and Liu, 2017] is devised to alleviate the problem. It selects a compact subset of informative features from the original features by dropping irrelevant and redundant features with advanced unsupervised learning. Due to the independence on semantic labels, high computing efﬁciency and well interpretation capability, unsupervised multiview feature selection has received considerable attention in It becomes a prerequisite component in various literature. machine learning models [Li et al., 2017]."
1904.11228,data,204,2022-05-13,0,"The key problem of multi-view feature selection is how to effectively exploit the diversity and consistency of multi-view features to collaboratively identify the feature dimensions, which could retain the key characteristics of the original features. Existing approaches can be categorized into two major families. The ﬁrst kind of methods ﬁrst concatenates multiview features into a vector and then directly imports it into the conventional single-view feature selection model. The candidate features are generally ranked based on spectral graph theory. Typical methods of this kind include Laplacian Score (LapScor) [He et al., 2005], spectral feature selection (SPEC) [Zhao and Liu, 2007] and minimum redundancy spectral feature selection (MRSF) [Zhao et al., 2010]. Commonly, the pipeline of these methods follows two separate processes: 1) Similarity structure is constructed with ﬁxed graph parameters to describe the geometric structure of data. 2) Sparsity and manifold regularization are employed together to identify the most salient features. Although these methods are reported to achieve certain success, they treat features from different views independently and unfortunately neglect the important view correlations."
1904.11228,data,205,2022-05-13,0,"Another family of methods considers view correlation when performing feature selection. Representative works in       clude adaptive multi-view feature selection (AMFS) [Wang et al., 2016], multi-view feature selection (MVFS) [Tang et al., 2013] and adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013]. These methods ﬁrst construct multiple view-speciﬁc similarity structures1 and then perform the subsequent feature selection based on the collaborative (combined) similarity structure. These two processes are separate and independent. The collaborative similarity structure remains ﬁxed during feature selection. The latently involved data noises and outlying entries in the view-speciﬁc similarity structures will adversely reduce the reliability of the ultimate collaborative similarity structure for feature selection. Furthermore, conventional approaches generally employ knearest neighbors assignment to construct the view-speciﬁc similarity structures and the simple weighted combination for ultimate similarity structure generation. This strategy can hardly achieve the ideal state for clustering that the number of connected components in the ultimate similarity structure is equal to the number of clusters [Nie et al., 2014]. Thus, suboptimal performance may be caused under such circumstance."
1904.11228,data,206,2022-05-13,0,"the important correlation of different feature views. Another kind of methods directly tackles the multi-view feature selection. They consider view correlations when performing feature selection. Adaptive multi-view feature selection (AMFS) [Wang et al., 2016] is an unsupervised feature selection approach which is developed for human motion retrieval. It describes the local geometric structure of data in each view with local descriptor and performs the feature selection in a general trace ratio optimization. In this method, the feature dimensions are determined with trace ratio criteria. Adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013] addresses the feature selection problem for visual concept recognition. It employs l2,1 norm [Nie et al., 2010] based sparse regression model to automatically identify discriminative features. In AUMFS, data cluster structure, data similarity and the correlations of different views are considered for feature selection. Multi-view feature selection (MVFS) [Tang et al., 2013] investigates the feature selection for multi-view data in social media. A learning framework is devised to exploit the relations of views and help each view select relevant features."
1904.11228,data,38,2022-05-13,0,"[Tang et al., 2013] Jiliang Tang, Xia Hu, Huiji Gao, and Huan Liu. Unsupervised feature selection for multi-view data in social media. In SDM, pages 270–278, 2013."
1904.11228,data,69,2022-05-13,0,"[He et al., 2005] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS, pages 507–514, 2005. [Huang et al., 2015] Jin Huang, Feiping Nie, and Heng Huang. A new simplex sparse learning model to measure data similarity for clustering. In IJCAI, pages 3569–3575, 2015."
1904.11228,data,75,2022-05-13,0,"As mentioned above, the data points can be directly partitioned into k clusters if the number of components in the similarity structure S is exactly equal to k. Theorem 1 indicates that this condition can be achieved if the rank of Laplacian matrix is equal to n − k. With the analysis, we add a reasonable rank constraint in Eq.(1) to achieve the condition. The optimization problem becomes"
1904.11228,data,76,2022-05-13,0,"With the advent of big data, multi-view features with high dimensions are widely employed to represent the complex data in various research ﬁelds, such as multimedia computing, machine learning and data mining [Liu et al., 2016; Liu et al., 2017; Zhu et al., 2017b; Zhu et al., 2015; Cheng and Shen, 2016; Cheng et al., 2016]. On the one"
1904.11228,"data, dataset",295,2022-05-13,1,"4 Experiments 4.1 Experimental Datasets 1) MSRC-v1 [Winn and Jojic, 2005]. The dataset contains 240 images in 8 class as a whole. Following the setting in [Grauman and Darrell, 2006], we select 7 classes composed of tree, building, airplane, cow, face, car, bicycle and each class has 30 images. We extract 5 visual features from each image: color moment with dimension 48, GIST with 512 dimension, SIFT with dimension 1230, CENTRIST feature with 210 dimension, and local binary pattern (LBP) with 256 dimension. 2) Handwritten Numeral [van Breukelen et al., 1998]. This dataset is comprised of 2,000 data points from 0 to 9 digit classes. 6 features are used to represent each digit. They are 76 dimensional Fourier coefﬁcients of the character shapes, 216 dimensional proﬁle correlations, 64 dimensional Karhunen-love coefﬁcients, 240 dimensional pixel averages in 2 × 3 windows, 47 dimensional Zernike moment and 6 dimensional morphological features. 3) Youtube [Liu et al., 2009]. This real-world dataset is collected from Youtube. It contains intended camera motion, variations of the object scale, viewpoint, illumination and cluttered background. The dataset is comprised of 1,596 video sequences in 11 actions. 4) Outdoor Scene [Monadjemi et al., 2002]. The outdoor scene dataset contains 2,688 color images that belong to 8 outdoor scene categories. 4 visual features are extracted from each image: color moment with dimension 432, GIST with dimension 512, HOG with dimension 256, and LBP with dimension 48."
1904.11228,dataset,1,2022-05-13,0,Dataset
1905.12665,code,82,2022-05-13,0,"This work was ﬁnanced in part by the S˜ao Paulo Research Foundation (FAPESP) under grants No. 2016/199476 and No. 2017/16597-7, the Brazilian National Council for Scientiﬁc and Technological Development (CNPq) under grant No. 307425/2017-7, and the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior—Brasil (CAPES)—Finance Code 001. We acknowledge the support of NVIDIA Corporation for the donation of a Titan X Pascal GPU used in this research."
1905.12665,"code, code available",1,2022-05-13,0,Code
1905.12665,data,102,2022-05-13,0,"Abstract Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classiﬁcation, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings."
1905.12665,data,22,2022-05-13,0,Presented at the ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data Copyright 2019 by the author(s).
1905.12665,data,37,2022-05-13,0,"Berg, R. v. d., Kipf, T. N., and Welling, M. Graph convolutional matrix completion. ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), 2018."
1905.12665,data,49,2022-05-13,0,"Donnat, C., Zitnik, M., Hallac, D., and Leskovec, J. Learning structural node embeddings via diffusion wavelets. In ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), pp. 1320–1329. ACM, 2018."
1905.12665,data,73,2022-05-13,0,"Bai, Y., Ding, H., Bian, S., Chen, T., Sun, Y., and Wang, W. SimGNN: A neural network approach to fast graph similarity computation. In ACM Inter. Conf. Web Search Data Min. (WSDM), WSDM ’19, pp. 384–392, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-5940-5."
1905.12665,data,94,2022-05-13,0,"Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In Gangemi, A., Navigli, R., Vidal, M.-E., Hitzler, P., Troncy, R., Hollink, L., Tordai, A., and Alam, M. (eds.), Semantic Web Conf. (ESWC), pp. 593–607, Cham, 2018. Springer International Publishing."
1905.12665,dataset,140,2022-05-13,1,"In this work, we evaluate our model as an edge classiﬁer, and simulate its performance as a graph generator by inputting noise as features and predicting on them. We perform experiments on three synthetic datasets that consist of images with Geometric Figures for segmentation, 3D surface function, and Community dataset (see Appendices A.1, A.2, and A.3, respectively). For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. Our evaluation metric is the Maximum Mean Discrepancy (MMD) measure (You et al., 2018), which measures the Wasserstein distance over three statistics of the graphs: degree (Deg), clustering coefﬁcients (Clus), and orbits (Orb)."
1905.12665,dataset,147,2022-05-13,0,"For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. For both models, we use the following settings. Our activation functions, σl, are sigmoid for all layers, except for the Eq. 7 where σl is a hyperbolic tangent. We use L = 5 layers to extract the ﬁnal adjacency and embeddings. The feature dimension, dl, is 32 for all layers. The learning rate is set 10−5 for the Community dataset, and in the rest of datasets, the learning rate is set 5 × 10−6. Additionally, the number of epochs changes depending on the experiment. Thus in the experiments of Communities, Surfaces and Geometrical Figures we use 150, 200 and 150 times respectively and, the number"
1905.12665,dataset,2,2022-05-13,0,A. Datasets
1905.12665,dataset,39,2022-05-13,0,"Figure 2. Results of the dissimilarity (MMD) between the prediction and ground truth (smaller values are better) while varying the number of recurrent steps, on the 3D Surface dataset (Surf400)."
1905.12665,dataset,4,2022-05-13,0,A.3. Community Dataset
1905.12665,dataset,40,2022-05-13,0,"We made the Geometric Figures dataset for the task of image segmentation within a controlled environment. Segmentation is given by the connected components of the graph ground-truth. Here, we provide RGB images and their expected segmentations."
1905.12665,dataset,44,2022-05-13,0,"Additionally, in Table 2, we present an ablation analysis of our model’s loss functions and regularization components on the Geometric Figures dataset. We emphasize a stable training and a fast convergence when we minimize both loss functions simultaneously."
1905.12665,dataset,5,2022-05-13,0,A.1. Geometric Figures Dataset
1905.12665,dataset,5,2022-05-13,0,A.2. 3D Surfaces Dataset
1905.12665,dataset,52,2022-05-13,0,"The Geometric Figures dataset contains 3000 images of size n×n, that are generated procedurally.1 Each image contains circles, rectangles, and lines (dividing the image into two parts). We also add white noise to the color intensity of the images to perturb and mixed their regions."
1905.12665,dataset,56,2022-05-13,0,"We generated 200 versions of each surface by randomly applying a set of transformations (from scaling, translation, rotation, reﬂection, or shearing) to the curve, moreover, two versions of the Surface dataset were created, Surf100 and Surf400 that use 100 and 400 vertices per surface, respectively."
1905.12665,dataset,68,2022-05-13,0,"We perform experiments on a synthetic dataset (Community dataset) that comprises two sets with C = 2 and C = 4 communities with 40 and 80 vertices each, respectively, created with the caveman algorithm (Watts, 1999), where each community has 20 people. Besides, Community C = 4 and C = 2 have 500 and 300 samples respectively."
1905.12665,dataset,69,2022-05-13,0,"Table C.1. Comparison of GLN, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric are accuracy (Acc), intersection-over-union (IoU), Recall (Rec), and Precision (Prec) shown row-wise per method, where larger numbers denote better performance."
1905.12665,dataset,73,2022-05-13,0,"Finally, in Fig. F.1, we present an application, even fundamental, on segmentation where each of the connected components represents different objects. For this, we apply our GLN model on Geometric Image dataset, using size image of 20 × 20. Besides, the white edges represent correct predictions, and light blue dashed edges are false negatives (i.e., not predicted edges)."
1905.12665,dataset,78,2022-05-13,0,"Figure E.1. Results on Community dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,78,2022-05-13,0,"Knowing the depth of the recursive model (i.e., the number of iterations) is not a trivial task since we must ﬁnd a tradeoff between the efﬁciency and effectiveness of the model. In Fig. 2, we show the dissimilarity metrics (MMD) while varying the number of applications of our proposed block on the 3D Surface dataset. According to our experiment, using ﬁve recurrent steps provides the right trade-off."
1905.12665,dataset,79,2022-05-13,0,"Figure D.1. Results on 3D Surface dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,81,2022-05-13,0,"To evaluate our method we needed a highly structured dataset with intricate relations and with easily understandable features. Hence, we convert parts of 3D surfaces into a mesh by sampling them. Each point in the mesh is translated into a node of the graph, with its position as a feature vector. We have a generator2 that creates different conﬁgurations for this dataset based on a number of nodes per surface, and transformation on it."
1905.12665,dataset,83,2022-05-13,0,"In Fig. D.1, we show the qualitative result of GLN for the 3D Surface dataset. We show the prediction on the elliptic hyperboloid, elliptic paraboloid, torus, saddle, and ellipsoid, all using 100 nodes (Surf100). We normalized the graphs (w.r.t. scale and translation) for better visualization. Besides, the red edges represent false negatives (i.e., not predicted edges) and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"In Fig. E.1, we predict the adjacency matrix the of Community dataset on two and four communities, C = 2 and C = 4 respectively (even rows). Note, our node embedding obtained after apply the λl function, shows a good grouping of individuals in the hyperspace (odd rows). Furthermore, the red edges represent false negatives (i.e., not predicted edges), and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"structure given a set of points and their feature embeddings, respectively. (ii) A recurrent architecture that deﬁnes our iterative process and our prediction functions. (iii) An endto-end learning framework for predicting graphs’ structure given a family of graphs. (iv) Additionally, we introduce a synthetic dataset, i.e., 3D surface functions, that contains patterns that can be controlled and mapped into graphs to evaluate the robustness of existing methods."
1905.12665,dataset,84,2022-04-21,0,"Table 1. Comparison of GLN against deep generative models, GraphRNN (G.RNN), Kronecker (Kron.), and MMSB, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric is MMD for degree (D), cluster (C), and orbits (O) shown row-wise per method, where smaller numbers denote better performance."
1905.12665,github,4,2022-05-13,0,at https://gitlab.com/mipl/
1905.12665,supplementary data,10,2022-05-13,0,Graph Learning Network: A Structure Learning Algorithm SUPPLEMENTARY MATERIAL
1906.08554,data,105,2022-05-13,0,"The smart devices are increasing exponentially day by day in the whole world. They provide much more facility to the end users and also attach with their daily life. Smart devices can connect to the internet easily for sending and receiving data within the network. The smart devices are not just smart phones, it may be smart refrigerator, Smart home automation entry point, smart air conditioners, Smart hubs, Smart thermostat, Color changing smart LEDs, Smart Watches and smart Tablets etc. in internet of things framework they are connected to each other through internet."
1906.08554,data,119,2022-05-13,0,"The objective of this research is to create the new reliable communication framework for the smart cities using the Tactile Internet a next revolution of internet of things. This research is based on low-latency, ultra-high availability and high-performance concepts of Tactile Internet. The framework provides QoS through reducing the latency (1ms in round trip) also the variety of the quantity of smart devices. In this research we consider idle state in order to makes our examination more efficient, at that point the general execution regarding the overall performance of the framework is evaluated. The framework will monitor and analyze the real-time data collected from network and then taking the action."
1906.08554,data,12,2022-05-13,0,Table.2: Comparison of peak data rate and latency [3]
1906.08554,data,127,2022-05-13,0,"The proposed research entitled “Tactile Internet based reliable communication framework for Smart cities in 5G” is a step forward in wireless networking and IoT where we propose new reliable framework based on Tactile Internet. The Wireless communication is the key of Internet of things and Tactile Internet. It is expected to exceed 50 billion connected devices by 2020 and most of these nodes cannot be connected by wireline. In order to enable critical applications such as smart factories or smart buildings, the networking protocols have to deal with the non-deterministic nature of wireless links. In the 5th generation communication system, the secure and reliable data packets will rely on the network with high availability and low latency."
1906.08554,data,169,2022-05-13,0,"The proposed research plan builds research on extending the performance of communication in internet of things using tactile internet. The transfer data from one configuration to another using wireless networks starts from 1973 in the form of packets radio network. They were able to communicate with another same configuration devices. Recent work is continuing on a project called the Serval Project. It provides networks facility to android devices for communication in infrastructure less network. Whereas our research is concerned about the high-performance communication in internet of smart devices for smart cities. The main contribution of this research is the creation of the reliable communication framework and provide secure, reliable and fast communication using Tactile Internet among the internet of smart devices. The previous studies have been focused on the creation and optimization the framework for communication, but such research doesn’t perform the full framework for secure and reliable communication among internet of smart devices for smart cities."
1906.08554,data,270,2022-05-13,0,"The main contribution of this research is designing a framework for ultra-reliable, low latency and high availability communication in Internet of smart devices for future smart cities using the Tactile Internet. The proposed framework is specifically appropriate for applications in which data is periodically transmitted in internet of smart devices environment. In these applications, on one hand, packets are being produced based on a certain periodic time pattern. On the other hand, service time is always a random variable with general distribution. Therefore, service time might temporarily exceed the period time which, as an inevitable consequence some packets might encounter a busy channel and be dropped. We solve this problem by proposing the new communication framework. We demonstrate that proposed reliable framework, not only increases the throughput, but also the direct connection between the generation (sensors) and communication packet systems are eliminated which make the system far more stable. Moreover, in order to enhance the proposed model, we have employed retransmission scheme, variable packet length, and saturated traffic condition. The solution of this research is summarized as follows. The implementation of proposed framework for communication among internet of smart devices in 5G will be programmed to execute on to the internet of things using Tactile Internet concepts. The idea will focus into three main concepts, these concepts are Reliability, Security and availability. The proposed study supports the wireless networking technology to establish a reliable framework among internet of devices for smart cities."
1906.08554,database,110,2022-05-13,0,"[27]. Aljohani, Mohammed, and Tanweer Alam. ""An algorithm for accessing traffic database using wireless technologies."" In Computational Intelligence and Computing Research (ICCIC), 2015 IEEE International Conference on, pp. 1-4. IEEE, 2015. DOI: https://doi.org/10.1109/iccic.2015.7435818  [28]. Alam, Tanweer, and Mohammed Aljohani. ""Design a new middleware for communication in ad hoc network of android smart devices."" In Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies, p. 38. ACM, 2016. DOI: https://doi.org/10.1145/2905055.2905244"
1912.0313,data,125,2022-05-13,0,"We observe that the ST-DIM based pre-trained models can easily be ﬁne-tuned only with small amount of downstream data. In this classiﬁcation task, our model can classify a randomly chosen time-series as a sample of VAR or SVAR. Note, with very few samples, models based on the pre-trained encoder (FPT and UFPT) outperform supervised models. However, as the number of samples grows, the accuracy achieved with or without pre-training levels out. We also notice that autoencoder based self-supervised pretraining does not assist in VAR vs. SVAR classiﬁcation. Consequently, we use only ST-DIM based pre-training for all the real data experiments. Refer to Figure 4 for the results of simulation experiments."
1912.0313,data,212,2022-05-13,0,"Unsupervised pre-training is a well-known technique to get a head start for the deep neural network. It may be considered as a regularizer which compares to classical regularizers (i.e. L1/L2) may not vanish even with more data and could ﬁnd a robust local minima for better generalization [6]. Classical methods are Deep Beliefs Networks (DBMs) [7] and stacked denoising autoencoders (SDAE) [8]. Unsupervised pre-training has broad implications in ﬁelds such as computer vision [9], natural language processing (NLP) (GPT2 [10], BERT [11], Word2Vec [12]) and automatic speech recognition (ASR) (with SDAE [13], with DBN-HMMs [14]). However, this unsupervised pre-training is considered to be less popular in ﬁelds other than NLP [15]. Speciﬁcally, in computer vision, researchers usually use the model which is pre-trained in supervised fashion on Imagenet as a starting point for downstream tasks. Furthermore, given enough data and technical strategies, it is possible to achieve better results on COCO object detection without supervised pre-training on Imagenet [16]."
1912.0313,data,25,2022-05-13,0,Simulations Real Data 16 Training Batch Size 200 Validation Batch Size 200 Test Batch Size Initial Learning Rate 3e-4 Learning Rate Scheduler None Max Epochs
1912.0313,data,30,2022-05-13,0,Figure 8. ICA time courses are computed from the resting state fMRI data. Results contain statistically independent spatial maps (top) and their corresponding time courses.
1912.0313,data,36,2022-05-13,0,"[19] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019."
1912.0313,data,38,2022-05-13,0,"namics generalizes across different data distributions, as our model pre-trained on healthy adults shows improvements in children and elderly. The generality of the approach is also demonstrated in an application to the keyword detection problem."
1912.0313,data,42,2022-05-13,0,"To simulate the data, we generate multiple 10-node graphs with 10 × 10 stable transition matrices. Using these we generate multivariate time series with autoregressive (VAR) and structural vector autoregressive (SVAR) models [44]."
1912.0313,data,42,2022-05-13,0,"[2] Vince D Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adalı. The chronnectome: time-varying connectivity networks as the next frontier in fmri data discovery. Neuron, 84(2):262–274, 2014."
1912.0313,data,45,2022-05-13,0,"[28] Vince D Calhoun, Tulay Adali, Godfrey D Pearlson, and JJ Pekar. A method for making group inferences from functional MRI data using independent component analysis. Human brain mapping, 14(3):140–151, 2001."
1912.0313,data,48,2022-05-13,0,"[31] R Devon Hjelm, Eswar Damaraju, Kyunghyun Cho, Helmut Laufs, Sergey M Plis, and Vince D Calhoun. Spatio-temporal dynamics of intrinsic networks in functional magnetic imaging data using recurrent neural networks. Frontiers in neuroscience, 12:600, 2018."
1912.0313,data,51,2022-05-13,0,"application [33–35] is considered as a way to enable learning from data and thus improve results in downstream classiﬁcation. To achieve improved performance, another idea is the data generating approach [36] which uses synthetic data generator for pre-training, relieving the scarcity of data."
1912.0313,data,61,2022-05-13,0,"U. Mahmood1, M. M. Rahman1, A. Fedorov2 , Z. Fu1, V. D. Calhoun1, 2, 3, S. M. Plis1 Tri-institutional Center for Translational Research in Neuroimaging and Data Science: 1Georgia State University, 2Georgia Institute of Technology, 3Emory University Atlanta, GA, USA {umahmood1,mrahman21}@student.gsu.edu afedorov@gatech.edu"
1912.0313,data,70,2022-05-13,0,"[50] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer, Mirella Dapretto, et al. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular psychiatry, 19(6):659, 2014."
1912.0313,data,96,2022-05-13,0,"As we are more interested in subjects for classiﬁcation task, we feed each time series (ICA time courses) into the encoder in the form of a sequence of windows. The encoder encodes the windows of input data into latent representation. The latent representation of the entire time series is then concatenated and passed to a biLSTM with hidden dimension of size 200. The output of biLSTM is then used as input to a feed forward network of two linear layers with 200 and 2 units to perform binary classiﬁcation."
1912.0313,"data, data https",154,2022-05-13,0,"We preprocessed the fMRI data using statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) under MATLAB 2016 environment. A rigid body motion correction was performed using the toolbox in SPM to correct subject head motion, followed by the slice-timing correction to account for timing difference in slice acquisition. The fMRI data were subsequently warped into the standard Montreal Neurological Institute (MNI) space using an echo planar imaging (EPI) template and were slightly resampled to 3 × 3 × 3 mm3 isotropic voxels. The resampled fMRI images were ﬁnally smoothed using a Gaussian kernel with a full width at half maximum (FWHM) = 6 mm. After the preprocessing. We included subjects in the analysis if the subjects have head motion ≤ 3◦ and ≤ 3 mm, and with functional data providing near full brain successful normalization [52]."
1912.0313,"data, data repository",11,2022-05-13,1,"2These data were downloaded from the Function BIRN Data Repository,"
1912.0313,"data, data repository",61,2022-05-13,0,"[48] David B Keator, Theo GM van Erp, Jessica A Turner, Gary H Glover, Bryon A Mueller, Thomas T Liu, James T Voyvodic, Jerod Rasmussen, Vince D Calhoun, Hyo Jong Lee, et al. The function biomedical informatics research network data repository. Neuroimage, 124:1074–1079, 2016."
1912.0313,"data, data repository, data https",100,2022-05-13,1,"Data for Schizophrenia classiﬁcation was used in this study were downloaded from the Function BIRN Data Repository (http://bdr.birncommunity.org:8080/BDR/), supported by grants to the Function BIRN (U24-RR021992) Testbed funded by the National Center for Research Resources at the National Institutes of Health, U.S.A. and from the COllaborative Informatics and Neuroimaging Suite Data Exchange tool (COINS; http://coins.trendscenter.org) and data collection was performed at the Mind Research Network, and funded by a Center of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 from the NIH to Dr.Vince Calhoun."
1912.0313,"data, dataset",106,2022-05-13,0,"COBRE The dataset has total 157 subjects — a collection of 68 HC and 89 affected with SZ. Like FBIRN, each subject has 53 non-noise components in its ICA time courses with 140 time points. We use two hold-out sets of size 32 each respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Unlike FBIRN, it has been impossible to increase the number of subjects for downstream training due to insufﬁciency of data."
1912.0313,"data, dataset",111,2022-05-13,0,"15255075100Number of Subjects Per Class0.30.40.50.60.70.80.91.0AUCSpeech CommandsFPTUFPTNPTFBIRNCOBREABIDEOASISHCPpre-trainingapplyingautismschizophreniaADdatasets contain labeled Schizophrenia (SZ) and Healthy Control (HC) subjects. FBIRN The dataset has total 311 subjects consisting of 150 HC and 161 affected with SZ. Each subject has 53 non-noise components with 140 time points. We use two hold-out sets with sizes 32 and 64 respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with 50% overlap along time dimension resulting in 13 windows for each subject. The details of the results are shown in Figure 9."
1912.0313,"data, dataset",111,2022-05-13,0,"As we see, the AUC scores of all the three models for 15 subjects is ∼ 0.5, which can be treated as merely random guess. However, as the number of subjects increases, pretrained models gradually start performing better than NPT which, in fact, even with 120 subjects fails to learn from the data. We suspect that the reason why pre-trained models do not work for 15 subjects is that the data set is much different than HCP. The big age gap between subjects of HCP and OASIS is a major difference and 15 subjects are even not enough for pre-trained models."
1912.0313,"data, dataset",126,2022-05-13,0,"Figure 7. Datasets used for pre-training and classiﬁcation tasks. Healthy controls from the HCP [5] are used for encoder pre-training guided by data dynamics alone1. The pre-trained encoder is then used in downstream classiﬁcation tasks of 3 different diseases, 4 independently collected datasets, many of which contain data from a number of sites, and consist of populations with signiﬁcant age difference. The age distributions in the datasets have the following means, medians and standard deviations: HCP: 29.31, 29.00, 3.667; ABIDE: 17.04, 15.40, 7.29; COBRE: 37.96, 37, 12.90; FBIRN: 37.87, 38, 11.25; OASIS: 67.67, 68, 8.92."
1912.0313,"data, dataset",131,2022-05-13,0,"In this section we study the performance of our model on both, synthetic and real data. To compare and show the advantage of pre-training on large unrelated dataset we use three different kind of models — 1) FPT (Frozen Pre Encoder for simulation experiment consists of 4 1D convolutional layers with output features (32, 64, 128, 64), kernel sizes (4, 4, 3, 2) respectively, followed by ReLU [41] after each layer followed by a linear layer with 256 units. For real data experiments, we use 3 1D convolutional layers with output features (64, 128, 200), kernel sizes (4, 4, 3) re 3"
1912.0313,"data, dataset",136,2022-05-13,0,"Recent advances in unsupervised learning using selfsupervised methods with mutual information objectives have reduced the gap between supervised and unsupervised learning on standard computer vision classiﬁcation datasets [17– 21] and scaled pre-training to very deep convolutional networks (e.g., 50-layer ResNet). Furthermore, it inﬂuences the neuroimaging ﬁeld for classiﬁcation of progression to Alzheimer’s disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identiﬁcation [24]. Speciﬁcally, authors [19] have shown that contrastive based self-supervised pre-training can outperform supervised methods by a large margin in case of small data (e.g., 13 images per class in ImageNet [25])."
1912.0313,"data, dataset",164,2022-05-13,0,"Differentiating multivariate dynamic signals is a difﬁcult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m (cid:29) n problem. In this paper, we show that the source of the problem—signal dynamics—can be used to our advantage and noticeably improve classiﬁcation performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets."
1912.0313,"data, dataset",17,2022-05-13,0,data experiments. Refer to Figure 7 for the details of the datasets for disease classiﬁcation.
1912.0313,"data, dataset",217,2022-05-13,0,"subjects provides beneﬁts that transfer across datasets, collection sites, and multiple disease classiﬁcation with varying age gap. Learning dynamics of fMRI helps to improve classiﬁcation results for schizophrenia, autism, Alzheimer’s dieseases and speed up the convergence of the algorithm on small datasets, that otherwise do not provide reliable generalizations. Although the utility of these results is highly promising by itself, we conjecture that direct application to spatio-temporal data will warrant beneﬁts beyond improved classiﬁcation accuracy in the future work. Working with ICA components is a smaller and thus easier to handle space that exhibits all dynamics of the signal, in future we will move beyond ICA pre-processing and work with fMRI data directly. We expect model introspection to yield insight into the spatio-temporal biomarkers of schizophrenia. In future work, we will use the same analogously pre-trained encoder on datasets with various other mental disorders such as MCI and bipolar. We are optimistic about the outcome because the proposed pre-training is oblivious of the downstream use and is done in a manner quite different from the classiﬁer’s work. It may indeed be learning crucial information about dynamics that might contain important clues into the nature of mental disorders."
1912.0313,"data, dataset",221,2022-05-13,0,"Mental disorders manifest in behavior that is driven by disruptions in brain dynamics [1, 2]. Functional MRI captures the nuances of spatio-temporal dynamics that could potentially provide clues to the causes of mental disorders and enable early diagnosis. However, the obtained data for a single subject is of high dimensionality m and to be useful for learning, and statistical analysis, one needs to collect datasets with a large number of subjects n. Yet, for any kind of a disorder, demographics or other types of conditions, a single study is rarely able to amass datasets large enough to go out of the m (cid:29) n mode. Traditionally this is approached by handcrafting features [3] of much smaller dimension, effectively reducing m via dimensionality reduction. Often, the dynamics of brain function in these representations vanishes into proxy features such as correlation matrices of functional network connectivity (FNC) [4]. Efforts that pull together data from various studies and increase n do exist, but it is difﬁcult to generalize to study of smaller and more speciﬁc disease populations that cannot be shared to become a part of these pools or are too different from the data in them."
1912.0313,"data, dataset",71,2022-05-13,0,"points. We use two hold-out sets of size 100 each respectively for validation and test purpose. The remaining data are used for downstream training i.e., autism vs. HC classiﬁcation. Like COBRE dataset, with 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Refer to Figure 11 for the details of the experimental results."
1912.0313,"data, dataset",93,2022-05-13,0,"For experiments on the downstream tasks, a hold out is selected for testing and is never used through the training/validation phase. For each downstream task, the number of subjects used for supervised training is gradually increased within a range to observe the effectiveness of pretraining in downstream task with varied number of training subjects . For each experiment, 10 trials are performed to ensure random selection of training subjects and, in each case, the performance is evaluated on the hold out dataset (test data)."
1912.0313,"data, dataset",94,2022-05-13,0,"For brain data, each of the models (FPT, UFPT, NPT) yields the best results based on its respective gain value of Xavier [43] initialization used for biLSTM. To ﬁnd the best gain value for each model, 20 values in the range 0 − 1 are tried with an increment of 0.05. For each value, 10 experiments are performed and best value is chosen based on the results on validation dataset. Refer to Table 1 for more parametric details of the models."
1912.0313,"data, dataset",98,2022-05-13,0,"15306090120Number of Subjects Per Class0.30.40.50.60.70.80.9AUCSTDIM vs AutoencoderNPTUFPT_STDIMFPT_STDIMUFPT_AutoencoderFPT_AutoencoderLibriSpeech+Mel-SpectrogramCoffee ShopBackgroundSpeech Commands    (Cat)Coffee ShopBackground+Mel-SpectrogramPre-TrainingTraining00.511.52Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dB+0 dB1111TTTT/2051015Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dBResearch Excellence) [49] project, from the release 1.0 of ABIDE (Autism Brain Imaging Data Exchange3) [50] and from release 3.0 of OASIS (Open Access Series of Imaging Studies4) [51]. Written informed consent was obtained from all participants of each dataset under protocols approved by the institutional review board (IRB)."
1912.0313,"data, dataset provided",53,2022-05-13,1,"Data for Alzheimer’s was provided by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, UL1TR000448, R01AG043434, R01EB009352. AV-45 doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly."
1912.0313,"data, dataset provided",60,2022-05-13,1,"Data for healthy subjects was provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University."
1912.0313,"data, dataset provided",82,2022-05-13,1,"15255075100120Number of Subjects Per Class0.400.450.500.550.600.650.70AUCOASISFPTUFPTNPTAutism data was provided by ABIDE. We acknowledge primary support for the work by Adriana Di Martino provided by the (NIMH K23MH087770) and the Leon Levy Foundation and primary support for the work by Michael P. Milham and the INDI team was provided by gifts from Joseph P. Healy and the Stavros Niarchos Foundation to the Child Mind Institute, as well as by an NIMH award to MPM (NIMH R03MH096321)."
1912.0313,"data, dataset, publicly available, data available",158,2022-05-13,1,"Our goal is to enable the direct study of systems dynamics in smaller datasets. In the case of brain data it, in turn, can enable an analysis of brain function. In this paper, we show how one can achieve signiﬁcant improvement in classiﬁcation directly from dynamical data on small datasets by taking advantage of publicly available large but unrelated datasets. We demonstrate that it is possible to train a model in a self-supervised manner on dynamics of healthy control subjects from the Human Connectome Project (HCP) [5] and apply the pre-trained encoder to a completely different data collected across multiple sites from healthy controls and patients. We show that pre-training on dynamics allows the encoder to generalize across a number of datasets and a wide range of disorders: schizophrenia, autism, and Alzheimer’s disease. Importantly, we show that learnt dy 1"
1912.0313,"data, dataset, used dataset",70,2022-05-13,1,"Next, we apply the same unsupervised pre-training method to brain imagining data. For encoder pre-training, we use HCP [5] consortium dataset and apply the pre-trained encoder for further downstream tasks. We apply the same pre-trained encoder for three different types of diseases spanning four datasets to classify schizophrenia, autism and Alzheimer’s diseases. We use resting fMRI data for all brain"
1912.0313,database,11,2022-05-13,0,"L. Fei-Fei. Image Database. In CVPR09, 2009."
1912.0313,dataset,10,2022-05-13,0,"Learnt dynamics generalizes across tasks, datasets, and populations"
1912.0313,dataset,101,2022-05-13,0,"The dataset OASIS [51] has total 372 subjects with equal number (186) of HC and AZ patients. We use two holdout sets each of size 64 respectively for validation and test purpose. The remaining are used for supervised training. Unlike other datasets described earlier, it has only 120 time points though the number of non-noise componets is same (53) as other datasets. With 120 time points, we use six 53 × 20-sized non-overlapping windows for each subject. Refer to Figure 12 for the details of the experiments."
1912.0313,dataset,104,2022-05-13,0,"Figure 5. Left: For pre-training, we combine audio ﬁles from LibriSpeech dataset with background noise of coffee shop. T is the length of audio ﬁles which ranges from 1 to 20 seconds. Right: For training, we superimpose a T/2 length audio of word ""cat"" padded with T/2 zeros onto a background noise of coffee shop of length T (T = 2). For both pre-training and training, we calculate the mel-spectrogram of the combined audio ﬁles that results in a matrix of size components × time courses for each audio ﬁle."
1912.0313,dataset,111,2022-05-13,0,"First, we generate 50 VAR times series with size 10 × 20000. Then we split our dataset to 50 × 10 × 14000 samples for training, 50 × 10 × 4000 —for validation and 50 × 10 × 2000 — for testing. Using these samples, We pre-train an encoder to learn consecutive windows (positive examples) from the same VAR time series. As mentioned in Section 3.1.2, we also use autoencoder for pre-training the same encoder and show the effectiveness of ST-DIM to learn time-series dynamics in self-supervised manner. After pretraining, we use our pre-trained encoder for complete-time series classiﬁcation."
1912.0313,dataset,141,2022-05-13,0,"In most cases, due to practical reasons, researchers in brain imaging are constrained to work with small datasets. In addition, earlier work [26, 27] in brain imaging have been based on unsupervised methods to learn the dynamics and structure of the brain while supervised approaches are used to perform predictions at individual level. Such unsupervised methods include models as linear ICA [28], HMM framework [29]. Moreover, some other nonlinear approaches are also proposed to capture the dynamics. Examples include using Restricted Boltzman Machines (RBMs) [30], RNN modiﬁcation of ICA [31], and reconstructions by recurrent U-Net architecture [32]. In some cases, where dataset is very small, transfer learning as observed in some neuroimaging"
1912.0313,dataset,142,2022-05-13,0,"Let D = {(ut, vs) : 1 ≤ t, s ≤ N, t (cid:54)= s} be a dataset of pairs of values at time point t and s sampled from sequence with length N . Then D+ = {(ut, vs) : 1 ≤ t ≤ N − 1, s = t + 1} is called a dataset of positive pairs and D− = {(ut, vs) : 1 ≤ t, s ≤ N, s (cid:54)= t + 1} — of negative pairs. The dataset D+ refers to a joint distribution and D− — a marginal distribution. Eventually, the lower bound with InfoNCE estimator [37] If (D+) is deﬁned as:"
1912.0313,dataset,19,2022-05-13,0,[46] Pete Warden. Speech commands: A dataset for limited vocabulary speech recognition. 2018.
1912.0313,dataset,2,2022-05-13,0,4.4.1 Datasets
1912.0313,dataset,23,2022-05-13,1,"For schizophrenia classiﬁcation, we conduct experiments on two different datasets, FBIRN [48] and COBRE [49]. The"
1912.0313,dataset,30,2022-05-13,1,"Four datasets used in this study are collected from the FBIRN (Function Biomedical Informatics Research Network2) [48] project, from the COBRE (Center of Biomedical"
1912.0313,dataset,32,2022-05-13,0,"As we have demonstrated, self-supervised pre-training of a spatiotemporal encoder gives signiﬁcant improvement on the downstream tasks in both keyword detection and brain imaging datasets. Pre-training on fMRI of healthy"
1912.0313,dataset,34,2022-05-13,0,"Figure 11. AUC scores for all the three models on ABIDE dataset. Like experiments on FBIRN and COBRE, it is evident that the pre-trained models consistently perform better than NPT."
1912.0313,dataset,34,2022-05-13,0,"The dataset ABIDE has total 569 subjects, of which, 255 are HC and 314 are affected with autism. Like other datasets, each subject has 53 non-noise components with 140 time"
1912.0313,dataset,43,2022-05-13,0,"Our method is two fold. We ﬁrst pre-train our encoder on large unrelated dataset to learn improved representation of the latent factors, and then use the pre-trained encoder for downstream task. We explain both steps in the following sections."
1912.0313,dataset,49,2022-05-13,0,"Figure 10. AUC scores for all the three models on COBRE dataset. It is obvious that even with 15 subjects for training, FPT outperforms NPT noticeably, that is, the difference between two median AUC scores is remarkable ((cid:39) 0.15)."
1912.0313,dataset,51,2022-05-13,0,"Figure 9. AUC scores for all the three models (Refer to Figure 3) on FBIRN dataset. It is noticeable that even with only 15 subjects for supervised training, the median AUC scores of FPT and NPT differ by a large margin (10%)."
1912.0313,dataset,53,2022-05-13,0,"Figure 12. AUC scores for all the models on OASIS dataset. As we continue increasing the number of subjects, the pre-trained models start learning and thus improve their respective scores. However, notice that the NPT model even with 120 subjects didn’t signiﬁcantly improve its predictability."
1912.0313,dataset,56,2022-05-13,0,"Figure 4. Area Under Curve (AUC) scores for VAR vs. SVAR time-series classiﬁcation using ST-DIM and autoencoder based pre-training methods. ST-DIM based pre-training greatly improves the performance of downstream task with small datasets. On the other side, autoencoder based pre-training fails to learn dynamics and thus exhibits poor performance."
1912.0313,dataset,59,2022-05-13,0,"Out of all the available subjects, we select 416 which have large number of time points (N (cid:62) 20k). We use 300 subjects for training and 116 for validation. For pretraining, we use the algorithm as described in section 3.1.1 and achieve accuracy of ∼ 0.95 on the validation dataset."
1912.0313,dataset,61,2022-05-13,0,15255075100Number of Subjects Per Class0.500.550.600.650.700.750.80AUCFBIRNFPTUFPTNPT152540Number of Subjects Per Class0.40.50.60.70.8AUCCOBREFPTUFPTNPT15255075100150Number of Subjects Per Class0.350.400.450.500.550.600.650.70AUCABIDEFPTUFPTNPT7.29 years. Refer to Figure 7 for the demographic information of all the datasets. The dissimilarity in the age range is supposed to cause signiﬁcant difference between these two datasets as the structure of brain and thought process of children is obviously different than adults.
1912.0313,dataset,69,2022-05-13,0,"For each dataset, 100 ICA components as shown in Figure 8 are acquired using the same procedure described in [52]. However, only 53 non-noise components as determined per slice (time point) are used in pre-training of encoder and on downstream task. For experiments, including both pretraining and classiﬁcation the fMRI sequence is divided into windows of 20 time points."
1912.0313,dataset,89,2022-05-13,0,"Trained): The pre-trained encoder is not further trained on the dataset of downstream task, 2) UFPT (Unfrozen PreTrained): The pre-trained encoder is further trained on the dataset of downstream task and 3) NPT (Not Pre-trained): The encoder is not pre-trained at all and only trained on the small dataset of downstream task. The models are shown in Figure 3. In each experiment, we compare all three models to demonstrate the effectiveness of unsupervised pre-training."
1912.0313,dataset,94,2022-05-13,0,"To show the broad implications of unsupervised pretraining, we ﬁrst apply it to a simple problem of keyword detection in audio ﬁles. We choose this problem as it has many practical applications (e.g., virtual assistants in smart phones, robots). We use LibriSpeech ASR corpus [45] for pre-training and Speech Commands Dataset [46] for supervised training. The audio ﬁles of both datasets are combined with a background noise of coffee shop collected from [47] to make pre-training and classiﬁcation harder."
1912.0313,"dataset, used dataset",108,2022-05-13,0,"As seen in the ﬁgure, same pre-trained encoder performs reasonably better than NPT for autism vs. HC classiﬁcation and thus reinforces our hypothesis that unsupervised pretraining learns signal dynamics useful for downstream tasks. Note the difference between age ranges of ABIDE and HCP datasets. The age range of ABIDE is much lower than that of HCP dataset used for pre-training. HCP dataset contains subjects of different ages with means 30.01 and 28.48, medians 30 and 28, and standard deviations 3.522 and 3.665 years respectively for female and male, whereas ABIDE dataset has overall mean 17.04, median 15.40 and standard deviation"
1912.09621,"code, code available",88,2022-04-21,0,"Images from digital microscopy are captured under different illumination conditions. To aid our classification architecture, we preprocess the images using a color constancy technique [10, 11] to maintain the color contrast across all images. Later, we resize the images to match with the ResNet and GoogLeNet architectures. Figures 4 and 5 present the results obtained using the color constancy preprocessing for different images [10]. The code utilized for color constancy is available at [11]."
1912.09621,data,156,2022-05-13,0,"Computer-aided detection has been a research area attracting great interest in the past decade. Machine learning algorithms have been utilized extensively for this application as they provide a valuable second opinion to the doctors. Despite several machine learning models being available for medical imaging applications, not many have been implemented in the real-world due to the uninterpretable nature of the decisions made by the network. In this paper, we investigate the results provided by deep neural networks for the detection of malaria, diabetic retinopathy, brain tumor, and tuberculosis in different imaging modalities. We visualize the class activation mappings for all the applications in order to enhance the understanding of these networks. This type of visualization, along with the corresponding network performance metrics, would aid the data science experts in better understanding of their models as well as assisting doctors in their decision-making process."
1912.09621,data,206,2022-05-13,0,"This paper describes a study that supports the apprehension of the results predicted by deep neural networks applied towards medical imaging analysis. Several machine learning and deep learning architectures have been proposed in the literature for automated Computer-Aided Detection (CAD) tools for various applications [1-4]. In the past few years, deep learning networks have been used widely in medical imaging applications [1, 3]. Residual Networks (ResNet) [5] and GoogLeNet [6] are some of the most popular networks used in this field. The availability of a vast variety of networks raises the question of choosing the optimal network for a given disease/condition. In a data science perspective, optimal results could be measured in terms of overall accuracy, confusion matrix, precision, recall, Receiver Operating Characteristics (ROC) curve, or any other performance metric. However, these optimal results might not be satisfactory for the doctors if the results are not interpretable. Determining the Region of Interest (ROI) that contributed to the decision making of the network will enhance the understanding for both data science experts and clinicians."
1912.09621,"data available, dataset",3,2022-05-13,0,5.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,2.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,3.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,4.1. Dataset
1912.09621,"data, dataset",213,2022-05-13,0,"In addition, we have presented a comprehensive study of these algorithms based on their CAM results. This type of CAD system would let the medical expert analyst choose the algorithm based on their discretion in terms of CAM results, overall accuracy, AUC, or any other performance metric along with computation time and memory consumption. CAM results could be utilized by data science experts to further optimize their respective models in terms of architecture and/or preprocessing techniques. This type of CAM study would also assist the researchers in understanding the discriminative regions determined by various architectures in different imaging modalities. For instance, if an architecture achieves a high CAD detection accuracy for a particular dataset but the discriminative region determined using CAM is present in an unrelated area, performance metrics might mislead both data science researchers and medical expert analysts. Hence, CAM visualization would provide the necessary documentation for the medical expert analysts to enhance the trust in CAD system. This research can be further extended by studying the ROI determined by CAM by changing different hyper-parameters and how they evolve throughout the epochs. Another way to extend this research is by fusing CAM regions determined using different architectures."
1912.09621,"data, dataset",59,2022-05-13,0,"For this application, we perform a hold-out validation study. We split the dataset into groups of 80% and 20% for training and testing respectively. We utilize a subset of 10% from our training data for validation purpose in order to fine-tune our hyperparameters. Table 1 presents the distribution of the dataset."
1912.09621,"data, dataset, publicly available",98,2022-05-13,1,"To maintain the homogeneity across all these applications, we solely study the performance of transfer learning approaches using GoogLeNet and ResNet. Figure 1 presents the top-level block diagram of the transfer learning methodology adopted in this study. We implement these techniques for publicly available datasets thereby setting a new benchmark for each application. Results presented for the publicly available datasets would grant the capability for researchers to replicate and enhance them further. This type of analysis would assist the doctors and data science experts in selecting the optimal model of their choice."
1912.09621,dataset,115,2022-05-13,0,"For this application, due to the limited availability of images, we perform a 10-fold validation study. We believe 10-fold validation would give a better estimate of our performance. We train 10 different networks based on 9 folds and test on the remaining fold in each iteration. For each fold, we train and tune our hyper-parameters solely based on the images from training fold. We make sure to exclude the testing fold in any manner to conduct a rigorous study. Note that we utilize the same set of cases in each fold for the architectures implemented. Overall distribution of the dataset is presented in Table 5."
1912.09621,dataset,135,2022-05-13,0,"Figures 21 and 22 present the CAM results obtained for two different cases from the brain tumor dataset. Figure 21 presents the results for the case marked as ‘positive brain tumor’ by the trained clinician and our algorithm accurately predicts the same. The discriminative region is near the tumor portion which would help the doctors pinpoint important features, spatially. Figure 22 presents the results for the case marked as ‘negative brain tumor’ and the visualization behind algorithm’s prediction. CAM visualization for 26 (one test fold) different cases using our approach is available at [17]. This type of automated CAD technology for brain tumor detection would assist the doctors in providing a valuable second opinion and enhancing their workflow."
1912.09621,dataset,147,2022-05-13,0,"In this section, we study the CAM results obtained using GoogLeNet and ResNet. Figures 8 and 9 present the CAM results obtained for two different cases from the malaria dataset using our proposed approach. Figure 8 presents the results for the case marked as parasitized by the expert reader, and our algorithm not only accurately predicts the same but also presents the discriminative region that contributed the most to its decision. The ROI is around the red spot containing plasmodium. Figure 9 presents the results for the case marked as uninfected. CAM visualization for 100 different cases using our presented approach is available at [12]. Typically, ResNet CAM converges to a smaller ROI in comparison to GoogLeNet. This type of automated CAD technology for malaria detection would assist the microscopists and enhance their workflow."
1912.09621,dataset,155,2022-05-13,0,"Figures 14 and 15 present the CAM results obtained for two different test cases from the APTOS dataset using our proposed approach. Figure 14 presents the results for a case marked as ‘positive DR’ by the expert clinicians. It is also interesting to note that CAM results presented by GoogLeNet and ResNet for DR detection have minimal intersection despite exhibiting similar performance, which could affect an expert clinician’s choice of network. Figure 15 presents the results for a case marked as ‘negative DR’ by an expert clinician and the discriminative region for this case is near the retinal portion. CAM visualization for 60 different cases using our presented approach is available at [15]. This type of automated CAD technology for DR detection could be implemented for immediate solutions and could be applied in places with scarcity of such expert clinicians."
1912.09621,dataset,3,2022-05-13,0,Type of Dataset
1912.09621,dataset,49,2022-05-13,0,"Similar to brain tumor detection, due to the limited availability of images, we perform a 10-fold validation study. Overall distribution of the dataset is presented in Table 7. There is no additional preprocessing except converting these images into the input size for the network."
1912.09621,dataset,6,2022-05-13,0,Table 7: Tuberculosis dataset distribution
1912.09621,dataset,66,2022-05-13,0,"Similar to malaria detection (Section 2), we perform a hold-out validation study. We split the dataset into groups of 72%, 8%, and 20% for training, validation, and testing respectively. Table 3 presents the distribution of each dataset. There is no preprocessing except converting these images into the input size of the network."
1912.09621,dataset,67,2022-05-13,0,"The remainder of this paper is organized as follows. Sections 2-5 present the results obtained for CAD of malaria, DR, brain tumor, and tuberculosis respectively. In each of these sections, we describe the dataset along with the experimental results obtained in terms of both performance metrics and CAM results. Finally, discussions and conclusions are offered in Section 6."
1912.09621,dataset,7,2022-05-13,0,Table 1: Malaria dataset distribution.
1912.09621,dataset,7,2022-05-13,0,Table 3: DR dataset distribution.
1912.09621,dataset,8,2022-05-13,0,Table 5: Brain Tumor dataset distribution.
1912.09621,dataset,92,2022-05-13,0,"MRI scans contain text information for some cases, which are not essential for classification and might mislead our deep neural networks. Hence, we preprocess MRI scans by cropping ROI of brain and removing any additional text from the image using simple morphological operations. In addition, we perform histogram equalization to enhance and maintain the contrast across the dataset. Later, we resize the images to match with the input of ResNet and GoogLeNet architectures. Figure 18 presents the results obtained using these preprocessing techniques."
1912.09621,"dataset, data https",17,2022-05-13,1,"[16] Brain Tumor Dataset, https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumordetection, Accessed December 7, 2019."
1912.09621,"dataset, data https",18,2022-05-13,1,"[13] Kaggle Diabetic Retinopathy Dataset, https://www.kaggle.com/c/diabetic-retinopathydetection/overview, Accessed December 7, 2019."
1912.09621,"dataset, data https",21,2022-05-13,1,"[8] Malaria Dataset, National Institutes of Health, https://ceb.nlm.nih.gov/repositories/malariadatasets/ , Accessed December 8, 2019."
1912.09621,"dataset, publicly available, dataset provided, used dataset",48,2022-05-13,1,We make use of the publicly available Shenzhen dataset [19] provided for the classification of chest radiographs. This dataset contains a total of 662 images. Figures 23 and 24 present sample images marked as ‘Normal’ and ‘Tuberculosis’ by radiologists.
1912.09621,"dataset, publicly available, used dataset",136,2022-05-13,1,"We make use of a publicly available dataset provided by the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 on Kaggle [14] to detect DR in retinal images. In this dataset, 3662 retinal images are graded by expert clinicians at Aravind Eye Hospital, India into 5 different categories: (i) Negative DR, (ii) Mild DR, (iii) Moderate DR, (iv) Proliferative DR, and (v) Severe DR. In this research, we solely focus on the detection of DR, hence, we merge all the mild, moderate, proliferative and severe cases into a single category ‘positive DR’. Figures 10 and 11 present sample images marked in different categories by expert clinicians."
1912.09621,"dataset, publicly available, used dataset",68,2022-05-13,1,We make use of a publicly available dataset provided on Kaggle [16] for the classification of MRI scans. The dataset contains a total of 253 images classified into two different categories ‘positive’ and ‘negative’ brain tumor. Figures 16 and 17 present sample images marked as ‘positive brain tumor’ and ‘negative brain tumor’ by trained clinicians.
1912.09621,"dataset, publicly available, used dataset",84,2022-05-13,1,"We make use of a publicly available dataset provided by the National Institutes of Health (NIH) for the classification of cell images [8, 9]. Any cell image that contains plasmodium is marked as ‘parasitized’ by expert analysts and ‘uninfected’ otherwise. The dataset contains a total of 27,558 images with equal distribution of parasitized and uninfected cells. Figures 2 and 3 present sample images marked as parasitized and uninfected by expert readers."
2002.00512,code,55,2022-05-13,0,"[32] M. Torrent, F. Jollet, F. Bottin, G. Zérah;, and X. Gonze, Implementation of the projector augmentedwave method in the ABINIT code: Application to the study of iron under pressure, Computational Materials Science, 42 (2008), pp. 337 – 351."
2002.00512,code,94,2022-05-13,0,"The numerical results using a Julia [1] homemade code are summarized in the following ﬁgures with Z = 3, R = 1 and L = 5. The atomic PAW function φk are the eigenfunctions of the hydrogenoid atom. For the pseudo atomic function (cid:101)φk, continuity of the function and of the ﬁrst four derivatives are enforced (i.e. d = 5). The lowest eigenvalue is computed using a conjugate-gradient algorithm stopped when the norm of the residual is less than 10−5."
2002.00512,data,43,2022-05-13,0,"[17] F. Jollet, M. Torrent, and N. Holzwarth, Generation of Projector Augmented-Wave atomic data: A 71 element validated table in the XML format, Computer Physics Communications, 185 (2014), pp. 1246–1254."
2002.04279,code,15,2022-05-13,0,"● Software that checks text-based documents, source code, or both (Chowdhury &"
2002.04279,data,126,2022-05-13,0,"Since the analysis and interpretation of the data are quite sensitive, we approached this period with the utmost care. As suggested by Guba and Lincoln (1989), member check is an effective technique for establishing the trustworthiness criteria in qualitative studies. Therefore, having analyzed and reported the data, we sent a preprint of the results to the vendors. Team members closely evaluated the issues raised by the vendors. Not all of them were able to be addressed in this paper, but as many as possible were incorporated. Because of the rigorous efforts to establish the validity of the results and the reliability of the study in this process, this study was further delayed."
2002.04279,data,18,2022-05-13,0,"Journal of Data Mining and Knowledge Discovery, 2(2), 50–53. doi: 10.11648/j.ajdmkd.20170202.12"
2002.04279,data,21,2022-05-13,0,This research did not receive any external funding. HTW Berlin provided funding for openly publishing the data and materials.
2002.04279,data,36,2022-05-13,0,"5. Because of European data privacy laws, for higher education institutions in the EU it must be certain that the companies are only using servers in the EU if they are storing material."
2002.04279,data,5,2022-05-13,0,Availability of data and materials
2002.04279,data,64,2022-05-13,0,"Sorokina, D., Gehrke, J., Warner, S., & Ginsparg, P. (2006, December). Plagiarism detection in arXiv. In J. Liu & B. W. Wah (Eds.), Proceedings of the Sixth International Conference on Data Mining (ICDM'06). Hong Kong. (pp. 1070–1075). doi: 10.1109/ICDM.2006.147"
2002.04279,"data, data available",125,2022-05-13,0,"Our testing took place between Nov 2018 and May 2019. During this time, we tested both coverage and usability. An additional test of multi-source document took place between August and November 2019. Since the present research did not benefit from any funding, the researchers were expected to fulfill their institutional workloads during the research period. Considering the size of the project team from various countries, we could make significant progress only during semester breaks, which explains the length of the testing process. It should be noted that we tested what the systems offered at the time of data collection. We used features that were allowed by the access given to us by the vendors."
2002.04279,"data, publicly available, data available",14,2022-05-13,1,Data and materials used in this project are publicly available from http://www.academicintegrity.eu/wp/wg-testing/
2002.04279,database,105,2022-05-13,0,"As has been shown in other investigations (Weber-Wulff et al., 2013) translation plagiarism is very seldom picked up by software systems. The worst performance of the systems in this test was indeed the translation plagiarism, with one notable exception—Akademia. This system is the only one that performs semantic analysis and allows users to choose the translation language. Unfortunately, their database—with respect to the languages of our testing—is much smaller than the database of other systems. However, the performance drop between copypaste and translation plagiarism is much smaller for Akademia than for the other systems."
2002.04279,database,107,2022-05-13,0,"PlagScan presents itself as a plagiarism checker. It is operated by the German company PlagScan GmbH and was launched in 2009. They state that they have more than 1,500 organizations as customers. Although they focus on higher education, high schools, and businesses, PlagScan is also available for single users. They search the internet using MS Bing, published academic articles, their so-called “Plagiarism Prevention Pool”, and optionally a customer’s own database. PlagScan offers multiple pricing plans for each type of customer, there are apparently also now options for a free trial."
2002.04279,database,108,2022-05-13,0,"One aspect of Wikipedia sources that is not adequately addressed by the text-matching software systems is the proliferation of Wikipedia copies on the internet. As discussed in Weber-Wulff et al. (2013), this can lead to the appearance of many smallish text matches instead of one large one. In particular, this can happen if the copy of the ever-changing Wikipedia in the database of the software system is relatively old and the copies on the internet are from newer versions. A careless teacher may draw false conclusions if they focus only on the quantity of Wikipedia similarities in the report."
2002.04279,database,13,2022-05-13,0,"2. Text-matching systems that maintain a database of potential sources, employ"
2002.04279,database,139,2022-05-13,0,"The Croatian researchers Birkić, Celjak, Cundeković, and Rako (2016) tested four tools that are widely used in Europe and have the possibility to be used at the national and institutional level. They compared such criteria as the existence of an API (application programming interface) and the possibility to integrate it as a plug-in for learning management systems, database scope, size of the user community, and other criteria. The researchers tested the tools using two papers for each type of submission: journal articles, conference papers, master’s and doctoral theses, and student papers. However, they did not include different types of plagiarism and evaluated the checking process with a focus on quote recognition, tool limitations, and interface intuitiveness."
2002.04279,database,151,2022-05-13,0,"Turnitin was founded in 1999 by four students and grew to be an internationally known company. In 2014, they acquired the Dutch system Ephorus and “joined forces” (Ephorus, 2015). In 2019 they themselves were taken over by a US investment company, Advance (Turnitin, 2019). With a focus on institutional users only, they are used by 15,000 institutions in 150 countries. Turnitin uses its own crawler to search the web including also an archive of all previously indexed web pages (Turnitin, n.d.). Turnitin further compares the texts against published academic articles, as well as their own database of all assignments which have ever been submitted to the system, and optionally institutional databases. They are also developing many additional software tools for educators to use in teaching and giving feedback."
2002.04279,database,156,2022-05-13,0,"Luparenko (2014) tested 22 tools that were selected as popular ones based on an analysis of scientific literature and web sources. She considered many criteria related to functional specification (such as type, availability of free trial mode, need for mandatory registration at a website, number of users that have access to the program, database, acceptable file formats, etc.) and also checked the performance of the tools using one scientific paper in the Ukrainian language and another one in English. Moreover, the checking was done using three different methods: entering the text in the field of website, uploading a file, and submitting the URL of the article. She measured the checking time and evaluated the quality of the report provided by tools, as well as reported the percentage of unique text found in each of the articles."
2002.04279,database,166,2022-05-13,0,"As for the general testing, the results are highly consistent with the Wikipedia results which contributes the validity of the single-source and multi-source testing. Again, in single-source documents, Urkund obtained the highest score, while PlagAware is the best performing system in multi-source documents. Dupli Checker, DPV and intihal.net obtained the least scores in both categories. Most of the systems demonstrated better performance for multi-source documents than for single-source ones. This is most probably explained by the chances the systems had for having access to a source. If one source was missing in the tool’s database, it had no chance to identify the text match. The use of multiple sources gave the tools multiple chances of identifying at least one of the sources. This points out quite clearly the issue of false negatives: even if a text-matching tool does not identify a source, the text can still be plagiarized."
2002.04279,database,249,2022-05-13,0,"Viper presents itself as a plagiarism checker. It was founded in 2007. Viper focuses on all types of customers; the pricing is based on the pay-as-you-go principle. Currently, it is owned by All Answers Limited (2019), which according to the information at the website, gives an impression of an essay mill. It is interesting to see the progress in the way Viper uses the uploaded content on their “Terms and conditions” page. In 2016 the page stated ""[w]hen you scan a document, you agree that 9 months after completion of your scan, we will automatically upload your essay to our student essays database which will appear on one of our network of websites so that other students may use it to help them write their own essays"" (Viper, 2016). The time span was shortened to 3 months some time afterwards (Viper, 2019a). These paragraphs have been removed from the current version of the page (Viper, 2019b). On a different page, it is noted that ""[w]hen you scan your work for plagiarism using Viper Premium it will never be published on any of our study sites"" (Viper, 2019c). In e-mail communication, Viper claims that they are not using any essay without the author's explicit consent."
2002.04279,database,25,2022-05-13,0,Innovation Centre Kosovo (2018). An Albanian Academic Database and a Qualitative AntiPlagiarism System created by Akademia. Retrieved from https://ickosovo.com/news/post/an-albanian-academic-database-and-a-qualitative-antiplagiarism-system-crea
2002.04279,database,90,2022-05-13,0,"Source-based coverage testing was made using four types of sources; Wikipedia, openaccess papers, a student thesis and online articles. For many students, Wikipedia is the starting point for research (Howard & Davies, 2009), and thus can be regarded as one of the primary sources for plagiarists. Since a Wikipedia database is freely available, it is expected that Wikipedia texts should easily be identifiable. Testing the tools with Wikipedia texts demonstrates the fundamental ability to catch text matches."
2002.04279,database,92,2022-05-13,0,"Copyscape declares itself to be a plagiarism checker. The primary aim is to provide a tool for owners of websites to check if their original content was not used by others. They also provide a service of regular checks and email alerts. Copyscape, which started in 2004 (Greenspan, 2019), is operated by a private company, Indigo Stream Technologies Ltd., which is apparently based in Gibraltar. It does not have its own database but uses Google services to crawl the web."
2002.04279,download,138,2022-05-13,0,"The presentation and understandability of the results reported by the systems were evaluated in a second usability criteria group. Since the systems cannot determine plagiarism, the results must be examined by one or more persons in order to determine if plagiarism is present and a sanction warranted. It must be necessary to download the result reports and to be able to locate them again in the system. Some systems rename the documents, assigning internal numbering to them, which makes it extremely difficult to find the report again. Many systems have different formats for online and downloadable reports. It would be useful for the report review if the system kept the original formatting and page numbers of the document being analyzed in order to ease the load of evaluation."
2002.04279,download,88,2022-05-13,0,"None of the systems was able to get the highest score in the usability group related to the test results. Two systems (PlagScan and Urkund) support almost all features, but six systems support half or fewer features. The most supported features are the possibility to download result reports and highlighting matched passages in the online report. Less supported features are a side-by-side demonstration of evidence in the downloaded report and in the online report, as well as keeping document formatting."
2002.04279,publicly available,11,2022-05-13,0,4–5 pages from a publicly available source in the given language
2002.04279,publicly available,125,2022-05-13,0,"Table 2 shows the aggregated results of the language comparisons based on the language sets. It can be seen that most of the systems performed better for English, Italian, Spanish, and German, whereas the results for Latvian, Slovak, Czech, and Turkish languages are poorer in general. The only system which found a Czech student thesis from 2010 which is publicly available from a university webpage, was StrikePlagiarism.com. The Slovak paper in an open-access journal was not found by any of the systems. Urkund was the only system that found an open-access book in Turkish. It is worth noting that a Turkish system, intihal.net, did not find this Turkish source."
2002.04279,publicly available,16,2022-05-13,0,● 4–5 pages from any publicly available source in a given language with 1/3 copy &
2005.10539,data,109,2022-05-13,0,"The second approach was intended to increase the coordination between each instrument, by combining time (i. e., horizontal) and harmony (i. e., vertically) information. This was achieved by changing the data extraction phase from obtaining separately each complete instrument part to obtain every instrument part at every beat. This allowed us to train a set of instruments at the same time from a concrete set of symphonies. This way, the generated parts present a considerable increment of coordination and it is easier to differentiate each musical phrase, as each instrument part respects or accompanies the others."
2005.10539,data,131,2022-05-13,0,"The goal of this work is to generate music, based on Beethoven’s compositional model, obtaining the conductor’s score with all the orchestra instrument’s parts. Two approaches to the goal were established; ﬁrstly, the system was trained with each instrument individually, to generate all the different instrument’s parts, and then put them all together in a conductor’s score. Nevertheless training each part individually lead to a lack of coordination, so a new approach was addressed. This second approach consisted in training the system with information from different instruments at the same time, extracting the data vertically, to maintain the harmony (vertical) and time (horizontal) information."
2005.10539,data,135,2022-05-13,0,"Introduction Romantic composer Ludwig van Beethoven wrote his Symphonies from 1799 to 1824, when he ﬁnished the No. 9 (Cooper 2000). Although there is no constancy of the existence of the 10th Symphony score, there exists some manuscripts found in Beethoven’s house after his death that are thought to be part of the upcoming Symphony. In 1988 Barry Cooper tried to ﬁnish it, building from 50 of those fragments the ﬁrst movement of the Symphony. Those manuscripts are kept in the museum dedicated to his life in his natal city, Bonn, although they can be seen online 1. The public manuscript is not easy to read and understand, so that existing data will not be used in this paper."
2005.10539,data,138,2022-05-13,0,"Ludwig van Beethoven composed his symphonies between 1799 and 1825, when he was writing his Tenth symphony. As we dispose of a great amount of data belonging to his work, the purpose of this paper is to investigate the possibility of extracting patterns on his compositional model from symbolic data and generate what would have been his last symphony, the Tenth. A neural network model has been built based on the Long Short-Therm Memory (LSTM) neural networks. After training the model, the generated music has been analysed by comparing the input data with the results, and establishing differences between the generated outputs based on the training data used to obtain them. The structure of the outputs strongly depends on the symphonies used to train the network."
2005.10539,data,147,2022-05-13,0,"Since the main goal of this paper is to obtain a score including every orchestra instrument’s part, the input ﬁles format were changed to symbolic data in mxl. This extension refers to a compressed music score, which Music21 easily processes. Mxl ﬁles are the compressed format of the so called MusicXML (Good 2001). In order to represent the output of the training, i.e. the weights of the different notes and durations, the model also returns a HDF5 ﬁle, i. e., Hierarchical Data Format version 5, commonly used to store big quantities of data. After the prediction process, given the obtained weights, Music21 allows us to generate the ﬁnal output in MIDI or MusicXML, formats accepted by Musescore (so the score can be visualised and played)."
2005.10539,data,153,2022-05-13,0,"Our ﬁnal network is composed 3 different types of layers. The most relevant ones are the LSTM layers, which take the sequences and return new ones. Then, the Dropout layers prevent overﬁtting, ignoring randomly selected neurons during the training, setting those inputs to 0. The Dense (Density) layer serves as a full connection mechanism. This layer is the last one, so the system returns the same number of outputs as the different numbers of tuples (note name, note duration) the input data had. Finally, the activation function used for every layer is set, and it determines how each node’s output is represented. In this case, the softmax function (i. e., linear activation) is used, allowing the output to be interpreted as a probability between 0 and 1."
2005.10539,data,160,2022-05-13,0,"Technical background Deep learning: LSTM Networks Included in the ﬁeld of Machine Learning, Deep Learning involves the use of artiﬁcial neural networks (Gulli and Pal 2017). There exists several types of neural networks, such as Deep Neural, Deep Brief and Recurrent Neural Networks (RNN). In this paper we work with the last ones, since we need to process sequential data, assuming that each event depends on previous ones. The most accurate RNN variant is the LSTM. As proved with Figure 1, we need the memory that this type of networks own. On it we ﬁnd the sequence F - F - F, a predictor without memory would return another F, although by learning from the notes before, it can extract that after three equal notes, it is probable that the upcoming note is two tones below the last one."
2005.10539,data,247,2022-05-13,0,"Proposed in 1997, LSTM neural networks can learn longterm dependencies, improving the cells or neurons in the RNN graph. They have the ability to connect previous knowledge to a present task. Each cell has memory, and it decides to store or forget a data based on a given priority (i. e., represented as weights), assigned by the algorithm after the learning process. Figure 3 shows a LSTM cell or neuron. The top line represents the ﬂow of the cell state, which can be altered up to three times. The ﬁrst layer, sigmoid (σ), takes information from the previous state and determines if it is useful or not, returning a value between 0 and 1. As it is shown with the vertical arrow, it directly affects to the ﬂow of the cell state. The second layer is composed of the combination of the sigmoid (σ) and tanh functions, which chooses the data to be updated from the previous state, and creates a vector of candidate values to be added to the current cell state. The ﬁnal sigmoid (σ) layer determines the output, by deciding which parts of the state are more relevant. Those will be combined with a tanh function, converting the current state into values between 1 and -1 (Gulli and Pal 2017)."
2005.10539,data,328,2022-05-13,0,"References [Biles 1994] Biles, J. 1994. Genjam: A genetic algorithm In Proc. of the International for generating jazz solos. Computer Music Conference, 131–137. Aarhus, Denmark: ICMC. [Cohen 1995] Cohen, H. 1995. The further exploits of aaron, painter. Stanford Humanities Review 4(2):141–158. [Colton 2012] Colton, S. 2012. The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04."
2005.10539,data,344,2022-05-13,0,"The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04. [Graves, Mohamed, and Hinton 2013] Graves, A.; Mohamed, A.; and Hinton, G. E. 2013. Speech recognition the with deep recurrent neural networks. International conference on acoustics, speech and signal processing, 6645–6649. Vancouver, Canada: IEEE. [Gulli and Pal 2017] Gulli, A., and Pal, S. 2017. Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow. Birmingham, UK: Packt Publishing Ltd."
2005.10539,data,347,2022-05-13,0,"2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor? AI Magazine 30(3):71–71. [Schwanauer and Levitt 1993] Schwanauer, S., and Levitt, D. 1993. Musact: A connectionist model of musical harmony. In Machine Models of Music. Cambridge, MS, USA: The MIT Press. 497–510. [Tao Li 2011] Tao Li, Mitsunori Ogihara, G. T. 2011. Music Data Mining. Boca Ratn, FL, USA: CRC Press. [Thiemel 2001] Thiemel, M. 2001. Dynamics. New Grove online."
2005.10539,data,35,2022-05-13,0,"Once the model is built and the input and output data are ready, it gets trained, generating an hdf5 ﬁle containing the weights (i. e., input notes’ priorities)."
2005.10539,data,35,2022-05-13,0,"Results The system output differs from the information given to the training, although once with the same trained data, the system predicts the same score, which denotes a lack of variability."
2005.10539,data,350,2022-05-13,0,"[Haynes and Cooke 2001] Haynes, B., and Cooke, P. 2001. Pitch. New Grove online. [Hiley 2001] Hiley, D. 2001. Clef (i). New Grove online. [Hiller and Isaacson 1958] Hiller, Jr., L. A., and Isaacson, L. M. 1958. Musical composition with a high-speed digital computer. J. Audio Eng. Soc 6(3):154–160. [Liang and Gotham 2017] Liang, F. T., and Gotham, M. e. a. 2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor?"
2005.10539,data,43,2022-05-13,0,"Training Finally, we can generate the training data (i. e., sequence input and output). By establishing a certain sequence length, the output for each input sequence will be the ﬁrst note that comes after that sequence."
2005.10539,data,76,2022-05-13,0,"For example, setting a sequence length equal to two, the ﬁrst stage of the system’s work ﬂow (i. e., data extraction) for the Figure 5 input would be the shown in Table 1. It is important to take into account that in case of establishing a big sequence length, the model may generalise, while setting a small sequence length may lead to an overlearning problem."
2005.10539,data,84,2022-05-13,0,"Data representation Several ways of representing the Beethoven Symphonies’ scores have been studied for this paper. Firstly, we used MIDI ﬁles as an input for our system, as it is a data ﬁle which contains information about the sounds: what note is played, when, and how long or loud. Figure 4 shows the problem we boarded with the MIDI input. Music21 was unable to differentiate between the different string instrument’s MIDI channels."
2005.10539,"data, dataset",137,2022-05-13,0,"This paper is structured as follows. Previous work on using Artiﬁcial Intelligence in computational creativity and concretely in music generation is exposed in the State of the art section. After that, musical deﬁnitions needed to understand requirements, limitations and characteristics found in the results are introduced. Later, the work developed for this paper is explained in detail, presenting the Deep Learning technique used, the needed toolkits, and how the data was represented. Then, the Music generation subsection is divided in: dataset creation, training, and prediction. The results section is focused on explaining the reason why the system returns a certain output when trained with a speciﬁc set of symphonies. The conclusions and future work are described in the last sections."
2005.10539,"data, dataset, python",314,2022-05-13,0,"Dataset creation As previously mentioned, the Beethoven symphonies have been converted to an mxl ﬁle, which constitutes the dataset or corpus that we have used to obtain the desired results. Also, the instrument or instruments with which the system works has to be established, so the Python module music21 can divide the mxl score into all the present instruments, and take only the desired parts. This way, in the ﬁrst approach, where the goal is to obtain each instrument’s part individually, the note names and durations are stored in an independent ﬁle, being the different tuples of note names and durations the training data. Nevertheless, as the second approach trains the model with a set of chosen instruments at the same time, we need to store, besides the note name and duration, the offset (i. e., time data relating to the moment in which the note is being played regarding the score) and the name of the instrument that plays it. The offset information will be used to sort the data. After making sure that the events are sorted in a time-line as they are in the original score, it can be removed from the dataset, in order to reduce the data dimensionality and to avoid an overlearning problem in the model. This way, the training data will be composed of the different tuples of note names, note durations and the instrument’s name playing it (introduced in the second approach). At this point, a dictionary to encode each data tuple as a number is created, so the neural network can work with it. This dictionary will be also used for the decoding phase, after the prediction."
2005.10539,"data, python, open-source",129,2022-05-13,0,"This project has been developed in Python. Data extracting and processing from the scores has been performed using the python’s library Music21 (Cuthbert and Ariza 2010), which allows parsing and generating scores in different formats. Furthermore, every musical action and representation that we needed to perform, was made possible using that library. For the Deep Learning engine we have used Keras 3 (Gulli and Pal 2017). Finally, in order to manage the score formats, Musescore 4 (i. e., open source program available for every platform) brought us the possibility to import and export the symphonies, so we could see the score and listen to it at the same time."
2005.10539,dataset,114,2022-05-13,0,"Prediction For this task, the network input is generated again, as in the previous process (see Table 1). Since it needs to work over the same model, it is created again, with the same parameters, but now, instead of training the model, it loads the generated weights from the previous process (i. e., the hdf5 ﬁle). It is important at this point that the network input shapes and the loaded weights have the same dimensions. Once the model is ready, the encoding dictionary built during the dataset creation phase is inverted, for decoding the prediction results."
2005.10539,python,123,2022-05-13,0,"In case of the input, reshaping into a 3 dimension matrix is needed so it is compatible with the LSTM layers, using Python’s numpy module. The ﬁrst dimension or shape of the network is the number of unique different sequences (i. e., sequence in in Table 1) obtained in the last step, the second one is the previously established sequence length and ﬁnally the last dimension is forced to be 1, so it has just one input information per sequence length. After that, the software normalises the input into sequential values, from 0 to 1. In case of the output, it is converted into a categorical model."
2010.02554,"code, github",45,2022-05-13,0,The dependencies of parameters in our Pytorch implementation (https://github.com/pmorenoz/RecyclableGP) are clearly shown and evident from the code structure oriented to objects. It is also amenable for the introduction of new covariance functions and more structured variational approximations if needed.
2010.02554,"code, publicly available, github, code available",11,2022-05-13,2,1The code is publicly available in the repository: github.com/pmorenoz/RecyclableGP/.
2010.02554,data,109,2022-05-13,0,"Stationarity and expressiveness. We assume that the non-linear function f is stationary across subsets of data. If this assumption is relaxed, some form of adaptation or forgetting should be included to match the local GPs. Other types of models can be considered for the ensemble, as for instance, with several latent functions (Lázaro-Gredilla and Titsias, 2011) or sparse multi-output GPs (Álvarez and Lawrence, 2011). The model also accepts GPs with increased expressiveness. For example, to get multi-modal likelihoods, we can use mixture of GP experts (Rasmussen and Ghahramani, 2002)."
2010.02554,data,146,2022-05-13,0,"The data D is assumed to be partitioned into an arbitrary number of K subsets that we aim to observe and process independently, that is, {D1, D2, . . . , DK}. There is not any restriction on the amount of subsets or learning nodes. The subsets {Dk}K k=1 do not need to have the same size, and we only restrict them to be Nk<N . However, since we treat with a huge number of observations, we still consider that Nk for all k ∈ {1, 2, . . . , K} is sufﬁciently large for not accepting exact GP inference due to temporal and computational demand. Notice that k is an index while k(·, ·) refers to the kernel."
2010.02554,data,151,2022-05-13,0,"where, once again, φ∗ = {µ∗, S∗} are the global variational parameters that we aim to learn. One important detail of the sum of expectations in (16) is that it works as an average contrastive indicator that measures how well the global q(u∗) is being ﬁtted to the local experts qk(uk). Without the need of revisiting any distributed subset of data samples, the GP predictive qC(uk) is playing a different role in contrast with the usual one. Typically, we assume the approximate posterior ﬁxed and ﬁtted, and we evaluate its performance on some test data points. In this case, it goes in the opposite way, the approximate variational distribution is unﬁxed, and it is instead evaluated over each k-th local subset of inducing-inputs Zk."
2010.02554,data,161,2022-05-13,0,"To obtain multiple independent approximations to the posterior distribution p(f |D) of the GP function, we introduce K variational distributions qk(f ), one per distributed partition Dk. In particular, each variational distribution factorises as qk(f ) = p(f(cid:54)=uk |uk)qk(uk), with qk(uk) = N (uk|µk, Sk) and p(f(cid:54)=uk |uk) being the standard conditional GP prior distribution given the hyperparameters ψk of each k-th kernel. To ﬁt the local variational distributions qk(uk), we build lower bounds Lk on the marginal log-likelihood (ELBO) of every data partition Dk. Then, we use optimisation methods, typically gradient-based, to maximise the K objective functions Lk, one per distributed task, separately. Each local ELBO is obtained as follows"
2010.02554,data,173,2022-05-13,0,"Heterogeneous single-output GP. Extensions to GPs with heterogeneous likelihoods, that is, a mix of continuous and discrete variables yi, have been proposed for multi-output GPs (Moreno-Muñoz et al., 2018). However, there are no restrictions in our single-output model to accept different likelihoods p(yi|f (xi)) per data point {xi, yi}. An inconvenience of the bound in (1), is that, each i-th expectation term could be imbalanced with respect to the others. For example, if mixing Bernoulli and Gaussian variables, binary outputs could contribute more to the objective function than the rest, due to the dimensionality. To overcome this issue, we ﬁt a local GP model to each heterogeneous variable. We join all models together using the ensemble bound in (6) to propagate the uncertainty in a principled way. Although, data-types need to be known beforehand, perhaps as additional labels."
2010.02554,data,20,2022-05-13,0,DATA ST. (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55)
2010.02554,data,214,2022-05-13,0,"4.3 Heterogeneous tasks. We analysed how ensembles of recyclable GPs can be used if one of the local tasks is regression and the other a GP classiﬁer. (viii) London household data: We have two subsets of input-output variables: the binary contract of houses (leasehold vs. freehold) and the price per latitude-longitude coordinate in the London area. Three quadrants (Q) of the city {Q2, Q3, Q4} are trained with a GP classiﬁer and Q1 as regression. To clarify, Q1 is the right-upper corner given the central axes. Our purpose is to combine the local latent uk, learned with the binary data on {Q2, Q3, Q4} and the uk learned on Q1 via regression. Then, we search the global f to be predict with a Bernoulli likelihood in Q1. The ensemble shows a test NLPD of 7.94 ± 0.01 in classiﬁcation while the recyclable task predicts with an NLPD of 8.00 ± 0.01 in the Q1. We asses that the heterogeneous GP prediction is better in Q1 than the local GP classiﬁer. The mean GP of regression is passed through the sigmoid function to show the multimodality."
2010.02554,data,22,2022-05-13,0,where yt and ft are the true output target and function values. Nt is the number of test data points.
2010.02554,data,27,2022-05-13,0,"J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in Artiﬁcial Intelligence (UAI), pages"
2010.02554,data,290,2022-05-13,0,"A priori, the ensemble GP bound is agnostic with respect to the likelihood model. There is a general derivation in Matthews et al. (2016) of how stochastic processes and their integral operators are affected by projection functions, that is, different linking mappings of the function f (·) to the parameters θ. In such cases, the local lower bounds Lk in (1) might include expectation terms that are intractable. Since we build the framework to accept any possible data-type, we propose to solve the integrals via Gaussian-Hermite quadratures as in Hensman et al. (2015); Saul et al. (2016) and if this is not possible, an alternative would be to apply Monte-Carlo methods. Computational cost and connections. The computational cost of the local models is O(NkM 2 k ), while the global GP reduces to O(((cid:80) k Mk)M 2) and O(M 2) in training and prediction, respectively. The methods in Table 1 typically need O((cid:80) k ) for global prediction. A last theoretical aspect is the link between the global bound in (6) and the underlying idea in Tresp (2000); Deisenroth and Ng (2015). Distributed GP models are based on the application of CI to factorise the likelihood terms of subsets. To approximate the posterior predictive, they combine local estimates, divided by the GP prior. It is analogous to (6), but in the logarithmic plane and the variational inference setup."
2010.02554,data,3,2022-05-13,0,DATA SIZE →
2010.02554,data,339,2022-05-13,0,"All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−5, ηL = 10−7, ηψ = 10−8, ηZ = 10−7} and we used M = 25 inducing-inputs."
2010.02554,data,34,2022-05-13,0,"In our experiments with toy data, we used two versions of the same sinusoidal function, one of them with an incremental bias. The true expressions of f (·) are"
2010.02554,data,343,2022-05-13,0,"viii) London household data: Based on the large scale experiments in Hensman et al. (2013), we obtained the register of properties sold in the Greater London county during the 2017 year (https://www.gov.uk/government/collections/ price-paid-data). All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information."
2010.02554,data,349,2022-05-13,0,"In terms of distributed inference for scaling up computation, that is, the delivery of calculus operations across parallel nodes but not data or independent models, we are similar to Gal et al. (2014). Their approach can be understood as a speciﬁc case of our framework. Alternatively, if we look to the property of having nodes that contain usable GP models (Table 1), we are similar to Deisenroth and Ng (2015); Cao and Fleet (2014) and Tresp (2000), with the difference that we introduce variational approximation methods for non-Gaussian likelihoods. An important detail is that the idea of exploiting properties of full stochastic processes (Matthews et al., 2016) for substituting likelihood terms in a general bound has been previously considered in Bui et al. (2017) and Moreno-Muñoz et al. (2019). Whilst the work of Bui et al. (2017) ends in the derivation of expectation-propagation (EP) methods for streaming inference in GPs, the introduction of the reparameterisation of Gal et al. (2014) makes our inference and performance different from Moreno-Muñoz et al. (2019). There is also the inference framework of Bui et al. (2018) for both federated and continual learning, but focused on EP and the Bayesian approach of Nguyen et al. (2018). A short analysis of its application to GPs is included for continual learning settings but far from the large-scale scope of our paper. Moreover, the spirit of using inducing-points as pseudo-approximations of local subsets of data is shared with Bui and Turner (2014), that comments its potential application to distributed setups. More oriented to dynamical modular models, we ﬁnd the work by Velychko et al. (2018), whose factorisation across tasks is similar to Ng and Deisenroth (2014) but oriented to state-space models."
2010.02554,data,45,2022-05-13,0,"We highlight several use cases for the proposed framework. The idea of recycling GP models opens the door to multiple extensions, with particular attention to the local-global modelling of heterogeneous data problems and the adaptation of model complexity in a data-driven manner."
2010.02554,data,48,2022-05-13,0,"Then, if we have (10), which is the ﬁrst version of our ensemble lower bound LE , we can use the augmented likelihood term p(y|f∞) to introduce the local approximations to f instead of revisiting the data. This is,"
2010.02554,data,52,2022-05-13,0,"We introduced a novel framework for building global approximations from already ﬁtted GP models. Our main contribution is the construction of ensemble bounds that accept parameters from regression, classiﬁcation and heterogeneous GPs with different complexity without revisiting any data. We analysed its performance on synthetic and real data with"
2010.02554,data,67,2022-05-13,0,"Model recycling and use cases. The ability of recycling GPs in future global tasks have a signiﬁcant impact in behavioral applications, where ﬁtted private-owned models in smartphones can be shared for global predictions rather than data. Its application to medicine is also of high interest. If one has personalized GPs for patients, epidemiologic surveys can be built without centralising private data."
2010.02554,data,70,2022-05-13,0,"In this paper, we investigate a general framework for recycling distributed variational sparse approximations to GPs, illustrated in Figure 1. Based on the properties of the Kullback-Leibler divergence between stochastic processes (Matthews et al., 2016) and Bayesian inference, our method ensembles an arbitrary amount of variational GP models with different complexity, likelihood and location of pseudo-inputs, without revisiting any data."
2010.02554,data,8,2022-05-13,0,Appendix C. Combined Ensemble Bounds with Unseen Data
2010.02554,data,85,2022-05-13,0,"A common theme in the previous approaches is the idea of model memorising and recycling, i.e. using the already ﬁtted parameters in another problem or joining it with others for an additional global task without revisiting any data. If we look to the functional view of this idea, uncertainty is still much harder to be repurposed than parameters. This is the point where Gaussian process (GP) models (Rasmussen and Williams, 2006) play their role."
2010.02554,data,89,2022-05-13,0,"Data-driven complexity and recyclable ensembles. One of the main advantages of the recyclable GP framework is that it allows data-driven updates of the complexity. That is, if an ensemble ends in a new variational GP model, it also can be recycled. Hence, the number of global inducing-variables M can be iteratively increased conditioned to the amount of samples considered. A similar idea was already commented as an application of the sparse order-selection theorems by Burt et al. (2019)."
2010.02554,data,9,2022-05-13,0,"Data Privacy and Conﬁdentiality at NeurIPS, 2019."
2010.02554,data,9,2022-05-13,0,Figure 2: Recyclable GPs with synthetic data.
2010.02554,"data, code, data available",178,2022-05-13,2,"In this section, we evaluate the performance of our framework for multiple recyclable GP models and data access settings. To illustrate its usability, we present results in three different learning scenarios: i) regression, ii) classiﬁcation and iii) heterogeneous data. All experiments are numbered from one to nine in roman characters. Performance metrics are given in terms of the negative log-predictive density (NLPD), root mean square error (RMSE) and mean-absolute error (MAE). We provide Pytorch code that allows to easily learn the GP ensembles.1 It also includes the baseline methods. The syntax follows the spirit of providing a list of recyclable_models = [GP1, GP2, GP3], where each GPk contains exclusively parameters of the local approximations. Further details about initialization, optimization and metrics are in the appendix. Importantly, we remark that data is never revisited and its presence in the ensemble plots is just for clarity in the comprehension of results."
2010.02554,"data, code, python, github",121,2022-05-13,2,"The code for the experiments is written in Python 3.7 and uses the Pytorch syntax for the automatic differentiation of the probabilistic models. It can be found in the repository https://github.com/pmorenoz/RecyclableGP, where we also use the library GPy for some algebraic utilities. In this section, we provide a detailed description of the experiments and the data used, the initialization of both variational parameters and hyperparameters, the optimization algorithm for both the local and the global GP and the performance metrics included in the main manuscript, e.g. the negative log-predictive density (NLPD), the root mean square error (RMSE) and the mean absolute error (MAE)."
2010.02554,"data, data available",121,2022-05-13,0,"Local likelihood reconstruction. The augmented likelihood distribution is perhaps, the most important point of the derivation. It allows us to apply conditional independence (CI) between the subsets of distributed output targets. This gives a factorized term that we will later use for introducing the local variational experts in the bound, that is, log p(y|f∞) = (cid:80)K k=1 log p(yk|f∞). To avoid revisiting local likelihood terms, and hence, evaluating distributed subsets of data that might not be available, we use the Bayes theorem but conditioned to the inﬁnite-dimensional augmentation. It indicates that the local variational distributions can be approximated as"
2010.02554,"data, dataset",108,2022-05-13,0,"As we already mentioned in the manuscript, there might be scenarios where it could be not necessary to distribute the whole dataset D in K local tasks or, for instance, a new unseen subset k + 1 of observations might be available for processing. In such case, it is still possible to obtain a combined global solution that ﬁts both to the local GP approximations and the new data. For clarity on this point, we rewrite the principal steps of the ensemble bound derivation in section A but without substituting all the log-likelihood terms by its Bayesian approximation, that is"
2010.02554,"data, dataset",166,2022-05-13,0,"One of the most desirable properties for any modern machine learning method is the handling of very large datasets. Since this goal has been progressively achieved in the literature with scalable models, much attention is now paid to the notion of efﬁciency. For instance, in the way of accessing data. The fundamental assumption used to be that samples can be revisited without restrictions a priori. In practice, we encounter cases where the massive storage or data centralisation is not possible anymore for preserving the privacy of individuals, e.g. health and behavioral data. The mere limitation of data availability forces learning algorithms to derive new capabilities, such as i) distributing the data for federated learning (Smith et al., 2017), ii) observe streaming samples for continual learning (Goodfellow et al., 2014) and iii) limiting data exchange for private-owned models (Peterson et al., 2019)."
2010.02554,"data, dataset",230,2022-05-13,1,"v) Pixel-wise MNIST classiﬁcation: We took images of ones and zeros from the MNIST dataset. To simulate a pixel-wise unsupervised classiﬁcation problem, true labels of images were ignored. Instead, we threshold the pixels to be greater or smaller than 0.5, and labeled as yi = 0 or yi = 1. That is, we turned the grey-scaled values to a binary coding. Then, all pixels were described by a two-dimensional input in the range [−1.0, 1.0], that indicates the coordinate of each output datum. In the case of the zero image, we splitted the data in four areas, i.e. the four corners, as is shown in the subﬁgure (A) of Figure 4. Each one of the local tasks was initialized with an equally spaced grid of Mk = 16 inducing-inputs. The ensemble GP required M = 25 in the case of the number zero and M = 16 for the one. The plotted curves correspond to the test GP predictive posterior at the probit levels [0.2, 0.5, 0.8]. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,"data, dataset",311,2022-05-13,0,"1.1 Background. The ﬂexible nature of GP models for deﬁnining prior distributions over non-linear function spaces has made them a suitable alternative in many probabilistic regression and classiﬁcation problems. However, GP models are not immune to settings where the model needs to adapt to irregular ways of accessing the data, e.g. asynchronous observations or missings input areas. Such settings, together with GP model’s well-known computational cost for the exact solutions, typically O(N 3) where N is the data size, has motivated plenty of aproaches focused on parallelising inference. Regarding the task of distributing the computational load between learning agents, GP models have been inspired by local experts (Jacobs et al., 1991; Hinton, 2002). Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al."
2010.02554,"data, dataset",320,2022-05-13,0,"ii) Distributed GPs: In this second experiment, our goal is to compare the performance of the recyclable framework with the distributed GP methods in the literature (Tresp, 2000; Ng and Deisenroth, 2014; Cao and Fleet, 2014; Deisenroth and Ng, 2015). To do so, we begin by generating toy samples from the sinusoidal function f (x). The comparative experiment is divided in two parts, in one, we observe N = 103 and in the other, N = 104 input-output data points. In the ﬁrst case, we splitted the dataset D in K = 50 tasks with Nk = 200 and Mk = 3 per partition. Any of these distributed subsets were overlapping, and their corresponding input-spaces concatenated perfectly in the range x ∈ [0.0, 5.5]. For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm."
2010.02554,"data, dataset",322,2022-05-13,0,"Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al. (2014) presented a new re-parameterisation of the lower bounds that allows to distribute the computational load accross nodes, also applicable to GPs with stochastic variational inference (Hensman et al., 2013) and with non-Gaussian likelihoods (Hensman et al., 2015; Saul et al., 2016). Out of the sparse GP approach and more inspired in Tresp (2000) and product of experts (Bordley, 1982), the distributed GPs of Deisenroth and Ng (2015) scaled up the parallelisation mechanism of local experts to the range of N >106. Their approach is focused on exact GP regression, not considering classiﬁcation or other non-Gaussian likelihoods. Table 1 provides a description of these different methods and their main properties, also if each distributed node is a GP model itself."
2010.02554,"data, dataset",330,2022-05-13,0,"Ideally, to obtain a global inference solution given the GP models included in the dictionary, the resulting posterior distribution should be valid for all the local subsets of data. This is only possible if we consider the entire data set D in a maximum likelihood criterion setting. Speciﬁcally, our goal now is to obtain an approximate posterior q(f ) ≈ p(f |D) by maximising a lower bound LE under the log-marginal likelihood log p(D) without revisiting the data already observed by the local models. We begin by considering the full posterior distribution of the stochastic process, similarly as Burt et al. (2019) does for obtaining an upper bound on the KL divergence. The idea is to use inﬁnite-dimensional integral operators that were introduced by Matthews et al. (2016) in the context of variational inference, and previously by Seeger (2002) for standard GP error bounds. The use of the inﬁnite-dimensional integrals is equivalent to an augment-and-reduce strategy (Ruiz et al., 2018). It consists of two steps: i) we augment the model to accept the conditioning on the inﬁnite-dimensional stochastic process and ii) we use properties of Gaussian marginals to reduce the inﬁnite-dimensional integral operators to a ﬁnite amount of GP function values of interest. Similar strategies have been used in continual learning for GPs (Bui et al., 2017; Moreno-Muñoz et al., 2019). Global objective. The construction considered is as follows. We ﬁrst denote y as all the output targets {yi}N i=1in the dataset D and f∞ as the augmented inﬁnite-dimensional GP. Notice that f∞ contains all the function values taken by f (·), including that ones at {xi}N k=1 for all partitions. The augmented log-marginal expression is therefore"
2010.02554,"data, dataset",350,2022-05-13,0,"For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm. The setup of the VEM algorithm during the ensemble ﬁtting was {VE = 30, VM = 10, ηm = 10−3, ηL = 10−6, ηψ = 10−8, ηZ = 10−8}. As in the previous experiment with toy data, we set M = 35 inducing-inputs. iii) Recyclable ensembles: For simulating potential scenarios with at least N = 106 input-output data points, we used the setting of the previous experiment, but with K = 5 · 103 tasks of Nk = 800 instead. However, as explained in the paper, its performance was hard to evaluate in the baseline methods, due to the problem of combining bad-ﬁtted GP models. Then, based on the experiments of Deisenroth and Ng (2015) and the idea of building ensembles of ensembles, we set a pyramidal way for joining the distributed local GPs. It was formed by two layers, that is, we joined ensembles twice as shown in the Figure 5 of this appendix."
2010.02554,"data, dataset, data https, data available",327,2022-05-13,1,"The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g. K(cid:29)103, it is problematic for baseline methods, due to partitions must be revisited for building predictions and if one-of-many GP fails, performance decreases. Then, we repeated the experiment in a pyramidal way. That is, building ensembles of recyclable ensembles, inspired in Deisenroth and Ng (2015). Our method obtained {NLPD=4.15, RMSE=2.71, MAE=2.27}. The results in Table 3 indicate that our model is more robust under the concatenation of approximations rather than overlapping them in the input space. (iv) Solar physics dataset: We tested the framework on solar data (available at https://solarscience.msfc.nasa.gov/), which consists of more than N =103 monthly average estimates of the sunspot counting numbers from 1700 to 1995. We applied the mapping log(1 + yi) to the output targets for performing Gaussian regression. Metrics are provided in Table 2, where std. values were small, so we do not include them. The perfomance with 50 tasks is close to the baseline solutions, but without storing all distributed subsets of data."
2010.02554,"data, dataset, data https, data available",332,2022-05-13,0,"4.1 Regression. In our ﬁrst experiments for variational GP regression with distributed models, we provide both qualitative and quantitative results about the performance of recyclable ensembles. (i) Toy concatenation: In Figure 2, we show three of ﬁve tasks united in a new GP model. Tasks are GPs ﬁtted independently with Nk=500 synthetic data points and Mk=15 inducing variables per distributed task. The ensemble ﬁts a global variational solution of dimension M =35. Notice that the global variational GP tends to match the uncertainty of the local approximations. (ii) Distributed GPs: We provide error metrics for the recyclable GP framework compared with the state-of-the-art models in Table 3. The training data is synthetic and generated as a combination of sin(·) functions (in the appendix). For the case with 10K observations, we used K=50 tasks with Nk=200 data-points and Mk=3 inducing variables in the sparse GP. The scenario for 100K is similar but divided into K=250 tasks with Nk=400. Our method obtains better results than the exact distributed solutions due to the ensemble bound searches the average solution among all recyclable GPs. The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g."
2010.02554,"data, dataset, data https, used dataset",206,2022-05-13,1,"iv) Solar physics dataset: We used the solar physics dataset (https://solarscience.msfc.nasa.gov/) which consists of N = 3196 samples. Each input-output data point corresponds to the monthly average estimate of the sunspot counting numbers from 1700 to 1995. The output targets y were transformed to the real-valued domain via the mapping log(yi + 1) to use a normal likelihood distribution. We also scaled the input area to the range x ∈ [0, 100] and normalized the outputs to be zero-mean. The number of tasks was K = 50 and 20% of the data observations were reserved for test. The initial values of kernel and likelihood hyperparameters was {(cid:96) = 0.2, σ2 n is the initial likelihood variance, that we also learn. In this case, the setup of the VEM algorithm was {VE = 20, VM = 20, ηm = 10−5, ηL = 10−8, ηψ = 10−10, ηZ = 10−10}. The number of global inducing-inputs used for the ensemble was M = 90, whilst we used Mk = 6 for each distributed approximation."
2010.02554,"data, dataset, dataset provided, data available",109,2022-05-13,0,"In practice, it might not be necessary to distribute the whole dataset D in parallel Recyclable GPs and new data. tasks, with some subsets Dk available at the global ensemble. It is possible to combine the samples in Dk with the dictionary of local GP variational distributions. In such cases, we would only approximate the likelihood terms in (3) related to the distributed subsets of samples. The resulting combined bound would be equivalent to (6) with an additional expectation term on the new data. We provide the derivation of this combined bound in the supplementary material."
2010.02554,dataset,156,2022-05-13,0,"We consider a supervised learning problem, where we have an input-output training dataset D = {xi, yi}N i=1 with x ∈ Rp. We assume i.i.d. outputs yi, that can be either continuous or discrete variables. For convenience, we will refer to the likelihood term p(y|θ) as p(y|f ) where the generative parameters are linked via θ = f (x), being f (·) a non-linear function drawn from a zero-mean GP prior f ∼ GP(0, k(·, ·)), and k(·, ·) is the covariance function or kernel. Importantly, when non-Gaussian outputs are considered, the GP output function f (·) might need an extra deterministic mapping Φ(·) that transforms it to the appropriate parametric domain of θ."
2010.02554,dataset,20,2022-05-13,0,Table 2: Performance metrics for distributed GP regression with the solar physics dataset. (std. ×102)
2010.02554,dataset,205,2022-05-13,1,"vii) Banana dataset: The banana experiment is perhaps one of the most used datasets for testing GP classiﬁcation models. We followed a similar strategy as the one used in the MNIST experiment. After removing the 33% of samples for testing, we partitioned the input-area in four quadrants, i.e. as is shown in Figure 4. For each partition we set a grid of Mk = 9 inducing-inputs and later, the maximum complexity of the global sparse model was set to M = 25. The baseline GP classiﬁcation method also used M = 25 inducing-inputs and obtained an NLPD value of 7.29 ± 7.85 × 10−4 after ten trials with different initializations. Our method obtained a test NLPD of 7.21 ± 0.04. As we mentioned in the main manuscript, the difference is understandable as the recyclable GP framework used a total amount of 4 × 16 inducing-inputs, that capture more uncertainty than the 16 of the baseline method. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,dataset,97,2022-05-13,0,"The construction of ensemble variational bounds from recyclable GP models is based on the idea of augmenting the marginal likelihood to be conditioned on the inﬁnite-dimensional GP function f∞. Notice that f∞ contains all the function values taken by f (·) over the input-space Rp, including the input targets {xi}N k=1 and the global ones Z∗. Having K partitions of the dataset D with their corresponding outputs y = {y1, y2, . . . , yK }, we begin by augmenting the marginal log-likelihood as"
2010.02554,"dataset, used dataset",218,2022-05-13,1,"of Van der Wilk et al. (2017), we threshold images of zeros and ones to black and white pixels. Then, to simulate a pixel-wise learning scenario, we used each pixel as an input-output datum whose input xi contains the two coordinates (x1 and x2 axes). Plots in Figure 4 illustrate that a predictive ensemble can be built from smaller pieces of GP models, four corners in the case of the number zero and two for the number one. (v) Compositional number: As an illustration of potential applications of the recyclable GP approach, we build a number eight predictor using exclusively two subsets of the approximations learned in the previous experiment with the image of a zero. The trick is to shift Zk to place the local approximations in the desired position. (vi) Banana dataset: We used the popular dataset in sparse GP classiﬁcation for testing our method with M =25. We obtained a test NLPD= 7.21 ± 0.04, while the baseline variational GP test NLPD was 7.29 ± 7.85×10−4. The improvement is understandable as the total number of inducing points, including the local ones, is higher in the recyclable GP scenario."
2010.09647,code,123,2022-05-13,0,"ing unconstrained. The base measure problem occurs, in a sense, but the remedy is both local and internal to the implementation of Stan. The latter Stan pushes on the user: if a user writes a Stan model with a parameter x and a transformed parameter f (x), and wishes to code a density on f (x) that corresponds to the pushforward of some known density p(x), then that user must think about base measures and partial derivatives themselves. Stan does not help; but then again, it also does not claim it would help, so Stan doesn’t compute anything visibly “incorrect”."
2010.09647,code,89,2022-05-13,0,"We have named the Base Measure Problem and provided a solution to it. Implementing the solution in probabilistic programming systems should cause negligible loss of performance for cases that were already correctly handled, and expand the set of models in which the system can compute correct probability densities. Implementation does carry a code complexity cost, but that cost is minimized by using two-argument dispatch, or emulating it with a Visitor pattern. Despite correctly accounting for measures, no non-local information is required."
2010.09647,data,80,2022-05-13,0,"The root of the base measure problem is that we didn’t want to compute with measures directly, but lost the base measure when representing probability distributions with densities. It’s not actually possible to infer the correct base measure from the data type representing the sample: a point on the unit circle in R2 is represented with two ﬂoating-point numbers, but using Lebesgue measure on R2 as the base is not helpful."
2010.09647,package,88,2022-05-13,0,"The Jacobian-determinant correction 1/| det(Jfx)| accounts for the possibility that f changes the volume of an inﬁnitesimal volume element near x. The Jacobian determinant can be computed by forming the Jacobian of f , for example with automatic diﬀerentiation; but for many functions f , it’s available more eﬃciently. Thus a conventional choice is to package such f , together with their inverse f −1, in a Bijector class with a method for computing said Jacobian determinant."
2011.00242,code,100,2022-05-13,0,"Therefore, in all applications, there are indications, in the form of source code, comments, and issues, that developers demand an effort to understand and improve cache keys, reasoning about alternative better ways to organize and identify content. We identiﬁed common content properties used to build cache keys, which are: (i) content ids, (ii) method signatures, and (iii) a tag-based identiﬁcation, in which the type of content, the class, the module or hierarchy are used (Evidence 19)."
2011.00242,code,11,2022-05-13,0,Category (Acronym) Code Scattering and Tangling (CST)
2011.00242,code,12,2022-05-13,0,Complex Naming Conventions (CNC) Additional Caching Code (ACC)
2011.00242,code,120,2022-05-13,0,"Given that implementing cache is challenging, we noticed that in six applications developers made use of supporting libraries and frameworks. This was done to prevent adding much cache-related code to the base code, because such components raise the abstraction level of caching, providing some ready-to-use features (Evidence 22). Examples of such external components are distributed cache systems, e.g. Redis [27] and Memcached [26], and libraries that can act locally, e.g. Spring Caching [51], EhCache [52], Inﬁnispan [53], Rails low-level caching [54], Google Guava [55] and Caffeine [56]."
2011.00242,code,152,2022-05-13,0,"First, we assigned concepts to pieces of extracted text (open coding), each representing application-level caching characteristics. Figure 2 exempliﬁes different codes identiﬁed during the analysis of an application. For instance, Code Tangling is created from the observation of cache-related implementation spread all over the application base code (underlined). Then, for each new concept, we veriﬁed whether they are connected somehow with existing ones, in order to generate categories (selective coding). Thus, the name assigned to a particular category aims at representing, at a higher abstraction level, all concepts related to it. Regarding the Code Tangling example, if Code Scattered were found afterwards, we could establish a relationship between the former and the latter to create a category, given that both are related to lack of separation of concerns."
2011.00242,code,161,2022-05-13,0,"[7] M. P. Robillard, W. Coelho, and G. C. Murphy, “How effective developers investigate source code: An exploratory study,” IEEE Transactions on Software Engineering, vol. 30, no. 12, pp. 889–903, 2004. [Online]. Available: http://dx.doi.org/10.1109/TSE.2016. 2532873 J. Sillito, G. C. Murphy, “Asking and answering questions during a programming change task,” in IEEE Transactions on Software Engineering, vol. 34, [Online]. Available: no. 4. http://dx.doi.org/10.1109/TSE.2008.26 S. Nadi, T. Berger, C. K¨astner, and K. Czarnecki, “Where do conﬁguration constraints stem from? An extraction approach and an empirical study,” IEEE Transactions on Software Engineering, vol. 41, no. 8, pp. 820–841, aug 2015. [Online]. Available: http://dx.doi.org/10.1109/TSE.2015.2415793"
2011.00242,code,17,2022-05-13,0,"to implementation issues of application-level caching, by providing solutions and guidance at the code level."
2011.00242,code,19,2022-05-13,0,A code comment before an expensive operation: TODO cache the global properties to speed this up??
2011.00242,code,21,2022-05-13,0,Labels of Evidence Sources: SC-Source Code (without comments); COM-Code Comments; IS-Issues; DOC-Documentation; DEV-Developers.
2011.00242,code,23,2022-05-13,0,A code comment: Is hibernate taking care of caching and not hitting the db every time? (hopefully it is)
2011.00242,code,236,2022-05-13,0,"As can be seen in Figure 3, caching implementation and associated design decisions are much more discussed and revised by developers than maintenance decisions. Caching implementation, which is spread in the code and involves the choice for appropriate locations to add and remove elements from the cache, is error-prone and can compromise code legibility. Consequently, many issues are associated with bug ﬁxes, technological details and code refactorings. Moreover, despite being less frequent, caching design is time-consuming and challenging, given that it requires understanding of the application behavior, as well as limitations, conditions and restrictions of content being cached. In applications analyzed, the mean (M) and standard deviation (SD) values of cache-related implementation issues are M = 47.45% and SD = 5.76%, while design issues achieve M = 37.92% and SD = 7.11%. Finally, because ﬁnegrained conﬁgurations require empirical analysis such as cache proﬁling, and there is little evidence that this was performed in investigated applications, maintenance decisions often result in the choice for default settings. Consequently, a lower number of issues is associated with such decisions, speciﬁcally M = 14.61% and SD = 3.44%. This issue analysis allowed us to understand the aspects of caching that require more effort from developers."
2011.00242,code,30,2022-05-13,0,"Code comments. Given that caching is an orthogonal concern in the application, unrelated to the business logic, but interleaved with its code, code comments are often"
2011.00242,code,36,2022-05-13,0,"reasoning, time, and modiﬁcations to the code (Evidence 14). We discussed our ﬁndings regarding cache design decisions and we next discuss observations made associated with how to implement design decisions."
2011.00242,code,38,2022-05-13,0,"We performed an analysis of the issues available in issue platforms to investigate the primary sources of cacherelated problems, typically bugs, in the applications. Based on user messages, code reviews and commit messages that"
2011.00242,code,43,2022-05-13,0,"A code snippet exposing a test of caching logic: it ”can set and get false values when return cache nil” do @store.set :test, false expect(@store.get(:test)).to be false end"
2011.00242,code,46,2022-05-13,0,"Before addressing each of our research questions, we show an objective analysis of the impact of caching in the investigated applications by identifying all code and issues related to them. This gives a broad sense of how caching is implemented in target systems."
2011.00242,code,48,2022-05-13,0,"Source code. Application source code is our core source of information. Since we focus on application-level caching, our analysis is concentrated in the core of the application (i.e. the business logic), which is where the caching logic is typically implemented."
2011.00242,code,49,2022-05-13,0,A code snippet: cache id = ’objectmodel ’ . $entity defs[’classname’] . ’ ’ . (int)$id . ’ ’ . (int)$id shop . ’ ’ . (int)$id lang;
2011.00242,code,56,2022-05-13,0,"Keep the cache API simple. (Evidence 15), (Evidence 17), (Evidence 20) and (Evidence 22) Caching logic tends to be spread all over the application, and a good solution should be employed to avoid writing messy code at the cost of high maintenance efforts."
2011.00242,code,59,2022-05-13,0,"Furthermore, in cases where updates in the base code are not an option (due to time or technical restrictions), a transparent and automatic caching component can provide fast results. These solutions address layers before and after application boundaries, and require only a few adaptations to the application needs (Evidence 24)."
2011.00242,code,60,2022-05-13,0,"All these caching design options may become complex and difﬁcult to understand. Indeed, identifying caching opportunities and ensuring consistency can add much code and may not be trivial to implement and understand. Due to the nature of application-level caching, such logic is spread all over the system. We noticed in 90% of the applications"
2011.00242,code,60,2022-05-13,0,"Such supporting libraries and frameworks not only provide partial ready-to-use features but also reduce the amount of additional effort required to guarantee that the cache is working. We observed in all applications that the cache includes code dedicated to test, debug and conﬁgure cache components, which can be expensive in some scenarios (Evidence 23)."
2011.00242,code,63,2022-05-13,0,"A developer quote: At ﬁrst glance, the cache code hinders the understanding of the business logic. Also, the cache logic itself it is not easy to get. A bug report on an issue platform: When you import categories with a parent category which does not exist, it prevents from duplicate it because of the cache."
2011.00242,code,87,2022-05-13,0,"Although this problem is present in the code, there are indications that developers know about it and express they are willing to improve the provided solution. In order to reduce the impact of an infrastructure component to the system business logic, we identiﬁed cases where there are suggestions to design more extensible classes and modules, refactoring and reducing cache-related code, and reusing components (Evidence 17). This acknowledgment of technical debt was observed in 90% of the applications."
2011.00242,code,91,2022-05-13,0,"such as design patterns, third-party libraries or aspects? 22 How is the caching logic mixed with the application code? 23 Is this extra cache logic tested? 24 What is the required format to cache? 25 How are objects translated to the cache? 26 How are names (keys) deﬁned for cached objects? 27 Do developers use another caching layer besides application-level? 28 Is any transparent or automatic caching component being used? 29 Do developers rely on automatic caching components? 30"
2011.00242,code,96,2022-05-13,0,"Presence of complex constructs such as batch processing and asynchronous communication, which require extra effort and reasoning from developers to be implemented. Indication of use of third-party caching solutions to help the implementation, raising the level of abstraction of some caching aspects. Choice for complex keys of caching content, causing developers to spend time and effort to elaborate and understand such keys. the Code implemented to support caching logic, such as implemented caching tests, logic to monitor cache statistics and additional interfaces to support available caching providers."
2011.00242,"code, github",72,2022-05-13,0,"Issues. An issue can represent a software bug, a project task, a help-desk ticket, a leave request form, or even user messages about the project in general. Usually, changes in the code are due to registered issues. Thus, implementation and design decisions are better explained by associated issues in issue platforms, such as GitHub Issue Tracker, JIRA, and Bugzilla."
2011.00242,"code, open-source",68,2022-05-13,0,"To identify patterns of application-level caching adopted by developers and understand what kinds of caching implementations and decisions can be automatically inferred, characterize and evaluate application-level caching-related design and implementation from a perspective of the researcher as they are implemented in the source code and described in issues of web-based applications in the context of 10 software projects, obtained from open-source repositories and software companies."
2011.00242,data,102,2022-05-13,0,"The high number of occurrences related to ensuring consistency refers to the expiration process of cached content, which requires extra reasoning from developers, because they should track which changes cause data content to become outdated, and be aware for how long the cache can provide stale data, in case the data source has been updated. In fact, consistency approaches have been widely investigated [4], [6], and the typical way of dealing with it is to analyze data dependency, from which conditions and constraints for consistency can be derived."
2011.00242,data,106,2022-05-13,0,"Avoid caching per-user data. (Evidence 4) and (Evidence 26) It is recommended to avoid caching per-user data unless the user base is small and the total size of the cached data does not require an excessive amount of memory; otherwise, it can cause a memory bottleneck. However, if users tend to be active for a while and then go away again, caching per-user data for short-time periods may be an appropriate approach. For instance, a search engine that caches query results by each user, so that it can page through results efﬁciently."
2011.00242,data,107,2022-05-13,0,"Regarding implementation choices, we observed common practices. The ﬁrst is associated with how to name cached data. In order to use in-memory caching solutions, there is no prescribed way to organize data. Typically, unique names are assigned to each cached content, thus leading to a key-value model—and this was the case in all investigated applications. Given that cache stores lots of data, the set of possible names must be large; otherwise, two names (keys) can conﬂict with each other and, thus stale (or even entirely wrong) data can be retrieved from"
2011.00242,data,11,2022-05-13,0,RQ1. What and when is data cached at the application
2011.00242,data,110,2022-05-13,0,"Despite choosing where and what to cache, cached values are valid only as long as the sources do not change, and when sources change, a consistency policy should be employed to ensure that the application is not serving stale data. Therefore, in nine analyzed applications, there are indications that developers demand an effort to design consistency approaches, reasoning about the lifetime of cached data, as well as eviction conditions and constraints. We identiﬁed common approaches to keep consistency, which are: (i) a less efﬁcient and straightforward approach is to invalidate cached values based on mapping actions that"
2011.00242,data,127,2022-05-13,0,"The theoretical categories are described in Table 5, which presents their description, an original piece of content (example of evidence) and sources of data classiﬁed in each category. Moreover, categories are also shown in Figure 5a with the associated number of occurrences in the applications analyzed and classiﬁed according to each research question. Figure 5b shows the percentage of contribution of each application to the categories emerged from the study. These ﬁgures also present categories identiﬁed in RQ2 (described in Table 6) and RQ3 (described in Table 7), which are discussed in the following sections. Acronyms used in Figures 5a and 5b are introduced in Tables 5, 6, and 7."
2011.00242,data,13,2022-05-13,0,Is there a relationship between the data cached and the application domain?
2011.00242,data,13,2022-05-13,0,RQ1. What and when is data cached at the application level?
2011.00242,data,131,2022-05-13,0,"Do not discard small improvements. (Evidence 3), (Evidence 8) and (Evidence 9) The user perceived latency is reduced by any caching solution employed. This means that even not obvious scenarios should be target of caching, i.e. it is not true that solely data that is frequently used and expensive to retrieve or create should considered for caching. Furthermore, data that is expensive to retrieve and is modiﬁed on a periodic basis can still improve performance and scalability when properly managed. Caching data even for a few seconds can make a large difference in high volume sites. If the data is handled more often than it is updated, it is also a candidate for caching."
2011.00242,data,136,2022-05-13,0,"Following the introduced phases of data analysis, we performed mainly a subjective analysis of the data, collecting: (i) typical caching design, implementation and maintenance strategies; (ii) motivations, challenges and problems behind caching, and (iii) characteristics of caching decisions. These collected data were evaluated to conceptualize how the open codes were related to each other as a set of hypotheses in accounting for resolving the primary concern. Furthermore, we also made a broad analysis of the target systems in order to investigate how application-level caching was conceived in them. All the research phases were performed manually, as the collected data (most of them expressed in natural language) analysis is associated with the interpretation of caching approaches."
2011.00242,data,14,2022-05-13,0,4.2 RQ1: What and when is data cached at the application level?
2011.00242,data,141,2022-05-13,0,"7 DISCUSSION AND CONCLUSION Application-level caching has been increasingly used in the development of web applications, in order to improve their response time given that they are becoming more complex and dealing with larger amounts of data over time. Caching has been used in different locations, such as proxy servers, often as seamless components. Applicationlevel caching allows caching additional content taking into account application speciﬁcities not captured by off-theshelf components. However, there is limited guidance to design, implement and manage application-level caching, which is often implemented in an ad-hoc way. In this paper, we presented a qualitative study performed to understand how developers approach application-level caching. The study consisted of the selection ten web applications and investigation of caching-related aspects, namely design, implementation and maintenance practices."
2011.00242,data,141,2022-05-13,0,"performance via adaptive content the Proceedings of Computing and Applications. [Online]. Available: http://dx.doi.org/10.1109/NCA.2011.55 [20] G. Soundararajan and C. Amza, “Using semantic information to improve transparent query caching for dynamic content web sites,” in Proceedings of the International Workshop on Data Engineering Issues in E-Commerce, vol. 2005. IEEE, 2005, pp. 132– 138. [Online]. Available: http://dx.doi.org/10.1109/DEEC.2005.25 [21] C. Amza, G. Soundararajan, and E. Cecchet, “Transparent caching with strong consistency in dynamic content web sites,” in Proceedings of international conference on Supercomputing. New York, New York, USA: ACM Press, jun 2005, p. 264. [Online]. Available: http://dx.doi.org/10.1145/ 1088149.1088185"
2011.00242,data,150,2022-05-13,0,"From all analyzed applications, we observed that none of them use a proactive approach to cache content. Content is always cached after it requested (i.e. reactive approach) and, as a consequence, the ﬁrst request always results in a cache miss. Due to this, prefetching techniques can be used in order to populate the cache and prevent misses by predicting and caching data that will potentially be requested in the future. It can be based on heuristics, usage observations or even with the use of complex prediction algorithms [15], [32], [58], [59]. However, the design and implementation of a reactive cache component already requires signiﬁcant effort and reasoning to be properly done, and a proactive approach increases the complexity of the caching solution even more."
2011.00242,data,16,2022-05-13,0,Deﬁne naming conventions Perform cache actions asynchronously Do not use cache as data storage Perform measurements
2011.00242,data,18,2022-05-13,0,"Design of some kind of consistency approach such as expiration policies and invalidation, preventing stale data."
2011.00242,data,183,2022-05-13,0,"the content being cached should be considered when using size-limited caches. In this case, an adequate trade-off between popularity (hits) and size of the items must be achieved. Keeping small popular items in the cache tends to optimize hit-ratio; however, a hit in a large item may be more beneﬁcial for an application than many hits on small items. At the same time, ﬁlling the cache with few large items may turn the cache performance dependent on a good replacement policy. Evaluate staleness and lifetime of cached data. (Evidence 2), (Evidence 3) and (Evidence 12) Every piece of cached data is already potentially stale, it is important to rethink the degree of integrity and potential staleness that the application can compromise for increased performance and scalability. Many cache implementations adopted an expiration policy to invalidate cached data based on a timeout since weak consistency is easier than deﬁning a hard-to-maintain, but more robust, invalidation process. In short, developers must"
2011.00242,data,186,2022-05-13,0,"Even though these approaches focus on providing an adaptive behavior to application-level cache, none of them takes application speciﬁcities into account to autonomously manage their target. They attempt only to optimize cache decisions based on cache statistics like hit-ratio and access patterns at a higher level, thus, ignoring cache meta-data expressed by application-speciﬁc characteristics, which are closely related to application computations and could help to reach an optimal performance of the caching service on time. Addressing this issues, content-aware approaches have been proposed [45], [46] for admitting and replacing items in the cache by exploring descriptive caching hits in the application model or spatial locality (relationships among web objects). Though adaptive caching is in general new and innovative, it is far from being adopted as standard practice in software industry; many improvements must be achieved before this can happen, such as reducing the overhead in terms of resource consumption and processing time of the learning process, and providing easy ways to integrate application and caching method."
2011.00242,data,189,2022-05-13,0,"The last issue is related to the maintenance of the cache system, which involves several aspects such as determining replacement policies and the size of the cache. According to Radhakrishnan [31], a traditional approach to maintain cache is to set up strategies based on initial assumptions or well-accepted characteristics of workload and access patterns. Thus, cache statistics such as hit ratio, the number of objects cached, and average object size can be collected by observing the application and cache at run-time. These data, compared in the context of desired values of these parameters, can be used to decide whether to change the ﬁrst choices. Usually, this process is repeated until an acceptable trade-off between conﬁguration and performance is reached, and then cache strategies are ﬁxed for the lifetime of the product or at least for several release cycles. However, its performance may decay over time due to changes in the workload characteristics and access patterns. Therefore, achieving acceptable application performance requires constantly tuning cache decisions, which implies extra time"
2011.00242,data,19,2022-05-13,0,Fig. 6. Cacheability ﬂowchart: intuitive process to decide whether to cache or not particular data.
2011.00242,data,19,2022-05-13,0,Guideline Evaluate different abstraction levels to cache Stack caching layers Separate dynamic static data Evaluate boundaries Specify selection criteria
2011.00242,data,198,2022-05-13,0,"issues about whether to cache some speciﬁc data or not, showing that the connection between observed bottlenecks in the application and opportunities for caching is not straightforward and requires a deeper analysis (Evidence 1). Therefore, selection criteria based on common sense or past experiences are initial assumptions to ease such decisions. Despite these criteria are usually unjustiﬁed, they guide developers while selecting content. We observed in 90% of the applications the deﬁnition of selection criteria to make the distinction of cacheable from uncacheable content easier. These criteria were observed in explanations of cache design choices in comments, issues, and documentation. We identiﬁed common criteria used to determine whether to cache or not a speciﬁc content, which are: (i) content change frequency (Evidence 2), (ii) content usage frequency (Evidence 3), (iii) content shareability (Evidence 4), (iv) content retrieval complexity (Evidence 5), (v) content size (Evidence 6), and (vi) size of the cache (Evidence 7)."
2011.00242,data,2,2022-05-13,0,Data Expiration
2011.00242,data,21,2022-05-13,0,A sentence in documentation: We use Redis [a thirdparty solution] as a cache and for transient data.
2011.00242,data,221,2022-05-13,0,"Abstract—Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users’ demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on speciﬁc details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications."
2011.00242,data,257,2022-05-13,0,"Specify selection criteria. (Evidence 1), (Evidence 2), (Evidence 3), (Evidence 5), (Evidence 6) and (Evidence 7) Selecting the right data to cache involves a great reasoning effort given that data manipulated by web applications range in dynamicity, from being completely static to changing constantly. To optimize this selection process, there are four primary selection criteria used by developers while detecting cacheable content, which should be used in decisions regarding whether to cache. These criteria are described below, ordered according to their importance; i.e. the higher the inﬂuence level, the earlier it is presented. Data change frequency. Developers should seek for data that have some degree of stability, i.e. those that are more used than changed. Even if data are volatile and change in time intervals, caching still brings a beneﬁt. This is the ﬁrst factor to be considered since caching volatile data implies the implementation of consistency mechanisms, which is not trivial and requires an extra effort and reasoning from developers. In short, the cost of consistency approaches cannot be higher than the beneﬁt of caching. Besides, when stale data is not a critical issue, an approach of weak consistency can be employed, such as time-to-live (TTL) eviction, where data is expired after a time in cache, regardless of possible changes."
2011.00242,data,29,2022-05-13,0,"Data retrieval complexity. Data that is expensive to retrieve, compute, or render, regardless of its dynamicity, is always considered a good caching opportunity."
2011.00242,data,35,2022-05-13,0,"order to avoid this, we followed a systematic analysis of our data, and conclusions are all founded on these data. Moreover, cross-checks were performed using our different sources of evidence."
2011.00242,data,39,2022-05-13,0,"ensure that the expiration policy matches the pattern of access to applications that use the data, which is based on determining how often the cached information is allowed to be outdated, and relaxing freshness when possible."
2011.00242,data,44,2022-05-13,0,7 What kinds of bottlenecks are addressed by developers? 8 What motivated the need for explicit caching manipulation? 9 What is the granularity of the cached objects? 10 What is the importance of the cached data to the application? 11
2011.00242,data,44,2022-05-13,0,"Separate dynamic from static data. (Evidence 3) and (Evidence 2) Content can be distinguished in static, dynamic, and user-speciﬁc. By partitioning the content, it is easier to select portions of the data to cache."
2011.00242,data,46,2022-05-13,0,Evaluate staleness and lifetime of cached data Avoid caching per-user data (Evidence 4) and (Evidence 26) DG-07 Avoid caching volatile data (Evidence 2) and (Evidence 3) DG-08 DG-09 Do not discard small provements Keep the cache API simple
2011.00242,data,5,2022-04-21,0,Data usage frequency. Frequent
2011.00242,data,55,2022-05-14,0,"[4] D. R. K. Ports, A. T. Clements, I. Zhang, S. Madden, and B. Liskov, “Transactional Consistency and Automatic Management in an Application Data Cache,” Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation, pp. 279–292, oct 2010."
2011.00242,data,56,2022-05-14,0,"Occurrences of multiple caching solutions, which can involve different third-party components or different application layers. As a consequence, the same content can be cached at different places in varying forms. Occurrences of caching content being added to the cache because they retrieve data from frameworks, libraries, or external applications."
2011.00242,data,57,2022-05-14,0,"6 THREATS TO VALIDITY We now analyze the possible threats to the validity of this study, and how we mitigated them. Researcher bias is a typical threat to qualitative research because the results are subject to the researcher interpretation of the data. In our study, prior knowledge might have inﬂuenced results. In"
2011.00242,data,59,2022-05-14,0,1 What are the motivations to employ cache? 2 What are the typical use scenarios? 3 Where and when data is cached? 4 What are the constraints related to data cached? 5 What are the selection criteria adopted to detect cacheable content? 6 Do developers adopt a pattern to decide which content should be
2011.00242,data,59,2022-05-14,0,"[29] K. Nguyen and G. Xu, “Cachetor: detecting cacheable data the 9th Joint Meeting on to remove bloat,” in Proceedings of Foundations of Software Engineering. New York, New York, USA: ACM Press, aug 2013, p. 268. [Online]. Available: http://dx.doi.org/10.1145/2491411.2491416"
2011.00242,data,59,2022-05-14,0,"of the application-level caching is not trivial and demands high effort, because it involves manual implementation by developers. Its design and maintenance involve four key challenging issues: determining how to cache the selected data, what data should be cached, when the selected data should be cached or evicted, and where the cached data"
2011.00242,data,6,2022-05-14,0,Controller layer. Caching data at
2011.00242,data,64,2022-05-14,0,"Due to the increasing demand and complexity involved with caching, developers try to decrease cache-related effort by adopting simple design solutions. We observed in seven of the ten analyzed applications design choices such as managing consistency based on expiration time, keeping default parameters, selecting data without any criteria, or even adopting external solutions, which do not claim extra"
2011.00242,data,68,2022-05-14,0,"Finally, determining maintenance strategies to manage efﬁciently where data is placed, such as replacement policies or size of the cache, requires additional knowledge and reasoning from application developers. Nevertheless, there are no foundations to make this choice adequately. Therefore, RQ3 complements the analysis above, questioning how application speciﬁcities are leveraged to provide the desired performance to the cache system."
2011.00242,data,69,2022-05-14,0,"Avoid caching volatile data. (Evidence 2) and (Evidence 3) Data should be cached when it is frequently used and is not continually changing. Developers should remember that caching is most effective for relatively stable data, or data that is frequently read. Caching volatile data, which is required to be accurate or updated in real time, should be avoided."
2011.00242,data,72,2022-05-14,0,"time-consuming, error-prone and, consequently, a common source of bugs [25]. Gupta et al. [6] and Ports et al. [4] both address these implementation issues by providing high-level caching abstractions, in which developers can simply designate application functions as cacheable, and the proposed system automatically caches their results and invalidates the cached data when the underlying source changes."
2011.00242,data,75,2022-05-14,0,"operations, requests, queries and shared content (accessed by multiple users) must be identiﬁed, focusing on recomputation avoidance. Even if some processing can be fast enough at a glance, it can potentially become a bottleneck when being invoked many times. Despite being frequently used, user-speciﬁc data cannot be shared and may not bring the beneﬁt of caching, being usually left out of the cache."
2011.00242,data,77,2022-05-14,0,"Do not use cache as data storage. (Evidence 27) An application can modify data held in a cache, but the cache should be considered as a transient data store that can disappear at any time. Therefore, developers should not save valuable data only in the cache, but keep the information where it should be as well, minimizing the chances of losing data if the cache unexpectedly becomes unavailable."
2011.00242,data,8,2022-05-14,0,Business or service layer. Caching data at
2011.00242,data,8,2022-05-14,0,Size of the data. The size of
2011.00242,data,81,2022-05-14,0,"[25] W. Wang, Z. Liu, Y. Jiang, X. Yuan, and J. Wei, “EasyCache: a transparent in-memory data caching approach for internetware,” in Proceedings of the 6th Asia-Paciﬁc Symposium on Internetware on Internetware. New York, New York, USA: ACM Press, nov 2014, pp. 35–44. [Online]. Available: http://dx.doi.org/10.1145/ 2677832.2677837 [26] “Memcached,” 2016."
2011.00242,data,84,2022-05-14,0,"Our study was designed based on comparative and interactive principles of grounded theory [47]. The purpose of grounded theory is to construct theory grounded in data by identifying general concepts, develop theoretical explanations that reach beyond the known, and offer new insights into the area of study. The systematic procedures of grounded theory enable qualitative researchers to generate ideas. In turn, these ideas can be later studied and veriﬁed through traditional quantitative forms of research."
2011.00242,data,88,2022-05-14,0,"Determining the cacheable content and the right moment of caching or clearing the cache content are a developer’s responsibility and might not be trivial in complex applications, motivating RQ1. In this research question, we aim to identify what data is selected to be cached, and the criteria used to detect such cacheable data. Furthermore, this question also explores when data should be evicted, as well as constraints, consistency conditions, and the rationale for all these choices."
2011.00242,data,91,2022-05-14,0,"By using grounded theory, we aimed to construct a wellintegrated set of hypotheses that explain how the concepts operated. Thus, the selective coding involves identifying the core category that best explains how study data refers to a large portion of the variation in a pattern and is considered the primary concern or problem related to the study, integrating closely related concepts. Finally, theoretical codes conceptualize how the codes may relate to each other as hypotheses to be incorporated into the theory [47]."
2011.00242,data,99,2022-05-14,0,"12 How much memory does the cached data consume? 13 What data is most frequently accessed? 14 How often is the cached data going to be used and changed? 15 What data is expensive? 16 What data depends on user sessions? 17 How up to date does the data need to be? 18 How is consistency assurance implemented? Why was it chosen? 19 Where and when is consistency assurance employed? 20 Which kind of operation/behavior affects cache consistency? 21 Do developers employ any technique to ease caching implementation,"
2011.00242,"data, code",107,2022-05-14,0,"4 ANALYSIS AND RESULTS This section details the results of our study and their analysis, according to the research questions we aim to answer. Our collected data consist mainly of source code and issues (expressed in natural language) and, as these are qualitative data, we have undertaken a subjective analysis of the application-level caching (hereafter referred to simply as “caching”) aspects represented in the target systems. Note that we labeled some ﬁndings with “Evidence X,” so that we can later refer to them to support the guidelines we derived from this analysis."
2011.00242,"data, code",108,2022-05-14,0,"We observed a higher number of categories, which are classes of observations made based on the analysis of these applications, associated with caching design and implementation than those associated with maintenance. Furthermore, the number of occurrences of each category, design and implementation categories also have higher numbers. This phenomenon was expected since the most representative portion of our qualitative data consists of source code and issues. Moreover, simple maintenance tasks and conﬁgurations are already executed and provided by external components, being commonly adopted. However, the number of occurrences do not reﬂect the importance of a category."
2011.00242,"data, code",135,2022-05-14,0,"The second and third issues refer to deciding the right content to cache and the best moment of caching or clearing the cache in order to avoid cache thrashing and stale content. These decisions involve (i) choosing the granularity of cache objects, (ii) translating between raw data and cache objects, and (iii) maintaining cache consistency, which are all tasks to be accomplished by developers and might not be trivial in complex applications. Della Toffola et al. [5], Nguyen and Xu [29] and Infante [30] address part of these issues by identifying and suggesting caching opportunities. However, developers should still review the suggestions and refactor the code, integrating cache logic into the application."
2011.00242,"data, code",139,2022-05-14,0,"To understand how developers integrate application-level caching logic in their applications, we analyzed the cache implementations from two perspectives. The ﬁrst consists of examining the explicit caching logic present in the application code, focusing on analyzing where the caching logic is placed, for what this logic is responsible, and when it is executed. The second evaluates the integration and communication between the application and an external caching component, which is usually used to relieve the burden on developers, easing cache implementation, raising abstraction levels, or even taking full control of caching. For this question, the most valuable data comes from source code and comments, which express implementation details. The theoretical categories referring to RQ2 are described in Table 6 and shown in Figure 5."
2011.00242,"data, code",158,2022-05-14,0,"should be placed and maintained. Although applicationlevel caching is commonly being adopted, it is typically implemented in an ad hoc way, and there are no available practical guidelines for developers to appropriately design, implement and maintain caching in their applications. To ﬁnd caching best practices, developers can make use of conventional wisdom, consult development blogs, or simply search online for tips. Nevertheless, this empirical knowledge is unsupported by concrete data or theoretical foundations that demonstrate its effectiveness in practice. Despite there are existing tool-supported approaches that can help developers to implement caching with minimal impact on the application code [4], [5], [6], they do not consider all the aforementioned issues and do not take application speciﬁcities into account, letting most part of the reasoning, as well as the integration with the tool, to developers."
2011.00242,"data, code",165,2022-05-14,0,"As previously described, this study is mainly based on development information of web applications. To investigate different caching constructs, we followed a set of steps to perform our qualitative study: (i) selection of a set of suitable web applications; (ii) speciﬁcation of a set of questions to guide us in the data analysis and (iii) analysis of each web application using the speciﬁed questions. The collected data consists of six different sources of information, explained as follows. Information about the application. Our goal is to identify caching patterns or decisions, which possibly depend on the application domain. Therefore, we collected general details of the applications to characterize them. The collected application data is (i) its description; (iii) programming languages and technologies involved; and (iii) size of the application in terms of the number of lines of code."
2011.00242,"data, code",243,2022-05-14,0,"We followed the analytical process of coding in our analysis [47], which makes it easier to search the data and identify patterns. This process combines the data for themes, ideas and categories, labeling similar passages of text with a code label, allowing them to be easily retrieved at a later stage for further comparison and analysis. We used what we learned from the analysis to adapt our evaluation approach and observation protocols. Insights we had while coding the data and clustering the codes were captured in memos. There are three coding phases in classical grounded theory: open coding, selective coding, and theoretical coding. Open coding generates concepts from the data that will become the building blocks for the theory [47]. The process of doing grounded theory is based on a concept indicator model of constant comparisons of incidents or indicators to incidents [50]. Indicators are actual data, such as behavioral actions and events observed or described in documents and interviews. In this case, an indicator may be an architectural style or design pattern adopted to implement the cache, a data structure, a class, a control ﬂow logic, a comment, a discussion in the issues platform, a paragraph in the documentation, or any other evidence we can get from the data being analyzed."
2011.00242,"data, code",50,2022-05-14,0,"We observed in eight from the ten analyzed applications that developers indicated that they had uncertainty in cache design, with respect to deciding what data should be cached and the right moment to do it, causing missed caching opportunities. There are comments in the source code and"
2011.00242,"data, code",80,2022-05-14,0,"We observed in eight from the ten analyzed applications that they present code scattering and tangling, on caching logic, causing low cohesion and high coupling in the code. Caching control code, responsible for caching data in particular places, was spread all over the base code, being invoked when application requests were processed. Consequently, there is a lack of separation of concerns, leading to increased maintenance effort (Evidence 15)."
2011.00242,"data, code",82,2022-05-14,0,"In addition to design issues, as shown in Figure 1, the cache system and the underlying source of data are not aware of each other, and the application must implement ways to interact with the cache. Therefore, our goal with RQ2 is to characterize patterns of how this implementation occurs in the application code; for example, ways to assigning names to cached values, performing lookups, and keeping the cache up to date."
2011.00242,"data, code",92,2022-05-14,0,"In this question, we focus on going beyond the facts exposed by source code and analyze the reasoning behind caching decisions such as what and why data is cached or evicted, when and where caching is done, and what are the conditions and constraints involved with this. Therefore, issues, code comments, and documentation about the cache of applications were the primary source of information to ﬁnd answers to this question, because they convey (in natural language) the rationale behind implementation decisions."
2011.00242,"data, code",93,2022-05-14,0,"Based on data presented in Table 4, we can observe a signiﬁcant amount of lines of code dedicated to implementing caching, ranging from 0.27% to 3.02%. It shows the importance of the caching in the project, considering that caching is a solution for a non-functional requirement (i.e. scalability and performance). Furthermore, caching logic is presented in a substantial amount of ﬁles, from 2.06% to 10.76%, which indicates the caching nature of being spread all over the application."
2011.00242,"data, code, database",210,2022-05-14,0,"Model or database layer. At the model or database layer, a large amount of data can be cached, for lengthy periods. It is useful to cache data from a database when it demands a long time to process queries, avoiding unnecessary round-trips. Stack caching layers. (Evidence 9) and (Evidence 10) It is reasonable to say that the more data cached, the lower the chance of being hit without any content already loaded. Caching might be at the client, proxy server, inside the application in presentation, business, and model logics, or database. Despite the same data may be cached in multiple locations, when the cache expires in one of them, the application will not be hit with an entirely uncached content, avoiding processing and network round trips. However, it is important to keep in mind that caching layers imply a range of responsibilities, such as consistency conditions and constraints, and extra code and conﬁguration. Due to this, it is important to consider many caching layers but, at the same time, achieve a good trade-off between caching beneﬁts and implementation effort."
2011.00242,"data, code, database",237,2022-05-14,0,"A code comment detailing the motivation for caching: Fetch all dashboard data. This can be an expensive request when the cached data has expired, and the server must collect the data again. A user announcement on issue platform: Caching has now landed: Fragment caching for each product; Fragment caching for the lists of products in home/index and products/index; Caching in the ProductsController, using expires in which caches for 15 minutes. A sentence in the documentation: Most stores spend much time serving up the same pages over and over again. [...] In such cases, a caching solution may be appropriate and can improve server performance by bypassing time-consuming operations such as database access. A user message on issue platform detailing the invalidation approach adopted: Ideally we cache until the topic changes (a new post is added, or a post changed) [...] less ideally we cache for N minutes A user message on issue platform: i can see that it may speed up the query responses, but is the saved time substantial enough to be worth the effort? A code snippet deﬁning a default setting: <defaultCache maxElementsInMemory=“1000” eternal=“false” timeToIdleSeconds=“60” timeToLiveSeconds=“0” overﬂowToDisk=“false” diskPersistent=“false”/>"
2011.00242,"data, code, database, data available",282,2022-05-14,0,"As already discussed in the introduction, the development of application-level caching involves four key issues: determining how, what, when, and where to cache data. The main problem is that solutions for all of these issues usually depend on application-speciﬁc details, and are manually designed and implemented by developers, as shown in the example presented in Listing 1. In this example, we assume that an e-commerce application where a class named ProductsRepository is responsible for loading products from database. To reduce database workload, caching logic is inserted in this class within the methods getProducts, updateProduct, and deleteProduct, which retrieves, updates and deletes content from the database, respectively. The ﬁrst issue is related to the cache-aside implementation and the fact that the cache system and the underlying source of data are not aware of each other (as illustrated in Figure 1). The cache system itself is a passive component, and developers must implement, directly in the application code, ways to assign names to cached values, perform lookups, and keep the cache up to date. The extra logic also requires additional testing and debugging time, which can be expensive in some scenarios. The implementation and maintenance of application-level caching are a challenge because developers must refactor all the data access logic to encapsulate cache data into the proper application-level object, and its direct management leads to a concern spread all over the system—mixed with business code—which leads to increasing complexity and maintenance time. Thus, this on-demand and manual caching process can be very"
2011.00242,"data, data available",115,2022-05-14,0,"Developers. In addition to the manual inspection of the above data, we asked developers to which we had access about caching-related implementation, decisions, challenges, and problems. To guide the extraction of the information needed from the above data for our qualitative analysis, we derived sub-questions for each research question, which convey characteristics that should be observed and evaluated while looking for answers to the main research questions, i.e. sub-questions were used to extract data. Results presented in Section 4 are derived from answers to those questions. Table 2 depicts the sub-questions derived, which serve as a checklist while analyzing our data."
2011.00242,"data, data available",120,2022-05-14,0,"Moreover, we observed that all analyzed web applications did not adopt caching since their conception. As they increased in size, usage and performance analysis were performed and led to requests for improvements. Thus, developers had to refactor data access logic to encapsulate cache data into the proper application-level object, which is a task that can be very time-consuming, error-prone and, consequently, a common source of bugs. As result, we found a signiﬁcant number of issues speciﬁcally related to caching, achieving the maximum of 4.99% of the Pencilblue issues, which can express in numbers the impact of caching in the entire project (Evidence 15)."
2011.00242,"data, data https, data available",32,2022-05-14,0,"caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 859–874, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2005.89"
2011.00242,"data, data https, data available",48,2022-04-21,0,"[38] Q. Yang and H. H. Zhang, “Web-log mining for predictive web caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 1050–1053, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2003.1209022"
2011.00242,"data, data https, database, data available",56,2022-05-14,0,"[24] P. Larson, J. Goldstein, and J. Zhou, “MTCache: Transparent mid-tier database caching in SQL server,” Proceedings of the International Conference on Data Engineering, vol. 20, pp. 177–188, mar 2004. [Online]. Available: http://dx.doi.org/10.1109/ICDE. 2004.1319994"
2011.00242,"data, database",138,2022-05-14,0,"Evaluate different abstraction levels to cache. (Evidence 9) and (Evidence 11) It is important to cache data where it reduces the most processing power and round trips, choosing locations that support the lifetime needed for the cached items, despite where it is located in the application. Different levels of caching provide different behavior, and possibilities must be analyzed. For instance, caching in the model or database level offers higher hit ratios, while caching in presentation layer can reduce the application processing overhead signiﬁcantly in the application in case of a hit. However, in the latter case, hit ratios are in general lower. It is possible to cache data at various layers of an application, according to the following layer-by-layer considerations."
2011.00242,"data, database",74,2022-05-14,0,"the business layer should be considered if an application needs to process requests from the presentation layer or when the data cannot be efﬁciently retrieved from the database or another service. It can be implemented by using hash tables, library or framework. However, at this level, a large amount of data tends to be manipulated and caching it can consume more memory and leads to memory bottlenecks."
2011.00242,"data, database",84,2022-05-14,0,"layer should be considered when data needs to be frequently displayed to the user and is not cached on a peruser basis. At this level, controllers usually work by serving parameterized content, which can be used as an identiﬁer in the cache. For example, if a list of states is presented to the user, the application can load these once from the database and then cache them, according to the parameters passed in the ﬁrst request."
2011.00242,"data, database",89,2022-05-14,0,"Recently, latency and cost of Internet-based services are driving the proliferation of application-level caching, which is placed on the server-side and typically uses a key-value inmemory store system to cache frequently accessed or expensive to compute data that remain not cached in other caching levels, lowering the load on database or services that are difﬁcult to scale up [3], [4], [5]. Therefore, application-level caching has become a popular technique for enabling highly scalable web applications."
2011.00242,"data, database, data available",237,2022-05-14,0,"Figure 1 illustrates an example where an applicationlevel cache is used to lower the application workload. First, the application receives an HTTP request (Step 1) from the web infrastructure. This HTTP request is eventually treated by an application component called M1, which in turn depends on M2. However, M2 can be an expensive operation (i.e. request the database, call a service or process a large amount of data). Therefore, M1 implements a caching layer, which veriﬁes whether the necessary processing is already in the cache before calling M2 (Step 2). Then, the cache component performs a look up for the requested data and returns either the cached result or a not found error (Step 3). If data is found, it means a cache hit and M1 can continue its execution without calling M2. If, however, a not found error is returned, it means a cache miss happened, then M2 needs to be invoked (Steps 4 and 5). The newly fetched result of M2 can be cached to serve future requests faster (Step 6) and eventually a response is sent to the user (Step 7). Furthermore, other caching layers can be deployed outside the application, at the web infrastructure."
2011.00242,"data, database, data https",67,2022-05-14,0,"[16] K. S. Candan, W.-S. Li, Q. Luo, W.-P. Hsiung, and D. Agrawal, “Enabling dynamic content caching for database-driven web sites,” Proceedings of the ACM SIGMOD International Conference on Management of Data, vol. 30, no. 2, pp. 532–543, jun 2001. [Online]. Available: http://dx.doi.org/10.1145/376284.375736"
2011.00242,"data, database, dataset provided",156,2022-05-14,0,"Regarding design choices, we observed common practices. The ﬁrst is associated with the lack of a speciﬁc approach to cache data. To process a client request, application components of distinct layers and other systems (databases, web services, and others) are invoked, and each interaction results in data transformation, which is likely cacheable. Due to this, nine analyzed applications present multiple caching solutions by not specifying cacheable layers, com ponents or data, and employing cache wherever can potentially provide performance and scalability beneﬁts (Evidence 8), no matter which is the application layer, component or data (Evidence 9). As a consequence, the same content can be cached at different places, from the database to controllers, in varying forms such as query results, page fragments or lists of objects (Evidence 10)."
2011.00242,"data, database, dataset provided",185,2022-05-14,0,"2 BACKGROUND AND RELATED WORK Focused on server-side, caching of dynamic web content can be implemented at several locations across a web-based system architecture [15]. Depending on where caching is implemented, different types of content can be stored, such as the ﬁnal HTML page [16], intermediate HTML or XML fragments [17], [18], [19], database queries [20], [21], [22], [23], or database tables [24]. These alternatives are conceived out of application boundaries and can cache dynamic data automatically, which provide transparent caching components for developers. However, they do not take application speciﬁcities into account, providing good results in general but are less effective where complex logic and personalized web content are processed within the application [25]. Therefore, application-level caching is an appealing technique to improve performance, reduce workload and make the overall user experience more pleasurable by reducing communication delays of web-based systems."
2011.00242,"data, dataset provided",200,2022-05-14,0,"Classiﬁcation: Design Intent: provide an intuitive process to decide whether to cache or not particular data. Problem: cache has limited size, so it is important to use the available space to cache data that maximizes the beneﬁts provided to the application. Otherwise, it can end up reducing application performance instead of improving it, consuming more cache memory and at the same time suffering from cache misses, where the data is not getting served from cache but is fetched from the source. Solution: even though there are many criteria that contribute for identifying the level of data cacheability, there is a subset that would conﬁrm this decision regardless of the values of the other criteria. Changeability is the ﬁrst criterion that should be analyzed while selecting cacheable data, then usage frequency, shareability, retrieval complexity, and cache properties should be considered. Figure 6 expresses a ﬂowchart of the reasoning process to decide whether to cache data, based on the observation of data and cache properties. All criteria are tightly related to the application speciﬁcities and should be speciﬁed by the developer."
2011.00242,"data, dataset provided",37,2022-05-14,0,"asynchronously with caching. Provide an intuitive process decide to whether to cache or not particular data. Given cacheable content, provide an intuitive process to choose a consistency management approach based on data speciﬁcities."
2011.00242,"data, dataset provided",85,2022-05-14,0,"Deﬁne naming conventions. (Evidence 19) and (Evidence 18) To deﬁne appropriate names for cached data, it is important to assign a name that is related to its context, the data itself, and the caching location. It can provide two direct beneﬁts: (a) prevention of key conﬂicts, and (b) guidance of cache actions such as updates and deletes of stale data in case of changes in the source of information."
2011.00242,"data, dataset, dataset provided",314,2022-05-14,0,"Rules of thumb: (a) Despite being frequently used, user-speciﬁc data are not shareable and may not bring the beneﬁt of caching, being usually avoided by developers. In this case, a speciﬁc session component is used to keep and retrieve user sessions. (b) If the data changes frequently, it should not be immediately discarded from cache. An evaluation of the performance beneﬁts of caching against the cost of building the cache should be done. Caching frequently changing data can provide beneﬁts if slightly stale data is allowed. (c) Expensive spots (when much processing is required to retrieve or create data) are bottlenecks that directly affect application performance and should be cached, even though it can increase complexity and responsibilities to deal with. Methods with high latency or that consists of a large call stack are some examples of this situation and opportunities for caching. In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects."
2011.00242,"data, dataset, dataset provided",338,2022-05-14,0,"In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects. Unless the size of the cache is large enough, do not cache large objects, it will probably result in a cache trashing problem, where the caching component is ﬂushed or replaced many times in a short period. Example: we list some typical scenarios where data should be cached and also give explanations based on the criteria presented. (a) Headlines. In most cases, headlines are shared by multiple users and updated infrequently. (b) Dashboards. Usually, much data need to be gathered across several application modules and manipulated to build a summarized information about the application. (c) Catalogs. Catalogs need to be updated at speciﬁc intervals, are shared across the application, and manipulated before sending the content to the client. (d) Metadata/conﬁguration. Settings that do not frequently change, such as country/state lists, external resource addresses, logic/branching settings and tax deﬁnitions. (e) Historical datasets for reports. Costly to retrieve or create and does not need to change frequently."
2011.00242,database,150,2022-05-14,0,"caching at a granularity best suited to the application, providing a way to cache entire HTML pages, page fragments, database queries or even computed results; rather than laying in front of the application servers (e.g., a proxy-level cache) or between the application and the database (e.g., a query or database cache). For example, many websites have highly-personalized content, thus rendering wholepage web caches is mostly useless; application-level caches can be used to separate shared content from customized content, and then the shared content can be cached and reused among users [4]. Memcached [26] and Redis [27] are popular solutions and are a critical web infrastructure component for some big players such as Amazon, Facebook, LinkedIn, Twitter, Wikipedia, and YouTube [28]."
2011.00242,database,36,2022-05-14,0,"components is a common bottleneck and, consequently, an opportunity for caching. Consider caching for database queries, remote server calls and requests to web services, which are made across a network."
2011.00242,database,76,2022-05-14,0,"The second design choice is associated with the concern of reducing the communication latency between the application and other systems, which increases the overall response time of a user request. Therefore, we noticed caching in application boundaries in nine of the analyzed applications, addressing remote server calls and requests to web services, database queries, and loads dependent on ﬁle systems, which are common bottlenecks (Evidence 11)."
2011.00242,"database, code, github",162,2022-05-14,0,"GitHub, the widely known common code hosting service. We selected GitHub projects that match the following criteria: (i) projects with some popularity (at least 350 stars); (ii) projects containing application-level caching implementation and issues (at least 50 occurrences of cache-related aspects); (iii) projects written in different programming languages; and (iv) projects of different domains. The ﬁrst criterion indicates that projects are interesting and were possibly evolved by people other than the initial developers. The second ensures that selected projects would present caching-related implementation and issues, which would contribute more to the study. The inclusion of applications was not restricted to any particular technology or programming language given that the study is focused on identifying language-independent caching patterns and guidelines expressed in the source code. Furthermore, we found applications that use cache in other architectural (e.g. database"
2011.00242,github,19,2022-05-14,0,"[55] “Google Guava,” 2016. [Online]. Available: https://github.com/"
2011.00242,github,9,2022-05-14,0,[Online]. Available: https://github.com/
2011.00242,open-source,113,2022-05-14,0,"Therefore, we analyzed systems with a broad range of characteristics. Aiming at reducing the inﬂuence of particular software features on the results, we selected systems of different sizes (from 21 KLOC to 250 KLOC, without comments and blank lines), written in different programming languages, adopting different frameworks and architectural styles, and spanning different domains. We studied ten systems in total, of which nine are open-source software projects, and one is from the industry. Due to intellectual property constraints, we will refer to the commercial system as S1. Table 3 summarizes the general characteristics of each target system."
2011.00242,open-source,119,2022-05-14,0,"are described in the issues, we classiﬁed them into three different categories, which follow the main topic of our research questions: Design (e.g. changes in the selection of content to be put in the cache), Implementation (e.g. bugs in the implementation) and Maintenance (e.g. performance tests, adjustments in replacement algorithms or size limits of the cache). Results of the performed analysis are presented in Figure 3, in which applications are ordered ascending by the number of cache-related issues, which is shown next to each application name. Only open source projects with issues related to caching are detailed in this ﬁgure."
2011.00242,open-source,134,2022-05-14,0,"is implemented, the number of LOC associated speciﬁcally with caching (without comments and blank lines), and the number of issues related to it. This analysis is shown in Table 4, in the columns #Cache Files, Cache LOC and #Cache Issues, respectively. This table also gives an overview of further information of each application to which we had access. They are some form of documentation and access to developers. It is important to note that available documentation about caching, in most cases, was limited to an abstract or general description of how caching was adopted. This documentation was available in some open source systems, and we only had access to developers of our system from the industry."
2011.00242,open-source,89,2022-05-14,0,"In order to investigate different aspects of caching, it was important to select representative systems that make extensive use of application-level caching. To obtain applications that employ application-level caching, we searched through open-source repositories, from which information can be easily retrieved for our study. Based on text search mechanisms, we assessed how many occurrences of cache implementation and issues were present in applications to ensure they would contribute to the study. The more caching-related implementation and issues, the better, so"
2011.00242,"open-source code, open-source",6,2022-05-14,0,The open-source applications were selected from
2011.00242,python,2,2022-04-21,0,PHP Python
2011.05411,data,10,2022-05-14,0,"Original training data End-users Service Provider Service Provider, Third-parties"
2011.05411,data,101,2022-05-14,0,"Along with the privacy-preserving techniques such as Secure Aggregation, diﬀerential privacy, and Homomorphic Encryption designated for protecting local ML parameters, the FL client application installed at end-users’ devices must be secure to prevent from unauthorised access, cyber-attack, or data breach directly from the devices or from the communications between the users’ devices and a coordination server. This precondition is same as any other systems in which a variety of security and privacy techniques are readily integrated into FL applications, as well as secure communications protocols such as IPSec, SSL/TLS and HTTPS"
2011.05411,data,102,2022-05-14,0,"[124] Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F., 2015. Is feature selection secure against training data poisoning?, in: International Conference on Machine Learning, pp. 1689–1698. [125] Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., Beaufays, F., 2018. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903 ."
2011.05411,data,103,2022-05-14,0,"Furthermore, the requirements of purpose limitation and data minimisation are not always feasibly carried out in MLbased systems. The majority of ML algorithms heavily rely on data quality and quantity, thus researchers tend to collect as much related data as possible. Therefore, determining 1) the purposes of data collection as well as 2) what data is adequate, limited, and relevant only to the claimed purposes before executing such ML algorithms are problematic challenges. These requirements overly restrict the natural operations of ML-based services and applications to a smaller range than ever before."
2011.05411,data,105,2022-05-14,0,"The purpose of this principle is to ensure that a Data Controller should keep personal data correctly, updated, and not misleading any matter of fact. In centralised FL settings, a coordination server does not store any individual locally trained ML model parameters except the aggregated results from a batch of participants, and the anonymised global ML model. This information is stored and processed (i.e., for updating global model) in its original form without any changes, and updated for every training round. For these reasons, FL systems automatically satisfy the GDPR accuracy principle."
2011.05411,data,107,2022-05-14,0,"Dr. Kai Sun is the Operation Manager of the Data Science Institute at Imperial College London. She received the MSc degree and the Ph.D degree in Computing from Imperial College London, in 2010 and 2014, respectively. From 2014 to 2017, she was a Research Associate at the Data Science Institute at Imperial College London, working on EU IMI projects including U-BIOPRED and eTRIKS, responsible for translational data management and analysis. She was the manager of the HNA Centre of Future Data Ecosystem in 20172018. Her research interests include translational research management, network analysis and decentralised systems."
2011.05411,data,11,2022-05-14,0,to protect data in transit between clients and the server.
2011.05411,data,112,2022-05-14,0,"ing set without having access to the original data. For instance, Hitaj et al. based on GANs have developed an attack at user-level which allows an insider to infer information from a victim just by analysing the shared model parameters in some consecutive training rounds [57]. This attack can be accomplished at client-side without interfering the whole FL procedure, even when the local model parameters are obfuscated using DP technique. A malicious coordination server can also recover partial personal data by inspecting the proportionality between locally trained model parameters sent to the server and the original data samples [4, 122]."
2011.05411,data,116,2022-05-14,0,"In most of the real-world scenarios, data, particularly personal data, is generated and stored in data silos, either end-users’ devices or service providers’ data centres. Most of conventional ML algorithms are operated in a centralised fashion, requiring training data to be fused in a data server. Essentially, collecting, aggregating and integrating heterogeneous data dispersed over various data sources as well as securely managing and processing the data are non-trivial tasks. The challenges are not only due to transporting highvolume, high-velocity, high-veracity, and heterogeneous data across organisations but also the industry competition, the complicated administrative procedures, and essentially, the"
2011.05411,data,116,2022-05-14,0,"We are now living in a data-driven world where most of applications and services such as health-care and medical services, autonomous cars, and ﬁnance applications are based on artiﬁcial intelligence (AI) technology with complex data-hungry machine learning (ML) algorithms. AI has been showing advances in every aspect of lives and expected to ""change the world more than anything in the history of mankind. More than electricity.” 1. However, the AI technology is yet to reach its full potential, also the realisation of such AI/ML-based applications has been still facing longstanding challenges wherein centralised storage and computation is one of the critical reasons."
2011.05411,data,119,2022-05-14,0,"Dr. Nguyen B.Truong is currently a Research Associate at Data Science Institute, Imperial College London, United Kingdom. He received his Ph.D, MSc, and BSc degrees from Liverpool John Moores University, United Kingdom, Pohang University of Science and Technology, Korea, and Hanoi University of Science and Technology, Vietnam in 2018, 2013, and 2008, respectively. He was a Software Engineer at DASAN Networks, a leading company on Networking Products and Services in South Korea in 2012-2015. His research interest is including, but not limited to, Data Privacy, Security, and Trust, Personal Data Management, Distributed Systems, and Blockchain."
2011.05411,data,119,2022-05-14,0,"[78] Lindell, Y., Pinkas, B., 2000. Privacy preserving data mining, in: Annual International Cryptology Conference, Springer. pp. 36–54. [79] Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, l-diversity: Privacy beyond k-anonymity. ACM Trans M., 2007. actions on Knowledge Discovery from Data (TKDD) 1, 3–es. [80] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017a. Communication-eﬃcient learning of deep networks from decentralized data, in: Artiﬁcial Intelligence and Statistics, pp. 1273– 1282."
2011.05411,data,120,2022-05-14,0,"Large-scale data collection, aggregation and processing at a central server in such ML-based systems not only entail the risks of severe data breaches due to single-point-offailure but also intensify the lack of transparency, data misuse and data abuse because the service providers are in full control of the whole data lifecycle [118]. In addition, as ML algorithms operate in a black-box manner, it is also challenging to provide insightful interpretation of how the algorithms execute and how certain decisions are made [83, 91]. Consequently, most of the ML-based systems ﬁnd it diﬃcult to satisfy the requirements of transparency, fairness, and automated decision-making in the GDPR."
2011.05411,data,127,2022-05-14,0,"ML is a disruptive technology for designing and building intelligent systems that can automatically learn and improve from experience to accomplish a task without being explicitly programmed. For this purpose, an ML-based system builds up a mathematical model (i.e., model training process) based on a sample set (i.e., training data) whose parameters are to be optimised during this training process. As a result, the system can perform better predictions or decisions on a new, unseen task. Typically, an ML task can be formulated as a mathematical optimisation problem whose goal is to ﬁnd the extremum of an objective function. Thus, an optimisation method is of paramount importance in any ML-based systems."
2011.05411,data,13,2022-05-14,0,Keywords: Federated Learning Data Protection Regulation GDPR Personal Data Privacy Privacy Preservation
2011.05411,data,133,2022-05-14,0,"Depending on encryption schemes and classes of computational operations that can be performed on an encrypted form, homomorphic encryption techniques are divided into diﬀerent categories such as partial, somewhat (SWHE), and fully homomorphic encryption (FHE)[2]. Some classic encryption techniques, including Rivest–Shamir–Adleman (RSA), is SWHE wherein simple addition and multiplication oper ations can be executed [2]. FHE, ﬁrstly proposed by Graig et al. in [45, 46], enables any arbitrary operations (thus, enables any desirable functionality) over cipher-text, yielding results in encrypted forms. In FHE, computation on the original data or the cipher-text can be mathematically transferred using a decryption function without any conﬂicts."
2011.05411,data,135,2022-04-21,0,"3. An assessment of the data security and privacy risks that might be induced by each operation, along with the technical measures implemented to mitigate and manage the risks. For instance, in an FL system, the operation of sending local ML model parameters to a coordination server for global ML model update might be the target of inference attacks, thus, inducing privacy leakage. The measures called Secure Aggregation and Homomorphic Encryption mechanisms are implemented along with the technical report. Even though such privacy-preserving methods are implemented to strengthen FL systems, there exist some risks that can be exploited for illegitimate purposes such as model poisoning with back-door sub-tasks. These possible attacks, which lead to non-compliance with the GDPR, should be addressed."
2011.05411,data,136,2022-05-14,0,"As depicted in Table 2, in FL settings, personal data is regarded as local model parameters, not the original data samples as in traditional cloud-based ML systems. A service provider, who implements an FL system, is Data Controller and Data Processor combined as the service provider (i) dictates end-users (i.e., Data Subject) to train an ML model using their local training data and to share such locally trained model, (ii) processes the local model parameters sent from end-users (i.e., aggregates and updates the global model), and (ii) disseminates the global models to all end-users and requests the end-users to update their local models. Furthermore, in centralised FL settings, a service provider can only"
2011.05411,data,136,2022-05-14,0,"This purpose limitation principle can be interpreted that an FL service provider needs to clearly inform clients about the purpose of a global ML model training as well as how clients’ local personal data and devices’ computation are used to locally train a requested ML model provided by the service provider. The principle also states that the service provider can further process the data for other compatible purposes. In this respect, FL systems fully satisfy with the principles if suﬃcient privacy-preserving mechanisms such as Secure Aggregation and diﬀerential privacy are readily implemented into the systems. This is because locally trained ML models from clients are aggregated only for the global model updates and cannot be individually extracted and exploited (by the coordination server) for other purposes."
2011.05411,data,138,2022-05-14,0,"clarify the whole data management processes along with the necessity and proportionality of these processes. Such assessments are important tools for accountability and essential to eﬃciently manage the data security and privacy risks, to demonstrate the compliance, as well as to determine the measure have been taken to address the risks. However, carrying out a DPIA or PIA is not mandatory for every data processing operation. It is only required when the operation is ""likely to result in a high risk to the rights and freedoms of natural persons"" (Article 35(1)). The guideline for the criteria on the DPIA/PIA obligatory is described under Article 35(3), 35(4) which are adopted by DPAs to carry out such assessments."
2011.05411,data,139,2022-05-14,0,"Attackers might carry out model inversion (MI) attack to extract sensitive information contained in training data samples, for instance, by reconstructing representatives of classes which characterising features in classiﬁcation ML models [38]. MI attacks do not require the attacker to actively participate in the training process (i.e., black-box or passive attacks). For example, it is possible to recover images from a facial recognition model for a particular person (i.e., all class members depict this person) using MI by deriving a correct weighted probability estimation for the target feature vectors [112, 43]. In this scenario, the experiment results show that this MI attack can reconstruct images that are visually similar to the victim’s photos [38]."
2011.05411,data,142,2022-05-14,0,"One of the fundamentals of FL is eﬃcient optimisation algorithms for federated settings wherein training data is nonIID, massively and unevenly distributed across local nodes, ﬁrst introduced by Konečn`y et al. in 2016 [70]. The distributed settings for the federated optimisation is formulated as follows. Let 𝐾 be the number of local nodes, ℙ𝑘 be the set of data samples stored on node 𝑘 ∈ {1, 2, .., 𝐾}, and be the number of data samples stored on node 𝑛𝑘 = |ℙ𝑘| 𝑘. As personal data in each local node is diﬀerent, we can assume that ℙ𝑘 ∩ ℙ𝑙 = ∅ if 𝑘 ≠ 𝑙 and ∑𝐾 𝑘=1 𝑛𝑘 = 𝑛. The distributed problem formulation for the minimisation objective is deﬁned as:"
2011.05411,data,142,2022-04-21,0,"sonal data, any previous infringement, and the nature, gravity, and duration of the current infringement. For instance, Facebook and Google were hit with a collective $8.8 billion lawsuit (Facebook, 3.9 billion euro; Google, 3.7 billion euro) by Austrian privacy campaigner, Max Schrems, alleging violations of GDPR as it pertains to the opt-in/opt-out clauses. Speciﬁcally, the complaint alleges that the way these companies obtain user consent for privacy policies is an ""all-ornothing"" choice, asking users to check a small box allowing them to access services. It is a clear violation of the GDPR’s provisions per privacy experts and the EU/UK. A list of ﬁnes and notices (with non-compliance reasons) issued under the GDPR can be found on Wikipedia12"
2011.05411,data,146,2022-05-14,0,"rameters updates and aggregation between local nodes and a central coordination server are strengthened by privacypreserving and cryptography techniques, which enhance data security and privacy [48, 123, 14, 15, 96]. The FL capability could potentially inaugurate new opportunities for service providers to implement some sorts of ML algorithms for their applications and services without acquiring clients’ personal data, hence naturally complying with data protection regulations like the GDPR. Unfortunately, despite the distributed collaborative learning model of FL empowered by privacy-preserving measures, personal information can be stealthily extracted from local training parameters [4, 96, 130, 57, 86]. As a consequence, FL-based service providers still stay within the regulatory personal data protection framework and are still liable for implementing GDPR-compliant mechanisms when dealing with EU/UK citizens."
2011.05411,data,148,2022-05-14,0,"for the aggregation of independently trained neural networks in [95]. Since then, this technique has been improved to return statistically indistinguishable results among participants while ensuring that such noise-added model parameters do not aﬀect much on the accuracy of the global model in FL settings [111, 4, 48, 1, 114, 82]. As a consequence, adversaries cannot distinguish individual records in the FL training process and do not know whether or not a targeted client participating in the training; thus, preserving data privacy and protecting against the inference attacks. Generally, there are two types of employing diﬀerential privacy techniques for local nodes in FL settings: batch-level and userlevel where random noise is added by measuring parameters’ sensitivity from data points in a mini-batch and users themselves, respectively."
2011.05411,data,15,2022-05-14,0,"Data Poisoning [11, 84, 124, 68, 23, 63]"
2011.05411,data,150,2022-05-14,0,"Furthermore, local nodes not only passively contribute local training results but also get updated about intermediate stages of a global training model from a coordination server. This practice enables the opportunity for malicious participants to manipulate the training process by providing arbitrary updates in order to poison the global model [41, 9], which calls for an investigation on security models along with insightful analysis of privacy guarantees for a centralised FL framework. Accordingly, the FL framework then needs to be strengthened by employing further privacy and security mechanisms to protect personal data eﬀectively and to comply with intricate data protection legislation like the GDPR. A summary of related articles in terms of attack models with associated privacy preservation methods in centralised FL is depicted in Table 1. Detailed descriptions along with analysis are carried out in the following subsections."
2011.05411,data,151,2022-05-14,0,"According to the ﬁrst principle, a service provider providing an FL application, as a Data Controller, must specify its legal basis in order to request end-users to participate in the FL training. There are total six legal bases required by the GDPR namely (1) Consent, (2) Contract, (3) Legal Obligation, (4) Vital Interest, (5) Public Task, and (6) Legitimate Interest (deﬁned in Article 6 of the GDPR in detail). These lawful bases might need to come along with other separate conditions for lawfully processing some special cate gory data including healthcare data, biometric data, racial and ethnic origin. Depending on speciﬁc purposes and context of the processing, the most appropriate one should be determined and documented before starting to process personal data."
2011.05411,data,154,2022-05-14,0,"One of the privacy-preserving objectives of centralised FL is that a coordination server is unable to inspect the data or administer the training process at a local node. This, however, prohibits the transparency of the training process; thus, imposes a new vulnerability of a new type of attack called model poisoning [12, 87, 22, 41, 9, 6]. Generally, model poisoning attacks aim at manipulating training process by feeding poisoned local model updates to a coordination server. This type of attack is diﬀerent from data poisoning [11, 84, 124, 68, 23, 63], which is less eﬀective in FL settings [9, 6] because the original training data is never shared with a server. Thus, this section is mainly dedicated to analysing the model poisoning attacks in FL."
2011.05411,data,16,2022-05-14,0,"[44] Gentry, C., 2010. Computing arbitrary functions of encrypted data."
2011.05411,data,166,2022-05-14,0,"In this paper, we examine the centralised FL in which there exists a centralised server (i.e., service provider) requests to coordinate the whole training process. Speciﬁcally, this coordination server (i) determines a global model to be trained, (ii) selects participants (i.e., local nodes) for each training round, (iii) aggregates local training results sent by the participants, (iv) updates the global model based on the aggregated results, (v) disseminates the updated model to the participants, and (vi) terminates the training when the global model satisﬁes some requirements (e.g., accurate enough). Local nodes passively train the model over their local data as requested, and send the training results back to the server whenever possible. The workﬂow cycle in a centralised FL framework consists of four steps (illustrated in Fig. 2) as follows:"
2011.05411,data,169,2022-05-14,0,"Nevertheless, both centralised and decentralised architectures are required to acquire model consistency, particularly when data parallelism is employed. There are numerous strategies to update parameters in order to maintain the consistency of a global model, respected to a synchronisaIn this regard, Asyntion model among compute nodes. chronous Parallel (ASP) [99, 29], Bulk Synchronous Parallel (BSP) [47], and Stale Synchronous Parallel (SSP) [58] are the most common approaches to update parameters in a distributed learning system. The BSP and the ASP update parameters once receiving all gradients from a bulk of compute nodes (barrier synchronisation) and from just any node (no synchronisation), respectively. Generally, the BSP is relatively slow due to the stall time of waiting whereas ASP is faster as it does not perform any synchronisation; as a tradeoﬀ, the convergence in BSP is guaranteed but uncertain in"
2011.05411,data,171,2022-05-14,0,"can be generated by injecting a hidden backdoor model intentionally, as illustrated in Fig. 4. Compromised participants analyse the targeted global model; the poisoned model is then trained on backdoor data samples using dedicated techniques such as constrain-and-scale accordingly, and feed the parameters to a coordination server as other honest participants. The objective of this attack is that the global model is replaced by a joint model consisting of both original task and the injected backdoor sub-task while retaining high accuracy on the two. The backdoor training at the adversary can be empowered by modifying minimisation strategies such as constrain-and-scale, which optimises both gradients of the loss and the backdoor objective [6]. A parameter estimation mechanism is then used for generating parameters submitted to the coordination server for honest participants’ updates. As secure aggregation is used for preventing the server from inspecting individual models, this poisoning model is unable to detect [9, 6]."
2011.05411,data,177,2022-05-14,0,"To ensure privacy, an FL system is designed in a way that does not let the service provider (i.e., the coordination server) to directly access and obtain either original training data or locally trained ML models at end-users’ devices. Instead, end-users, as participants in the FL system, will only send the results back to the coordination server when they are ready. An FL client-side application should oﬀer several options for clients to participate in the training process proactively that allows a client to fully control the local training as well as of the sending/receiving ML model updates to/from a coordination server. Furthermore, FL systems only process data (i.e., local ML model parameters) for an explicit purpose (i.e., aggregates results and updates a global model), which is in ways that clients would reasonably expect whilst having minimal privacy impact. For these reasons, either Consent or Legitimate Interest legal basis can be appropriate for an FL application10."
2011.05411,data,180,2022-05-14,0,"In this federated setting, minimising the number of iterations in the optimisation algorithms is paramount of importance as there is limited communication capability of the local nodes. In the same paper, Konečn`y et al. proposed a novel distributed gradient descent by combining the Stochastic Variance Reduced Gradient (SVRG) algorithm [64, 72] with the Distributed Approximate Newton algorithm (DANE) [110] for distributed optimisation called Federated SVRG (FSVRG) [70]. The FSVRG computes gradients based on data on each local node 𝑘, obtains a weighted average of ℙ𝑘 the parameters from all the 𝐾 local nodes, and updates new parameters for each node after round. This algorithm is then experimented based on public Google+ posts, clustered by about 10, 000 users as local nodes, for predicting whether a post will receive any comments. The results show that the FSVRG outperforms the native gradient descent algorithm as it converges to the optimum within only 30 iterations."
2011.05411,data,181,2022-04-21,0,"4. Privacy-Preservation in Centralised Federated Learning Framework As an ML model can be cooperatively trained while retaining training data and computation on-device, FL naturally oﬀers privacy-guarantee advantages compared to the traditional ML approaches. Unfortunately, although personal data is not directly sent to a coordination server in its original form, the local ML model parameters still contain sensitive information because some features of the training data samples are inherently encoded into such models [5, 81, 4, 96, 86]. For example, authors in [5] have shown that during the training process, correlations implied in the training data are concealed inside the trained models, and personal information can be subsequently extracted. Melis et al. have also pointed out that modern deep-learning models conceal internal representations of all kinds of features, and some of them are not related to the task being learned. Such unintended features can be exploited to infer some information about the training data samples. FL systems, consequently,"
2011.05411,data,186,2022-05-14,0,"Model poisoning attacks are always inherent in collaborative learning including FL. As shown by Bagdasaryan et in [6], just by controlling less than 1% Byzantine paral. ticipants, an adversary can successfully insert a backdoor functionality into a global model without reducing much accuracy, preventing the coordination server from detecting the attack. Solutions to mitigate model poisoning attack at server-side have to detect and ﬁlter out poisoned model updates from malicious clients (i.e., model anomaly detection) [41, 63]. For this purpose, the server needs to access either participants’ training data or parameter model updates, which breaks the privacy-preservation catalyst of FL. Besides, Secure Aggregation protocol is assumed to be implemented at both client- and server-side, which prevents the server from inspecting individual model updates; consequently, ruling out any solutions for model poisoning attacks [41]. Indeed, no resolutions have been proposed that eﬀectively tackle model poisoning attacks at server-side, which imposes as a critical research topic for FL."
2011.05411,data,194,2022-05-14,0,"The data minimisation principle in the GDPR necessitates a Data Controller (e.g., a service provider) to collect and process personal data that is adequate, limited, and relIn traditional centralised evant only to claimed purposes. ML algorithms, this data minimisation requirement is a challenge as it is not always possible to envision what data and the minimal amount of data are necessary for training an ML model. In this regard, FL appears as a game-changer as an FL system does not need to collect and process original training data; instead, a service provider only needs to gather local ML models from participants for assembling the global model. Generally, with privacy-preserving techniques introduced in Section 4, an FL system can assure that the coordination server obtains aggregated local model parameters from participants for global model updates only (i.e., the claimed purposes) while acquiring nothing about individual’s contribution. The aggregation mechanism also assures that the global model itself contains no individual sensitive features that can be exploited by adversaries to extract or infer any personal information."
2011.05411,data,196,2022-05-14,0,"The challenge to provide this right to Data Subjects is that the GDPR demands the Data Controller to concisely, intelligibly, and speciﬁcally specify what and how the local ML model is used in the FL training, along with expected outputs of the mechanism11. Same as many complex ML mechanisms, FL is as a black-box model; thus, it cannot be precisely interpreted of how it works and predicting the outcomes. The GDPR supervisory board recognises the challenges and relaxes the requirement for AI/ML mechanisms by accepting a general explanation as an indication of how and what personal data is going to be processed. As a result, for an FL system, the right to be informed is achieved as privacy information including purposes for processing local ML model (i.e., to build a global ML model), retention periods (i.e., no longer in use after each training round), and who it will be shared with (only the coordination server) can be determined as in Terms and Conditions when a client accepts to participate in an FL system."
2011.05411,data,20,2022-05-14,0,"[24] Chen, X.W., Lin, X., 2014. Big data deep learning: challenges and"
2011.05411,data,204,2022-05-14,0,"Federated Learning Systems FL emerges a new approach to tackle data privacy challenges in ML-based applications by decoupling of data storage and processing (i.e., local model training) at end-users’ devices (i.e., local nodes) and the aggregation of a global ML model at a service provider’s server (i.e., a coordination server). The privacy-preservation advantage of FL compared to the traditional centralised ML approaches is undeniable: It enables to train an ML model whilst retaining personal training data on end-users’ devices. Only locally trained model parameters, which contain the essential amount of information required to update the global model, are shared with a coordination server. Nevertheless, such model parameters still enclose some sensitive features that can be exploited to reconstruct or to infer related personal information as depicted in Section 4. Subsequently, an FL system still retains within the GDPR and is liable for complying with obligatory requirements. This section closely examines whether a GDPR requirement should be complied or inapplicable and should be waived in FL settings. Unsolved challenges on fully complying with the GDPR are also determined and discussed."
2011.05411,data,205,2022-05-14,0,"In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with diﬃculties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not suﬃcient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR."
2011.05411,data,210,2022-04-21,0,"number of data samples and data distributions among personal mobile devices. Training over non-IID data has been shown to be much less accurate as well as slower convergence than IID data in federated settings [128]. Konečn`y with his colleagues at Google went further on improving the eﬃciency of the FSVRG algorithms in distributed settings by minimising the information in parameter update to be sent to an orchestration server [71]. Two types of updates are considered called structured updates and sketched updates in which the number of variables used in an ML model is minimised as many as possible, along with the compression of the information in the full model updates. Another ambitious federated optimisation approach is that local nodes are independently trained diﬀerent ML models as a task in a multi-learning objective simultaneously [113]. Generally, local nodes generate data under diﬀerent distributions which naturally ﬁt separate learning models; however, these models are structurally similar resulting in the ability to model the similarity using a multi-tasking learning (MTL) framework. Therefore, this approach improves performance when dealing with non-IID data as well as guarantees the learning convergence [113]."
2011.05411,data,22,2022-05-14,0,"[59] Horvitz, E., Mulligan, D., 2015. Data, privacy, and the greater good."
2011.05411,data,234,2022-04-21,0,"A Data Subject is assumed to have the right ""not to be subject to a decision based solely on automated processing, including proﬁling"" - Article 22(1), the GDPR. Therefore, an FL client, as a Data Subject, has the right to receive meaningful information and explanation about whether the result of the processing (i.e., a global ML model) used in an automated decision-making system will produce legal eﬀects concerning the client or similarly signiﬁcantly aﬀects the client. Unfortunately, due to the black-box operation model and the limitation of the transparency in ML, including FL, training process, results (e.g., a global ML model in FL) are generally generated without any proper explanation [119]. Thus, it is infeasible to predict whether outcomes of an ML model might aﬀect the legal status or legal rights of the Data Subject, or negatively impact on its circumstances, behaviour or choices. Consequently, any FL system fails to comply with the GDPR requirements of the data subject’s right in control of automated decision making. Fortunately, this requirement can be neglected if a Data Controller explicitly mentions the lack of automated decision making and proﬁling right when asking for Data Subject’s consent to process personal data."
2011.05411,data,242,2022-05-14,0,"data; instead, inferring attributes or membership of the original trained data from local model parameters can also induce serious privacy leakage [42, 85, 86, 93, 94] (e.g., an attacker can ﬁgure out whether a speciﬁc data sample (of a patient) is used to train a model of a disease). This is the baseline for the membership attack. Authors in [85, 86, 94] have investigated membership attacks in FL and demonstrated the capability of these attacks in both passive and active approaches. For instance, the gender of a victim can be inferred with a very high accuracy of 90% when conducting this attack in a binary gender classiﬁer on the FaceScrub dataset7. Other features, which are uncorrelated with the main task, can also be inferred such as race and facial appearance (e.g., whether a face photo is wearing glasses) [86]. Nasr et al. proposed an active attack approach called gradient ascent by exploiting the privacy vulnerabilities of SGD optimisation algorithms. This attack based on the correlation between the local gradients of the loss and the direction and the amount of changes of model parameters when minimising the loss to ﬁt a model to train data samples in the SGD algorithms. This active membership attack was conducted on the CIFAR100"
2011.05411,data,279,2022-05-14,0,"The nature of decoupling between data storage and processing at client-side and global ML model aggregation at server-side in centralised FL leads to the unnecessity of providing the (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure, (5) Right to restrict processing, (6) Right to data portability, and (7) Right to object. For instance, regarding the ""Right to erasure"", if a user requests to delete its data (i.e., local ML model parameters sent to an FL server), literally, the only way to fulﬁl the user’s request is to thoroughly re-train the global model without using user’s data from the round that the user ﬁrst participates [50]. This is unnecessary and impractical in FL settings as only local ML model parameters (possibly privacy guarantee-strengthened with diﬀerential privacy) in aggregated encrypted forms (by using Secure Aggregation and other advanced cryptography techniques) are shared with a coordination server. Consequently, it is worthless for a Data Subject to have control over its local ML model as (i) the model parameters are protected by privacy-preserving techniques from inference attacks; (ii) the server is unable to separate the user’s data from the others, the server also does not store the model once it is aggregated to update the global model; and (iii) the global model is wholly anonymised and cannot be exploited to extract or infer any individual information."
2011.05411,data,283,2022-05-14,0,"Standing on these federated optimisation research works, McMahan et al. proposed a variation of the SGD called FederatedSGD along with the Federated Averaging algorithm that can train a deep network at 100 times fewer communications compared to the naive FSVRG [81, 80]. The catalyst of such algorithms is to leverage the increasingly powerful processors in modern personal mobile devices to perform high-quality updates than simply calculating gradient steps. Speciﬁcally, each client not only calculates the gradients but also computes the local model for multiple times; the coordination server only performs aggregation of the local models from the clients. This results in fewer training rounds iterations (thus fewer communications) while producing a decent global model. These proposed algorithms well suited for scenarios that are highly limited communication bandwidth with high jitter and latency. In these scenarios, the naive FSVRG algorithms proposed in [70, 71] are not eﬃcient enough. Indeed, the algorithms are utilised for a realworld application for text prediction in Google keyboard in Android smartphones (i.e., G-board)5 [125]. In this system setting, the FederatedSGD is executed locally on the smartphone to compute gradient descent using local data. The gradient is then sent to an aggregation server. This server performs the FederatedAveraging algorithm which randomly selects a fraction of smartphones for each training round, and takes the average of all gradients sent from the selected participants to update the global model. This updated global model is distributed to all participants; the local nodes will then update their local models accordingly."
2011.05411,data,293,2022-05-14,0,"As FL is in the early stage, a fruitful area of multi-disciplinary research is commenced in order to ﬂourish the technology and to comply with the GDPR fully. Firstly, eﬃcient cryptographic and privacy primitives for decentralised collaborative learning must be further developed, particularly for counteracting model poisoning and inference attacks. Furthermore, as these privacy-preserving techniques such as SMC impose non-trivial performance overheads, further eﬀort on how to eﬃciently utilise such techniques on FL applications are required. Secondly, research on transparency, interpret-ability and algorithm fairness in FL systems should be profoundly carried out. Even though a sub stantial amount of research has been conducted in centralised AI/ML settings, there is still an open question whether these approaches could be employed and how to sensibly adapt them to the decentralised settings where training data is highly skewed non-IID and unevenly distributed across sources. The sampling constraints should be investigated to see how much extend they aﬀect and how to mitigate the bias of the global training model. For instance, the agnostic FL framework introduced in [89] naturally yields good-intent fairness as it modelled the target distribution as an unknown mixture of the distributions instead of the uniform distribution in typical FL training algorithms. This agnostic FL framework, as a result, can control for bias in the training objective. Thirdly, it requires more research on interpretable and unbiased ML models and algorithms that can be employed over encrypted settings to well consolidate with advanced encryption schemes in FL systems. Besides, the trade-oﬀs between privacy utility, accuracy, interpretability, and fairness in an FL framework need to be thoroughly explored."
2011.05411,data,3,2022-04-21,0,11https://ico.org.uk/for-organisations/guide-to-data-protection/guideto-the-general-data-protection-regulation-gdpr/individual-rights/right-tobe-informed
2011.05411,data,31,2022-05-14,0,This research was supported by the HNA Research Centre for Future Data Ecosystems at Imperial College London and the Innovative Medicines Initiative 2 IDEA-FAST project under grant agreement No 853981.
2011.05411,data,316,2022-05-14,0,"This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]. We present a systematic summary of the threat models, possible attacks, and the privacy-preserving techniques in FL systems, along with the analysis of how these techniques can mitigate the risk of privacy leakages. Furthermore, insightful analysis of how an FL-system complies with the GDPR is also provided. Obligations and appropriate measures for a service provider to implement a GDPR-compliant FL system are examined in details following the rational guidelines of the GDPR six principles."
2011.05411,data,32,2022-05-14,0,"member of the Academia Europaea. His research interests are in the areas of data mining for largescale scientiﬁc applications including distributed data mining methods, machine learning and informatics systems."
2011.05411,data,330,2022-05-14,0,"AI/ML-based applications and services are high on the agenda in most sectors. However, the unregulated use or misuse of personal data is dramatically spreading, resulting in severe concerns of data privacy. A series of severe personal data breaches such as Facebook’s Cambridge Analytica scandal, along with urgent mobile applications during the SARS-CoV2 pandemic for large-scale contact tracing and movement tracking [61] trigger worldwide attention respecting to a variety of privacy-related aspects including algorithm bias, ethics, implications of politic settings, and legal responsibility. This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]."
2011.05411,data,35,2022-05-14,0,"[128] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V., arXiv preprint Federated learning with non-iid data."
2011.05411,data,37,2022-05-14,0,"It is worth noting that standard distributed ML algorithms are generally designed to train independent identically-distributed (IID) data, and this assumption does not hold in federated settings due to the signiﬁcant diﬀerences of the"
2011.05411,data,37,2022-05-14,0,"[108] Sarwate, A.D., Chaudhuri, K., 2013. Signal processing and machine learning with diﬀerential privacy: Algorithms and challenges for continuous data. IEEE signal processing magazine 30, 86–94."
2011.05411,data,38,2022-05-14,0,• Provide insightful examination on pros and cons of the existing privacy-preserving techniques as well as prospective solution approaches in order for a FL-based service to comply with the EU/UK General Data Protection Regulation (GDPR).
2011.05411,data,4,2022-04-21,0,10https://ico.org.uk/for-organisations/guide-to-data-protection/guide to-the-general-data-protection-regulation-gdpr/lawful-basis-forprocessing/
2011.05411,data,4,2022-04-21,0,2.2.1. Data Anonymisation
2011.05411,data,4,2022-04-21,0,5.2.3. Data Minimisation
2011.05411,data,40,2022-05-14,0,"[23] Chen, X., Liu, C., Li, B., Lu, K., Song, D., 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 ."
2011.05411,data,42,2022-05-14,0,"[36] Dwork, C., Smith, A., Steinke, T., Ullman, J., 2017. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application 4, 61–84."
2011.05411,data,45,2022-05-14,0,"[50] Ginart, A., Guan, M., Valiant, G., Zou, J.Y., 2019. Making ai forget you: Data deletion in machine learning, in: Advances in Neural Information Processing Systems, pp. 3518–3531."
2011.05411,data,46,2022-05-14,0,"[31] Du, W., Han, Y.S., Chen, S., 2004. Privacy-preserving multivariate statistical analysis: Linear regression and classiﬁcation, in: Proceedings of the 2004 SIAM international conference on data mining, SIAM. pp. 222–233."
2011.05411,data,53,2022-05-14,0,"The natural advantage of FL compared to the traditional cloud-centric ML approaches is the ability to reassure data privacy and (presumably) comply with the GDPR because personal data is stored and processed locally, and only model parameters are exchanged. In addition, the processes of pa 2https://gdpr-info.eu/"
2011.05411,data,54,2022-04-21,0,"1. A systematic description of data processing operations, associated purposes, along with clariﬁcation and justiﬁcation of the operations. For instance, the operation of asking Data Subject’s consent for local ML training and sending the ML model parameters to a coordination server should be documented in detail."
2011.05411,data,54,2022-05-14,0,"Even though homomorphic encryption oﬀers rigorous privacy-guarantee to individuals as the original data in plaintext has never been disclosed, there is a practical limitation in performing computation over cipher-text due to the tremendous computational overhead. As a consequence, employing homomorphic encryption in large-scale data training remains impractical [49]."
2011.05411,data,55,2022-05-14,0,"[14] Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K., 2016. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482 ."
2011.05411,data,55,2022-05-14,0,"[49] Gilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M., Wernsing, J., 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy, in: International Conference on Machine Learning, pp. 201–210."
2011.05411,data,56,2022-05-14,0,"[34] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M., 2006. Our data, ourselves: Privacy via distributed noise generation, in: Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer. pp. 486–503."
2011.05411,data,56,2022-05-14,0,"[5] Ateniese, G., Mancini, L.V., Spognardi, A., Villani, A., Vitali, D., Felici, G., 2015. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classiﬁers. International Journal of Security and Networks 10, 137–150."
2011.05411,data,62,2022-05-14,0,"Diﬀerential privacy technique has been widely employed in various ML algorithms such as linear and logistic regression [19], Support Vector Machine (SVM) [102] and deep learning [20, 1], as well as in ML-based applications such as data mining [39] and signal processing with continuous data [108]."
2011.05411,data,62,2022-05-14,0,"This principle obligates Data Controllers to implement appropriate measures in place to eﬀectively protect personal data. Thus, in order to comply with this principle, a centralised FL system requires to implement security and privacy mechanisms not only at a coordination server but also at end-users’ devices as the FL system itself does not guarantee security and privacy."
2011.05411,data,66,2022-04-21,0,The GDPR deﬁnes 6-core principles as rational guidelines for service providers to manage personal data as illustrated in Fig. 7 (The GDPR Articles 5-11). These principles are broadly similar to the principles in the Data Protection Act 1998 with the accountability that obligates Data Controllers to take responsibility for complying with the principles and implementing appropriate measures to demonstrate the compliance.
2011.05411,data,68,2022-05-14,0,"Basically, this principle ensures that a Data Controller does not keep personal data for longer if the data is no longer needed for the claimed purposes. In this case, data should be erased or anonymised. There is an exception for data retention only if the Data Controller keeps the data for public interest archiving, scientiﬁc or historical research, or statistical purposes."
2011.05411,data,75,2022-05-14,0,"Mr. Siyao Wang is a PhD student of the Data Science Institute at Imperial College London. He received the BSc degree in Computer Science and Technology from the University of Chinese Academy of Sciences in 2018. He received the MRes degree in Medical Robotics and ImageGuided Intervention from Imperial College London in 2019. His research interests include machine learning, deep learning, computer vision and artiﬁcial intelligence applications in healthcare."
2011.05411,data,75,2022-05-14,0,"[61] Ienca, M., Vayena, E., 2020. On the responsible use of digital data to tackle the covid-19 pandemic. Nature medicine 26, 463–464. [62] Jagannathan, G., Wright, R.N., 2005. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 593–599."
2011.05411,data,77,2022-04-21,0,"3. Federated Learning: A Distributed Collaborative Learning Approach In many scenarios, the traditional cloud-centric ML approaches are no longer suitable due to the challenges of complying with strict data protection regulations on vast aggregation and processing personal data. By nature, most personal data is generated at the edge by end-users’ devices (e.g., smart phones, tablets, and wearable devices) which are equipped with increasingly powerful computing capability"
2011.05411,data,8,2022-05-14,0,Personal Data Data Subject Data Controller Data Processor
2011.05411,data,81,2022-05-14,0,"[39] Friedman, A., Schuster, A., 2010. Data mining with diﬀerential privacy, in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 493–502. [40] Fung, B.C., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: A survey of recent developments. ACM Computing Surveys (Csur) 42, 1–53."
2011.05411,data,85,2022-05-14,0,"[118] Truong, N.B., Sun, K., Lee, G.M., Guo, Y., 2019. Gdpr-compliant personal data management: A blockchain-based solution. IEEE Transactions on Information Forensics and Security 15, 1746–1761. [119] Wachter, S., Mittelstadt, B., Floridi, L., 2017. Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7, 76– 99."
2011.05411,data,87,2022-05-14,0,"As illustrated in Fig. 8, the investigation of non-compliance and decision of punishment are carried out by DPAs once there is a suspicion or a claim ﬁled by a customer. The compliance inspection will conduct some analysis to see whether a suspicious organisation follows the legal requirement of Privacy&Security-by-design approach and satisﬁes some standard assessments such as Data Protection Impact Assessment (DPIA) and Privacy Impact Assessment (PIA), which are essential parts of the GDPR accountability obligations."
2011.05411,data,87,2022-05-14,0,"data protection regulations and restrictions such as the EU General Data Protection Regulation (GDPR)2 [59]. In traditional ML algorithms, large-scale data collection and processing at a powerful cloud-based server entails the singlepoint-of-failure and the risks of severe data breaches. Foremost, centralised data processing and management impose limited transparency and provenance on the system, which could lead to the lack of trust from end-users as well as the diﬃculty in complying with the GDPR [118]."
2011.05411,data,94,2022-05-14,0,"The GDPR requires Data Controllers to provide the following rights for Data Subjects if capable (The GDPR Articles 12-23): (1) Right to be informed, (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure (Right to be forgotten), (5) Right to restrict processing, (6) Right to data portability, (7) Right to object, and (8) Rights in relation to automated decision making and proﬁling."
2011.05411,data,94,2022-05-14,0,"methods. Frontiers in Applied Mathematics and Statistics 3, 9. [73] Li, N., Li, T., Venkatasubramanian, S., 2007. t-closeness: Privacy beyond k-anonymity and l-diversity, in: 2007 IEEE 23rd International Conference on Data Engineering, IEEE. pp. 106–115. [74] Li, O., Liu, H., Chen, C., Rudin, C., 2017. Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions. arXiv preprint arXiv:1710.04806 ."
2011.05411,data,95,2022-05-14,0,"Dr. Yike Guo (FREng, MAE) is the director of the Data Science Institute at Imperial College London and the Vice-President (Research and Development) of Hong Kong Baptist University. He received the BSc degree in Computing Science from Tsinghua University, China, in 1985 and received the Ph.D in Computational Logic from Imperial College London in 1993. He is a Professor of Computing Science in the Department of Computing at Imperial College London since 2002. He is a fellow of the Royal Academy of Engineering and a"
2011.05411,data,95,2022-05-14,0,"Mr. Florian Guitton received a BSc in Software Engineering from Epitech (France) in 2011 and a MSc in Advanced Computing from the University of Kent (United Kingdom) in 2012. In 2012 he joined the Discovery Sciences Group at Imperial College London where he became Research Assistant working on iHealth, eTRIKS and IDEA-FAST EU programs. He is currently a PhD candidate at Data Science Institute, Imperial College London working on distributed data collection and analysis pipeline in mixed-security environments with the angle of optimising user facing experiences."
2011.05411,data,98,2022-05-14,0,"share a global ML model, which can be considered as anonymous information, with third-parties as it does not possess any other personal data (e.g., original training data as in traditional ML systems). Therefore, Data Processors in FL settings are also the service providers, but not other players (i.e., third-parties). The processing mechanisms in FL are also uncomplicated compared to the traditional ones as they are only related to the aggregation of the local ML models as well as the update of the global ML model."
2011.05411,"data, data available",150,2022-05-14,0,"Normally, DPAs might require a variety of information with a detailed explanation from Data Controller to perform the analysis including documents of organisational and technical measures related to the implementation the GDPR requirements as well as independent DPIA and PIA reports frequently conducted by the Data Controller. DPAs may also require to be given access to data server infrastructure and management system including personal data that is being processed. In this respect, besides the legal basis such as consents from end-users, an FL service provider can only provide documentation of how FL-related mechanisms are implemented along with privacy-preserving technical measures such as secure aggregation, diﬀerential privacy, and homomorphic encryption. Other inquiries from DPAs such as direct access to the FL model training operations and inspection of individual local model parameters from a particular end-user are technically infeasible for any FL systems."
2011.05411,"data, data available",85,2022-05-14,0,"FL settings. Training data in FL is unbalanced and non-IID, which is scattered across millions of personal mobile devices with signiﬁcant higher-latency, lower-throughput connections compared to the traditional techniques working on a cloud-centric data server. In addition, the data and computing resources in personal devices are only intermittently available for training. Therefore, to actualise FL, optimisation algorithms must be well adapted and eﬃciently performed for federated settings (i.e., federated optimisation [70])."
2011.05411,"data, dataset",114,2022-05-14,0,"Another approach to preserve data privacy and security in ML is to utilise homomorphic encryption techniques, particularly in centralised systems, e.g., cloud servers, wherein data is collected and trained at a server without disclosing the original information. Homomorphic encryption enables the ability to perform computation on an encrypted form of data without the need for the secret key to decrypt the ciphertext [44]. Results of the computation are in encrypted form and can only be decrypted by the requester of the computation. In addition, homomorphic encryption ensures that the decrypted output is the same as the one computed on the original unencrypted dataset."
2011.05411,"data, dataset",120,2022-05-14,0,"As aforementioned, a trained ML model contains unintended features that can be utilised to extract personal information. Thus, local ML model parameters from a federated optimisation algorithm can be exploited by an adversary to infer personal information, particularly when combining with related information such as model data structure and meta-data. This information can be either original training data samples (i.e., reconstruction attack) [38, 111, 81, 4, 57, 96, 112, 6, 93, 130, 43] or membership tracing (i.e., to check if a given data point belongs to a training dataset) [15, 112, 86]."
2011.05411,"data, dataset",136,2022-05-14,0,"The authors in [1] have proposed an SGD algorithm integrated with diﬀerential privacy performing over some batches (a group) of data samples. This algorithm estimates the gradient of the group by taking the average of the gradient loss of these batches and adds noise (generated by Gaussian mechanism) to the group to protect the privacy. This algorithm is implemented to train on the MNIST and CIFAR-10 datasets showing sensible results as it achieves only 1.3% and 7% less accurate compared to the non-diﬀerentially private conventional baseline algorithms on the same datasets, respectively. Similar to the mechanism proposed by Shokri and Shmatikov in [111], the authors have proposed a mechanism to monitor the total privacy budget (i.e., privacy accounting)"
2011.05411,"data, dataset",161,2022-05-14,0,"Generally, there are three gradient descent methods that are categorised based on the amount of training data used in the gradient calculation of the objective function 𝑓 (𝜃) [103]. The ﬁrst category is batch gradient descent, in which the gradients are computed over the entire training dataset  for one update. The second category is stochastic gradient descent (SGD), that, in contrast to batch gradient descent, randomly selects a sample (or a subset) from  and performs the parameters update based on the gradient of this sample only (one sample per step, the whole process sweeps through the entire dataset). The third one is mini-batch gradient descent in which the dataset is subdivided into mini-batches of 𝑛 training samples (𝑛 is the batch-size); the parameters update is then performed on every mini-batch (single minibatch per step)."
2011.05411,"data, dataset",172,2022-05-14,0,"Furthermore, the local nodes can leverage the perturbation method to prevent a coordination server and other adversaries from disclosing model parameters updates and original training dataset. The idea of employing perturbation technique to FL is that a local node adds random noise to its local model parameters in order to obscure certain sensitive attributes of the model before sharing. As a result, adversaries, in case it can successfully derive such model parameters, is unable to accurately reconstruct the original training data or infer some related information. In other words, the perturbation method could prevent adversaries from carrying out inference attacks on a local model trained by a particular client. This privacy-preservation method typically adopts diﬀerential privacy technique that adds random noises to either training dataset or model parameters, oﬀering statistical privacy guarantees for individual data [35, 33, 7]. Indeed, before the proposal of FL, diﬀerential privacy with SMC has been suggested as a privacy-preserving technique"
2011.05411,"data, dataset",178,2022-05-14,0,"Data anonymisation or de-identiﬁcation is a technique to hide (e.g., hashing) or remove sensitive attributes, such as personally identiﬁable information (PII), so that a data subject cannot be identiﬁed within the modiﬁed dataset (i.e., the anonymous dataset) [92]. As a consequence, data anonymisation has to balance well between privacy-guarantee and utility because hiding or removing information may reduce the utility of the dataset. Furthermore, when combined with auxiliary information from other anonymous datasets, a data subject might be re-identiﬁed, subjected to a privacy attack called linkage attack [40]. To prevent from linkage attack, numerous techniques have been proposed such as k-anonymity [116], l-diversity [79], a k-anonymity-based method, and tcloseness - a technique built on both k-anonymity and l-diversity that preserves the distribution of sensitive attributes in a dataset so that it reduces the risk of re-identifying a data subject in a same quasi-identiﬁer group [73]."
2011.05411,"data, dataset",186,2022-05-14,0,"To overcome such challenges, Federated Learning (FL), proposed by Google researchers in 2016, has appeared as a promising solution and attracted attention from both industry and academia [70, 71, 81, 80]. Generally, FL is a technique to implement an ML algorithm in decentralised collaborative learning settings wherein the algorithm is executed on multiple local datasets stored at isolated data sources (i.e., local nodes) such as smart phones, tablet, PCs, and wearable devices without the need for collecting and processing the training data at a centralised data server. FL allows local nodes to collaboratively train a shared ML model while retaining both training dataset and computation at internal sites [70]. Only results of the training (i.e., parameters) are exchanged at a certain frequency, which requires a central server to coordinate the training process (centralised FL) or utilises a peer-to-peer underlying network infrastructure (i.e., decentralised FL) to aggregate the training results and calculate the global model."
2011.05411,"data, dataset",199,2022-05-14,0,"In this regard, FL is an alternative for the cloud-centric ML technique that facilitates an ML model to be trained collaboratively while retaining original personal data on their devices, thus potentially mitigates data privacy-related vulnerabilities. It is a cross-disciplinary technique covering multiple computer science aspects including ML, distributed computing, data privacy and security that enables end-users’ devices (i.e., local nodes) to locally train a shared ML model on local data. Only parameters in the training process are exchanged for the model aggregation and updates. The diﬀerence between FL and the standard distributed learning is that in distributed learning, local training datasets in compute nodes are assumed to be independent and identically distributed data (IID) whose their sizes are roughly the same. FL is, thus, as an advancement of distributed learning as it is designed to work with unbalanced and non-independent identically-distributed data (non-IID) whose sizes may span several orders of magnitude. Such heterogeneous datasets are resided at a massive number of scattering mobile devices under unstable connectivity and limited communication bandwidth [81, 80, 65]."
2011.05411,"data, dataset",200,2022-05-14,0,"SMC, also known as multi-party computation (MPC) or privacy-preserving computation, was ﬁrstly introduced by Yao in 1986 [126] and further developed by numerous researchers. Its catalyst is that a function can be collectively computed over a dataset owned by multiple parties using their own inputs (i.e., a subset of the dataset) so that any party learns nothing about others’ data except the outputs [51, 18, 27]. Speciﬁcally, 𝑛 parties 𝑃1, 𝑃2, .., 𝑃𝑛 own 𝑛 pieces of pri, respectively to collectively comvate data 𝑋1, 𝑋2, ..., 𝑋𝑛 pute a public function 𝑓 (𝑋1, 𝑋2, .., 𝑋𝑛) = (𝑌1, 𝑌2, .., 𝑌𝑛). The only information each party can obtain from the computation is the result (𝑌1, 𝑌2, .., 𝑌𝑛) and its own inputs 𝑋𝑖 . Classical secret sharing such as Shamir’s secret sharing [109, 17] and veriﬁable secret sharing (VSS) schemes [26] are the groundwork for most of the SMC protocols."
2011.05411,"data, dataset",217,2022-05-14,0,"Regarding the Fairness and Transparency requirements, as AI/ML algorithms like deep learning are normally operated in a black-box fashion, it is limited of transparency of how certain decisions are made, as well as limited understanding of the bias in data samples and training process [30, 83, 3, 91]. An FL system is not an exception. Generally, if the training data is poorly collected or intentionally prejudicial and fed to an ML, including FL, system, the results apparently turn out to be biased. If the trained model is then utilised for an automated decision-making system, then it probably leads to discrimination and injustice. Furthermore, the nature of preventing service providers from accessing original training dataset as well as the inability to inspect individuals’ locally trained ML model due to Secure Aggregation mechanism ampliﬁes the lack of transparency and fairness in FL systems. As a result, an FL system ﬁnds it problematic to transparently execute the training operations as well as to ensure any automated decisions from the system are impartially performed. This, consequently, induces the impracticality for any FL systems and fails to fully comply with the GDPR requirements of fairness and transparency."
2011.05411,"data, dataset",226,2022-05-14,0,"Geyer et al. in [48] have developed another method to implement diﬀerential privacy for federated optimisation in FL settings that conceals the participation of a user in a training task; as a result, the whole local training dataset of the user is protected against diﬀerential attacks. This approach is diﬀerent from the batch-level one, which aims at protecting a single data point in a training task. The proposed method utilises a similar concept of privacy accounting from [1] that allows a coordination server to monitor the accumulated privacy budget by observing the moment accountant and privacy loss proposed in [1]. The training process is halted once the accumulated privacy budget reaches a pre-deﬁned threshold, implying that the privacy guarantee is no further tolerated. The Gaussian mechanism is also used to generate random noise which is then added to distort the sum of gradients updates to protect the whole training data. The proposed method has been experimented on MNIST dataset, and the results show that with a suﬃciently large number of participants (e.g., about 10,000 clients), the accuracy of the FL trained model almost achieves as high as the nondiﬀerential-privacy baseline while a certain level of privacy guarantee over the local training data still holds."
2011.05411,"data, dataset",67,2022-05-14,0,"In traditional ML approaches, this sort of algorithms performs a vast number of fast iterations over a large dataset homogeneously partitioned in data servers. Such algorithms require super low-latency and high-throughput connections to the training data [80]. Therefore, solving this optimisation problem in the context of FL is diﬀerent from the traditional ML approaches as such conditions do not hold in"
2011.05411,"data, dataset",71,2022-05-14,0,"Reconstruction attacks using MI and GANs are only feasible if and only if all class members in an ML model are analogous which entails a similarity between the MI/GANreconstructed outputs and the training data (e.g., facial recognition of a speciﬁc person, or MNIST dataset for handwritten digits6 used in [4]). Fortunately, this precondition is less practical in most of the FL scenarios."
2011.05411,"data, dataset",90,2022-05-14,0,"learning system target two main objectives: (i) privacy of the training dataset and (ii) privacy of the local model parameters (from an optimisation algorithm such as a gradient descent variant) which are exchanged with other nodes and/or a centralised server [111]. In this respect, prominent privacy-preserving techniques in ML include data anonymisation [92], diﬀerential privacy [34], secure multi-party computation (SMC) [126], and homomorphic encryption [44]."
2011.05411,"data, dataset",96,2022-05-14,0,"SMC is beneﬁcial to data privacy preservation in distributed learning wherein compute nodes collaboratively perform model training on their local dataset without revealing such dataset to others. Indeed, SMC has been employed in numerous ML algorithms such as secure two-party computation (S2C) in linear regression [31], Iterative Dichotomiser3 (ID3) decision tree learning algorithm [78], and k-means clustering algorithm for distributed data mining [62]. However,most of SMC protocols impose non-trivial overheads which require further eﬃciency improvements with practical deployment."
2011.05411,"data, dataset provided",139,2022-05-14,0,"The GDPR clearly diﬀerentiates three participant roles, namely: Data Subject, Data Controller and Data Processor, along with associated requirements and obligations under the EU/UK data protection law. While serving as a better privacy and security framework, the GDPR also aims at protecting data ownership by obligating Data Controllers to provide fundamental rights for Data Subjects to control over their data (""How?"" in Fig. 1). For these purposes, the GDPR introduces and sets high-standard for the consent lawful basis in which Data Controller shall obtain consent from Data Subject in order to process data. Data Controller takes full responsibility to regulate the purposes for which and the methods in which, personal data is processed under the Terms and Conditions deﬁned in the consent."
2011.05411,"data, dataset provided",142,2022-05-14,0,"In this article, we conduct a survey on existing FL studies with an emphasis on privacy-preserving techniques from the GDPR-compliance perspective. Firstly, we brieﬂy review the challenges on data privacy preservation in conventional centralised ML approaches (Section 2) and introduce FL as a potential approach to address the challenges (Section 3). Secondly, the state-of-the-art privacy-preserving techniques for centralised FL are described with the analysis of how these solutions can mitigate data security and privacy risks (Section 4). Thirdly, we provide an insightful deliberation with potential solution approaches of how an FL system can be implemented in order to comply with the EU/UK GDPR (Section 5). Unsolved challenges hindering an FL system from complying with the GDPR are also speciﬁed along with the future research directions."
2011.05411,"data, dataset provided",178,2022-05-14,0,"The GDPR diﬀerentiates three participant roles, namely Data Subject, Data Controller and Data Processor, and designates associated obligations for these roles under the EU data protection law. Data Controllers are subject to comply with the GDPR by determining the purposes for which, and the method in which, personal data is processed by Data Processors - who will be responsible for processing the data on behalf of Data Controllers. Furthermore, Data Controllers should take appropriate measures to provide Data Subjects with information related not only to how data is shared but also to how data is processed in the manner ensuring security and privacy of personal data. The GDPR also clearly speciﬁes rights of Data Subjects, giving data owners the rights to inspect information about how the personal data is being processed (e.g., Right to be informed and Right of access) as well as to fully control the data (e.g., Right of rectiﬁcation and erasure, and Right to restriction of processing)."
2011.05411,"data, dataset provided",6,2022-04-21,0,5.3. Rights of Data Subject
2011.05411,"data, dataset provided",81,2022-05-14,0,"To meet stringent requirements of the GDPR, conventional ML-based applications and services are required to implement measures that eﬀectively protect and manage personal data adhering to the six data protection principles in the GDPR, as well as to provide mechanisms for data subjects to fully control their data. Although ML-based systems are strengthened by several privacy-preserving methods, implementing these obligations in a centralised MLbased system is non-trivial, sometimes technologically impractical [119, 53]."
2011.05411,"data, dataset, data available",222,2022-05-14,0,"called compute nodes and grouped into clusters. For efﬁciency, the calculations in the training process should be parallelised using concurrency methods such as model parallelism and data parallelism [24]. Model parallelism distributes an ML model into diﬀerent computing blocks; available computing nodes are then be assigned to compute some speciﬁc blocks only. Model parallelism requires mini-batch data is replicated at computing nodes in a cluster, as well as regular communication and synchronisation among such nodes [29]. Data parallelism, instead, keeps the completeness of the model on each computing node but partitions the training dataset into smaller equal size shards (also known as sharding), which are then distributed to computing nodes in each cluster [8]. The computing nodes then train the model on their subset as a mini-batch, which is especially eﬀective for SGD variants because most operations over mini-batches are independent in these algorithms. Data parallelism can be found in numerous modern ML frameworks including TensorFlow3 and Pytorch4. The two parallelism techniques can also be combined (so-called Hybrid parallelism) to intensify the advantages while mitigating the drawbacks of each one; as a result, a hybrid system can achieve better eﬃciency and scalability [25]."
2011.05411,"data, publicly available",151,2022-05-14,0,"The new GDPR legislation has come into force from May 2018 in all European Union (EU) countries which is a major update to the EU Data Protection Directive (95/46/EC) (DPD-95) introduced in the year 1995. The GDPR aims to protect personal data (more comprehensive range depicted in ""Which?"" - Fig. 1) with the impetus that ""personal data can only be gathered legally, under strict conditions, for a legitimate purpose"". The full regulation is described in detail across 99 articles covering principles, and both technical and admin requirements around how organisations need to process personal data. The GDPR creates a legal data protection framework throughout the EU/UK member states which has impacted commercial and public organisations worldwide processing EU/UK residents’ data (""Global"" in Fig. 1)."
2011.05411,"data, publicly available",77,2022-05-14,0,"The GDPR establishes supervisory authorities in each member state which are independent public authorities called Data Protection Authorities (DPAs). DPAs are responsible for supervising and inspecting whether a Data Controller is compliant with the data protection regulations whilst the Data Controller is responsible for demonstrating the compliance. The questions are judiciously raised: How can an FL system be investigated and validated by DPAs, and how can it demonstrate the compliance?"
2011.05411,dataset,126,2022-05-14,0,"Proposed by Dwork et al. in 2006, diﬀerential privacy [34] is an advanced solution of the perturbation privacy-preserving technique in which random noise is added to true outputs using rigorous mathematical measures [40]. As a result, it is statistically indistinguishable between an original aggregate dataset and a diﬀerentially additive-noise one. Thus, a single individual cannot be identiﬁed as any (statistical) query results to the original dataset is practically the same regardless of the existence of the individual [34, 33, 35]. However, there is a trade-oﬀ between privacy-guarantee and utility as adding too much noise and improper random Nguyen Truong et al.: Preprint submitted to Elsevier"
2011.05411,dataset,141,2022-05-14,0,"where the training dataset is in form of a set of input-output pairs (𝑥𝑖, 𝑦𝑖), 𝑥𝑖 ∈ ℝ𝑑 and 𝑦𝑖 ∈ ℝ, ∀𝑖 ∈ {1, 2, .., 𝑛}. In Equation 2, 𝑛 is the number of samples in the dataset, 𝑤 ∈ ℝ𝑑 is the parameter vector, and 𝑓𝑖(𝑤) is a loss function. This formulation covers both linear and logistic regressions, support vector machines, as well as complicated non-convex problems in Artiﬁcial Neural Networks (ANN) including Deep Learning [70]. This problem requires an optimisation process that can be eﬃciently computed by using a gradient descent algorithm with back-propagation technique [105, 101] for minimising the overall loss with respect to each model parameters."
2011.05411,dataset,156,2022-05-14,0,"It is worth to emphasise that the separation of the four steps in the cycle is not a strict requirement in every training round. For instance, an asynchronous SGD algorithm can be used in which results of the local training can be immediately applied to update the local model before obtaining updates from other participants [21]. This asynchronous approach is typically utilised in distributed training for deep learning models on a large-scale dataset as it maximises the rate of updates [29, 25]. However, in FL settings, the synchronous approach, which requires the coordination from a centralised server, has substantial advantages over the asynchronous ones in terms of both communication eﬃciency and security because it allows advanced technologies to be integrated such as aggregation compression, secure aggregation with SMC, and diﬀerential privacy [80, 71, 55, 120]."
2011.05411,dataset,18,2022-05-14,0,"ness will signiﬁcantly depreciate reliability and usability of the dataset [33, 35, 40]."
2011.05411,dataset,247,2022-05-14,0,"Shokri and Shmatikov in [111] have proposed a communication eﬃcient privacy-preserving SGD algorithm for deep learning in distributed settings in which local gradient parameters are asynchronously shared among participants with an option of adding noise to such updates for the differentially private protection of the individual model parameters. In this algorithm, participants can choose a fraction of parameters (randomly selected or following a strategy) to be updated at each round so that their local optimal can converge faster while being more accurate. In order to integrate diﬀerential privacy technique into the algorithm, the 𝜀 total privacy budget parameter and the sensitivity of gradient are taken into account to control Δ𝑓𝑖 the trade-oﬀ between the diﬀerential privacy protection and the model accuracy. Laplacian mechanism is used to generate noise during both parameter selection and exchange processes based on the estimation of the Δ𝑓𝑖 sensitivity and the allocated 𝜀 privacy budget. The proposed algorithm has experimented on MNIST and SVHN datasets showing the trade-oﬀ between strong diﬀerential privacy guarantees and high accuracy of the training model. However, with a large number of participants sharing a large fraction of gradients, the accuracy of the proposed algorithm with diﬀerential privacy is better than the standalone baseline. It is worth noting that in this algorithm, local gradients can be exchanged directly or via a central server, which can feasibly be implemented in the FL settings."
2011.05411,dataset,38,2022-05-14,0,"as accumulated privacy loss by observing privacy loss random variables. Based on the experiment, the authors also indicate that privacy loss is minimal for large group size (with a large number of datasets)."
2011.05411,dataset,40,2022-05-14,0,"[92] Narayanan, A., Shmatikov, V., 2008. Robust de-anonymization of large sparse datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008), IEEE. pp. 111–125."
2011.05411,dataset,47,2022-05-14,0,"FL is well-suited for sorts of ML models that are formulated as minimisation of some objective functions (loss functions) on a training dataset for parameter estimation, particularly for gradient-based optimisation algorithms [70]. The minimisation objective can be formulated as follows:"
2011.05411,dataset,71,2022-05-14,0,"Although gradient descent-based optimisation methods were successfully engaged in various ML algorithms, they have recently re-gained much attention since the emergence of large-scale distributed learning, including FL [16, 29]. In these scenarios, a complex model, e.g., a deep neural network (DNN) with millions of parameters, is trained on a very large dataset across multiple nodes. These nodes are"
2011.05411,dataset,89,2022-04-21,0,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-users’ devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.0684,data,106,2022-05-14,0,"(cid:12) (cid:12) 2, 𝑥𝑛 is the 𝑛th data symbol (cid:205)𝑈 −1 where 𝐵1,𝑛 = (cid:12)ℎB,𝑛+𝑖 𝑁 (cid:12) 𝑖=0 (cid:12) (cid:12) (cid:205)𝑈 −1 (cid:12) is the 𝑛th noise symbol (cid:12)𝑣B,𝑛+𝑖 𝑁 at Bob, and 𝐵2,𝑛 = 1√ 𝑖=0 𝑈 component at Bob and where it is observed that 𝐵1,𝑛 ⊥⊥ 𝑥𝑛 ⊥⊥ 𝐵2,𝑛. As detailed in A-A and A-B, the components can respectively be derived as:"
2011.0684,data,106,2022-05-14,0,"Prior to the secure data transmission between Alice and Bob, a handshake protocol must take place. Depending on it, Eve may obtain different degrees of information regarding the channels, which leads to different decoding capabilities and so, different security performance. PLS performance highly depends on the availability of CSI at the communication parties. It is assumed that Alice knows Bob CSI but does not know Eve CSI who is assumed to be an external passive node of the network that tries to eavesdrop the data. Furthermore, Bob and Eve CSI’s are considered spatially independent."
2011.0684,data,109,2022-05-14,0,"In this paper, a new scheme is introduced in order to establish a secure communication at the physical layer between a base station, Alice, and a legitimate user, Bob, in the presence of a passive eavesdropper, Eve. Alice uses a time reversal precoder, implemented in the frequency domain with OFDM, to add to the transmitted data an artiﬁcial noise that lies in the null-space of Bob but degrade Eve’s channel. The proposed technique only requires a single transmit antenna and is therefore well suited for devices with limited capabilities, such as in IoT for instance."
2011.0684,data,11,2022-05-14,0,1) Data term: E (cid:2)|E𝑆𝐷𝑆 1
2011.0684,data,113,2022-05-14,0,"Equation (9) shows that the addition of AN in the FD TR SISO OFDM communication can secure the data transmission. The degree of security depends on G and the amount of data energy, 𝛼, that is injected into the communication, with respect to the amount of AN energy injected (via 1 − 𝛼), as explained in Section III. It is to be noted that, since w is generated from an inﬁnite set of possibilities, even if Eve knows its equivalent channel HEH∗ B and the spreading sequence, she cannot estimate the AN signal to try retrieving the data."
2011.0684,data,124,2022-05-14,0,"It has to be pointed out that lower bounds of the SINR at Bob and Eve were determined for the three investigated scenarios. From simulations, the closed form approximated SINR lower bounds, derived in (15), (22), (28), and (34), are observed to be very tight and are therefore used in the remaining as an approximation. By doing so, an analytical expression of the SR can be determined using (11) as a function of 𝛼. It is therefore straightforward to determine the amount of data energy to inject in the communication, with respect to AN, in order to maximize the ergodic SR."
2011.0684,data,131,2022-05-14,0,"and covariance matrix E (cid:2)(S𝐻 vB)(S𝐻 vB) 𝐻 (cid:3) = 𝜎2 V,BI𝑁 . In (6), each transmitted data symbol is affected by a real gain √ 𝛼 2 at the position of the legitimate receiver. 𝑈 This frequency diversity gain consequently increases the received useful signal power at Bob in fading environments and increases with the BOR value. Considering a ﬁxed bandwidth, the TR focusing effect is enhanced for higher BOR’s at the expense of the data rate. It is also observed that no AN contribution is present in (6) since (3) is respected. A ZF equalization is performed at the receiver leading to:"
2011.0684,data,137,2022-05-14,0,"where G is a 𝑁 × 𝑄 decoding matrix performed by Eve and vE is a complex AWGN. The nature of the decoding matrix is determined by the considered scenarios, which are presented in the next Section II-B. The noise variance is E (cid:2)|𝑣E,n|2(cid:3) = 𝜎2 V,E. The gain of the data component in (8) depends on G and does not necessarily provide a SNR enhancement due to a TR effect. Similarly, the AN component does not necessarily cancel out, depending on G. After ZF equalization, the estimated symbols are: BS(cid:1) −1 (cid:16)√ ˆxE = (cid:0)GHEH∗ √ √ 1 − 𝛼 (cid:0)GHEH∗ 𝛼x +"
2011.0684,data,14,2022-05-14,0,Eve can decode the data thanks to G = S𝐻 H∗ sequence is:
2011.0684,data,14,2022-05-14,0,"Therefore, the optimal amount of data energy to inject is given by:"
2011.0684,data,141,2022-05-14,0,"The scheme consists in data transmission onto OFDM blocks with 𝑄 subcarriers. Without loss of generality, it is considered that only one data block x is sent and is composed of 𝑁 symbols 𝑥𝑛 (for 𝑛 = 0, ..., 𝑁−1, with 𝑁 ≤ 𝑄). The symbol 𝑥𝑛 is a zero-mean random variable (RV) with variance E (cid:2)|𝑥𝑛|2(cid:3) = 𝜎2 𝑥 = 1, i.e., a normalized constellation is considered. The block is then spread in the FD by a factor 𝑈 = 𝑄/𝑁, called back-off rate (BOR), thanks to the spreading matrix S of size 𝑄 × 𝑁. S is designed in such a way not to increase the PAPR, as suggested in [46]."
2011.0684,data,142,2022-05-14,0,"Fig. 8 presents the SR enhancement thanks to the waterﬁlling optimization. The SR gain is deﬁned as the difference between the maximal SR obtained after and before optimization. As a reminder, before and after optimization, the mean energy radiated dedicated to the useful data remains unchanged, and the AN signal always remains in Bob’s null space. The optimal amount of data energy to inject is computed thanks to (36), (38), and (40) in order to ensure a maximal ergodic SR. The SR is then further increased via the waterﬁlling optimization procedure, as described in section III-E. As it can be observed, there is an increase of the SR gain for all three models and all BOR values thanks to the waterﬁlling."
2011.0684,data,16,2022-05-14,0,"From (7), a perfect data recovery is possible in high SNR scenarios."
2011.0684,data,164,2022-05-14,0,"Channel-based adaptation secrecy schemes were ﬁrst introduced in [17]–[19]. In these works, it was proven that positive secrecy rate (SR) can be obtained even if, on average, the channel between Alice and Bob is a degraded version of the one between Alice and Eve, by optimizing or adapting at the transmitter side the communication parameters. In doing so, the precoded signal is optimal for Bob’s channel but not for Eve’s one since they experience different fading. The concept of AN addition was ﬁrst established in [20]–[22]. The idea is to make Eve’s channel condition artiﬁcially degraded by intentionnaly adding an AN signal to the transmitter data. This AN signal is designed in such a way not to degrade Bob’s channel, therefore leading to a PLS enhancement, [1]."
2011.0684,data,177,2022-05-14,0,"AN is added either on all the channel taps or on a set of selected taps. While the condition for AN generation is given, its derivation is however not detailed. In [9], [41], [42], FD precoders using OFDM and AN injection are presented. In [9], the AN is injected in the null space of Bob but only limited decoding capacilities were attributed to Eve. In [41], [42], the idea is to use several OFDM subcarriers for dummy data transmission, i.e., several subcarriers are used for data obfuscation. However, the encryption information must be shared between the transmitter and the legitimate receiver, leading to more processing needed at the receiver. In addition, the security is enhanced when more subcarriers are used for data obfuscation, at the expense of the data rate. Furthermore, it is assumed that Eve has no knowledge about the legitimate link."
2011.0684,data,190,2022-05-14,0,"the optimal amount of transmitted data energy is derived thanks to eq.(36), (38), and (40). It leads to the coefﬁcient 𝛼𝐺 that maximizes the ergodic opt SR of the communication depending on G. It is a unique power coefﬁcient weighting the 𝑄 components of the useful transmitted data. Since the channel capacity is proportionnal to the subcarrier energy, and since Alice has access only to the instantaneous channel capacity at Bob, she can tune the amount of transmitted data energy at each subcarrier, i.e., she can apply a different weight at each subcarrier, to enhance the instantaneous capacity at Bob. In doing so, at each channel realization, she determines a new set of coefﬁcients, denoted w = [𝛼𝐺 𝜶𝐺 ]𝑇 , that enhances the instantaneous w,0 capacity at Bob. Because Bob and Eve channels are independent, enhancing the channel capacity at Bob does not change the ergodic capacity at Eve. This power allocation strategy is described below. 𝑤 = [𝛼𝐺 𝑤 ,0"
2011.0684,data,202,2022-05-14,0,"The left part of Fig.6 illustrates the values of 𝛼opt given by (36), (38), and (40) that maximize the ergodic SR determined from the closed-form approximations (35), (37), and (39), as well as obtained from the numerical simulations, as a function of the BOR. There is a slight discrepency between the analytical estimations of the optimal amount of data energy to inject using (36), (38), and (40), and its numerical estimation. However, the resulting analytical SR does not differ much from the maximal SR obtained in simulation, as it can be observed on the right part of Fig.6. Indeed, as observed in Fig.5, the SR is a function that varies slowly about its maximum, for all models. So, for a given BOR value, Alice can make a rough determination of 𝛼𝐺 opt depending on Eve decoding structure, and therefore the available SR, if 𝛿𝐵 and 𝛿𝐸 are known. One can also note that much more AN power should be injected to"
2011.0684,data,217,2022-05-14,0,"Abstract—A frequency domain (FD) time-reversal (TR) precoder is proposed to perform physical layer security (PLS) in single-input single-output (SISO) systems using orthogonal frequency-division multiplexing (OFDM) and artiﬁcial noise (AN) signal the data transmission to the legitimate receiver but degrades the decoding performance of the eavesdropper. This scheme guarantees the secrecy of a communication towards a legitimate user when the transmitter knows the instantaneous channel state information (CSI) of the legitimate link thanks to the channel reciprocity in time division duplex (TDD) systems, but does not know the instantaneous CSI of a potential eavesdropper. Three optimal decoding structures at the eavesdropper are considered in a fast fading (FF) environment depending on the handshake procedure between Alice and Bob. Closed-form approximations of the AN energy to inject the communication are derived. In addition, the required conditions at the legitimate receiver’s end to guarantee a given SR are determined when Eve’s signal-to-noise ratio (SNR) is inﬁnite. Furthermore, a waterﬁlling power allocation strategy is presented to further enhance the secrecy of the scheme. Simulation results are presented to demonstrate the security performance of the proposed secure system."
2011.0684,data,24,2022-05-14,0,"• The data and noise are independent of each other. • ℎ𝐵,𝑖 ⊥⊥ ℎ𝐵, 𝑗 , ∀𝑖 ≠ 𝑗,"
2011.0684,data,280,2022-05-14,0,"SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains. In [33], the information-theoretic secrecy capacity of an Offset-QAMbased ﬁlterbank multicarrier (FBMC-OQAM) communication over a wiretap frequency selective channel is studied. The authors compare the secrecy capacity of the FMBC-OQAM modulation with a cyclic preﬁx-orthogonal frequency-division multiplexing (CP-OFDM) modulation."
2011.0684,data,285,2022-05-14,0,"First, it can be seen that the analytical models given by (35), (37), and (39) well approximate the simulation curves and remain tight upper bounds for all scenarios. In addition, one can notice the importance of the AN addition on the SR. In fact, one can observe a SR enhancement with the addition of AN except for very high percentages of AN sent, i.e., when 1 − 𝛼 → 1, or for very low percentages of AN sent, i.e., 1 − 𝛼 → 0. Furthermore, for all three models, SR→ 0 when 1 − 𝛼 → 1 since the SINR at Bob and Eve drops to zero. As anticipated from sections III-B2a and III-B2c, high SR values are obtained, i.e., low decoding performance at Eve, when she has the same capabilities as Bob, and when she only knows her own channel. It is also observed that these two scenarios exhibit very similar behaviours except when 1 − 𝛼 → 0, as explained in section III-B2c. Finally, one can observe lower SR values when Eve implements a matched ﬁltering decoding structure. This can be understood from (23) where it is noticed that each transmitted data symbol is afffected by a real gain at Eve such that it beneﬁts from a frequency diversity gain, leading to higher decoding performances at Eve, and so, lower SR values. In fact, Eve SINR is about U times larger with the MF decoder compared to the SDS and the OC decoders."
2011.0684,data,31,2022-05-14,0,"In order to transmit secure data between Alice and Bob, the useful data is precoded and an AN signal w is added before transmission, as depicted in Fig.1."
2011.0684,data,335,2022-05-14,0,"While many works implement these schemes with multiple antennas at the transmitter, using for instance frequency diverse array beamforming [23], [24], directional modulation (DM) [25], antenna subset modulation (ASM) [26], nearﬁeld direct antenna modulation (NFDAM) [27], [28], spatial diversity [29]–[32], or waveform design [33], only few works perfom PLS using single-input single-output (SISO) systems [9], [34]–[42]. SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains."
2011.0684,data,4,2022-05-14,0,A. Data term
2011.0684,data,42,2022-05-14,0,"V,E + (𝑈 + 1) −𝑈𝜎2 V,B, and 𝑇4 = (𝑈 + 1)(𝑈 + 3)𝜎2 V,B, the optimal amount of data energy to transmit is:"
2011.0684,data,45,2022-05-14,0,"precoded data without pilot to Bob. From the FF assumption, Eve cannot learn the precoding performed by the transmitter. In this conﬁguration, Eve implements a decoding structure that takes beneﬁt of her own channel knowledge, denoted by OC."
2011.0684,data,48,2022-05-14,0,"However, if Alice sends a precoded pilot in addition to the precoded data to Bob, Eve is then able to evaluate her equivalent channel H∗ BHE, and therefore to implement a matched ﬁltering decoding structure, denoted by MF. This is depicted in Fig.3"
2011.0684,data,5,2022-05-14,0,1) Data term:
2011.0684,data,52,2022-05-14,0,"In doing so, each data symbol is transmitted onto 𝑈 different subcarriers with a spacing of 𝑁 subcarriers, introducing frequency diversity. The spread sequence is then precoded with the complex conjugate of Bob’s channel H∗ B, before addition of the AN signal w and transmission."
2011.0684,data,55,2022-05-14,0,"1) At the intended position: At Bob, a simple despreading operation is performed. Thanks to the precoding at the transmitter side, every received data symbol is affected by a real gain, as expressed in (6). The ergodic SINR for transmitted symbol 𝑛 is given by:"
2011.0684,data,57,2022-05-14,0,"The AN should not have any impact at Bob’s position but should corrupt the data everywhere else since Alice does not have any information about Eve’s instantaneous CSI, i.e., Eve is a passive node. Furthermore, this signal should not be guessed at the unintended positions to ensure the secure"
2011.0684,data,69,2022-05-14,0,"If Alice only transmits precoded data to Bob, Eve is not able to know anything but H𝐵𝐸 , the channel between Bob and Eve. In that situation, she cannot do better but to implement the same decoding structure as Bob, denoted by the abbreviation SDS. In that scenario, Eve only despreads the received sequence. This situation is presented in Fig.2."
2011.0684,data,72,2022-05-14,0,"where 𝐸 𝐺 3,𝑛 are respectively the data, noise and AN 𝑛th symbol components of the received signal at Eve’s position, for a particular decoding structure G. The expression of the SINR at Eve depends on the receiving structure G whose design depends on the amount of knowledge Eve can obtain. The expression (16) is therefore derived for the three considered scenarios."
2011.0684,data,75,2022-05-14,0,"Equations (45), and (46) are convex expressions that can be minimized as a function of 𝛼. Let’s denote 𝛼𝑆𝐷𝑆 ∞ , 𝛼𝑀 𝐹 ∞ , as the amount of data energy to inject, with respect to AN, in order to guarantee a desired communication SR when 𝛿𝐸 = ∞, respectively for the ﬁrst, second and third scenario. It can be shown that:"
2011.0684,data,76,2022-05-14,0,"PLS can be achieved by increasing the signal-plusinterference to noise ratio (SINR) at Bob and decreasing the SINR at Eve. This can be done by designing a suitable channel-based adaptive transmission scheme, and/or by injecting an artiﬁcial noise (AN) signal to the data. These time and/or techniques can be implemented in the space, frequency domains, [1], [15], [16]."
2011.0684,data,77,2022-05-14,0,"III. PERFORMANCE ASSESSMENTS The classical metric used to evaluate the degree of secrecy in a communication in the PLS ﬁeld is the secrecy channel capacity (SC). The SC is deﬁned as the maximum transmission rate that can be supported by the legitimate receiver’s channel while ensuring the impossibility for the eavesdropper to retrieve the data, [47]. In the ergodic sense, it can be expressed as:"
2011.0684,data,78,2022-05-14,0,"life. Wireless communication has become the dominant access for most of these services but it is intrisically unsecure due to its unbounded nature. Therefore, several issues have emerged and need to be urgently adressed such as data conﬁdentiality and integrity. The amount of leaked information is also an important feature that needs to be considered and minimized in order to guarantee secrecy of wireless transmissions, [1]–[3]."
2011.0684,data,8,2022-05-14,0,B. MF Decoder 1) Data term:
2011.0684,data,8,2022-05-14,0,C. Optimal amount of data energy to inject
2011.0684,data,87,2022-05-14,0,"This paper shows, consequently, with analytical and simulation results, that a scheme exploiting only frequency degrees of freedom can achieve a positive ergodic secrecy rate to considerably jeopardize any attempt of an eavesdropper to retrieve the data. This approach can be easily integrated into existing standards based on OFDM and does not necessitate extra hardware. However, a perspective of this work is to extend it to multiple antenna systems to assess the beneﬁt of the extra spatial degree of freedom."
2011.0684,data,89,2022-05-14,0,"the unintended position, the received signal before ZF equalization is given by √ (8). Let’s introduce E𝐺 𝛼GHEH∗ 2 = GvE and E𝐺 1 − 𝛼GHEw being respectively the data component, 3 = the noise component, and the AN component of the received signal at Eve for a particular decoding structure G. Using the Jensen’s inequality, an approximation of a lower-bound of the averaged SINR of the symbols 𝑛 at the unintended position can be derived as2:"
2011.0684,data,99,2022-05-14,0,"With the closed-form approximations of the SR (35), (37), and (39), it is possible to determine the SNR at Bob and the amount of data energy 𝛼 that guarantees a given SR, as a function of the communication parameters. Let’s introduce Δ being the targetted SR in bit per channel use, 𝛿𝑆𝐷𝑆 , 𝛿𝑀 𝐹 𝐵 , and 𝛿𝑂𝐶 being respectively Bob’s required SNR for the ﬁrst, 𝐵 second and third investigated scenario. Remembering that 𝜎2 V,B = 1"
2011.0684,"data, data available",86,2022-05-14,0,"[36] Y. Lee, H. Jo, Y. Ko, and J. Choi, “Secure index and data symbol modulation for ofdm-im,” IEEE Access, vol. 5, pp. 24 959–24 974, 2017. [37] J. M. Hamamreh, E. Basar, and H. Arslan, “Ofdm-subcarrier index selection for enhancing security and reliability of 5g urllc services,” IEEE Access, vol. 5, pp. 25 863–25 875, 2017."
2011.07981,code,35,2022-05-14,0,"[33] “Distribution code,” ESB Networks,” Grid code, April 2016. [34] W. H. Kersting, “Radial distribution test feeders,” IEEE Transactions on"
2011.07981,data,126,2022-05-15,0,"To examine the performance of the proposed data recovery approach, the interruption of the communication channels from the substation or one of the DERs is simulated, resulting in simultaneous loss of four signals in the former case or three signals in the latter cases. Without a recovery plan, considering the mean values in the training set for the missing signals seems like the most logical choice. Fig. 5 presents the performance of DA in this scenario. As noted, DA can no longer predict the correct conﬁguration, especially for the cases in which the communication from DER4, DER5, or DER6 is interrupted. This establishes the vulnerability of DA under the interruption of communication channels."
2011.07981,data,130,2022-05-15,0,"To simulate anomalous measurements, at each step, the measurements from one of the DERs are multiplied by 0.9, to examine a negative bias, and then 1.1, to examine a positive bias. α is calculated with the manipulated values, as well as the original values. Fig. 9 presents the percentage of the data points placed in each speciﬁed range of α. As noted, with the original values, for the majority of the data points α is below 60 (more than 92% of the data points, in all the scenarios), while with the manipulated values, α takes larger values (above 60 for over 95% of the data points, in all the"
2011.07981,data,133,2022-05-15,0,"While deploying information technologies streamlines the system management, it can cause more dependencies between the physical and cyber components, which makes the system more vulnerable to natural disasters and cyber-attacks [15]. In fact, proneness to loss of online data has undermined the popularity of the approaches that rely on online measurements in practice, especially when the system may face the loss of multiple signals, because of, e.g., interruption of communication channels, or cyber-attacks. Securing the power system management against cyber-attacks has been the subject of many recent studies, like [16] and [17]. This highlights the importance of resiliency in the development of a TI function. None of the reviewed TI approaches is resilience-oriented."
2011.07981,data,170,2022-05-15,0,"This paper is dedicated to developing a topology identiﬁcation function for distribution networks that relies only on the measurements available to DERMS. These measurements include the operating condition of the grid and DERs [1]. Considering the applicability of discriminant analysis (DA) as a widely-used statistical classiﬁer, DA is employed for this purpose. While originally developed for DERMS, this method is applicable for DMS too since DMS and DERMS share the DERs measurements [1]. To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,18,2022-05-15,0,It is worth emphasizing that the quadratic programming data recovery approach is applicable only when some mea 5
2011.07981,data,186,2022-05-15,0,"In addition to the loss of multiple signals, the TI approach should be resilient against malicious data. In this subsection, a benchmark is introduced to detect if one or a group of measurements contain anomalous values. Suppose the measurements from a speciﬁc meter, i.e., X s = [xs l ], are suspected to contain anomalous values. Considering (10), the information of the other measurements can be exploited to estimate the normal value of X s, i.e., X r = [xr l ]. To discern if X s is anomalous, the idea is to compare the likelihood of X r associating with any of the network topologies, i.e., ∪K i=1(ki) denotes the union of all the possible topologies. For this purpose, the likelihood ratio, denoted by α = Λ(X r : X s| ∪K i=1 (ki)), is employed as a benchmark to detect if X s is anomalous. The likelihood ratio is deﬁned as:"
2011.07981,data,188,2022-05-15,0,"Abstract—Network topology identiﬁcation (TI) is an essential function for distributed energy resources management systems (DERMS) to organize and operate widespread distributed energy resources (DERs). In this paper, discriminant analysis (DA) is deployed to develop a network TI function that relies only on the measurements available to DERMS. The propounded method is able to identify the network switching conﬁguration, as well as the status of protective devices. Following, to improve the TI resiliency against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. By deploying the propounded data recovery approach and Bayes’ theorem together, a benchmark is developed afterward to identify anomalous measurements. This benchmark can make the TI function resilient against cyberattacks. Having a low computational burden, this approach is fast-track and can be applied in real-time applications. Sensitivity analysis is performed to assess the contribution of different measurements and the impact of the system load type and loading level on the performance of the proposed approach."
2011.07981,data,51,2022-05-15,0,"[30] J. Hallinan, “Chapter 2 - data mining for microbiologists,” in Systems Biology of Bacteria, ser. Methods in Microbiology, C. Harwood and A. Wipat, Eds. Academic Press, 2012, vol. 39, pp. 27 – 79."
2011.07981,data,58,2022-05-15,0,"Fig. 9. The percentage of the data points in the test set placed in each speciﬁed range of α, with the normal values and manipulated values of the measurements from: a) DER1, b) DER2, c) DER3, d) DER4, e) DER5, f) DER6."
2011.07981,data,66,2022-05-15,0,"As discussed, the loss of multiple signals and malicious data are two phenomena that jeopardize the applicability of approaches relying on online measurements in practice. In this section, an approach is proposed to recover the original values of the measurements that are missing or suspected to contain anomalous values. From now on, they are referred to as the missing signals."
2011.07981,data,78,2022-05-15,0,"It should be emphasized that in this approach, we do not consider the objective of network reconﬁguration operations, but instead, to form the training and test data sets, all the possible conﬁgurations are considered and in each conﬁguration, all the probable variations of loads consumption and DERs generation are simulated. In this order, whatever the objective of network conﬁguration is, it will include a subset of this data set."
2011.07981,data,82,2022-05-15,0,"Afterward, a quadratic programming optimization approach, based on the recovery of the lost signals, was presented to make the proposed TI approach resilient against the interruption of communication channels. The speciﬁc optimization problem can be settled efﬁciently and a global extremum is guaranteed. Furthermore, by exploiting this data recovery approach, a benchmark was introduced to detect anomalous measurements. This benchmark can be employed to enhance the resiliency of the proposed TI against cyber-attacks."
2011.07981,data,85,2022-05-15,0,"To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,864,2022-05-15,0,"This approach requires a low processing time, which makes it suitable to be employed in real-time applications. This approach is able to consider weakly-meshed conﬁgurations, works with any type of mea (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)(cid:12)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:11)(cid:22)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:23)(cid:23)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:11)(cid:24)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:19)(cid:25)(cid:26)(cid:27)(cid:17)(cid:18)(cid:25)(cid:26)(cid:27)(cid:17)(cid:28)(cid:25)(cid:26)(cid:27)(cid:17)(cid:12)(cid:25)(cid:26)(cid:27)(cid:17)(cid:22)(cid:25)(cid:26)(cid:27)(cid:17)(cid:24)(cid:25)(cid:26)(cid:27)(cid:17)(cid:20)(cid:25)(cid:26)(cid:27)(cid:17)(cid:29)surements, can treat unbalanced networks, and can be applied to identify the switching conﬁguration and the operation of protective devices."
2011.07981,data,9,2022-05-15,0,of the proposed data recovery approach are presented.
2011.10563,code,23,2022-05-15,2,The code of HINDSIGHT++ is exclusively written in R and a proof of concept implementation is available in [8]. The
2011.10563,code,80,2022-05-15,0,"backbone is the CRAN interface to Keras, a high-level, userfriendly Application Programming Interface (API) which enables quick and dynamic experimentation with DL algorithms, while it further allows for easy neural network architectural prototyping. Keras is designed to run on top of multiple back-end environments, including Theano, CNTK, and Tensorflow (default). Furthermore, it offers the option for executing the code either on top of CPUs or GPUs."
2011.10563,data,108,2022-05-15,0,"[11] V. Raida, P. Svoboda, M. Kruschke, and M. Rupp, “Constant rate ultra short probing (crusp): Measurements in live lte networks,” in ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1–6, IEEE, 2019. [12] C. Maier, P. Dorﬁnger, J. L. Du, S. Gschweitl, and J. Lusak, “Reducing consumed data volume in bandwidth measurements via a machine learning approach,” in 2019 Network Trafﬁc Measurement and Analysis Conference (TMA), pp. 215–220, IEEE, 2019."
2011.10563,data,109,2022-05-15,0,"For the bandwidth prediction task under study, therefore, we ﬁnd that hyperparameter optimization is especially vital in 5G networks, since they introduce higher data rates and larger variations over time. Coupled with the mobility aspect, a suboptimal hyperparameter set will fail to capture the intrinsic network characteristics. Among the two available optimization solutions under study, we observe that BOA provides slightly better results, since it utilizes probability theory concepts for minimizing the error. We further observe that 4G networks offer signiﬁcantly lower data rates with a lower variation factor which signiﬁcantly diminishes the need for advanced hyperparameter optimization solutions."
2011.10563,data,160,2022-05-16,0,"• Random Search (RS). Instead of evaluating all possible hyperparameter combinations, RS iterates over a smaller sample [3]. As the number of iterations increase, the probability of converging to a better solution increases as well. The number of iterations required to reach to a good solution depends on several factors including the size of the hyperparameter space, search range, data complexity, and so forth. RS has a fairly simple implementation and does not require any scientiﬁc knowledge for the hyperparameters under study. In addition, it provides signiﬁcant gains in terms of computational complexity, since it only tests a limited number of combinations, which is a safe approach under the assumption that non all hyperparameters are equally important. As a rule of thumb, there is a 95% chance that RS will reach to a good solution with only 60 iterations."
2011.10563,data,18,2022-05-16,0,Multivariate data analysis often involves features with variable scales. Since LSTM weight allocation is prone to such
2011.10563,data,260,2022-05-16,0,"The topic of bandwidth forecasting in MBB networks has been in the spotlight for a long time. However, it has lately become more prominent due to the fast-expanding next generation network infrastructures, which in turn, yield to a larger exchange of network data, hence, increasing the need for higher and more accurate predictions. Furthermore, 5G introduces new frequency bands, such as the mmWave, which drives data rates to groundbreaking limits, thus, posing new challenges to the forecasting process. Research studies focus on integrating robust and effective algorithms, while maintaining the implementation and computational complexity in acceptable levels. Over the years, numerous solutions have been proposed ragning from traditional statistical methods, such as the Naive, Autoregressive Integrated Moving Average (ARIMA), and Vector Autoregression (VAR), to considerably higher complexity algorithms, namely dynamic linear models, TBATS [1] (i.e., based on exponential smoothing), and Prophet [2] (i.e., widely used by Facebook). Within the context of Artiﬁcial Intelligence (AI), both Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM) networks have been projected as high efﬁcient algorithms for time series modeling. Between the two, LSTM networks appears to be the most ﬁtting solution for the bandwidth forecasting study, considering the complex long term dependencies and the unforeseeable behavior that mobile traces display due to mobility."
2011.10563,data,270,2022-05-16,0,"A supervised Machine Learning (ML) solution for downlink throughput prediction in MBB networks is proposed in [9]. The authors argue that ML can be used as a tool for signiﬁcantly reducing the data volume consumption over the network, while maintaining the predictive error in acceptable levels. Furthermore, in [10], Raida et. al leverage constant rate probing packets to estimate available bandwidth in a controlled Long-Term Evolution (LTE) environment, while in [11], they extend their work by further testing and validating the developed framework in live LTE networks. In [12], Maier et al. introduce a novel AI model with feed-forward neural networks for both downlink and uplink bandwidth forecasting. To ﬁnd a ﬁtting speed test duration, the authors investigate two scenarios. First, they train a model with a duration ﬁxed to a value lower than the default, while second, they dynamically determine a duration by using the results of a pretrained neural network model. Experimental approaches studying the problem of bandwidth prediction have also been addressed in [13, 14, 15, 16, 17, 18, 19]. Beyond empirical-based studies, theoretical models have also been published. In [20], Gao et al. introduce a theoretical learning based throughput prediction system for reactive ﬂows, while authors in [21] propose a novel stochastic model for user throughput prediction in MBB networks that considers fast fading and user location."
2011.10563,data,3,2022-05-16,0,3.1.2 Data Import
2011.10563,data,3,2022-05-16,0,3.2 Data Preprocessing
2011.10563,data,30,2022-05-16,0,"The objective of this ﬁrst phase is two-fold. First, to provide support with all required software components, and second, to initiate the data import process."
2011.10563,data,32,2022-05-16,0,"data, it is critical that we consider a transformation function leveraging one of the following state-of-the-art normalization methods, known as min-max, z-score, and tanh [37]."
2011.10563,data,39,2022-05-16,0,"Fig. 2: A high-level representation of the HINDSIGHT++ control ﬂow diagram. The operations for each block are described in Section 3, highlighted with a matching title. Dashed lines delimit the data preprocessing stage."
2011.10563,data,42,2022-05-16,0,"[13] Y. Liu and J. Y. Lee, “An empirical study of throughput prediction in mobile data networks,” in 2015 IEEE Global Communications Conference (GLOBECOM), pp. 1– 6, IEEE, 2015."
2011.10563,data,44,2022-05-16,0,"Finally, data undergoes transformation and reshaping functions to comply with the LSTM input requirements. The outcome of this process are the 2-D and 3-D matrices summarized in Table 1. X and Y represent the LSTM input and output, respectively."
2011.10563,data,53,2022-05-16,0,"[34] R. Yu, Y. Li, C. Shahabi, U. Demiryurek, and Y. Liu, “Deep learning: A generic approach for extreme condition trafﬁc forecasting,” in Proceedings of the 2017 SIAM international Conference on Data Mining, pp. 777–785, SIAM, 2017."
2011.10563,data,54,2022-05-16,0,"[33] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, “Long short-term memory neural network for trafﬁc speed prediction using remote microwave sensor data,” Transportation Research Part C: Emerging Technologies, vol. 54, pp. 187–197, 2015."
2011.10563,data,55,2022-05-16,0,Experimental Design: All experiments are carried out on an x86-64 architecture with a 16GB RAM while an NVidia Titan X graphics card is used to accelerate data preprocessing. The operating system is based on a Linux Ubuntu 16.04.6 LTS distribution. All reported RT values could signiﬁcantly vary under different architectural designs.
2011.10563,data,60,2022-05-16,0,"[32] J. Wang, J. Tang, Z. Xu, Y. Wang, G. Xue, X. Zhang, and D. Yang, “Spatiotemporal modeling and prediction in cellular networks: A big data enabled deep learning approach,” in IEEE INFOCOM 2017-IEEE Conference on Computer Communications, pp. 1–9, IEEE, 2017."
2011.10563,data,61,2022-05-16,0,"4. In addition, HINDSIGHT++ supports one-hot-encoding for converting categorical variables to binary features and a number of options for dealing with missing data (e.g., interpolation, Moving Average (MA), Kalman ﬁltering, etc.). We do not cover these additional aspects in detail, since they are not part of our data."
2011.10563,data,77,2022-05-16,0,"The process of data preparation in LSTM networks is rather perplexed time consuming. The complexity factor signiﬁcantly increases considering the different data format combinations (e.g., multivariate, parallel) and parameters (e.g., lags, prediction steps). Figure 2 (dashed frame) encloses all steps followed for providing compatibility with the LSTM models. In line with the AutoML concept, data preprocessing is treated as a black box4."
2011.10563,data,80,2022-05-16,0,"Michael Alexander Riegler, Chief Research Scientist, SimulaMet, holds degrees in computer science from the University of Oslo (UiO) and Klagenfurt experienced in University. He is medical multimedia data analysis and understanding, image processing, image retrieval, parallel processing, crowdsourcing, social computing and user intent. He is a recognized expert in his ﬁeld, serving as a member of the Norwegian Council of Technology on Machine Learning for Healthcare."
2011.10563,data,93,2022-05-16,0,"Takeaways: The hyperparameter optimization paradigm is critical for improving several performance aspects in LSTM networks. However, its signiﬁcance is highly dependant on the underlying data attributes. For example, sequences that display large signs of variance over time are notably harder to predict, thus requiring hyperparameter conﬁgurations of high precision. On the contrary, sequences that either meet the stationarity test requirements or show consistent trends across time have limited gains, since most combinations of hyperaparameters within the predeﬁned search range will provide comparable performance."
2011.10563,data,96,2022-05-16,0,"Training of neural networks is an iterative process that involves identifying unique data properties and learning the model parameters. At each iteration, we track both training and validation error, which we use as bias and variance measures, or else, indicators of underﬁtting or overﬁtting. The validation set always reﬂects the last portion of the initial training set. We test the ﬁnal model on an never-seenbefore testing set and use the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as our error metrics."
2011.10563,"data available, code, open-source, code available, data, open-source code",70,2022-05-16,0,"3.1.1 Load Prerequisites HINDSIGHT++ relies on a number of open-source packages, or else, a collection of functions, compiled code, and data, available in CRAN. To guarantee software stability and robustness, it is critical that all dependencies are pre-installed in the system. Therefore, we enable a veriﬁcation process that automatically locates, installs, and loads any missing packages and libraries."
2011.10563,"data, code, open-source",186,2022-05-16,2,"In this work, we present HINDSIGHT++, a novel, lightweight, versatile, and open-source R-based framework for bandwidth forecasting experimentation in MBB networks with LSTM networks2. The main source of motivation behind the design of HINDSIGHT++ is two-fold. First, to limit the implementation complexity of LSTM networks, and second, to minimize the predictive error. Toward this goal, we adopt the concept of Automated Machine Learning (AutoML), an acronym used to describe the process of data pipeline automation. In particular, AutoML covers the complete learning routine, from raw data preprocessing to the ﬁnal model production. Moreover, HINDSIGHT++ is compliant with parallel and multivariate data and supports a wide forecasting horizon. Last, it offers different hyperparameter optimization options and LSTM variants. The source code of HINDSIGHT++ is structured in a dynamic fashion, so it can accommodate new features and algorithms. Users are encouraged to integrate more libraries and software components to satisfy their cause, but also to enrich the capabilities of the framework."
2011.10563,"data, dataset",112,2022-05-16,0,"Next is the data import process where users select a number of f ∈ [1, z] .csv ﬁles via an interactive window. Due to its universal design, HINDSIGHT++ operates under the assumption that the selected datasets are parallel3. In addition, equidimensional datasets are required, since LSTM models expect input arrays with equal sizes. The last prerequisite is that the dependent variable locates in the ﬁrst array position followed by any number of additional of regressors. We annotate the matrix dimensions for each dataset as df ∈ Rn×m, where n and m represent the number of samples and features, respectively."
2011.10563,"data, dataset",138,2022-05-16,0,"Furthermore, we observe that LSTM models operate on a low MAE region which is the result of the low-variance data rates experienced in 4G under mobility, thus restricting the room for vast performance improvement. Across the two dataset portions, we ﬁnd that, in average, validation error is lower than testing error, which is the common scenario, since model tuning takes place in the former set. Last, Figure 5 illustrates the bandwidth time series graph of three NYU-METS traces10. Color is used for mapping the forecasting lines ( (RSV L)) with the associated part of the trace, i.e., training, validation, and testing set. As evident, LSTM models follow the data trends along the traces with minimal error deviation."
2011.10563,"data, dataset",172,2022-05-16,0,"5Gophers: Exploring potential patterns across the 5G bandwidth measurements, we observe that in some scenarios the optimized LSTM models feature a third HL. Again, this is a rather expected result considering the higher data rates and variation in 5G in addition with the challenging bandwidth trends over time and under mobility. Likewise, the average number of neurons per HL across all layers equals to 197, 79, and 32 for HL-1, HL-2 and HL-3, respectively. We observe that the number of neurons per HL for both datasets follow a decreasing trend across the network, which also coincides with the neuron selection from authors in [30] (i.e., HL-1=256 and HL-2=128). Additionally, the average LR and BS across all traces equal to 0.004 and 28, respectively, while the number of epochs averages 76, which is an ≈ 17% increase comparing to the 4G counterpart result hinting a slower gradient descent convergence."
2011.10563,"data, dataset",62,2022-04-21,0,"At a glance, we observe that 5Gophers error show an ≈ 100-fold increase when compared to NUY-METS.This observation is in line with our expectations, considering the higher data rates that users experience in a 5G network environment (i.e., up to 1500Mbps, see Figure 6). Next, we follow up the results discussion per dataset."
2011.10563,"data, dataset",93,2022-05-16,0,"Takeaways: The designated results reveal that hyperparameter optimization is crucial for achieving superior performance in 5G networks, while trading computational resources. The above ﬁnding is further reinforced by the reported model hyperparameters, which signiﬁcantly vary across technology standards, mobility scenarios, and to some extent network operators. Across the two datasets, we ﬁnd that 5G data require LSTM conﬁgurations with an increased number of HLs and epochs, while featuring signiﬁcantly lower BS values, which further reveals the challenging network conditions in such scenarios."
2011.10563,"data, dataset, open-source",240,2022-05-16,3,"6 CONCLUSIONS AND FUTURE WORK In this paper, we studied the challenging task of bandwidth forecasting in next-generation MBB networks under mobility. To ease our goal, we designed HINDSIGHT++, an open-source R-based framework that allows for LSTM experimentation in time series data. We speciﬁcally focused on the hyperparameter optimization aspect which is critical for achieving state-of the performance. We analysed and performed a comparative analysis between two opensource datasets of bandwidth measurements in operational 4G and 5G networks, respectively, aiming to quantify the LSTM performance improvement under different conﬁguration setups. Results show that hyperparameter optimization provides signiﬁcant beneﬁts under 5G settings compared to 4G settings and the optimal parameters for 4G cannot be directly applied considering the substantially higher data rates and variation over time that users experience in 5G network conditions. As for the future, we plan on integrating feature selection algorithms, additional LSTM variants (e.g., Convolutional Neural Networks (CNN) LSTM and multiplicative LSTM [51]), and provide support for supervised classiﬁcation tasks. Furthermore, we will carry out a dedicated measurement campaign to collect additional network features (e.g., signal strength, latency, etc.). Using the new data, we aim to explore potential correlation or causation relationships and show whether or not they bring any signiﬁcant gains to the LSTM performance."
2011.10563,"data, dataset, open-source data, open-source",223,2022-05-16,1,"Abstract—Bandwidth forecasting in Mobile Broadband (MBB) networks is a challenging task, particularly when coupled with a degree of mobility. In this work, we introduce HINDSIGHT++, an open-source R-based framework for bandwidth forecasting experimentation in MBB networks with Long Short Term Memory (LSTM) networks. We instrument HINDSIGHT++ following an Automated Machine Learning (AutoML) paradigm to ﬁrst, alleviate the burden of data preprocessing, and second, enhance performance related aspects. We primarily focus on bandwidth forecasting for Fifth Generation (5G) networks. In particular, we leverage 5Gophers, the ﬁrst open-source attempt to measure network performance on operational 5G networks in the US. We further explore the LSTM performance boundaries on Fourth Generation (4G) commercial settings using NYU-METS, an open-source dataset comprising of hundreds of bandwidth traces spanning different mobility scenarios. Our study aims to investigate the impact of hyperparameter optimization on achieving state-of-the-art performance and beyond. Results highlight its signiﬁcance under 5G scenarios showing an average Mean Absolute Error (MAE) decrease of near 30% when compared to prior state-of-the-art values. Due to its universal design, we argue that HINDSIGHT++ can serve as a handy software tool for a multitude of applications in other scientiﬁc ﬁelds."
2011.10563,"data, open-source",218,2022-05-16,2,"Over the past years LSTM networks have been successfully applied for network performance related tasks. Cui et al. proposed a stacked bidirectional LSTM architecture for network-wide trafﬁc estimation [29], while authors in [30] studied the applicability of LSTM for a real-time bandwidth prediction problem. Furthermore, Zhao et. al. focused on short-term trafﬁc estimation by considering temporal–spatial correlation [31], whereas, authors in [32] proposed a hybrid framework that combines LSTM networks and an auto-encoder based model for network prediction from a spatio-temporal angle. Additional work on mobile trafﬁc forecasting has been further proposed in [33, 34, 35]. Distinct from the preceding studies, this paper stands out and distinguishes itself by putting focus on the hyperparameter optimization aspect applied on data from different countries, network operators, technology standards, and mobility scenarios. In addition, to the best of our knowledge, this is the ﬁrst study that attempts to perform a bandwidth forecasting comparative analysis between 4G and 5G networks under mobility with LSTM networks. Last, we make available to the community an open-source framework which follows an end-to-end AutoML paradigm and allows for ample experimentation with LSTM networks."
2011.10563,dataset,107,2022-05-16,1,"We study the topic of bandwidth forecasting in commercial 5G networks under mobility, while we revisit Fourth Generation (4G) scenarios and draw additional insights by performing a comparative analysis between the two technology standards. Thereupon, we leverage two opensource datasets, 5Gophers, which features network measurements from operational 5G networks in the US, and NYU Metropolitan Mobile Bandwidth Trace (NYU-METS), a counterpart dataset that comprises of hundreds of 4G bandwidth measurements in the wild. We adopt systematic investigation to showcase the signiﬁcance of hyperparameter optimization at each use case when compared to existing state-of-the-art approaches."
2011.10563,dataset,122,2022-05-16,0,"made after closely observing the fast saturation rate of the gradient descent error. However, we argue that future 5G applications may require a higher number of iterations to converge to a good solution, although, this would result to a substantial increase of the RT complexity. Second, the hyperparameter search range can be stretched to accom modate a higher pool of values, which could potentially add to the performance. The selected values were decided as a compromise to the performance versus RT complexity tradeoff. Last, both datasets are composed of traces with a limited number of samples which can have a negative effect on the LSTM performance since they are known to perform"
2011.10563,dataset,123,2022-05-16,0,"At present time, 4G networks are considered the norm of wireless telecommunication systems sustaining a big portion of society’s global network functions. However, the recent advances in technology indicate that in the upcoming decade 5G will become the next big thing with several network operators already moving toward its adoption. Therefore, it is dire that we identify the principal network characteristics and trends between the two technology standards and show to what extent they dictate the performance of LSTM networks. In addition, our experiments aim to identify the potential gains of hyperparameter optimization in delivering more efﬁcient and robust models. Next, we provide a description of the two datasets under study."
2011.10563,dataset,174,2022-05-16,0,"VALTE012301230123MAERMSE7TrainA7TrainBBus57Bus62ABus62BNTrainLIMSVLRSVLRSBDBOAVLVALTE050100150200050100150200050100150200MAERMSESprintASprintBVerizonAVerizonBWalkingMSVLRSVLRSBDBOAVL010002000300040005000051525t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)0500150025000246812t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)to the bandwidth forecasting problem. Likewise, we compare RSV L and RSBD to remove the hyperparameter optimization bias. On the one hand, we ﬁnd that Bidirectional LSTM networks outperform the Vanilla counterpart in two out of the four driving traces (i.e., SprintA and VerizonA) with an average testing MAE improvement of ≈ 15%. On the other hand, however, they seem to be performing worse in the other two driving traces (i.e., SprintB and VerizonB) by an average testing MAE percentage of ≈ 15%. This ﬁnding veriﬁes the fact that bidirectional LSTM networks are very dependant on the respective dataset underlying characteristics. Therefore, the question of whether or not they are ﬁtting for a given application can only be answered after systematic empirical analysis."
2011.10563,dataset,193,2022-05-16,0,"Tables 4 and 5 provide additional insights with respect to the RT complexity. In particular, each row reports the RT [min] per trace and across the available conﬁgurations. In average, we observe that RSBD present a higher RT of ≈ 54% when compared to the RSV L. The above result is not balanced across the two datasets due to the randomness bias during the hyperparameter selection process. For example, instances of the algorithm where higher values for HLs or neurons are selected will require more computational power to accomplish. On the other hand, the average increase in RT between BOAV L and RSV L reaches to a percentage of ≈ 154%, which can be attributed to the following. First, since BOA leverages the Bayes Theorem for minimizing the gradient descent error, it requires more time to reach to a good solution, opposed to RS which randomly scans the search space. And second, the R BOA implementation is not optimized to run on top of a GPU, which adds to the RT overhead."
2011.10563,dataset,215,2022-05-16,0,"Setting the Baseline: For the baseline experiments (M SV L), we adopt a stacked Vanilla architecture with two HLs and the model hyperparameters7 reported in [30] and illustrated in Table 2. Note that these hyperparameters was prior optimized (e.g., by means of sensitivity analysis or by using indicative values based on previous experience with datasets of similar nature) on the same dataset [30]. LSTM experimentation takes place in a per-trace fashion, i.e., the ﬁrst x% part of the sequence is used for training and validation, while the remaining y% is used for testing. In addition, we adopt a walk-forward validation, or else, rolling forecast approach, where each prediction value is used as new input to the model for forecasting the next step. Last, we overcome the random LSTM weights initialization bias by repeating each baseline experiment 25 times and reporting the median values for both of our error metrics8. Toward Performance Optimization: We repeat the experimental campaign by enabling hyperparameter optimization while testing the available LSTM variants. Our goal is to test whether the hypothesis stating that a set of ﬁxed model hyperparameters is sub-optimal for modelling"
2011.10563,dataset,39,2022-05-16,0,"To the best of our knowledge, little research has been conducted to provide guidelines on the LSTM hyperparameter selection, since most studies focus on particular architectures and datasets [28]. Therefore, we are moving"
2011.10563,dataset,46,2022-05-16,0,0204060801001200200400t (s)Bandwidth (Mbps)0204060801001200200400600t (s)Bandwidth (Mbps)02040608010012005001500t (s)Bandwidth (Mbps)02040608010012004008001200t (s)Bandwidth (Mbps)01002003004005006000400800t (s)Bandwidth (Mbps)1024102410240.011001283units1units2units3lrepochsbslayers7TrainA1024102410240.011001283units1units2units3lrepochsbslayersBus62A1024102410240.011001283units1units2units3lrepochsbslayersLIMSVLRSVLRSBDBOAVL1024102410240.011001283units1units2units3lrepochsbslayersSprintA1024102410240.011001283units1units2units3lrepochsbslayersSprintB1024102410240.011001283units1units2units3lrepochsbslayersVerizonA1024102410240.011001283units1units2units3lrepochsbslayersVerizonB1024102410240.011001283units1units2units3lrepochsbslayersWalkingMSVLRSVLRSBDBOAVLbest with larger datasets.
2011.10563,dataset,54,2022-05-16,0,"across the available traces and datasets. Figures 7 and 8 illustrate the selected hyperparameters along the selected NYU-METS traces and 5Gophers datasets, respectively. The edges of each radar plot map to the maximum search range values across the seven hyperparameters, while color is used to discriminate between each conﬁguration."
2011.10563,dataset,66,2022-05-16,0,"LSTM performance heavily relies on the selection of the model hyperparameters and the underlying dataset characteristics (i.e., periodic patterns and trends). Since these two aspects are tightly tied with each other, knowing how to efﬁciently tune an LSTM model becomes an important but rather challenging task, thus, urging for a hyperparameter optimization solution. Among the most popular approaches"
2011.10563,dataset,72,2022-05-16,0,"The next phase involves splitting each dataset into a training (80%) and a testing (20%) set. Both of these percentages can be conﬁgured as appropriate. We deﬁne the dimensions for f ∈ Rnte×m, where ntr and each set as dtr nte are the number of samples for the training and testing set, respectively, while m represents the number of features."
2011.10563,dataset,73,2022-05-16,0,"Time series visualization can be used to highlight certain phenomena at glance, including periodic patterns or trends, extreme outliers, odd observations, abrupt changes over time, and so forth. Moreover, it allows for visual inspection of the developed models to assess how well they ﬁt the datasets under study. A different color scheme is used to distinguish the training, validation, and testing set."
2011.10563,"dataset, open-source",24,2022-05-16,2,"• Last, we open-source HINDSIGHT++ to the community allowing for further experimentation with a variety of datasets and applications [8]."
2011.10563,"dataset, open-source data, open-source",188,2022-05-16,1,"5.1 Datasets We exploit 5Gophers5 [48, 49], the ﬁrst open-source dataset that attempts to study the performance of 5G in commercial settings, featuring three operational operators (two mmWave and one mid-band carrier) in the US. 5Gophers allows for a wide range of analyses, including impact of mobility, handoffs, network operator performance, and many more. In this study, we focus on a single walking, and two driving traces (i.e., medium mobility between 20 to 50Km), since we are primarily interested in the mobility aspect. The walking trace comprises from both 4G and 5G readings, followed by additional features, such as cell and handover identiﬁers6. The experimental setup under medium mobility features three SGS10 devices mounted on a vehicle’s front windshield and measuring the iPerf performance for three major operators in Atlanta, i.e., Sprint, T-Mobile, and Verizon. A bandwidth estimation measure is recorded every 120sec. All traces alongside the number of available samples are reported in Table 4."
2011.10563,"dataset, open-source data, open-source",96,2022-05-16,1,"We complement our study with NYU-METS [50], an open-source dataset that comprises of 4G LTE bandwidth measurements carried out in the New York University (NYU) metropolitan area. NYU-METS covers several transportation modes including bus, subway, and ferry. The experimental setup features an LTE-enabled mobile device and a remote server located at the NYU lab. Transmission Control Protocol (TCP) measurements are carried out using the iPerf cross-platform tool with a sampling rate of one second. Likewise, Table 5 lists the available bandwidth traces"
2011.10563,"dataset, used dataset",63,2022-05-16,0,"Overall, it is evident that the selected LSTM models adopt disparate strategies for minimizing the gradient descent error function, a result that veriﬁes the high degree of complexity in the neural network ecosystem. Next, we isolate a number of use cases per dataset in an effort to discover any hidden patterns or trends alongside the pool of hyperparameters."
2011.10563,github,37,2022-05-16,0,"[50] https://github.com/NYU-METS/Main. [51] B. Krause, L. Lu, I. Murray, and S. Renals, “Multiplicative lstm for sequence modelling,” arXiv preprint arXiv:1609.07959, 2016."
2011.10563,provide implementation,61,2022-05-16,0,"4 FRAMEWORK VALIDATION We organize the following content into three main parts. First, we provide an overview of HINDSIGHT++ technical and implementation details. Next, we discuss the training, validation, and testing process, while we outline the key LSTM parameters. Last, we present the available benchmarking options alongside a brief visualization description."
2011.10916,data,10,2022-05-16,0,"5. handling larger dimension-ed data efﬁciently across the modalities,"
2011.10916,data,122,2022-05-16,0,"Our observation is that aligning the entire data doesn’t improve the accuracy by a signiﬁcant margin, while we also agree that adding more depth to the DCCA in section 4.2 would certainly give us better results as the parameter count would go up. Generally, with multimodal fusion, we need a lot of parameters to capture all the nuances and complexity attached with cross-modal context and capturing these long-range minute dependencies might be improved as the parameter count go up. Thus, currently, the best way to improve accuracy seems to be adding more parameters on the upper sections of the model. We also suggest the following other ideas for the future work:"
2011.10916,data,137,2022-05-16,0,"Now we brieﬂy discussion the hyper-parameter selection. Most of the attention literature uses Adam optimizer and we found that for this model, it gave stable decrease in the loss as well. Due to the large parameter count, model time complexity and limited computational resources; we only tried handful of hyper-parameter setups. We split train:validation:test data into approxiamtely 18.5k:2.25k:2.25k sample ratio, which is a 80:10:10 percentage ratio. We used the validation set for hyper-parameter tuning, upon ﬁnding the satisfactory setup, we merged validation and train set into one and re-learned the model with it; testing the generalize-ability of the model only on the test set. The selected hyper-parameters are given in Table 1. The model is shown in Figure 2c."
2011.10916,data,147,2022-05-16,0,"We ﬁrst use our classiﬁcation head on all 3 unimodal delta self-attention module individually, to compare the gains of the proposed multimodal fusion technique later. Here we expect that multimodal fusion will add more context to the data and thus, the gains should be comparably higher. Third experiment is between aligned and unaligned data, with aligned data, we have aligned the sequence intervals of all 3 modalities according to the base text modality, parts of any modality not in the interval have been discard; with the unaligned data, we are not discarding any parts regardless of their inclusion in any interval, nor we have any “base” or reference modality. Our expectation is that aligned data of course gives us an indicator of which range to look into for a spoken word’s effect"
2011.10916,data,184,2022-05-16,0,"Here we evaluate the results in the table 2 quantitatively. We couldn’t compare out per class binary classiﬁcation results with the MulT[22] model as they hadn’t reported it, so it’s unfair to claim that this model can outperform MulT on per class basis. We can still observe a couple of things from the all class classiﬁcation accuracy Acc6 and the F1 score: 1) Our model is giving very competative results to the MulT model. We had only used 3 base attention modules in place of 6 in the MulT model. This would signiﬁcantly reduce the computation requirement. 2) We can see that this multi-modal fusion has been effective in capturing the context of the situation because of its superior performance over uni-modal models. 3) Given that the model is performing on-par for unaligned data, we can prove our hypothesis that attention would indeed naturally provide the alignment and thus we do not explicitly need to align the sequences as a pre-processing step."
2011.10916,data,4,2022-05-16,0,6.1 Pre-processing the Data
2011.10916,data,4,2022-05-16,0,Aligned Data Unaligned Data
2011.10916,data,51,2022-05-16,0,"If we ﬁnd out that we do not need to align the data, it will be a great time-saver for realtime inference. Our hypotheses is that due to the natural alignment provided by cross-attention, we wouldn’t need our data to be aligned for this architecture."
2011.10916,data,54,2022-05-16,0,"Table 2: Table showing results of experiments on the test date for unimodal performance of the delta self-attended modules, fused DCCA modules, and then multi-modal hierarchical attention modules for both aligned and unaligned data. The results are compared with MulT, G-MFN, SotA1, SotA2, and SotA3."
2011.10916,data,91,2022-05-16,0,"For all the approaches taken so far, the below mentioned missing gaps will be addressed in this project. In reccurent networks, looking at the entire sequence of data and outputting the last, single hidden state as its representation, often turns into the model forgetting information way past the current timestamp. They do not preserve the long-range dependencies [22] due to this bottleneck. This domino effect of updating a single hidden state each timestep can also result in exploding or vanishing gradients."
2011.10916,data,98,2022-05-16,0,"Second, data can be unaligned across modalities. A frowning face may relate to a pessimistic word spoken in the past. It’s not always between current word and current expression. And thus, third related point is, neutral expressions are quite idiosyncratic [8]. Some people may always look angry given their facial conﬁguration. This raises the need for delta attention [28], we need to take cross-modal context plus the temporal context within each individual modality to negate the effect of “monotonous-across-the-time” features."
2011.10916,"data, data available",30,2022-05-16,0,"[17] S. Nemati, R. Rohani, M. E. Basiri, M. Abdar, N. Y. Yen, and V. Makarenkov. A hybrid latent space data"
2011.10916,dataset,123,2022-05-16,0,"To have the same dimensionality for the fusion at later stage, we take word-level granularity and for the words spoken at each interval (note that there is interval information alongwith features for all 3 modalities in the dataset), we pad and stack the visual (Facet 4.2) and acoustic (COVAREP) sequences, making all three sequences for a certain time interval [t, t + (cid:15)] having the same length. To make sure that the attention modules don’t interpret padded values (“[PAD]” for words and 0s for the other two), we use attention masks which will mask away the padded values."
2011.10916,dataset,2,2022-05-16,0,5 Dataset
2011.10916,dataset,41,2022-05-16,0,"[13] P. P. Liang and R. Salakhutdinov. Computational modeling of human multimodal language: The mosei dataset and interpretable dyanamic fusion. In First Workshop and Grand Challenge on Computational Modeling of Human Multimodal Language, 2018."
2011.10916,dataset,54,2022-05-16,0,"Due to the novelty of the concept and components, we mainly focused the efforts on building the model architecture and trying out various layers more than trying the same architecture on various datasets. Thus, these experiments are only on the dataset described in section 5, CMU-MOSEI [2]."
2011.10916,dataset,60,2022-05-16,0,"The dataset comes with high-level features in form of glove embeddings having 300 dimensions. But for the purpose of using a transformer, we used raw text to get the advantage of dynamic context alignment. The CMU-MOSEI SDK provides the facility to align the visual and vocal computational sequences with the verbal as the base modality reference."
2011.10916,dataset,73,2022-05-16,0,"[2] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236–2246, Melbourne, Australia, July 2018. Association for Computational Linguistics."
2011.10916,dataset,75,2022-05-16,0,"We trained the uni-modal self-attention modules for all 3 modalities L, V, A separately ﬁrst, this can be done in parallel as these 3 modules are independent of each other after the initial interval alignment performed in the pre-processing step. The selected hypermeter is given in Table 1. The hypermeter selection method of splitting the dataset into train:valid:test sets is given in the section 4.4."
2011.10916,"dataset, used dataset",122,2022-05-16,1,"Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) 1 [2] is the largest and the latest dataset of sentence level sentiment analysis and emotion recognition. It contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics. It has around 23k samples, each with around 1000 text features, 300 visual frames at 15Hz sampling rate, and 150 acoustic features at 20Hz sampling frequency. Most of the transformer based papers have been benchmarked on this dataset, because of its size and the variety in “in-the-wild” emotions. Thus, this project will also use this dataset, to be able to compare the results fairly."
2011.10916,github,3,2022-05-16,0,1https://github.com/A2Zadeh/CMU-MultimodalSDK
2012.01288,data,13,2022-05-16,0,Research supported by BRD — Groupe Societe Generale Data Science Research Fellowships.
2012.01288,data,31,2022-05-16,0,"1 Faculty of Mathematics and Computer Science 2 Human Language Technologies Research Center 3 Data Science Center University of Bucharest ana.uban@gmail.com, alina.ciobanu@my.fmi.unibuc.ro, liviu.p.dinu@gmail.com"
2012.01288,data,50,2022-04-21,0,"17. Vulic, I., Moens, M.: Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014. (2014) 349–362"
2012.01288,dataset,42,2022-05-16,0,"6. Ciobanu, A.M., Dinu, L.P.: Building a Dataset of Multilingual Cognates for the Romanian Lexicon. In: Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014. (2014) 1038–1043"
2012.01288,dataset,57,2022-05-16,0,"The Romance Languages We compute the cosine similarity between cognates for each pair of modern languages, and between modern languages and Latin as well. We compute an overall score of similarity for a pair of languages as the average similarity for the entire dataset of cognates. The results are reported in Table 5."
2012.01288,"dataset, used dataset",48,2022-05-16,0,"The Romance Languages vs English Further, we introduce English into the mix as well. We run this experiment on a subset of the used dataset, comprising the words that have a cognate in English as well4. The subset has 305 complete cognate sets."
2012.01288,publicly available,152,2022-05-16,1,"1. Obtain word embeddings for each of the two languages. 2. Obtain a shared embedding space, common to the two languages. This is accomplished using an alignment algorithm, which consists of ﬁnding a linear transformation between the two spaces, that on average optimally transforms each vector in one embedding space into a vector in the second embedding space, minimizing the distance between a few seed word pairs (for which it is known that they have the same meaning), based on a small bilingual dictionary. For our purposes, we use the publicly available multilingual alignment matrices that were published in [12]. 3. Compute semantic distances for each pair of cognates words in the two languages, using a vectorial distance (we chose cosine distance) on their corresponding vectors in the shared embedding space."
2012.11723,data,131,2022-05-16,0,The paper is organized as follows. In section II we start by describing a representative network that we use to illustrate the methodology to calculate latency bounds. Section III explains the analytical results behind these calculations. Section IV presents the analysis of the representative network. Section V discusses a free-rider principle which states that a fast network designed to transport the signals from cameras and other high data rate sensors can transport the ﬂows from CAN buses and other slow sources for free. Section VI explains a simple method to derive bounds on the latency and storage of fast ﬂows. Section VIII summarizes the main points of the paper and gives hints on which future in vehicle network architectures may beneﬁt from this work.
2012.11723,data,166,2022-05-16,0,"From a more academic point of view, one could also assume links to be asymmetric in line rate. We examine the latency and memory utilization when links are designed to have a line rate equal to 125% of the peak bandwidth of data required, independent from the standardized Ethernet modes of operation. That is, we design the links so that their maximum utilization is 80%. We consider the case when the processor only sends slow trafﬁc to fast devices (e.g., cameras), as it it the situation that corresponds to the slowest links which might result in larger latency and memory occupancy. In particular, we assume that the processor can send at most a burst of 4 packets of 512 bits to a given camera. The delay of these four packets on the 10Mb/s link attached to the camera is then at least 4 × 512/(10Mb/s) = 200µs."
2012.11723,data,169,2022-05-16,0,"the aggregation of camera data requires high line rate links. Again the speciﬁc execution of this model could vary greatly between manufacturers and few have actually come to the market so far. It could be argued that a shift away from privately owned individual vehicles towards automated robotaxi ﬂeets may drive this transition from functional clusters to geometrical zones, as the aspects of functional variance between customer chosen options, generational carry over and nameplate spread become less important for automated robotaxi ﬂeets while integration of systems and sensors becomes more important to solve the task of perception and control for driver-less operation in such vehicles. A further building block to allow this data convergence, which can be observed in the telecommunications business for quite some years already, is the only recent availability of multi-Gb/s Ethernet physical layer links for application inside a vehicle. All these aspects together create the environment wherein the concepts of this paper can be successfully deployed."
2012.11723,data,204,2022-05-16,0,"Roy Myers received his Bachelor’s and Master of Science in Electrical Engineering from Georgia Institute of Technology in 1990 and 1991, respectively. He then joined National Semiconductor LAN Division designing mixed signal Ethernet IC products. In 1996, he joined Enable Semiconductor, as a founding member, developing Ethernet 100Base-TX transceivers. Enable Semiconductor was acquired by Lucent Technologies in March 1999. After leaving Lucent, he co-founded Terablaze Inc in 2000 where he was Director of Engineering developing highly scalable network fabrics and layer 2/3 Gigabit Ethernet switching solutions. TeraBlaze Inc was acquired by Agere Systems in 2004. Roy continued to lead the Gigabit Ethernet switch development at Agere and then at LSI after acquisition of Agere Systems in 2007. He left LSI in 2007 to join Aquantia Corp (AQ:NYSE) where he was Chief Architect developing a series of 10GBase-T and Multi-Gig MAC/PHY products to serve client, enterprise, and data center markets. Since April 2018, Roy is a co-founder and SVP of Engineering of Ethernovia Inc developing network solutions for the next generation of automobiles. Roy holds 14 granted patents in communication technology."
2012.11723,data,219,2022-05-16,0,"Due to the very different approaches of OEMs towards their product strategy and resulting network architectures, we need to classify them based on more abstract concepts in order to compare them. The most common starting point is and was a functional clustering. Here electronic control units (ECUs) which are generally related in their functionality and thereby often designed within the same organizational branch of the OEM [29] are connected together directly, offering only very limited interfaces to systems of other functional clusters. Such clusters may for example be the engine, the drive train, or the infotainment system. The theoretical extreme of this is often described as a domain-based system. How these different functional clusters or domains interact is very different for different OEMs. As the need to exchange data between domains has increased, e.g., to avoid duplication of expensive sensors in constrained packaging spaces, the so-called zonal model has gained much attention. In the zonal architecture the focus lies on integrating different functionalities onto a smaller set of ECUs, which share access to sensors and actuators. This also leads to shorter cables and more importantly for this paper, a reduced number of hops in the network where particularly"
2012.11723,data,219,2022-05-16,0,"Each source, such as a camera, a radar, an audio/video server, a 5G antenna, a CAN electronic control unit, a button, and so on, is attached to a bridge that polices and shapes the trafﬁc. The policing veriﬁes that the source is not misbehaving, such as a faulty switch that thinks it is being pushed every millisecond. The shaping separates the packets by the maximum gap consistent with the deadline to deliver a group of packets or with the rate of a stream or ﬁle transfer. For instance, say that a camera produces 2400 packets of 1500 Bytes every 16 milliseconds and that one wishes to deliver these 2400 packets in about 12 milliseconds. Then, the bridge transmits one packet every 12ms/2400 = 5µs. As another example, a bridge for a 5G antenna could send one 1500 Byte sized packet every 10µs to carry a data rate of 1.2Gb/s. Thus, every source of trafﬁc is shaped, including best effort sources. We assume that three of the four fast devices attached to a core switch send 12, 000b (or 1, 500 Bytes) packets every 5µs and one sends 12, 000b packets every 15µs."
2012.11723,data,42,2022-05-16,0,"For instance, a network designed to transport the data from cameras, radars, lidars, audio/video servers to processors and user interfaces can also transport the slow ﬂows between CAN buses, push-buttons, relays, and so on."
2012.11723,data,52,2022-05-16,0,"The network devices are classiﬁed as core, fast, or slow. Using Rene Cruz’s results, one shows that a network fast enough to transport signals from cameras and other fast sensors can also transport the data from slow sources and guarantee them their desired bounded latency."
2012.11723,"data, data available",85,2022-05-16,0,"required to control the camera. With the introduction of Energy Efﬁcient Ethernet (EEE)[30] this issue has partially been addressed. In EEE the power consumption of the lower bandwidth direction is reduced by turning off the transmitter while no data is available. Thus, the line rates at which the frames are transmitted remain symmetrical. As the Small Flow Approximation only depends on the line rate of a frame, it is completely untouched by EEE."
2012.11723,download,152,2022-05-16,0,"Thus, this architecture uses two central components of the IEEE time-sensitive networks standards: policing and shaping. It locates these components in the source bridges, instead of the network switches. The main beneﬁts of this approach are that the network uses a single priority class and that no further trafﬁc reshaping or scheduling is required in the switches. As we explain in the paper, control signals are delivered with guaranteed sub-millisecond latency even though they share queues with bursty best effort trafﬁc and audio/video streams. These latency guarantees are deterministic, not probabilistic: they are worst-case guarantees derived from analyzing the worst case behavior of the network, not by simulations. The architecture ensures that the web download of a new movie cannot interfere with the break signal from the self-driving control system or from the break pedal. Such guarantees are"
2101.03069,"data, data available",134,2022-04-21,0,"dominant Twitter accounts to be found in the data. One would expect to find highly active Twitter users in cases where the platform is used to amplify messaging. In the data, only two accounts (both from Spain) were found to have tweeted more than 100 times during the eight-month period. One of the accounts belongs to a paediatrician while the other to the Spanish Society for Paediatric Infectious Diseases. In the absence of an ideologically-motivated group, movement or collective, and some evidence that scientific rather than political activity is the driver of social media activity, Twitter is not in this case being used as a communication platform to amplify messaging about the risks or benefits of children attending school during the COVID-19 pandemic."
2101.03069,"data, data available",255,2022-05-16,0,"The scientific output results in mixed evidence of infection and transmission as they pertain to children. The limitations of the scientific studies and the consequent levels of uncertainty were conveyed when reporting findings. This is, however, not a unanimous approach. For example, a viewpoint in the Archives of Disease in Childhood (Munro and Faust, 2020), is entitled “Children are not COVID-19 super spreaders: time to go back to school”. The title appears to be inflated by the urgent need for policy decisions. The authors write “At the current time, children do not appear to be super spreaders. Serosurveillance data will not be available to confirm or refute these findings prior to the urgent policy decisions that need to be taken in the next few weeks such as how and when to reopen schools.” They continue “Governments worldwide should allow all children back to school regardless of comorbidities. Detailed surveillance will be needed to confirm the safety of this approach, despite recent analysis demonstrating the ineffectiveness of school closures in the recent past (Viner et al. 2020b). The media highlight of a possible rare new Kawasaki-like vasculitis that may or may not be due to SARS-CoV2 does not change the fact that severe COVID-19 is as rare as many other serious infection syndromes in children that do not cause schools to be closed”. The title suggests no uncertainty"
2101.03069,"data, database",149,2022-05-16,0,"The two databases (WHO and CORD-19) do not represent distinctive sets of publications, having quite  substantial overlap. In order to avoid duplicates, the two databases were merged and cleaned. For a reliable merging of the two databases, as well as for the further tracing of the (social) media reception of the publications, it was necessary to count with unique document identifiers (e.g., PubMed Identifiers, Digital Object Identifiers, etc.). Particularly Digital Object Identifiers (DOI) are commonly assigned to scientific publications to univocally identify scientific documents across databases and the web-at-large. The main inconvenience of using DOIs is that we can only identify and combine publication data for half of the papers included in the CORD-19 database and a third of those included in the WHO database (Figure 1)."
2101.03069,"data, dataset, data available",125,2022-05-16,0,"We also investigated the profiles of tweeters whose tweets were collected in our dataset. In particular, we looked into the share of tweeters who had tweeted about science (i.e., tweeted an academic publication) before the pandemic, that is, tweeters from our sample present in altmetric data from 2019. We found that 59.3% in Spain, 60.6% in the Netherlands and 65.1% in South Africa had mentioned other scientific articles in their tweets prior to the pandemic. From the total of 8,597 distinct tweeters identified in all three countries, 5,141 had already referenced scientific output before the COVID-19 pandemic. Moreover, we attempted to determine the professions of the 8,597 tweeters."
2101.03069,"data, dataset, database",131,2022-05-16,0,"Data collection The data collected for this study was extracted from a variety of sources: scientific publications, news outlets, and social media discussions and policy interventions. Since the outbreak of the pandemic, different community- and organization-led initiatives have been conducted to make scientific publications on COVID-19 openly accessible. In this study, we made use of the COVID-19 Open Research Dataset (CORD-19) and the World Health Organization (WHO) COVID-19 Global literature on coronavirus disease database. These two databases are of special interest due to the combination of sources they include, containing not only studies published in scientific journals but also preprints from the main global repositories (e.g., BioRxiv, MedRxiv, SSRN, etc.)."
2101.03069,"data, dataset, database",216,2022-05-16,0,"A final number of 5,713 publications along with their DOIs have been collected in our final dataset of scientific output. We proceeded to identify news outlets and social media discussions around the scientific publications in our dataset. News media items mentioning a DOI in our set were identified with data from Altmetric.com, retrieved in October 2020. From a total of 19,922 news items found globally for the set of DOIs in our database, 424 news articles could be identified as originating from the Netherlands, Spain, or South Africa. This was done by matching the URLs of the news outlet coming from Altmetric.com with the URLs of Dutch, Spanish, and South African national newspapers and broadcasting services, as extracted from Wikipedia and other websites listing news outlets. The final list of news outlets from each country was verified and curated manually. We identified 200 news items from Spain, which referenced 81 distinct DOIs. In South Africa, 79 news pieces referenced 72 distinct DOIs and in the Netherlands, 145 news items referenced 83 distinct DOIs. The titles and short abstracts of the news articles (where available in the data from Altmetric.com) were analyzed manually  for our study."
2101.03069,database,1,2022-05-16,0,database
2101.03069,database,145,2022-05-16,0,"The available scientific output about the role of children and schools in the COVID-19 pandemic has not been picked up in the social media in the three countries in our study to the same degree. We found that only 17.9% of the publications in our database have been tweeted about in the three countries; this is much less than the coverage of about 63.0% of all attention for CORD-19 publications as overall captured by Altmetric (Colavizza et al. 2020). A total of 932 DOIs (16.3% of the scientific output) has been mentioned in the Spanish tweets on the topic. In the Dutch tweets, only 4% of the scientific output (229 articles) has been mentioned, whereas in South Africa 289 articles (5%) have been mentioned."
2101.03069,database,222,2022-05-16,0,"Spain While in the Netherlands and South Africa schools reopened after around two months of closure, in Spain school reopening was delayed until after the summer holidays. Figure 3 depicts the announced and implemented measures, in chronological order, both at the national, as well as the regional levels. The policy measures registered no difference between primary and secondary schools. The figure also includes the timeline distribution of the news outlets and tweets in our database, which have been identified as originating from Spain. A total of 188 news articles and 15,603 tweets were identified between the beginning of February and the end of September 2020. News articles on the topic registered brief appearances before the school closure in March, as well as more consistent appearances around the reopening of schools in September. As for tweets, we can observe small peaks around the time of the announcements in March, as well as shortly before and after the schools reopening in September. Further activity has been registered during the school closure, with peaks around end of April, when the government announced a plan for easing lockdown restrictions, as well as in July and August, when no other policy intervention has been announced nor occurred."
2101.03069,database,67,2022-05-16,0,"Note: Number of total publications by database [Pubs], publications in 2020 [Pubs in 2020], share of publications with Document Object Identifier (DOI) [%DOI in 20202], number of publications related to children and schools [Pubs children] and share of publications with a DOI related to children and schools [%DOI children]"
2101.03069,dataset,90,2022-05-16,0,"An overview of the topics covered by tweets from Spain is depicted by a VOSviewer map in Figure 4. The nodes in the map present the co-occurrence of the most relevant keywords identified from the titles of the 5,713 articles in our dataset. The color coding of the map reflects the prevalence of mentions of those articles in tweets from Spain relative to the worldwide collected tweets: The darker the color, the more focus on the keywords relative to the worldwide tweets on the topic."
2101.03069,dataset,92,2022-05-16,0,"We identified 740 researchers, 741 health professionals, and 296 journalists based on terms found in the user descriptions of the tweeters. We note a possible overlap between the groups, as someone can be both a health professional and, e.g., hold a PhD (one of the indicators for being a researcher). Given the limited available information on Twitter and the limitation of our search algorithms, we expect that these results are underestimating the true presence of those professions in our dataset.  Discussion"
2101.03069,"dataset, database",88,2022-05-16,0,"We downloaded the two complete databases on October 15, 2020. Table 1 shows some descriptive values of the size of the database at the time. We searched within the title and abstract fields for documents containing the words ‘children’ and ‘schools’. After merging the ‘Pubs children’ documents of both datasets, a total of 5,713 publications were retrieved. This is our final set of scientific publications from which we trace their (social) media reception."
2101.10245,"data, dataset",149,2022-05-16,0,"We would also like to point out the problems with the Samsung S5 gesture sensing API. Samsung has deprecated support for the device and access to the sensor output is limited. Moreover, there is no way to access the raw sensor values without rooting the phone. This limits the impact of our current approach to pervade the current market, but doesn’t limit the research contribution. This deprecation did affect our user study. Because the sensor API was deprecated, many of the angle and velocity measures were flagged “unknown.” We removed those incomplete records from our dataset but the reliability of the sensor reading is called into question. As such, our results might represent a lower bound of performance and may be further increased with more reliable sensor readings or more expressive IR sensor data."
2101.10245,github,38,2022-05-16,0,"François Chollet et al. 2015. Keras. https://github.com/fchollet/keras. Bruno Dumas, Denis Lalanne, and Sharon Oviatt. 2009. Multimodal interfaces: A survey of principles, models and frameworks. Human"
2101.10245,open-source,41,2022-05-16,0,"To normalize and control dynamic range, we take the decibel magnitude of the STFT. The implementation of the STFT grid search and feature extraction techniques have been made open source and are available at [Mundada 2017]."
2101.10245,python,105,2022-05-16,0,"To create, train and validate machine learning algorithms we use a combination of packages in Python. Specifically, we use the “scikit-learn” library [Pedregosa et al. 2011] and Keras [Chollet et al. 2015] with the TensorFlow [Abadi et al. 2015] back-end. We chose to investigate several different machine learning baselines and also several different convolutional neural network architectures. It was unclear what neural network architecture and parameters of the architecture would be optimal, so we chose to train several variants and perform hyper parameter tuning for each architecture."
2101.10245,"python, github, open-source",70,2022-05-16,0,"Raunak Mundada. 2017. AirWare Open Source Repository. https://github.com/raunakm90/AirWare. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, Oct (2011), 2825–2830."
2102.0082,"data, dataset",112,2022-05-16,0,"An Adaptive Neuro Fuzzy Network with a TSK fuzzy type combined with an improved quantum subtractive clustering method to obtain appropriate number of fuzzy rules is proposed. The subtractive clustering, a density based algorithm, is used to determine number of cluster centers. Moreover a modified quantum clustering, an idea from quantum mechanics is applied to obtain cluster centers. Cluster centers represent a general model with essential characteristics of data which can be use as premise part of fuzzy rules. It caused impressive decrease in number of fuzzy rules and network accuracy. Finally we construct our model to predict fuel consumption in MPG dataset."
2102.0082,"data, dataset",181,2022-05-16,1,"Generally effective partitioning of input space can reduce number of fuzzy rules and increase learning speed of Anfis. In this paper we use a quantum subtractive clustering to determine fuzzy rules, and then a modified Anfis is applied to data set to predict the output. The experiments used the well-known automobile mile-per-gallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile. At first records with missing values has been eliminated, then we normalize data to [0,1] interval. Data set divided into two partition; train data and test data, according to even record and odd records from original dataset, respectively. Here we use training data set to construct and learn the model while test data is used to validate the model."
2102.0082,"data, dataset",203,2022-05-16,0,"We have compared our new Quantum Subtractive Clustering method with traditional QC applied in Anfis [1].  Fig. 2(a) shows comparison results between the desired and traditional QC model outputs for test data. Here horizontal axis shows number of test data and vertical axis determines desired and model fuel consumption. Also Fig .2(b) shows comparison diagrams between the desired and our model outputs for test data set.  i s  A s  d i f f e r e n c e  o b v i o u s  a n d  b e t w e e n  p r o p o s e d  m o d e l  o u t p u t  i s  l e s s  t h e  t h a n  p r e v i o u s  m e t h o d .  A s  h a s  r e s u l t ,  a n d  b e t t e r  g e n e r a l i z a t i o n  c a p a b i l i t y ."
2102.0082,dataset,138,2022-05-16,0,"Therefore we propose a new method to construct an adaptive neuro fuzzy network with a TSK fuzyy type. Also we use a modified QC to determine the premise part of fuzzy rules. Moreover subtractive clustering method is applied to determine the optimal number of clusters. We performed a learning method by a hybrid learning scheme using back propagation (BP) and a least-square estimator (LSE). The experiments used the well-known automobile mile-pergallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of a car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile."
2102.03237,data,30,2022-04-21,0,"Kim, J., & Owen-Smith, J. (In print). ORCID-linked labeled data for evaluating author name disambiguation at scale. Scientometrics. Doi: 10.1007/s11192-020-03826-6"
2102.03237,"data, database",259,2022-05-16,0,"Another group of studies has relied on third-party data sources that control the accuracy of researcher information. For example, Kawashima and Tomizawa (2015) evaluated the disambiguation performance of SCOPUS on a list of 75,405 Japanese author names in 573,338 papers. For this, they used the Database of Grants-in-Aid for Scientific Research (KAKEN) that maintains a unique ID number of a funded researcher in Japan with a list of her/his verified publications. An author name instance in a SCOPUSindexed paper was compared to each KAKEN researcher profile by comparing name strings, publication records, and affiliations. If a match was found, the KAKEN researcher ID was assigned to the author name instance. Such a record linking technique has been used in other studies to label name instances of Italian researchers (D'Angelo, Giuffrida, & Abramo, 2011) and Dutch researchers (Reijnhoudt, Costas, Noyons, Borner, & Scharnhorst, 2014) using each nation’s administrative scholarly databases. Other sources for labeling include NIH-funded researcher profiles2 (e.g., K. Kim, Sefid, Weinberg, & Giles, 2018; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009) and Highly Cited Researchers data3 (e.g., Liu et al., 2014; Torvik & Smalheiser, 2009). While these record linkage procedures produce large-scale, accurate labeling results, it also provides biased results (Lerchenmueller"
2102.03237,"data, database",40,2022-05-16,0,"Kim, J. (2017). The impact of author name disambiguation on knowledge discovery from large-scale scholarly data. (Ph.D.), University of Illinois at Urbana-Champaign, Retrieved from http://hdl.handle.net/2142/98269 IDEALS database."
2102.03237,"data, dataset",101,2022-05-16,0,"Furthermore, ORCID-linkage can help researchers label the name instances of authors who work in diverse research fields for which labeled data are scarce. Most existing labeled datasets for author name disambiguation were created to disambiguate author names in a few scientific domains, especially Computer Science and Biomedical Sciences (A. A. Ferreira, Gonçalves, & Laender, 2012; Müller et al., 2017). For those who need to mine ambiguous bibliographic data that represent diverse fields, ORCIDlinkage can be an effective way to generate labeled data for their ad-hoc disambiguation tasks."
2102.03237,"data, dataset",137,2022-05-16,0,"Second, ORCID-linked labeled data can complement other types of linkage-based labeled data. Our comparisons across three different types of linked labeled data showed that , ORCID-linked labeled data could captured the aspects of Author-ity2009’s performance that were also identified in the other two datasets. This means ORCID linkage can be used as an alternative to other labeling methods if they are unavailable. In addition, ORCID-linkage can be used to help researchers evaluate the labeling quality of other labeled data. Out of 312,951 instances in AUT-NIH, for example, a total of 32,131 instances were also linked to 3,578 ORCID ids. Among them, 99 name instances were assigned to different authors by the ORCID-linkage and the NIH-ExPORTER linkage used in Lerchenmueller and Sorenson (2016)."
2102.03237,"data, dataset",142,2022-05-16,0,"Different levels of name ambiguity might arise from the different sizes of labeled data in our study: AUTORC contains more than 3 million instances, while AUT-NIH consists of 312K instances. As name ambiguity in bibliographic data tends to increase with data size (Fegley & Torvik, 2013; J. Kim, 2017), AUT-ORC might be naturally more ambiguous than AUT-NIH. Other differences between these datasets may result from the data sources from which they were drawn. AUT-NIH relied on funded PI information. So, the name instances that could be labelled were restricted to those of researchers who have ever received funds from NIH, a group likely to be more prominent and more homogenous than science itself. In contrast, AUT-ORC utilized ORCID profile data for more than 5 million researchers"
2102.03237,"data, dataset",196,2022-05-16,0,"English name instances constitute the majority in AUT-NIH (diagonal-line bar, 54.53%), while other ethnicities are heavily underrepresented compared to their ratios in Author-ity2009. This might be because AUT-NIH is created based on information of PIs who have ever received funds from NIH in the U.S. Non-US investigators are generally ineligible to apply for NIH funds, so it makes sense that the name instance distribution in this dataset would skew toward English names. This English-skewed distribution is also confirmed in Lerchenmueller and Sorenson (2016) who found that 84% of all ethnicity-identified instances in the whole Author-ity2009 linked to NIH ExPORTER are ‘Caucasian’ (including many European names as well as English). Meanwhile, many instances in self-citation relation are also English (horizontal-line bar; 34.65%) but the ratio differences of other ethnicities against Authority2009 are smaller compared to those in AUT-ORC and AUT-NIH. As such, three labeled data are common in that English name instances are prevalent but none of them represents well the ethnicity distribution in Author-ity2009 because some ethnicities are over-represented while others underrepresented."
2102.03237,"data, dataset",21,2022-05-16,0,(1) How well do ORCID-linked labeled data represent the population of name instances in a large-scale bibliographic dataset?
2102.03237,"data, dataset",217,2022-05-16,0,"This study also creates a benchmark labeled dataset by linking Author-ty2009 with Principal Investigator (PI) information recorded in the National Institutes of Health (NIH) funded research data (ExPORTER). This NIH-linkage has been used in several studies to evaluate author name disambiguation for MEDLINE because ExPORTER provides the PMIDs of research papers in MEDLINE that result from NIH funds (e.g., K. Kim, Sefid, & Giles, 2017; K. Kim et al., 2018; Liu et al., 2014; Torvik & Smalheiser, 2009). After an Author-ity2009 paper’s PMID is found to be associated with a specific NIH grant, the author names in the paper are compared to the names of the PI who received the funding. If a PI’s name is found to match an author name, her/his unique NIH PI ID is assigned to the author name as a label. This study reuses the list of NIH PI IDs linked to the Author-ity2009 in Lerchenmueller and Sorenson (2016)13. To make this NIH-linked labeled data (AUT-NIH) comparable to AUT-ORC, each name instance in AUTNIH is assigned an ethnicity and a gender using Ethnea and Genni each."
2102.03237,"data, dataset",227,2022-05-16,0,"Block Size Distribution: Another way to discover how three labeled datasets represent Author-ity2009 is to compare the distributions of block sizes in each dataset. A common practice in author name disambiguation research is to collect author name instances into a block if they match on the full surname and first forename initial. Comparisons that support disambiguation are then performed within blocks (K. Kim et al., 2018). Many studies have used the block size distribution to characterize labeled data (e.g., J. Kim et al., 2019; Levin et al., 2012; Müller, Reitz, & Roy, 2017; Torvik & Smalheiser, 2009). Block sizes can become huge because labeled data contain a few hundreds of thousands (AUT-NIH) or millions (AUT-ORC) of name instances. So, block size distributions are plotted using a cumulative density function on log-log axes. Figure 5 visualizes the block size distributions in AUT-ORC and AUT-NIH. Note that AUT-SCT cannot produce a block size distribution because self-citation pairs only contain match information at the pair level and, thus, the matching status of name instance pairs that are not in self-citation relation but that may nevertheless fall within a block defined by surname and first initial are still unknown."
2102.03237,"data, dataset",253,2022-05-16,0,"Gender Distribution: To provide another indicator of how well each labelled dataset represents Authority2009, we turn to comparisons of the gender composition of author name instances. Figure 3 shows that the majority of name instances in all datasets are male (black bar; 57%) while female instances (22.32%) and NULL (i.e., gender unidentifiable) instances (20.28%) make up the rest with similar percentages. Such an imbalanced gender distribution is broadly characteristic of scientific authorship in general (Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013) as of biomedical science (Jagsi et al., 2006). The gender imbalance is also observed in AUT-ORC (gray bar) in which male names constitute 67.46 percent of all name instances while the percentage of female instances (22.70%) is quite similar to that in Autority2009. The higher ratio of male instances in AUT-ORC than in Author-ity2009 seems to be a trade-off with the reduced ratio of Null name instances. The same pattern is observed in AUT-NIH and AUT-SCT in which the dominance of male names are more prevalent (i.e., 73.95% and 65.44% each) than in Autority2009 and AUT-ORC but with lower ratios of Null names and similar ratios of female names. These observations indicate that despite the minute differences in gender ratios, three labeled data shared similar patterns of gender distribution."
2102.03237,"data, dataset",272,2022-05-16,0,"The answers to these questions can help disambiguation researchers to make informed choices of labeled data and to create evaluation and ground-truth datasets at scale. Several studies have attempted to answer similar questions by discussing how ORCID profiles represent the author population in Web of Science (Youtie, Carley, Porter, & Shapira, 2017), what issues need to be addressed before ORCID can be used as a gold standard for author disambiguation (Albusac, de Campos, Fernández-Luna, & Huete, 2018; Eichenlaub & Morgan, 2017), and how record-linkage-based labeling may or may not work in author disambiguation under certain conditions (Anderson A Ferreira, Gonçalves, & Laender, 2020; Reijnhoudt et al., 2014). This study contributes to that growing literature by demonstrating the use of ORCID-linked labeling against another large-scale disambiguated dataset constructed using different linkage-based labeling methods. Specifically, this study labels name instances in MEDLINE by linking them with ORCID researcher profiles. Then, the performances of Author-ity2009 which disambiguates MEDLINE author names, is evaluated using the labeled data. For comparison, two labeled datasets are created using two widely-used sources - NIH-funded researcher information and self-citation information. The three labeled datasets are compared for their representativeness of Author-ity2009 as well as to evaluate results of the Author-ity2009’s disambiguation performances. After that, a discussion follows about the implications and challenges of using ORCID for labeling. In the following section, labeling procedures via record-linkage for Author-ity2009 are described in detail."
2102.03237,"data, dataset",319,2022-05-16,0,"In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b. Both these distances may be due to the relatively larger European focus of ORCID and the US focus of NIH data. The largest name blocks in Author-ity2009 tend to be created by highly ambiguous Asian name instances. If the distributions of both labeled datasets were representative of Author-ity2009, we would expect to see their distributions track closely with those of the random subset of Author-ity2009 name instances."
2102.03237,"data, dataset",336,2022-05-16,0,"In Figure 5, the x-axis shows block sizes ranging from 1 (i.e., a single instance block) to 25,917 in Author-ity2009 (Figure 1a). They-axis represents the ratio of blocks with a specific size or larger (cumulative) over all blocks is shown. For example, in Author-ity2009, blocks with 2 or more name instances (blue circles) constitute 63.47% of all blocks, which in reverse means that 36.53% of blocks contain only one instance. In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b."
2102.03237,"data, dataset",98,2022-05-16,0,"Distribution of Publication Years: As shown in Table 3, the three labeled datasets – AUT-ORC, AUTNIH, and AUT-SCT –contain different numbers of labeled name instances (pairs). How do those instances differ and which is most representative of the overall Author-ity2009 dataset? To characterize the composition of labeled data, publication years of papers in which a labeled instance appears are counted. Figure 2 compares the publication year distributions of three labeled datasets. Note that for AUT-SCT, years associated with each self-citing name instance pair are counted."
2102.03237,"data, dataset provided",167,2022-05-16,0,"Second, the accuracy of ORCID records still needs to be verified. As acknowledged by ORCID, some records may contain errors due to “benign” (unintentional) mistakes by profile creators (e.g., claiming other researcher’s work as their own)18. Note that other labeled data may have the same verification problems. Human experts can produce inaccurate labels and often disagree on labeling decisions even given the same information (Shin et al., 2014; Song et al., 2015). Although NIH PI data are curated with special care by NIH, the linkage process for labeling may entail erroneous matching between PI names and author names in NIH-funded papers. As shown above regarding the labeling quality of AUT-NIH (see 3rd paragraph in Conclusion and Discussion), ORCID-linked data provided more accurate labeling results than the other method but still contained erroneous labels. To ensure that errors in ORCID records"
2102.03237,"data, dataset provided",299,2022-05-16,0,"Third, ORCID-linked labeled data can provide more enriched evaluation results. They can be used together with other labeled data for triangulating a disambiguation method’s performance. Unlike selfcitation-based labeled data, ORCID-linked labeled data can be used to measure both clustering and classification performances. Unlike NIH-linked labeled data, ORCID-linked labeled data contain a greater range of ambiguous names across ethnicities, which can enable a disambiguation method to be evaluated on name instances with different ambiguity levels. This in turn allows for more focused analysis to address difficult disambiguation tasks such as those presented by synonyms and homonyms. Moreover, ORCID-linkage can produce labeled instances that are challenging to disambiguate but are not easily collectable by other labeling methods. For example, FINI could not reach perfect recall in AUT-ORC (Figure 6a and Figure 7a). As detailed above (see Clustering Performance: AUT-ORC and AUT-NIH), 273,782 name instances of 12,646 authors (= unique ORCID ids) are recorded in a way that their ‘surname + first forename initial’ strings of the same author are different. This means ORCID-linkage could produce labeled name instances that refer to the same authors but do not belong to the same blocks. Such synonymous name variants existing across blocks have been insufficiently studied in disambiguation research (Backes, 2018; Gomide et al., 2017) because many studies have created labeled data by collecting (= blocking) ambiguous name instances sharing at least the full surname and first forename initial (J. Kim, 2018; Müller et al., 2017). Using ORCID-linked labeled data, scholars can develop disambiguation models that address synonyms as well as homonyms."
2102.03237,"data, dataset, data https",148,2022-05-16,1,"9 https://databank.illinois.edu/datasets/IDB-9087546 10 26 ethnicities include: African, Arab, Baltic, Caribbean, Chinese, Dutch, English, French, German, Greek, Hispanic, Hungarian, Indian, Indonesian, Israeli, Italian, Japanese, Korean, Mongolian, Nordic, Polynesian, Romanian, Slav, Thai, Turkish, and Vietnamese. Some name instances are assigned compound ethnicities (e.g., “Jane Kim” → Korean-English) if the surname and forename of an author name are associated frequently with different ethnicities.  11 Genni + Ethnea for the Author-ity 2009 dataset. (2018). Retrieved from: https://doi.org/10.13012/B2IDB9087546_V1 12 https://en.wikipedia.org/wiki/Andrea 13 https://dx.doi.org/10.6084/m9.figshare.3407461.v1. Instead of 355K instances in the original linked data, this study filters 313K instances recorded in papers published between 1991 and 2009."
2102.03237,"data, dataset, publicly available, data available",130,2022-05-16,0,"This study suggests several implications for researchers and practitioners of author name disambiguation. First, ORCID can be an effective source of authority for creating labeled data. This study illustrated that ORCID-linkage can generate millions of labeled name instances in a bibliographic data, which is not easily achievable by manual or other record-linkage-based labeling. In addition, ORCID-linkage can be repeated without much additional cost once technical procedures for record-linkage are implemented. Moreover, ORCID data continue to be expanded, publicly available, and released annually. This means labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated dataset and updated on a regular basis, enabling sustained evaluation of author name disambiguation in ever-growing digital libraries."
2102.03237,"data, publicly available",169,2022-05-16,1,"Author-ity2009 is chosen as an evaluation target for three reasons. First, Author-ity2009 conducts author name disambiguation on a digital library scale: 61.7M name instances in 18.6M papers published between 1966~2009 as indexed in MEDLINE. Evaluating disambiguation results for such a large bibliographic corpus can be a daunting challenge. So, Author-ity2009 can be a good use case to illustrate how ORCIDlinkage can contribute to the performance and evaluation of an important, large-scale disambiguation task. Second, the performance of Author-ity2009 have been evaluated on different types of labeled data in several studies (e.g., J. Kim, 2019b; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009), as summarized in Table 1. This provides a context for comparing ORCID with other labeling sources to better understand its strengths and weaknesses. Third, Author-ity2009 is publicly available for research, enabling scholars to replicate and validate this study."
2102.03237,"data, publicly available, dataset provided",132,2022-05-16,1,"In evaluating the clustering results of Author-ity2009, ORCID-linked labeled data effectively captured the ‘high precision over high recall’ strategy of Author-ity2009. Although comparative labeled data also produced the same evaluation results, ORCID-linked labeled data could provide more nuanced details about the Author-ity2009’s performance when name instances were evaluated across ethnic name groups. As such, ORCID-linkage can be used as a labeling method to produce large-scale truth data to evaluate the performance of a disambiguation method from various aspects. Three large-scale labeled data – AUTORC, AUT-NIH, and AUT-SCT – used in this study are publicly available17. The data sharing is expected to assist researchers to develop, compare, and validate disambiguation models using diverse, large-scale labeled data."
2102.03237,"data, publicly available, dataset provided, data available",221,2022-05-16,1,"How can we evaluate the performance of a disambiguation method implemented on big bibliographic data? This study suggests that the open researcher profile system, ORCID, can be used as an authority source to label name instances at scale. This study demonstrates the potential by evaluating the disambiguation performances of Author-ity2009 (which algorithmically disambiguates author names in MEDLINE) using 3 million name instances that are automatically labeled through linkage to 5 million ORCID researcher profiles. Results show that although ORCID-linked labeled data do not effectively represent the population of name instances in Author-ity2009, they do effectively capture the ‘high precision over high recall’ performances of Author-ity2009. In addition, ORCID-linked labeled data can provide nuanced details about the Author-ity2009’s performance when name instances are evaluated within and across ethnicity categories. As ORCID continues to be expanded to include more researchers, labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated data and updated on a regular basis. This can benefit author name disambiguation researchers and practitioners who need large-scale labeled data but lack resources for manual labeling or access to other authority sources for linkage-based labeling. The ORCID-linked labeled data for Authortiy2009 are publicly available for validation and reuse."
2102.03237,database,14,2022-05-16,0,"funding database in Japan. Scientometrics, 103(3), 1061-1071. doi:10.1007/s11192-015-1580-z"
2102.03237,database,140,2022-05-16,0,"This study shows the potential of ORCID-linkage-based labeling for evaluating author name disambiguation by assessing the disambiguation performance of Author-ity2009 (Torvik & Smalheiser, 2009; Torvik, Weeber, Swanson, & Smalheiser, 2005). Author-ity2009 is a bibliographic database that contains disambiguated author names in MEDLINE5, the world’s largest digital library of biomedical research, maintained by the U.S. National Library of Medicine (NLM). In Author-ity2009, author names are disambiguated in two steps. First, name pairs are compared for similarity over various features such as middle name initial, coauthor name, affiliation, and Medical Subject Headings. Next, the instance pairs are grouped into clusters by a maximum-likelihood-based, hierarchical agglomerative clustering algorithm using the pairwise similarity calculated in the first step."
2102.03237,database,27,2022-05-16,0,"author names in a large-scale bibliographic database. Paper presented at the Library of Congress International Symposium on Science of Science, Washington D.C. http://hdl.handle.net/2142/88927"
2102.03237,dataset,158,2022-05-16,0,"Other differences between labeled datasets also help illuminate particular strengths and weaknesses in Author-ity2019. AUT-ORC and AUT-NIH have different levels of name ambiguity. When the Authority2009’s performance was compared with two commonly-used baseline methods, it was less impressive on AUT-NIH where simpler the baseline methods accomplished equivalently high precision and recall to the more sophisticated Author-ity2009. In contrast, in AUT-ORC, the performance gaps between Authority2009 and baseline methods widened substantially. Considering that the baseline methods are deterministic (matching name instances on full surname and initialized forename), their strong performances mean that (1) while many name instances in AUT-ORC and AUT-NIH are not ambiguous, (2) AUT-ORC contains more ambiguous names than AUT-NIH. We observed the same patterns in comparisons of performance across groups of ethnic name instances known to vary in their ambiguity (Figure 7 and Figure 8)."
2102.03237,dataset,161,2022-05-16,0,"Clustering Measure: To assess the performances of Author-ity2009 on three labeled datasets, author name instances referring to the same author are grouped into a cluster. Specifically, a truth cluster is the collection of author name instances that share the same ORCID ID (AUT-ORC) or the same NIH PI ID (AUT-NIH). Meanwhile, a predicted cluster is the collection of author name instances that share the same Author-ity2009 ID (AUT-ORC and AUT-NIH). Then, the predicted cluster is compared to the truth cluster to quantify how well it contains only and all instances that belong to the truth cluster. This study uses B-Cubed (B3), one of most frequently used clustering metrics in author name disambiguation (J. Kim, 2019a). This measure is comprised of three metrics: B3 Recall, B3 Precision, and B3 F1, which are defined as follows:"
2102.03237,dataset,255,2022-05-16,0,"As reported above, three labeled datasets together highlight different aspects of Author-ity2009’s disambiguation performance. They all showed that Author-ity2009 is highly accurate in disambiguating author name instances. It demonstrated special strength in distinguishing author name instances that belong to different authors and in producing almost perfect clustering precision (AUT-ORC and AUTNIH). In addition, Author-ity2009 performed well in finding name instances of unique authors, producing very high clustering recall (> 0.96; AUT-ORC and AUT-NIH) and classification accuracy (= 98.06%; AUT-SCT) scores. Note that the Author-ty2009 is by design aimed to disambiguate with high precision because incorrectly matched name instances (merged author identities created by false positives) are more harmful than wrongly mismatched ones (split author identities created by false negatives) for bibliometric analyses (Fegley & Torvik, 2013; Liu et al., 2014; Torvik & Smalheiser, 2009). The evaluation results described so far strongly suggest that Author-ity2009 achieved its stated precision-over-recall goals. Using the name instances stratified into different ethnic groups, the three labeled datasets discussed here provide a deeper understanding of Author-ity2009’s disambiguation performance. Author-ity2009 achieved high precision regardless of ethnic name types (AUT-ORC and AUT-NIH). But its recall was relatively weak in disambiguating some ethnic names, when compared with baseline performances (AUTORC, AUT-NIH, and AUT-SCT), suggesting possibilities to improve the algorithm."
2102.03237,dataset,303,2022-05-16,0,"Another benchmark labeled dataset is a list of name instance pairs that represent self-citation relations. This self-citation information has been used in several studies to develop and test automatic labeling methods (e.g., J. Kim, 2018; J. Kim et al., 2019; Liu et al., 2014; Schulz, Mazloumian, Petersen, Penner, & Helbing, 2014; Torvik & Smalheiser, 2009). This labeling method is based on the assumption that if a paper cites another and they have the same author names, those names refer to the same author. To generate a list of citing references for a paper, reference lists of papers in MELDINE are connected to their cited papers via matching PMIDs. Then, author names in a cited paper are compared to those in citing papers. Following the common practice using this labeling method, if two name instances in cited and citing papers each match on the full surname and the first forename initial, we treat them as instances of the same author. More than 6.2M self-citation pairs are detected in Author-ity2009. To be comparable to AUT-ORC and AUT-NIH, each name instance in a self-citation pair is assigned an ethnicity and a gender, too. Table 3 characterizes the sources of record linkage and labeling methods of the three labeled datasets – AUT-ORC, AUT-NIH, and AUT-SCT – and presents the numbers of labeled instances and unique authors in each dataset. Note that the number of unique authors is unavailable for AUT-SCT because only name instances that have self-citation relationships can be labelled. It is thus impossible to know from this dataset alone whether name instances without self-citation refer to the same author.14."
2102.03237,dataset,327,2022-05-16,0,"To evaluate the disambiguation performances of Author-ity2009, author name instances disambiguated by Author-ity2009 need to be labeled. This study attempts to link ORCID ids to 40M author name instances that appear in about 9M papers published between 1991 and 2009 in Author-ity2009. Author-ity2009 disambiguates author name instances in MEDLINE but does not provide their raw name strings. So, this study proceeds from the whole MEDLINE corpus (2016 baseline version) retrieved from the National Library of Medicine repository7. We select MEDLINE records for papers published between 1991 and 2009 (MEDLINE2009) to align with the publication year range of Author-ity2009. Next, name instances in MEDLINE2009 are compared to the author profiles in ORICD. For this MEDLINE2009-ORCID linkage, a 2018 ORCID release version is used8. To find author name instances recorded in both MEDLINE2009 and ORCID, paper titles with five or more words in MEDLINE2009 are encoded into ASCII format, deprived of non-alphabetical characters, and lowercased. Any duplicate titles after the preprocessing are removed. Then, each title (which is associated with a unique PMID) is compared to the publication lists in ORCID researcher profiles. If a match is found between bibliographic records in MEDLINE2009 and ORCID, author name strings that appear in the matched MEDLINE2009 paper are compared with the name string of the ORCID researcher whose list of publications contains the matched title. If two name strings in MEDLINE2009 and ORCID are matched on the full surname plus the first forename initial, they are assumed to refer to the same author and the ORCID ID of the matched researcher profile is assigned to the name instance in MEDLINE2009. As shown in Figure 1, this matching process produces a labeled dataset, MED-ORC, in which an author name instance in a MEDLINE paper is associated with an ORCID ID."
2102.03237,dataset,39,2022-05-16,0,indicates that AUT-NIH most closely matches Author-ity2009 in terms of the publication year distribution of name instances. The other two labeled datasets over-represent recent years heavily (AUTORC) and slightly (AUT-SCT) relative to Author-ity2009..
2102.03237,dataset,8,2022-05-16,0,5 https://www.nlm.nih.gov/bsd/medline.html 6 https://databank.illinois.edu/datasets/IDB-4222651
2102.03237,dataset,88,2022-05-16,0,"Although three labeled datasets produced similar evaluation results, they had different characteristics. First, AUT-ORC and AUT-NIH were used to evaluate both the precision and the recall of Authority2009’s clustering of name instances that refer to the same unique authors. But AUT-SCT could be used only to evaluate how well Author-ity2009 decided that self-citing name instance pairs refer to the same authors (≈ recall). This means that AUT-SCT could only provide partial evaluation of Author-ity2009’s disambiguation performance."
2102.03237,dataset,9,2022-05-16,1,17 Datasets can be downloaded at https://doi.org/10.6084/m9.figshare.13404986.v1
2102.03237,"dataset, data https",23,2022-05-16,0,"Torvik, V. I., & Smalheiser, N. R. (2018). Author-ity 2009 - PubMed author name disambiguated dataset."
2102.03681,code,124,2022-05-16,0,"Sometimes, Stan is able to produce vectorized code such as in matrix multiplication. This is consistent with our benchmark results since Stan came closest to FastAD for this operation (see Section 5.3). It is also consistent with how it is implemented, since they allocate extra memory for double values for each matrix and the multiplication is carried out with these matrices of primitive types. However, this vectorization does come at a cost of at least 4 times extra memory allocation than what FastAD allocates. Moreover, the backward-evaluation requires heap-allocating a matrix on-the-ﬂy every time. FastAD incurs no such cost, only allocates what is needed, and never heap-allocates during AD evaluation."
2102.03681,code,145,2022-05-16,0,"N values. For a complicated model as such, there are many opportunities for FastAD to cache certain evaluations for constants as mentioned in Section 4.3 and 5.4. In particular, the exponential function eh reuses its forward-evaluated result, and many log-pdfs cache the log of its parameters such as log(σ) in the Normal log-pdfs and log(γ) in the Cauchy log-pdfs (σ, γ are the second parameters of their respective distributions, which are constant in this model). Note that this caching is automatically done in FastAD, which would be tedious to manually code for the baseline. Hence, this shows that due to automatic caching, FastAD forward and backward-evaluation combined can be faster than a manually-written forward evaluation only, which puts FastAD at an optimal performance."
2102.03681,code,167,2022-05-16,0,"One example is choosing the correct specialization of an operation depending on the shapes of the input. As seen in Section 4.1, all nodes are given a shape trait. Depending on the input shapes, one may need to invoke diﬀerent routines for the same node. For example, the normal log-pdf node behaves quite diﬀerently depending on whether the variance parameter is a scalar σ2 or a (covariance) matrix Σ. Namely, if the variance has a matrix shape, we must perform a matrix inverse to compute the log-pdf, which requires a diﬀerent code from the scalar case. Using a C++ design pattern called Substitution-Failure-Is-Not-An-Error (SFINAE), we can choose the correct routine at compile-time. The beneﬁt is that there is no time spent during run-time in choosing the routine anymore, whereas in libraries like CppAD, they choose the routines at run-time for every evaluation of the node [2]."
2102.03681,code,41,2022-05-16,0,"Fig. 4: Sum and product benchmarks of other libraries against FastAD plotted relative to FastAD average time. Fig. 4a,4c use built-in functions whenever available. Fig. 4b,4d use the naive iterative-based code for all libraries."
2102.03681,code,47,2022-05-16,0,"comments on the ﬁrst draft and for taking the time to optimize the benchmark code for their respective libraries. We also thank Art Owen and our colleagues Kevin Guo, Dean Deng, and John Cherian for useful feedback and corrections on the ﬁrst draft."
2102.03681,code,60,2022-05-16,0,"This example really highlights the beneﬁts of vectorization. As noted in Section 4.1, this was the one benchmark example where Stan was able to produce vectorized code, which is consistent with Figure 6 that Stan is the only library that has the same order of magnitude as FastAD. Other libraries did not produce vectorized code."
2102.03681,code,71,2022-05-16,0,"If we assume that the most expensive operation is the matrix multiplication, AD evaluation approximately takes two matrix multiplications between a matrix and a vector. We can then approximate a lower bound for the manually-written gradient computation time to be two times that of the baseline. The relative time of FastAD to this approximated time is 1.1, implying about 10% overhead from a manually-written code."
2102.03681,code,73,2022-05-16,0,"Hence, in total, one AD evaluation requires three matrix multiplications between two K ×K matrices. If we approximate a manually-written gradient computation to take three times as long as the baseline (one multiplication), FastAD time relative to this approximated time is 3.27 3 = 1.09. This shows then that FastAD only has about 9% overhead from a manually-written code, which is extremely optimal."
2102.03681,"data, code",208,2022-05-16,0,"Vectorization refers to the parallelization of operations on multiple data at the hardware level. On a modern Intel 64-bit processor supporting AVX, four doubleprecision ﬂoating point numbers can be processed simultaneously, roughly improving performance by a factor of four. While the compiler optimization is able to vectorize a user’s code sometimes, it is not guaranteed because vectorization requirements are quite stringent. For example, vectorization is not guaranteed if memory access is not done in a contiguous fashion and is impossible if there is any dependency between loop iterations. This makes it quite challenging to design an AD system that can always predict compiler optimization to create vectorized code. However, vectorization can make AD extremely fast, powerful, and practical even in complex problems. In practice, we come across many examples where operations can be vectorized during gradient computation. For example, matrix multiplication, any reduction from a multi-dimensional variable to a scalar such as summation or product of all elements, and any unary and binary function that is applied element-wise such as exponential, logarithm, power, sin, cos, tan, and the usual arithmetic operators."
2102.03681,github,7,2022-05-16,2,1 github page: https://github.com/JamesYang007/FastAD
2102.03681,github,7,2022-05-16,1,2 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,github,7,2022-05-16,1,3 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,package,18,2022-05-16,0,"2. Bell, B.: Cppad: a package for c++ algorithmic diﬀerentiation http://www.coin-or."
2102.03681,package,56,2022-05-16,0,"6. Griewank, A., Juedes, D., Utke, J.: Algorithm 755: Adol-c: A package for the automatic diﬀerentiation of algorithms written in c/c++. ACM Trans. Math. Softw. 22(2), 131–167 (Jun 1996). https://doi.org/10.1145/229473.229474, https: //doi.org/10.1145/229473.229474"
2103.02044,code,9,2022-05-16,0,Code availability (software application or custom code)
2103.02044,"data, data available",16,2022-05-16,0,Our future plan is to deploy a federated XNAT portal to collect preclinical imaging data from
2103.02044,"data, data available",17,2022-05-16,0,"Identifier is missing, preventing the data to be found by both humans and computers. In"
2103.02044,"data, database",18,2022-05-16,0,"images from several sources, to save data in a safe database, and to share data among"
2103.02044,"data, database",37,2022-05-16,0,"I. B. Ozyurt et al., “Federated web-accessible clinical data management within an extensible neuroimaging database,” Neuroinformatics, vol. 8, no. 4, pp. 231–249, Dec. 2010."
2103.02044,"data, dataset",11,2022-05-16,0,possibility to export proprietary raw image datasets to internationally recognized data
2103.02044,"data, dataset",17,2022-05-16,0,"process imaging datasets. It offers a platform to store and distribute the data, manage the"
2103.02044,"data, dataset provided",11,2022-05-16,0,data. Commercial softwares distributed by imaging device manufacturers provide the
2103.02044,"data, dataset provided",18,2022-05-16,0,"(Figure 3). Notably, XNAT-PIC users can adjust the custom variables accordingly to their data"
2103.02044,"data, open-source",17,2022-05-16,2,"data, all within an XNAT environment. All the developments are free and open-source. We"
2103.02044,"data, publicly available",15,2022-05-16,0,studies and the relative costs are prompting imaging scientists to share image data in public
2103.02044,"data, publicly available",38,2022-05-16,0,"J. Ellenberg et al., “A call for public archives for biological image data,” Nature Methods, vol. 15, no. 11. Nature Publishing Group, pp. 849–854, 01-Nov-2018."
2103.02044,"data, publicly available, data repository",14,2022-05-16,0,principles whose goal is to provide a public data repository compliant to open science
2103.02044,database,10,2022-05-16,0,eventually uploaded to the database as project level resources.
2103.02044,database,11,2022-05-16,0,"studies, the Medical Imaging Research Management and Associated Information Database"
2103.02044,database,112,2022-05-16,0,"Abbreviations ADNI AIM API CEST CIM COINS CT DICOM DICOM SEG DICOM Segmentation Digital Object Identifier DOI Decimal String DS Diffusion Weighted Imaging DWI European Open Science Cloud  EOSC European Population Imaging Infrastructure  EPI2 Findable, Accessible, Interoperable and Reusable  FAIR 18-fluorodeoxyglucose  FDG France Life Imaging FLI Human Imaging Database  HID Hypertext Transfer Protocol  HTTP Long String LO Longitudinal Online Research and Imaging System  LORIS Medical Database Multi-Modal Molecular Imaging Italian Node  Magnetic Resonance Imaging  Neuroimaging Informatics Technology Initiative Open Access Series of Imaging Studies  Optical Coherence Tomography  Open Health Imaging Foundation  Optical Imaging  Other Picture Archiving and Communication System  Photoacoustic Imaging  Positron Emission Tomography Representational State Transfer  Region of Interest"
2103.02044,database,13,2022-05-16,0,Human Imaging Database (HID) and the Collaborative Informatics and Neuroimaging Suite
2103.02044,database,13,2022-05-16,0,"based web application, exploiting the PostgreSQL database system. Users can personalize"
2103.02044,database,19,2022-05-16,0,the ROI and uploads the results back to the database as XNAT resources (Figure 8). A
2103.02044,database,20,2022-05-16,0,"facilitates scripting interactions with the XNAT database [41], [42], iii) Requests 2.23.0 that"
2103.02044,database,3,2022-04-21,0,"Science, Database"
2103.02044,dataset,12,2022-05-16,0,The processing of large volumes of biomedical image datasets requires dedicated platforms
2103.02044,dataset,12,2022-05-16,0,processing preclinical image datasets. MRI2DICOM is a MR image converter from
2103.02044,dataset,13,2022-05-16,0,"discover image datasets normally not accessible, promoting the free exchange and reuse"
2103.02044,dataset,13,2022-05-16,0,"neither tools for importing large, multimodal preclinical image datasets nor pipelines for"
2103.02044,dataset,13,2022-05-16,0,processing large image datasets. This workflow is based on the steps schematically
2103.02044,dataset,15,2022-05-16,0,"2. XNAT-PIC Uploader to upload large, multimodal image datasets in DICOM standard to"
2103.02044,dataset,15,2022-05-16,1,"The datasets analyzed in the current study are openly available in the CIM-XNAT repository,"
2103.02044,dataset,15,2022-05-16,0,complexity and the variety of preclinical trial datasets. The time needed to perform these
2103.02044,dataset,15,2022-05-16,0,dynamically added to the ‘standard’ DICOM dictionary to describe CEST-MRI datasets. The
2103.02044,dataset,15,2022-05-16,0,"heterogeneous datasets,” Front. Neuroinform., vol. 5, Dec. 2011."
2103.02044,dataset,15,2022-05-16,0,once the dataset is successfully imported to XNAT. The original raw images can be
2103.02044,dataset,16,2022-05-16,0,"Upon conversion to DICOM format, the image dataset can be uploaded to XNAT. XNAT-PIC"
2103.02044,dataset,16,2022-05-16,0,associate the dataset stored in XNAT with a Digital Object Identifier (DOI) or Persistent
2103.02044,dataset,16,2022-05-16,0,needs to process large scale image datasets. The urgency was therefore to scale up this
2103.02044,dataset,16,2022-05-16,0,"to our CIM-XNAT instance, upload their image datasets and add the pipelines to their own"
2103.02044,dataset,17,2022-05-16,0,"datasets need to be established. In this paper, we present an extension of XNAT for"
2103.02044,dataset,18,2022-05-16,0,"as imaging dataset, as well as the tools, workflows, and pipelines needed to process the"
2103.02044,dataset,19,2022-05-16,0,"image datasets of different modalities to XNAT such as MRI, PET, CT, and US, allowing"
2103.02044,dataset,26,2022-05-16,0,"several image repositories have emerged enabling the discovery of datasets from peer reviewed publications or research studies in the life science domain, from biological imaging"
2103.02044,dataset,30,2022-05-16,0,Figure 1: Schematic workflow of image archiving and processing. XNAT-PIC is a suite of tools aimed at facilitating the management and the analysis of preclinical image datasets.
2103.02044,"dataset, publicly available",17,2022-05-16,0,"[21] P. Kalendralis et al., “Multicenter CT phantoms public dataset for radiomics reproducibility"
2103.02044,download,14,2022-05-16,0,"in the resource folder, download the morphological image in DICOM standard and the"
2103.02044,download,14,2022-05-16,0,"parametric image in NIfTI format corresponding to the user selection, download the ROI"
2103.02044,download,16,2022-05-16,0,"2. For each subject, create a local folder and then download the corresponding DICOM"
2103.02044,download,16,2022-05-16,0,"contains XML instructions to take this user input, create the working directory, download the"
2103.02044,download,23,2022-05-16,0,projects (See: Adding Pipelines To Your Project: https://wiki.xnat.org/documentation/how to-use-xnat/adding-pipelines-to-your-project); ii) XNAT Admins can download XNAT-PIC
2103.02044,download,4,2022-05-16,0,available for download.
2103.02044,github,10,2022-05-16,0,"pipelines from https://github.com/szullino/XNAT-PIC-Pipelines, install and register the"
2103.02044,github,14,2022-05-16,0,https://github.com/szullino/XNAT-PIC [31]. The application uses the numpy 1.15.4 and
2103.02044,github,3,2022-05-16,0,https://github.com/szullino/XNAT-PIC-Pipelines
2103.02044,"github, download",25,2022-05-16,2,Table 2: Processing pipelines currently installed on our CIM-XNAT instance (http://cim-xnat.unito.it) and available for download at https://github.com/szullino/XNAT-PIC-Pipelines.
2103.02044,"github, download",51,2022-05-16,2,The latest releases of the source codes of XNAT-PIC are available to download from the and GitHub https://github.com/szullino/XNAT-PIC. XNAT-PIC is a free software and is distributed under the terms of the GNU General Public License v3 or any later version as stated by the Free Software Foundation.
2103.02044,open-source,13,2022-05-16,0,"available micro-services in SAS comprise Dicomifier, a generic and open-source Bruker to"
2103.02044,open-source,14,2022-05-16,1,"In this work we have developed XNAT-PIC, a free and open-source application consisting"
2103.02044,open-source,15,2022-05-16,0,"Preclinical Imaging Centers (XNAT-PIC). XNAT is a worldwide used, open-source platform"
2103.02044,open-source,15,2022-05-16,0,preclinical imaging available at http://cim-xnat.unito.it. XNAT is a free and open-source Java
2103.02044,open-source,22,2022-05-16,0,"[34] D. Mason, “SU‐E‐T‐33: Pydicom: An Open Source DICOM Library,” in Medical Physics,"
2103.02044,open-source,43,2022-05-16,0,"[54] T. Urban et al., “LesionTracker: Extensible open-source zero-footprint web viewer for cancer imaging research and clinical trials,” Cancer Res., vol. 77, no. 21, pp. e119–e122, Nov. 2017."
2103.02044,package,14,2022-05-16,0,"package to run MRI2DICOM and XNAT-PIC Uploader as a stand-alone executable, in both"
2103.02044,"publicly available, github",16,2022-05-16,0,GNU General Public License v3 or any later version and available on GitHub. Some work
2103.02044,python,14,2022-05-16,0,XNAT-PIC Uploader is built in Python 3.7.6. The communication with XNAT is possible
2103.02044,python,14,2022-05-16,0,descriptor invokes a bash script running a Python wrapper consisting of a sequence of
2103.02044,python,14,2022-05-16,0,script passes the resulted mask to a Python script that computes statistical calculations in
2103.02044,python,15,2022-05-16,0,to process multiple subjects within the same project. A Python 2.7 virtual environment has
2103.02044,python,15,2022-05-16,0,“Python Software Foundation.” [Online]. Available: https://www.python.org/.
2103.02044,python,16,2022-05-16,0,3.5 has been used to bundle the Python applications and all its dependencies into a single
2103.02044,python,16,2022-05-16,0,pipeline runs on Python 3.8.3 and uses the following libraries: numpy 1.18.5 [44]
2103.02044,python,18,2022-05-16,0,"for Python to run MATLAB scripts within a Python session [40], ii) pyxnat-1.2.1.0.post3 that"
2103.02044,python,19,2022-05-16,0,"acqp) into Python dictionaries [36]. Lastly, it saves all the relevant information into the"
2103.02044,python,19,2022-05-16,0,"mask [45], the image processing library opencv-python 4.4.0.40 [46], and nibabel 3.1.1 for"
2103.02044,python,20,2022-05-16,0,"Python library to encrypt the files containing the XNAT login credentials [37], [38]. PyInstaller"
2103.02044,python,21,2022-05-16,0,“opencv-python · PyPI.” [Online]. Available: https://pypi.org/project/opencv-python/4.4.0.40/. [Accessed: 18-Feb-2021].
2103.02044,python,30,2022-05-16,0,"[41] Y. Schwartz et al., “PyXNAT: XNAT in Python,” Front. Neuroinform., vol. 6, p. 12, May 2012."
2103.02044,python,30,2022-05-16,0,“Get Started with MATLAB Engine API for Python - MATLAB & Simulink.” [Online]. Available: https://www.mathworks.com/help/matlab/matlab_external/get-started-with-matlabengine-for-python.html. [Accessed: 18-Feb-2021].
2103.02044,python,6,2022-05-16,0,Python scripts in order to:
2103.02044,"python, code",28,2022-05-16,0,resource descriptor. The resource descriptor invokes a bash script passing both the T2 weighted DICOM image and the DICOM RT-STRUCT directories to a Python code to be
2103.02044,"python, github",24,2022-05-16,0,"[36] M. Caffini, “Project-Beat--Pyhton.” [Online]. Available: https://github.com/mcaffini/Project Beat---Python."
2103.02044,"python, open-source",13,2022-05-16,0,MRI2DICOM is a free and open-source tool built in Python 3.7.6 downloadable at
2103.02044,"python, open-source",15,2022-05-16,0,"through xnatpy 0.3.22, a new and open-source XNAT Python client, and pyAesCrypt 0.4.3"
2103.02044,"python, package",17,2022-05-16,0,converted into a NIfTI mask by the Python package dcmrtstruct2nii [55]. The same bash
2103.03806,code,154,2022-05-16,0,"In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to ﬁnd automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the ﬁeld of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers’ architecture to automatically detect malicious software. We propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection."
2103.03806,code,56,2022-05-16,0,"2. We model Android malware detection as a binary and a multi-label text classiﬁcation problem and propose a novel feature representation by considering the software applications’ source code as a set of features. We apply text preprocessing on these features to keep the important information like permissions, intents, and activities."
2103.03806,code,59,2022-05-16,0,"While in our work, we propose a noval approach, we used BERT to better detect malware, we reﬁned the pre-trained model to efﬁciently learn representations of source code language syntax and semantics. Our Context-Aware network learns contextual characteristics from a natural language sentences perspective thanks to the attention mechanism layers in the Transformer-based architecture."
2103.03806,code,89,2022-05-16,0,"The wildly used type is static analysis [3]. It is a known way of identifying malicious applications among benign applications, and this analysis focuses on the source code of software components that may be affected by malware. It is less expensive in terms of resources and time since no need to activate the malware by executing the code to capture the features, it can identify the maliciousness at the code level. For static analysis, there are mainly three practices to detect"
2103.03806,"data, dataset",108,2022-05-16,0,"Also, RoBERTa [13] an extension of BERT with some modiﬁcations in the pre-training procedure of the architecture, uses the same architecture. The modiﬁcations include training the model longer, with larger batches, and on a larger amount of data. Also, the predictive objective of the following sentence was deleted. And in order to on longer sequences, the authors applied dynamic modiﬁcation on the masking scheme [12] applied to the training data. They also collect a new dataset of comparable size to other privately used datasets to better control the effects of training set size."
2103.03806,"data, dataset",161,2022-05-16,0,"This paper studies the challenge of malware classiﬁcation using our novel approach for Transformer-based malware detection. We detailed the malware text classiﬁcation methodology and used it for feature representation. The BERT based model achieved high accuracy results for both binary and cross-category classiﬁcation, compared to the other baseline pre-trained language models. The results from the experiments show that the best binary accuracy is 0.9761 and for the multi-classiﬁcation, it is 0.9102. We can conclude that the proposed approach’s results of feature representation as text input for a Transformer-based model, are very good. So, the implementation of pre-trained linguistic models based on Transformer architectures in cybersecurity tasks can outperform standard RNN models like LSTM, when applied on a state-of-the-art dataset like Androzoo. Future work will therefore consist of testing other models, testing other types of data, and creating an API to detect malware in new applications."
2103.03806,"data, dataset",170,2022-05-16,0,"Once the data is created and annotated in the right format, we split the data into train and test. We conduct all our experiments using BERT, we ﬁne-tuned it on our train dataset. We ﬁxed the hyperparameters based on each classiﬁcation type. We train BERT to predict Malware/Benign (i.e., binary classiﬁcation) for each sample, then, to predict the categories of malware (i.e, multi-classiﬁcation). The Transformer architecture has speciﬁc input formatting steps including the creation of special tokens and ids. We use the Transformers implementation of the hugging face library [34] for the binary classiﬁcation of Android applications. Only the Transformer architecture, layers, and weights are implemented, while all data formatting must be done beforehand to be compatible with the Transformer. While most pre-trained Transformers have essentially the same steps. Here we test this approach with BERT. Figure 4 gives a detailed overview of our approach."
2103.03806,"data, dataset, code",145,2022-05-16,0,"In this paper, we propose a malware detection approach using a Transformer-based algorithm. We experiment with different Transformer model architectures on our data. The dataset includes 11 different malware categories namely adware, spyware, ransomware, clicker, dropper, downloader, riskware, SMS-sender, horse-trojan, backdoor, and banker [9]. Our methodology focuses on the static analysis level on the source code of Android applications, to identify different categories of malware. Indeed, we did not limit the features to permission-based only but considered the whole software code as an important set of feature representation for the analysis. We started with training the model with the features after preprocessing, then a binary classiﬁcation of the apps to malicious and benign, and ﬁnally a cross-category classiﬁcation at the malware level."
2103.03806,"data, dataset, code, download",270,2022-05-16,0,"Once the list of APKs is deﬁned, we write a script to download the ﬁles. Then, we decompiled the downloaded APKs using Jdax [32], which creates folders of the apps’ ﬁles [33]. We extracted the AndroidManifest.xml ﬁle from each sample. The manifest presents essential information about the application to the Android system, information the system must have before it can run any of the application’s code, including the list of permissions, the activities, services, broadcast receivers, content providers, the version, and the meta-data. These ﬁles are then treated as text ﬁles and passed through the preprocessing phase, in this step and to conserve the important information about the features, we apply speciﬁc cleaning of the not important, mostly repeated words, in the code. We manually analyzed different examples and created a list of words and expressions that do not provide additional info, so the cleaning included lexicon removal, punctuation removal and we conserved the digits and the cases of the characters. The purpose of the preprocessing is to reduce the size of the input. The ﬁnal dataset format has 4 columns, the ID column, represented by the APK hash name, the Text column representing the Manifest ﬁles after preprocessing, the Label column, a binary format equal to 1 if the app is malware and 0 if not, and ﬁnally the Category column representing the malware type name (exp: adware)."
2103.03806,"data, dataset, data https, download",310,2022-05-16,0,"Based on state-of-the-art taxonomies for Android malware categories [29] & [30] We selected 11 categories 3 namely; adware (displays advertising and entice a user to install it on their device), spyware (installs itself on the user device with the aim of collecting and transferring information without the user is aware of it), ransomware (takes personal data hostage), clicker (a type of trojan that performs a form of ad fraud. These “clickers” continuously make connections to websites, consequently awarding threat actors with revenue on pay-per-click bases), dropper (a syringe program or dropper virus, is a computer program created to install malicious software on a target system), downloader (a type of Trojan horse that downloads and installs malicious ﬁles), riskware (a software whose installation can represent a risk for the security of the computer, but however, not inevitably), SMS-sender (presents itself as a regular SMS messaging application and uses its basic permissions to send/receive short messages), horse-trojan (is designed to damage, disrupt, steal, or in general inﬂict some other harmful action on your data or network), backdoor (when introduced into the device, usually without the user’s knowledge, turns the software into a Trojan horse, and banker (is designed to steal data from users’ online bank accounts as well as data from online payment systems and plastic card systems). We select the list of APKs to download based on the recent creation and analysis date, then re-analyze this list with VirusTotal [31], to ﬁnally create our dataset list including 12,000 benign apps and 10,000 malware apps."
2103.03806,"data, dataset, publicly available",112,2022-05-16,1,"We collected the Android applications from the Androzoo public dataset. Androzoo, one of the stae of the art android malware dataset [27], is a growing collection of Android applications from several sources, including the ofﬁcial Google Play app market. It currently contains 13,320,014 different APKs, each of which has been analyzed by dozens of different antivirus products to ﬁnd out which applications are detected as malware. This public data is up to date with weekly analysis on the samples [28]. The data is labeled based on these analyses into malware and benign, and different malware categories and families."
2103.03806,dataset,145,2022-05-16,0,"We conducted the experiments on the preprocessed dataset. Fine-tuning the pre-trained models, clearly gave the highest accuracy results for this classiﬁcation task compared to the LSTM baseline. The best classiﬁcation model is BERT. The test metrics results of Table 1 show that each Transformer learns differently depending on each architecture. The results in Table 1 and Table 2 prove that BERT outperformed the other baseline models in both binary and multi-classiﬁcation malware detection. For BERT, the best learning rate shows that only two epochs are required before the loss starts to increase. Our ﬁne-tuning with the training set included changing the hyperparameters to boost the results. To evaluate the ﬁnal results, we used different evaluation metrics. The pretrained models achieved good results overall, but BERT obtained the best performance in both tasks."
2103.03806,dataset,22,2022-05-16,0,Table 1: Detection results using the feature representation approach across difference networks on the test dataset for both binary classiﬁcation.
2103.03806,dataset,22,2022-05-16,0,Table 2: Detection results using the feature representation approach across difference networks on the test dataset for cross-category malware classiﬁcation.
2103.03806,dataset,44,2022-05-16,0,"applies a CNN with an attention mechanism to images converted from binary datasets, by calculating an attention map to extract characteristic byte sequence. The distinction of regions in the attention map shows regions having higher importance for classiﬁcation in the image."
2103.03806,dataset,55,2022-05-16,0,2. MCC: The Matthews Correlation Coefﬁcient (MCC) is bast used for binary classiﬁcation with an unbalanced dataset. It has a range of -1 to +1. We chose MCC over F1-score for binary classiﬁcation as recommended in this study [36]. MCC equation is deﬁned as fellow :
2103.03806,dataset,55,2022-05-16,0,"To test the proposed approach, we evaluate it in terms of three main aspects: (1) the proﬁtability on large and recent categorical datasets, (2) the feature representation ability for information context extraction from android apps, and (3) the performance compared to the state-of-the-art approaches."
2103.03806,"dataset, code",118,2022-05-16,0,"This section explains the overall process of malware detection. The core idea of this work is to create, a malware detection framework using a Transformer-based approach. To reach this goal, we conducted a static analysis on the collected corpora from a natural language sentences perspective. So, we need a dataset including source code ﬁles and different categories of malware types. Figure 2 explains the logical ﬂow of our Android malware detection. This Process is mainly divided into 4 phases. First, the Android ﬁles collection, then the Decompilation phase of the APK ﬁles, Feature Mining, and ﬁnally Deep Learning (DL) models training experiments."
2103.03806,"dataset, code, package",277,2022-05-16,0,"Among these approaches this study [16] builds AMalNet, a DN framework to learn multiple integration representations and family assignment with Graph CNN (GCNs) to model high-level graph semantics and use an Independent RNN (IndRNN) to decode deep semantic information. SeqMobile [17], is a behavior-based sequence approach. it uses different recurrent neural networks (RNN). It extracts the semantic feature sequence, which can provide information of certain malicious behaviors, from binary ﬁles under a certain time constraint. This paper [18], presents a new approach based on OpCode-level FCG. The FCG is obtained through static analysis of Operation Code (OpCode) using a Long Short-Term Memory (LSTM). the authors conduct experiments on a dataset on 1,796 Android malware samples classiﬁed into two categories and 1,000 benign Android apps. The authors of [19] focused on step size as an important factor in relation to input size using RNN. They tested the model with three different feature vectors (hot-coding feature vector, random feature vector and Word2Vec feature vector) using hyper parameters. [20] transform the android package kit (APK) ﬁle into a lightweight RGB image using a predeﬁned dictionary and intelligent mapping, then apply a CNN on the obtained images for malware family classiﬁcation. Multiple other examples of DNN based approach [21] and [22], have been developed, with varying the feature extraction, selection, and representation methods in the aim of boosting the detection results."
2103.03806,"dataset, publicly available",32,2022-05-16,1,3. We conduct extensive experiments on our preprocessed Android dataset collected from public resources with different category-annotated labels. This preprocessed dataset will be released publicly for the research community.
2103.03806,github,68,2022-05-16,0,"[31] Virus Total. Virustotal-free online virus, malware and url scanner. Online: https://www. virustotal. com/en, 2012. [32] jadx. jadx - dex to java decompiler. Online: https://github.com/skylot/jadx, 2012. [33] Nicolas Harrand, César Soto-Valero, Martin Monperrus, and Benoit Baudry. Java decompiler diversity and its"
2103.03806,open-source,49,2022-05-16,0,"[28] Pei Liu, Li Li, Yanjie Zhao, Xiaoyu Sun, and John Grundy. Androzooopen: Collecting large-scale open source android apps for the research community. In Proceedings of the 17th International Conference on Mining Software Repositories, pages 548–552, 2020."
2103.03968,database,210,2022-05-17,1,"We ﬁrst applied our algorithm on scans simulated from a brain phantom, which we obtained from the BrainWeb database [3]. We simulated nθ projections from this phantom, for two values of nθ = 1440 and 960. For each nθ, we ﬁrst reconstructed the image of the phantom from the full set of nθ projections and from nθ/2 projections; we denote these images with xnθ and xnθ/2, respectively. We then applied the proposed algorithm and the dictionary-based interpolation algorithm to interpolate the subset of nθ/2 projections to generate nθ projections and reconstructed the image of the phantom from the interpolated projections. We will denote these images with xproposed nθ/2. We simulated two levels of noise in the projections with diﬀerent number of incident photons: N0 = 106 and N0 = 5 × 104. We will refer to these simulations as low-noise and high-noise, respectively. For both simulations, we assumed the detector electronic noise to be additive Gaussian with a standard deviation of 40. As the reference scan that we need for block matching for computation of Rs, we used the simulated scan of a diﬀerent brain phantom from the same database."
2103.03968,database,62,2022-05-17,0,"3. Cocosco, C.A., Kollokian, V., Kwan, R.K.S., Pike, G.B., Evans, A.C.: Brainweb: Online interface to a 3d mri simulated brain database. In: NeuroImage (1997) 4. Feldkamp, L.A., Davis, L.C., Kress, J.W.: Practical cone-beam algorithm. J. Opt."
2103.06238,"data, code",165,2022-05-17,0,"The study uses a Modified Iterative Development as a guide for the design and development of the VR 3D Model and its environment. Iterative development is a procedure on which there is a breaking down of the computer program advancement of a huge application into little portions. In the iterative development process, the highlighted code is planned, created, and tried in rehashed cycles. In each repetition, there is re-designing of additional features, developed and verified until it is ready to be deployed or installed. Since this project used Modified Iterative Development, there are some changes of the iteration on the stages to ensure that the program follows the desired outcomes set by the developers. Figure 1 illustrates the Modified Iterative Development to produce an increment project that is being developed. The process starts with the Planning, Data Gathering and Data Analysis, System Requirements, Designing, Testing, and Evaluation."
2103.12883,github,3,2022-05-17,2,1https://github.com/ricardoGrando/hydrone_deep_rl_icra
2103.12883,package,51,2022-05-17,0,"[31] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “Uuv simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in OCEANS MTS/IEEE Monterey. IEEE, 2016, pp. 1–8."
2103.12883,package,55,2022-05-17,0,"• We demonstrate that, with our approaches, the robot is capable to arrive at the desired target avoiding collisions. However, with a geometrically dependent tracking controller, the robot is unable to bypass the drilling risers. We also provide a completely built ROS package with a real-world described HUAUV."
2103.12883,python,63,2022-05-17,0,"The whole system was implemented using ROS and Gazebo frameworks. The Deep-RL approaches were implemented using Python programming language, while the vehicles’ related plugins were partially implemented in C++ and Python. The implementation of the neural networks was carried out with the PyTorch2 library. The performance of our approaches can also be observed in a complementary video3."
2103.12895,"code, package, python, data, dataset, dataset provided",151,2022-05-17,0,"the loading end), each named accordingly (front, middle, rear) in the dataset. Hence, there were 9 loggers in total for each instrumented shipment labeled with respect to the loggers’ location in the pallet and the pallets’ location in the container (front−top:FT, front−middle:FM, front−bottom:FB, ..., rear−bottom:RB). Figure 3 displays each sensor proﬁle separately for each of the 6 shipments. Please observe that these ﬁgures display real−world, noisy and complex multivariate time series as signature representatives of each shipment. Summary statistics for all variables from the shipments datasets are presented in Table 4. These statistics were obtained using the ‘Pandas’ Python package [8]. Python Code used for ﬁltering and analyzing these data will be provided based on request."
2103.12895,"data, data https",22,2022-05-17,0,Please visit the following website for more information about the sensors used in collecting the data: https://www. deltatrak.com/reusable-real-time-loggers
2103.12895,"data, data https, supplementary data, data available",16,2022-05-17,0,Supplementary data associated with this article can be found in the online version at Supplemental ﬁles
2103.12895,"data, dataset",7,2022-05-17,1,Direct URL to data: Dataset link
2103.12895,"data, dataset",78,2022-05-17,0,"• The analysis and processing of temperature time-series data for predictive tasks represent a signiﬁcant challenge especially when proﬁles may have variable lengths, high variability, and abnormalities as is common in many cold chain applications. The dataset will enable researchers from a wide array of ﬁelds and backgrounds to apply analytical tools such as machine learning and physical models in testing and comparing the performance of their predictive or diagnostic algorithms on the cold-chain."
2103.12895,"data, python",89,2022-05-17,0,Instruments: DeltaTrak’s Reusable Real−Time−Logger (RTL) Mini devices are used to log both temperature and location data in real time. The RTLs have a wide operational temperature range of −30◦C to 95.55◦C with a temperature accuracy of +/−1◦C. More information about the hardware used in this study can be found in the appendix. Data was extracted via the cloud application which can establish secure communications with the GSM loggers. Python [4] was employed to perform subsequent data analysis.
2103.12895,dataset,13,2022-05-17,0,A TIME-TEMPERATURE DATASET FOR THE STRAWBERRY COLD CHAIN ACROSS MULTIPLE SHIPMENTS AND LOCATIONS
2103.12895,dataset,196,2022-05-17,1,"Monitoring and controlling the refrigeration of food during the cold-chain (transportation, storage, and distribution of perishable food items) are critical to reducing the amount of food waste.However, the cost of installation of the monitoring devices such as wireless sensor networks (WSNs) and radio frequency identiﬁcation (RFID) systems limits monitoring resolution in commercial applications generally to one per container [[2], [3]]. Hence, with signiﬁcant collaboration between the stakeholders from growers to distributors to retailers to academics, six strawberry shipments across the continental United States datasets are shared to help in overcoming this limitation. The datasets now made available were collected aiming at understanding the holistic temperature behavior of the strawberry cold-chain and the development of prediction models to predict the future behavior of the strawberries during transportation from harvest to the DC. Nevertheless, due to the temporal heterogeneity, complexity, similarity, and discrepancy characteristics of the variables included in these datasets, their use goes beyond this future prediction problem to location-based-prediction, binary control criteria, classiﬁcation, clustering, etc."
2103.12895,python,22,2022-05-17,0,"[4] Guido Van Rossum and Fred L Drake Jr. Python tutorial. Centrum voor Wiskunde en Informatica Amsterdam, The"
2103.13219,"data, dataset",108,2022-05-17,0,"These music excerpts were derived from pieces by 84 different composers from Baroque to the Modern period. Their durations distribute between 0.3 and 2.3 seconds. To prepare ﬁxed-length data for training, excerpts that are shorter or longer than 2 seconds were repeated or trimmed to create a 2-second excerpt. Considering the large size of our dataset, we randomly took a thousand samples from the excerpts of each composer. In total, 62424 excerpts form a smaller dataset3. This also helps to compare convnet of different architectures in a more efﬁcient way, since the training time can be signiﬁcantly reduced."
2103.13219,"data, dataset",135,2022-05-17,0,"For the target task, the dataset consists of ten well known passages of Chopin’s piano music. A pianist was asked to perform the passages using a Yamaha baby grand piano situated in the MAT studios at Queen Mary University of London. The audio were recorded at 44.1 kHz and 24 bits using the spaced-pair stereo microphone technique with a pair of Earthworks QTC40 omnidirectional condenser microphones positioned about 50 cm above the strings. The positions were kept constant during the recording. Meanwhile, movement of the sustain pedal was recorded along with the audio with the help of the measurement system proposed in [4]. The audio data were annotated with frame-wise on or off labels as the ground truth, representing whether the sustain pedal"
2103.13219,"data, dataset",262,2022-05-17,0,"Abstract—Detecting piano pedalling techniques in polyphonic music remains a challenging task in music information retrieval. While other piano-related tasks, such as pitch estimation and onset detection, have seen improvement through applying deep learning methods, little work has been done to develop deep learning models to detect playing techniques. In this paper, we propose a transfer learning approach for the detection of sustainpedal techniques, which are commonly used by pianists to enrich the sound. In the source task, a convolutional neural network (CNN) is trained for learning spectral and temporal contexts when the sustain pedal is pressed using a large dataset generated by a physical modelling virtual instrument. The CNN is designed and experimented through exploiting the knowledge of piano acoustics and physics. This can achieve an accuracy score of 0.98 in the validation results. In the target task, the knowledge learned from the synthesised data can be transferred to detect the sustain pedal in acoustic piano recordings. A concatenated feature vector using the activations of the trained convolutional layers is extracted from the recordings and classiﬁed into frame-wise pedal press or release. We demonstrate the effectiveness of our method in acoustic piano recordings of Chopin’s music. From the crossvalidation results, the proposed transfer learning method achieves an average F-measure of 0.89 and an overall performance of 0.84 obtained using the micro-averaged F-measure. These results outperform applying the pre-trained CNN model directly or the model with a ﬁne-tuned last layer."
2103.13219,"data, dataset, publicly available",279,2022-05-17,1,"input.midpedal.wavno-pedal.wavexcerpts in pairsconvnetaudio recording.wavresultsconvnet featuresSVM classiﬁertransferlearningtrainsource tasktarget taskIII. DATASET For the source task, pedal and no-pedal versions of music excerpts are required to train a convnet, which is able to highlight the spectral or temporal characteristics that change with the sustain pedal instead of note events. For this reason, 1392 MIDI ﬁles publicly available from the Minnesota International Piano-e-Competition website1 were downloaded. They were recorded using a Yamaha Disklavier piano from the performance of skilled competitors. To render these MIDI ﬁles into high quality audio, the Pianoteq 6 PRO2 software was used. This physically modelled virtual instrument approved by Steinway & Sons can export audio using models of different instruments and recording conditions. We employed the Steinway Model D grand piano instrument and the closemiking recording mode. Audio with or without sustain-pedal effect was then generated with a sampling rate of 44.1 kHz and a resolution of 24 bits. These were rendered while preserving or removing the sustain-pedal message in the MIDI data. For each pedal-version audio, we can obtain the temporal regions when the sustain pedal is on or off by thresholding the MIDI message at 64 given its range of [0,127]. A pedalled segment is determined to start at a pedal onset (where the pedal state changes from off to on) and ﬁnish when the state returns to off. We can clip all the pedalled segments to form the pedal excerpts. The start and end times of the pedalled segments were also used to obtain no-pedal excerpts from the corresponding no-pedal-version of the audio."
2103.13219,database,64,2022-05-17,0,The rest of this paper is organised as follows. We ﬁrst introduce related works in Section II. The process of database construction is described in Section III. The methods of sustain-pedal detection including convnet design and transfer learning are discussed in Section IV. Experiments and results are presented in Section V. We ﬁnally conclude our work in Section VI.
2103.13219,dataset,43,2022-05-17,0,"2) A transfer learning method that allows the convnet trained from the source task to be adapted to the target task, where the recording instruments and room acoustics are different. This also allows effective learning with a smaller dataset."
2103.13219,"dataset, used dataset",321,2022-05-17,0,"In this paper, we focus on detecting the technique of the sustain pedal, which is the most frequently used one among the three standard piano pedals. All dampers are lifted off the strings when the sustain pedal is pressed. This mechanism helps to sustain the current sounding notes and allows strings associated to other notes to vibrate due to coupling via the bridge. A phenomenon known as sympathetic resonance [1] is thereby enhanced and embraced by pianists to create a “dreamy” sound effect. We can observe how the phenomenon reﬂects on the melspectrogram in Figure 1, where note F4 is played without (ﬁrst) and with (second) the sustain pedal in two bars respectively. Note that the symbol under the second bar of the music score in Figure 1 can be used to indicate the sustain-pedal techniques. Yet, even if pedal notations are provided, pedalling in the same piano passage can be executed in many different ways. Playing techniques are typically adjusted to the performer’s sense of tempo, dynamics, as well as the location where the performance takes place [2]. Given that detecting pedalling nuances from the audio signal alone is a rather challenging task [3], several measurement systems have been developed to capture the pedal movement. For instance, the Yamaha Disklavier piano can encode this movement into MIDI messages (0-127) along with note events. A dedicated system proposed in [4] enables synchronously recording the pedalling gestures and the piano sound. This can be deployed on common acoustic pianos, and it is used to provide the ground truth dataset introduced in Section III. Detection of pedalling techniques from audio recordings is necessary in the cases where installing sensors on the piano is"
2103.13219,python,46,2022-05-17,0,"[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel et al., “Scikit-learn: Machine learning in python,” Journal of machine learning research, vol. 12, no. Oct, pp. 2825–2830, 2011."
2104.00622,"data, dataset",106,2022-05-17,0,"and qualitative comparison on different training data respectively. We can see that training the model on both datasets can get best results. Ray Pooling. Table 12 shows that argmax performs consistently better than weighted sum on all types of testing data. Figure 15 also shows that argmax can better estimate missing depth of transparent objects on real images. Candidate points selection. Table 13 shows that directly learning offsets of candidate points is better than sampling points heuristically. Figure 16 further provides some examples on real images, showing that learning offset is more robust to strong background textures."
2104.00622,"data, dataset",226,2022-05-17,1,"We compare our approach to several state-of-the-art methods in Table 1. For fair comparison, we evaluate all related works using their released checkpoints (denoted by method name) as well as retraining on our data (denoted by method name with subscript ours. All baselines are trained on both datasets together which is the same setting as our proposed method). RGBD-FCNours is a strong baseline proposed by ourselves. It directly regresses depth maps using fully convolutional networks from RGB-D images. We use Resnet34-8s [51] as the network architecture and train the network on our data. NLSPN [38] is the state-ofthe-art method for depth completion on NYUV2 [48] and KITTI [49] dataset. Cleargrasp [46] is the state-of-the-art method for depth completion of transparent objects. For our approach, we use the best model: LIDF plus the depth reﬁnement model. Our method achieves the best result on all datasets even when baseline methods are trained on the same data. It also shows that training on Omniverse Object dataset can boost the performance of baseline methods. In Figure 6, we provide qualitative comparison by rendering point cloud in a novel view. Our approach can generate more meaningful depth than baseline methods."
2104.00622,"data, dataset",344,2022-05-17,1,"Function (LIDF) deﬁned on ray-voxel pairs consisting of camera rays and their intersecting voxels. The motivations for LIDF are: 1) The depth of a transparent object can be inferred from its color and the depth of its non-transparent In particular, color can provide useful vineighborhood. sual cues for the 3D shape and curvature while local depth helps to reason about the spatial arrangement and location of transparent objects. 2) Directly regressing the complete depth map using a deep network can easily overﬁt to the objects and scenes in the training data. By learning at the local scale (a voxel in our case) instead of the whole scene, LIDF can generalize to unseen objects because different objects may share similar local structures. 3) Voxel grids provide a natural partition of the 3D space. By deﬁning implicit function on ray-voxel pairs, we can signiﬁcantly reduce the inference time as the model only needs to consider occupied voxels intersected by the camera ray. Based on these motivations, we present a model to estimate the depth of a pixel by learning the relationship between the camera ray and its intersecting voxels given the color and local depth information. To further utilize the geometry of transparent object itself, we propose a depth reﬁnement model to update the prediction iteratively by combining the input RGB, input depth points and the predicted depth from LIDF. To train the whole pipeline, we create a large scale synthetic dataset, Omiverse Object dataset, using the NVIDIA Omniverse platform [2]. Our dataset provides over 60,000 images including both transparent and opaque objects in different scenes. The dataset is generated with diverse object models and poses, lighting conditions, camera viewpoints and background textures to close the sim-to-real gap. Experiments show that training on the Omniverse Object dataset can boost the performance for both our approach and competing methods in real-world testing cases."
2104.00622,"data, dataset",52,2022-05-17,1,"Training Data. We analyze the effects of training data in Table 5. We ﬁnd training our method purely on ClearGrasp or Omniverse leads to similar results, but training on both datasets can improve the performance a lot. This indicates that Omniverse dataset can be a good complementary to"
2104.00622,"data, dataset",80,2022-05-17,1,"more details about Omniverse Object dataset. The evaluation is done on the ClearGrasp dataset [46]. It has 4 types of different testing data: Synthetic images of 5 training objects (Syn-known); Synthetic images of 4 novel objects (Syn-novel); Real world images of 5 training objects (Realknown); Real world images of 5 novel objects (Real-novel), 3 of them are not present in synthetic data."
2104.00622,"data, dataset provided",10,2022-05-17,0,Training Data. Table 11 and Figure 14 provide quantitative
2104.00622,"data, dataset, code, data https",193,2022-05-17,3,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a selfcorrecting reﬁnement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs signiﬁcantly better than the current state-of-the-art In addimethods on both synthetic and real world data. tion, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp [46]. Code and dataset will be released at https: //research.nvidia.com/publication/202103_RGB-D-Local-Implicit."
2104.00622,"data, dataset, supplementary data",132,2022-05-17,0,"Datasets Our full pipeline is trained on the ClearGrasp dataset [46] and a new dataset we generated using the Omniverse Platform [2], which we call Omniverse Object dataset. The dataset provides various supervisions for transparent and opaque objects in cluterred scenes. Figure 5 visualizes some examples of the dataset. 3D object models in Omniverse Object dataset are collected from ClearGrasp and ShapeNet [7]. To get natural poses of objects, we use NVIDIA PhysX engine to simulate objects falling to the ground. Then we randomly select some objects and set their materials to the glass. We also augment the data by changing textures for the ground and opaque objects, lighting conditions and camera views. See supplementary for"
2104.00622,"data, dataset, supplementary data",57,2022-05-17,0,"In this section, we ﬁrst evaluate the effect of the depth reﬁnement model. After that, we compare several conﬁgurations for our ﬁrst stage networks. To focus on the generalization ability, we only report quantitative results on ClearGrasp Real-novel dataset. Please refer to the supplementary for results on other testing data."
2104.00622,dataset,10,2022-05-17,0,Figure 5. Examples from our Omniverse Object Dataset.
2104.00622,dataset,108,2022-05-17,0,"We have presented a novel framework for depth completion of transparent objects. Our method consists of a local implicit depth function deﬁned on ray-voxel pairs and an iterative depth reﬁnement model. We also introduce a large scale synthetic dataset for transparent objects learning, which can boost the performance for both our approach and other competing methods. Our pipeline is only trained on synthetic datasets but can generalize well to real world scenarios. We thoroughly evaluated our method compared to prior art and ablation baselines. Both quantitative and qualitative results demonstrate substantial improvements over the state-of-the-art in terms of accuracy and speed."
2104.00622,dataset,141,2022-05-17,0,"Transparent objects. Transparent objects have been studied in various computer vision tasks, including object pose estimation [26, 32, 31, 41, 30], 3D shape reconstruction [3, 20, 42, 28, 46] and segmentation [23]. However, most of these works assume known background patterns [20, 42], known object 3D models [26, 32, 41], or multi view/stereo input [30, 28]. Our approach does not require any priors and can estimate the depth of transparent objects from a single view RGB-D image. Sajjan et al. [46] is the closest work to ours. However, they pretrain their networks on out-ofdomain real datasets while our method is trained purely on"
2104.00622,dataset,309,2022-05-17,0,"Depth estimation. Depth estimation can be classiﬁed into three categories based on the input. Several methods have been proposed to directly regress the depth map from the color image using convolutional neural networks [14, 44, 27, 8, 16, 17, 19]. Most of them are trained on large scale datasets generated from RGB-D cameras, thus they can only reproduce the raw depth scan. Our method, on the contrary, focuses on the depth estimation for transparent objects where depth sensor typically fails. Another line of related work explores the task of depth completion given RGB images and sparse sets of depth measurements [33, 9, 43, 53, 11, 38]. These works improve the depth estimation over color-only methods, but they still produce low quality results because of limited information provided by sparse depth. Our method falls into the third category which tries to complete depth maps given noisy RGB-D images. Barron and Malik [4] propose a joint optimization for intrinsic images. Firman et al. [15] predict unobserved voxels from a single depth image using the voxlet representation. Matsuo and Aoki [34] reconstruct depths by ray-tracing to estimated local tangents. Recent works [56, 46] estimate surface normals and occlusion boundaries only from color images using deep networks and solve a global optimization based on those predictions as well as observed depths. The optimization is very slow and produces bad results if the network predictions are not accurate. We address these limitations by learning a implicit function using color and local depth jointly. Experiment shows that our method can achieve better results and 20× speedup compared to [46]."
2104.00622,dataset,47,2022-05-17,0,"[44] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset IEEE Transactions on Pattern Analysis and Matransfer. chine Intelligence (TPAMI), 2020. 2"
2104.00622,dataset,6,2022-05-17,0,D. Qualitative Results on NYUV2 Dataset
2104.00622,dataset,67,2022-05-17,0,"Figure 18. Qualitative results on NYUV2 dataset. For every example, ﬁrst row from left to right: input RGB, input depth, predicted depth, groundtruth depth; second row from left to right: input point cloud, predicted point cloud, groundtruth point cloud. Point clouds are rendered in a novel viewpoint. Please zoom in to see details."
2104.00622,dataset,73,2022-05-17,1,"We have done experiments on the NYUV2 dataset [48] to evaluate the performance of our method on general scenes and non-transparent objects. We corrupt the depth map by randomly creating some large holes. Our models are trained to predict the complete depth map given the corrupted depth map and RGB image. As shown in Figure 18, our method can predict reasonable missing depth in general scenes."
2104.00622,dataset,99,2022-05-17,0,"Our contributions are summarized as follows: 1) We pro pose LIDF, a novel implicit representation deﬁned on rayvoxel pairs, leading to fast inference speed and good generality. 2) We present a two-stage system, including networks to learn LIDF and a self-correcting reﬁnement model, for depth completion of transparent objects. 3) We build a large scale synthetic dataset proved to be useful to transparent objects learning. 4) Our full pipeline is evaluated qualitatively and quantitatively, and outperform the current state-of-theart in terms of accuracy and speed."
2104.00622,"dataset, dataset provided",115,2022-05-17,0,"In this section, we provide more details about our Omniverse Object Dataset. To generate the dataset, following categories from ShapeNet [7] are chosen: phone, bowl, camera, Following objects from ClearGrasp dataset [46] are chosen: cup-with-waves, ﬂower-bath-bomb, heart-bath-bomb, square-plastic-bottle, stemless-plastic-champagne-glass. Note that we only select training objects from ClearGrasp dataset to make sure testing objects are never seen during training. The background textures are randomly selected from the CC0 TEXTURES Dataset [1]. The textures for opaque objects are randomly selected from CC0 TEXTURES Dataset [1] and Describable Textures Dataset [13]."
2104.00622,"dataset, dataset provided",15,2022-05-17,0,Real-Known dataset. We also provide qualitative comparison of ablation studies on real images.
2104.00622,"dataset, dataset provided",4,2022-05-17,0,B. Omniverse Object Dataset
2104.00863,code,116,2022-05-17,0,"CrypTFlow [11] is a system that converts TensorFlow (TF) code automatically into secure multi-party computation protocol. The system has three parts: a compiler, from TF code into two and three-party secure computations, an optimized three-party computation protocol for secure interference, and a hardwarebased solution for computation integrity. The most salient characteristic of CrypTFlow is the ability to automatically translate the code into MPC protocol, where the speciﬁc protocol can be easily changed and added. The optimized three-party computational protocol is speciﬁcally targeted for NN computation and speeds up the computation. This approach is similar to the holistic approach of [1]."
2104.00863,data,9,2022-04-21,0,"encrypted data. CoRR, abs/1711.05189, 2017."
2104.00863,database,42,2022-05-17,1,"All tests were performed on the Fashion database of MNIST, which contains a training set of 60,000 and a testing set of 10,000 28x28 images of 10 fashion categories. The task is a multi-class classiﬁcation of a given image."
2104.00863,dataset,133,2022-05-17,0,"Abstract. The structure and weights of Deep Neural Networks (DNN) typically encode and contain very valuable information about the dataset that was used to train the network. One way to protect this information when DNN is published is to perform an interference of the network using secure multi-party computations (MPC). In this paper, we suggest a translation of deep neural networks to polynomials, which are easier to calculate eﬃciently with MPC techniques. We show a way to translate complete networks into a single polynomial and how to calculate the polynomial with an eﬃcient and information-secure MPC algorithm. The calculation is done without intermediate communication between the participating parties, which is beneﬁcial in several cases, as explained in the paper."
2104.11641,"data, data available",123,2022-05-17,0,"a) Graph Neural Networks: Graph Neural Networks (GNNs) [12, 13] have rapidly grown to become a popular research area, providing a highly competitive approach for tasks involving graph data. One line of research focuses on unsupervised models, e.g. VGAE [9] and Graphite [14]. These unsupervised variational models typically aim to use generative modelling of graphs for graph reconstruction, link prediction and clustering. Additionally, supervised models have attracted signiﬁcant attention, such as SCNN [15], ChebyNet [16], GAT [11] and GCN [10], which are widely used in tasks where labelled data is available."
2104.11641,"data, data available",168,2022-05-17,0,"Graph neural networks (GNNs) [1] have been shown to be effective in various graph machine learning tasks, such as link prediction and node classiﬁcation. The rapid growth of online social networks has led to the development of numerous methods for studying social behaviour online. However, many learning tasks on social networks have relied heavily on manual feature extraction. GNNs have provided an alternative to this with their ability to automatically learn representations end-to-end. One such task of interest, which has been shown to be enhanced using GNNs, is social inﬂuence prediction [2]. Data augmentation [3], which increases the amount of data available by creating informative variations of existing data, can improve the performance of machine learning models and has been widely used in many machine learning tasks [4, 5, 6]. In the ﬁelds of computer vision (CV) [7, 3]"
2104.11641,"data, dataset",110,2022-05-17,0,"• The Twitter dataset has been built by collecting Twitter data corresponding to tweets collected before, during and after the announcement of the discovery of the Higgs boson in 2012. The graph is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as whether a user retweets Higgs boson tweets. • The Weibo graph was built from 100 randomly selected users and their followers and followees. The social network is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as retweeting behaviors in the Weibo social network."
2104.11641,"data, dataset",118,2022-05-17,0,"Referring to the statistics of the datasets in Table I, we can see that the performance improvement of AugInf-GAT is particularly clear on the datasets with much fewer edges (Digg) but limited on datasets with more edges (Twitter and this is because the Twitter and Weibo). We believe that Weibo datasets contain enough edges to learn a sufﬁciently comprehensive representation, hence less beneﬁt is gained from the augmentation. We will further investigate the effect of removing edges from graphs as part of data augmentation in future work. Nonetheless, particularly for smaller graphs, we believe our proposed approach of train- and test-time augmentation can provide additional performance."
2104.11641,"data, dataset",151,2022-05-17,0,"b) Data Augmentation: Data augmentation has been shown to be an effective approach in machine learning which expands a dataset by producing transformed copies of data, thereby making the model invariant to these transformations. Data augmentation has been widely used to improve generalizability of machine learning models in natural language processing (NLP) and computer vision (CV). Most of the work on data augmentation has focused on improving augmentation at the training phase, e.g., batch augmentation [5] and UDA [6]. There are also studies that focus on augmentation during the testing phase [17]. However, data augmentation for graph neural networks has only been recently studied, such as SUBG-CON [18] and NodeAug [19]. Particularly, there is no research on test-time augmentation for GNNs. Inﬂuence:"
2104.11641,"data, dataset",165,2022-05-17,0,"Abstract—Data augmentation has been widely used in machine learning for natural language processing and computer vision tasks to improve model performance. However, little research has studied data augmentation on graph neural networks, particularly using augmentation at both train- and test-time. Inspired by the success of augmentation in other domains, we have designed a method for social inﬂuence prediction using graph neural networks with train- and test-time augmentation, which can effectively generate multiple augmented graphs for social networks by utilising a variational graph autoencoder in both scenarios. We have evaluated the performance of our method on predicting user inﬂuence on multiple social network datasets. Our experimental results show that our end-to-end approach, which jointly trains a graph autoencoder and social inﬂuence behaviour classiﬁcation network, can outperform stateof-the-art approaches, demonstrating the effectiveness of trainand test-time augmentation on graph neural networks for social inﬂuence prediction. We observe that this is particularly effective on smaller graphs."
2104.11641,"data, dataset",21,2022-05-17,1,1OAG dataset details: www.openacademic.ai/ 2Digg dataset details: www.isi.edu/ lerman/downloads/digg2009.html/ 3Twitter dataset details: snap.stanford.edu/data/higgs-twitter.html 4Weibo dataset details: www.aminer.cn/inﬂuencelocality
2104.11641,dataset,1,2022-05-17,0,Dataset
2104.11641,dataset,149,2022-05-17,0,"inﬂuence prediction method, AugInf, which incorporates train- and testtime augmentation with a jointly trained graph neural network approach. During training, this method takes into account the losses of both the graph representation learning and downstream social inﬂuence prediction task. We improve performance by applying numerous augmentations to the graphs using variational graph auto-encoders at both train- and testtime. Via an ablation study we show that the jointly trained model obtains more effective latent feature representations by using the joint loss along with both the train- and test-time augmentations. We compare our proposed end-to-end method with the state-of-the-art on several social network datasets. The experimental results show that our proposed method, AugInf-GAT, can improve the performance of predicting social inﬂuence on a number of social networks, and in particular, on the smallest of the social network graphs."
2104.11641,dataset,2,2022-05-17,0,A. Datasets
2104.11641,dataset,225,2022-05-17,0,"In our experiments we apply three augmentations to each graph with the augmentation hyperparameter threshold value set to 0.8 and train for 500 epochs. We will discuss the performance of varying these parameters in a later section. For the GAE component, each of the two hidden layers contain 64 hidden units for Digg and Twitter, and 32 for OAG and Weibo. They are trained with the Adagrad optimizer, using a 0.2 learning rate for OAG and Weibo, 0.05 for Digg and 0.1 for Twitter. Weight decay is set to 0.0005 all datasets except Digg, where it is 0.001. Additionally, we use dropout rate of 0.2. For the GNN prediction module, the ﬁrst and second layers each contain 128 hidden units and the third layer, as the output layer, has two hidden units. There are eight attention heads in each GAT layer, which means each head needs to process 16 hidden units for Digg and Twitter, with four attention heads for OAG and Weibo, which means each head needs to process 32 hidden units. The nonlinear activation function we use for both augmentation and prediction (σ in Eq. 1 and 5) is the exponential linear unit (ELU) [31]."
2104.11641,dataset,23,2022-05-17,0,"TABLE II THE PERFORMANCE OF TWO AUGINF MODELS ON DIFFERENT DATASETS, ALONG WITH THE PERFORMANCE OF THE BASELINES WITHOUT VERTEX FEATURES."
2104.11641,dataset,25,2022-05-17,0,"We evaluate using four datasets across different social network domains, namely OAG1 (Open Academic Graph), Digg2, Twitter3 and Weibo4."
2104.11641,dataset,26,2022-05-17,0,"1) Hyperparameter Analysis: We conduct an hyperparameter analysis on the Digg dataset with the same hyperparameters values mentioned previously, unless stated otherwise."
2104.11641,dataset,35,2022-05-17,0,TABLE I THE STATISTICS OF THE DATASETS. |V | AND |E| ARE THE TOTAL NUMBERS OF NODES AND EDGES OF THE ORIGINAL DATASET RESPECTIVELY AND N IS THE NUMBER OF SUBGRAPHS AFTER PREPROCESSING.
2104.11641,dataset,44,2022-05-17,0,"• The Digg dataset contains the timestamped voting behaviours of users on stories on a social news aggregation website. The edges of Digg graph are deﬁned as following relationships and the inﬂuence actions, which we are predicting, are voting behaviours."
2104.11641,dataset,66,2022-05-17,0,"We have further evaluated AugInf-GAT and AugInf-GCN on the individual components of our approach, to determine the contribution of each component to the overall performance. There are three main components in our approach: (1) train-time augmentation (2) test-time augmentation and (3) the jointly trained model. We have evaluated the following combinations on the Digg dataset:"
2104.11641,dataset,67,2022-05-17,1,"These datasets were used previously by Qiu et al. [2]. Qiu et al. [2] sampled the entire social network into sub-networks with 50 nodes in each sub-network by using a random walk with restart, extracted features for each node and provided a ground-truth for the dataset. The statistics of the three datasets are shown in Table I."
2104.11641,dataset,80,2022-05-17,0,"b) The Threshold for Augmentation: Another parameter we analyze is the threshold that determines which edges may be added. The results of this are shown in Figure 3. When the threshold is set to 0.8 for Digg dataset, our method achieves the highest performance, while on average the number of edges per dataset increases by 2.7%. As we increase the number of added edges, the performance of our method decreases."
2104.11907,"code, publicly available, code available",9,2022-05-17,2,4) The source code is publicly available.
2104.11907,"code, publicly available, code available, github, data, dataset",202,2022-05-17,3,"Abstract—As an essential procedure of data fusion, LiDARcamera calibration is critical for autonomous vehicles and robot navigation. Most calibration methods rely on hand-crafted features and require signiﬁcant amounts of extracted features or speciﬁc calibration targets. With the development of deep learning (DL) techniques, some attempts take advantage of convolutional neural networks (CNNs) to regress the 6 degrees of freedom (DOF) extrinsic parameters. Nevertheless, the performance of these DL-based methods is reported to be worse than the non-DL methods. This paper proposed an online LiDAR-camera extrinsic calibration algorithm that combines the DL and the geometry methods. We deﬁne a two-channel image named calibration ﬂow to illustrate the deviation from the initial projection to the ground truth. EPnP algorithm within the RANdom SAmple Consensus (RANSAC) scheme is applied to estimate the extrinsic parameters with 2D-3D correspondences constructed by the calibration ﬂow. Experiments on KITTI datasets demonstrate that our proposed method is superior to the state-of-the-art methods. Furthermore, we propose a semantic initialization algorithm with the introduction of instance centroids (ICs). The code will be publicly available at https://github.com/LvXudong-HIT/CFNet."
2104.11907,"data, dataset",231,2022-05-17,0,"utilized in the above experiments, se3 error [36] is a more direct evaluation metric. The off-range of the test dataset T3 is a litter smaller than that in dataset T2. The metric MSEE for β-RegNet and RGGNet is smaller in T3 than in T2. Nevertheless, the MSEE is the same for CFNet in these two test datasets, which proves that CFNet is more robust than βRegNet and RGGNet with different off-range settings. In test dataset T4, the performance of β-RegNet degrades heavily, and RGGNet needs to re-train on an additional dataset by adding a small number of data from 2009 10 03 sequence to achieve a good calibration result 0.010 (83.22%). CFNet does not need any additional training dataset and re-train process. The calibration results 0.001 (98.08%) demonstrates that CFNet has good generalization capability. Thus, FNet outperforms all of the learning-based calibration algorithms, RegNet, CalibNet, RGGNet, and even the motion-based calibration method. We can also see that, compared to the motion-based algorithm [30], our proposed method is more generalized, without the requirements of hand-crafted features or the extra IMU sensor. Furthermore, the semantic initialization process does not require motion information which is vital to the hand-eye calibration."
2104.11907,dataset,10,2022-05-17,0,TABLE IV THE COMPARISON RESULTS ON THE T1 TEST DATASET
2104.11907,dataset,101,2022-05-17,0,"We use the odometry recordings from the KITTI dataset, speciﬁcally the left color image and Velodyne point clouds recordings. We use the sequence 06 to 21 for training and validation (29416 frames), sequence 01 to 05 for evaluation/test (4854 frames). The initial calibration off-range ∆T is (±1.5m, ±20◦). To compare with other learning-based (CNNbased) methods, we design four different test datasets on the raw recordings of the KITTI dataset. Each test dataset is independent of the training dataset with the following test name conﬁgurations:"
2104.11907,dataset,111,2022-05-17,0,"In this paper, we presented a novel online LiDAR-camera extrinsic calibration algorithm. To represent the deviation from the initial projection of LiDAR point clouds to the ground truth, we deﬁne an image called calibration ﬂow. Inspired by the optical ﬂow network, we design a deep calibration ﬂow network CFNet. The initial projected points are rectiﬁed to construct accurate 2D-3D by the prediction of CFNet correspondences. EPnP algorithm within the RANSAC scheme is utilized to estimate the extrinsic parameters with iterative reﬁnement. Our experiments demonstrate the superiority of CFNet. The additional experiments on the KITTI360 datasets illustrate the generalization of our method."
2104.11907,dataset,111,2022-05-17,0,"The evaluation results on the KITTI odometry dataset are shown in Table I. It can be seen that in all of these test sequences, the mean translation error Et < 2cm and the mean rotation error ER < 0.13◦. Figure 5 shows two examples of CFNet predictions. We can see that the reference objects in the projected depth image and RGB image align accurately after re-calibration. In all of these test sequences, the calibration error of sequence 01 is the largest. The main reason is that this sequence is collected from a high-way scene, which is not included in the training dataset."
2104.11907,dataset,15,2022-05-17,0,"TABLE V THE COMPARISON RESULTS ON THE T2, T3, AND T4 TEST DATASET"
2104.11907,dataset,174,2022-05-17,1,"We evaluate our approach on the KITTI benchmark dataset including RGB images and Velodyne point clouds [42], recordings collected from different scenes. The timestamps of LiDAR and camera are synchronized, so the images and point clouds in each sequence correspond. To train our proposed calibration ﬂow prediction network CFNet, we need to ensure the input, output, and corresponding calibration ﬂow ground truth. We deﬁne the extrinsic parameters ground truth Tgt between LiDAR and camera as the transformation matrix from the camera coordinate to the LiDAR coordinate. By adding a random variable ∆T , we can obtain the initial calibration parameters Tinit = ∆T · Tgt. The LiDAR point cloud is projected onto the image plane with initial extrinsic parameters Tinit and the camera intrinsic matrix K to generate the LiDAR-image Dinit. The network takes an RGB image I, and the corresponding projected LiDAR-image Dinit as input. The calibration ﬂow ground truth can be provided by Eq. 7."
2104.11907,dataset,21,2022-05-17,0,Fig. 8. Examples of the reconstructed 3D color map on the KITTI360 datasets with the prediciton of CFNet.
2104.11907,dataset,3,2022-05-17,0,A. Dataset Preparation
2104.11907,dataset,34,2022-05-17,0,"The comparison results on the test datasets T2, T3, and T4 shown in Table V also illustrate the superior of the CFNet. Compared to the translation errors and the rotation errors"
2104.11907,dataset,49,2022-05-17,0,Fig. 7. Examples of CFNet predictions with semantic initialization on the KITTI360 datasets.(First Row) 2D-IC. (Second Row) 3D-IC. (Third Row) Semantic Initialization. (Forth Row) CFNet Prediction. (Fifth Row) Ground Truth.
2104.11907,dataset,62,2022-05-17,0,"KITTI360 datasets are utilized as an additional dataset to test our proposed LiDAR-Camera calibration algorithm CFNet. The models trained on KITTI odometry training datasets are regarded as the pre-trained models. We only use sequences 0000, 0002, and 0003 for training and validation during training to ﬁne-tune the CFNet models. Other sequences are selected as test datasets."
2104.11907,dataset,75,2022-05-17,1,"We also test the performance of CFNet on the KITTI360 benchmark dataset. The results are shown in Table VI and Figure 7. Despite re-training on a tiny sub dataset with one epoch, excellent results are obtained in the test sequences. Therefore, when the sensor parameters change, such as the camera focal length or the LiDAR-Camera extrinsic parameters, an excellent prediction model can be obtained with simple re-training."
2104.11907,dataset,9,2022-05-17,0,TABLE VI THE CALIBRATION RESULTS ON KITTI360 TEST DATASET
2104.14114,data,18,2022-04-21,0,"Following data processing, we obtained the time series of the number of publications by each author,"
2104.14114,dataset,10,2022-05-17,0,"in our model, as are the test datasets."
2104.14114,dataset,13,2022-05-17,0,time series and a target of any author in the training dataset.
2104.14114,dataset,16,2022-05-17,0,We extracted parts from the dataset dblp to construct training and test datasets for the experiments
2104.14114,dataset,16,2022-05-17,0,"dataset is comprised of 315,677 publications produced by 441,501 authors, which have been published in"
2104.14114,dataset,16,2022-05-17,0,"their annual number of publications in years 1951–2013, involving 105,806 publications. The test dataset"
2104.14114,dataset,17,2022-05-17,0,"involving 83,302 publications. The test dataset consists of those the same as the training dataset and"
2104.14114,dataset,17,2022-05-17,0,the power-law distribution. The training datasets used in these three experiments are the same as those
2104.14114,dataset,17,2022-05-17,1,"this method was validated by applying it to a high-quality dblp dataset, demonstrating that the proposed"
2104.14114,dataset,18,2022-05-17,0,"Here, we applied the piecewise Poisson model to the dataset. The training dataset consists of the"
2104.14114,dataset,18,2022-05-17,0,"in Section 6. The training dataset consists of 5,741 authors who produced publications at year 2000 and"
2104.14114,dataset,18,2022-05-17,0,"the piecewise Poisson model, the eﬀectiveness of which has been veriﬁed on the dblp dataset for the"
2104.14114,dataset,18,2022-05-17,0,"the piecewise Poisson model. Herein, this model was applied to the dblp dataset and exhibited good"
2104.14114,dataset,18,2022-05-17,0,"with a small training dataset; hence, integrating their advantages to design a mixed architecture can be"
2104.14114,dataset,19,2022-05-17,0,function. RMSprop is chosen to optimize the network. The training dataset used here is the same as
2104.14114,dataset,19,2022-05-17,0,the predicted number of publications. We applied this model to the datasets that are the same as those
2104.14114,dataset,20,2022-05-17,0,"1951–2000, which count for about 94% authors in the dblp dataset. Notably, the hyperparameters in the"
2104.14114,dataset,20,2022-05-17,0,of publications for each author. The dblp dataset is used to test the practicability of our model. The
2104.14114,dataset,20,2022-05-17,0,"on the given dataset. This indicates a direction for improving our model. Second, in the distributions of"
2104.14114,dataset,21,2022-05-17,0,"author s in years 1951–t. The proposed model is trained on every author s in the training dataset, where"
2104.14114,dataset,22,2022-05-17,0,"the training dataset is randomly divided into 4 packets: one of the packets is used as the test set, while"
2104.14114,dataset,23,2022-05-17,0,"The training dataset is the same as that in Section 6, but the test dataset is diﬀerent. It still consists of"
2104.14114,dataset,23,2022-05-17,0,"[4, 5]. Statistical factor analysis seems to be useful for predicting this index, as it helps address datasets"
2104.14114,dataset,26,2022-05-17,0,"In this model, the training dataset is divided into 4 parts, namely, Part I, Part II, Part III, and Part"
2104.14114,dataset,26,2022-05-17,0,"hs(tX−1)} and the target hs(tX ) of any author s in the training dataset. For fourfold cross validation,"
2104.14114,dataset,32,2022-05-17,0,"The dblp computer science bibliographic dataset was applied in this study, which provides open biblio graphic information on most of the journals and conference proceedings in computer science. The quality"
2104.14114,dataset,37,2022-05-17,0,"of this dataset is guaranteed by a range of measures, such as applying several methods of name disam biguation, linking 60,000 manually conﬁrmed external IDs to dblp author bibliographies, and so on. The"
2104.14114,dataset,53,2022-05-17,0,"We used the deep architecture of our model directly to predict the number of publications. The deep archi tecture, the LSTM, was applied to the test dataset, with the time series {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,56,2022-05-17,0,"The training and test datasets used here are the same as those in Section 5. Figs. 15-17 show the com parisons between the GRU and our model on the test dataset, where the input is {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,6,2022-05-17,0,requiring a large training dataset.
2105.00129,code,113,2022-05-17,0,"Problem Statement – Given W , the objective is to produce the code for a workﬂow generator that generates realistic synthetic workﬂow instances. This workﬂow generator takes as input an integer, n ≥ minw∈W (|w|). It outputs a workﬂow w(cid:48) with n(cid:48) ≥ n vertices that is as realistic as possible. n(cid:48) may not be equal to n, because real worfklows for most scientiﬁc applications cannot be feasibly instantiated for arbitray numbers of tasks. Our approach guarantees that n(cid:48) is the smallest feasible number of tasks that is greater than n."
2105.00129,code,122,2022-05-17,0,"generator for any given workﬂow application. WfChef takes as input a set of real workﬂow instances from an application, and outputs the code of a synthetic workﬂow generator for that application. WfChef analyzes the real workﬂow graphs in order to identify subgraphs that represent fundamental task dependency patterns. Based on the identiﬁed subgraphs and on measured task type frequencies in the real workﬂows, WfChef outputs a generator that can generate realistic synthetic workﬂow instances with an arbitrary numbers of tasks. In this work, we evaluate the realism of the synthetic workﬂows generated by our approach, both in terms of workﬂow structure and execution behavior. Speciﬁcally, this work makes the following contributions:"
2105.00129,code,153,2022-05-17,0,"The pseudo-code for REPLICATEPOS is shown in Algorithm 3. It takes as input a desired number of vertices (n), a base workﬂow (base), the list of POs in the base workﬂow (bP Os), and the list of POs in the workﬂow whose number of vertices is the closest to n (cP Os). The intent is to replicate POs in the base workﬂow, picking which pattern to replicate based on the frequency of POs for that pattern in the closest workﬂow. At Line 2, the algorithm ﬁrst sets the generated workﬂow to be the base workﬂow. Lines 4-9 are devoted to computing a probability distribution. More speciﬁcally, for each PO in bP Os, the algorithm computes the probability with which this PO should be replicated. Given a PO in bP Os, nc"
2105.00129,code,22,2022-05-17,0,The pseudo-code for WFCHEFGENERATE is shown in Algorithm 2. It takes as input a recipe (rcp) and a desired
2105.00129,code,267,2022-05-17,0,"The pseudo-code for WFCHEFRECIPE is shown in Algorithm 1. Lines 2 to 16 are devoted to detecting all POs in W . For each w in W , the algorithm visits w’s vertices (Lines 515). An arbitrary unvisited vertex v is visited, and another arbitrary unvisited vertex v(cid:48) is found, if it exists, that has the same type-hash as v (Lines 6-7). If no such v(cid:48) exists then the algorithm visits another vertex v (Line 8). Otherwise, it marks v(cid:48) as visited (Line 9) and computes the set of closest common ancestor and successor vertices for v and v(cid:48) (Lines 10-11). The pseudo-code of the CLOSESTCOMMONANCESTORS and CLOSESTCOMMONDESCENDANTS functions is not shown as they are simple DAG traversals. If v and v(cid:48) do not have at least one common ancestor and one common descendant, then the algorithm visits another vertex v (Line 12). Otherwise, two POs have been found, which are constructed and appended to the list of POs that occur in w at Lines 13 and 14. The pseudocode for function SUBDAG is not shown. It takes as input a vertex in a DAG, a set of ancestors of that vertex, and a set of descendants of that vertex. It returns a DAG that contains all paths from all ancestors to all descendants to traverse v, but"
2105.00129,code,334,2022-05-17,0,"Addressing these challenges requires a solid experimental methodology for evaluating and benchmarking workﬂow algorithms and systems. A fundamental component of this methodology is the availability of sets of representative workﬂow instances. One approach is to infer workﬂow structures from real-world execution logs. We have ourselves followed this approach in previous work [4], [5], resulting in a repository that provides ∼20 workﬂow instances for each of a handful of scientiﬁc applications. These instances have been used by researchers, often for driving simulation experiments designed to evaluate scheduling and resource management algorithms. Real workﬂow instances are by deﬁnition representative of real applications, but they cover only a limited number of scenarios. To overcome this limitation, in previous work we have developed tools for generating synthetic workﬂows by extrapolating the patterns seen in real workﬂow instances. The work in [4] presented a synthetic workﬂow generator for four workﬂow applications, which has been used extensively by researchers3. The method for generating the synthetic workﬂows was ad-hoc and based on expert knowledge and manual inspection of real workﬂow instances. Our more recent generator in [5] improves on the previous generator by using information derived from statistical analysis of execution logs. It was shown to generate more realistic workﬂows than the earlier generator, and in particular to preserve key workﬂow features when generating workﬂows at different scales [5]. The main drawback of these two generators is that implementing the workﬂow generation procedure is labor-intensive. Generators are manually crafted for each application, which not only requires signiﬁcant development effort (several hundreds of lines of code) but also, and more importantly, expert knowledge about the scientiﬁc application semantics that deﬁne workﬂow structures. As a result, this approach is not scalable if synthetic workﬂow instances are to be generated for a large number of scientiﬁc applications."
2105.00129,code,38,2022-05-17,0,"The pseudo-code in this section is designed for clarity. Our actual implementation, described in the next section, is more efﬁcient and avoids all unnecessary re-computations (e.g., the probabilities computed in WFCHEFGENERATE)."
2105.00129,"data, data available",53,2022-05-17,0,"work, for the purpose of evaluating WfChef and of comparing it to previously proposed approaches, we use as training data all available real workﬂow instances with fewer than the desired number of workﬂow tasks. But it may be that using fewer such instances would still lead to good results."
2105.00129,"data, data https, data available",205,2022-05-17,0,"[17] M. Rynge, G. Juve, J. Kinney, J. Good, G. B. Berriman, A. Merrihew, and E. Deelman, “Producing an infrared multiwavelength galactic plane atlas using montage, pegasus and amazon web services,” in 23rd Annual Astronomical Data Analysis Software and Systems (ADASS) Conference, 2013. [Online]. Available: http://pegasus.isi.edu/ publications/2013/rynge-montage-pegasus-amazon-adass2013.pdf [18] E. Deelman, K. Vahi, G. Juve, M. Rynge, S. Callaghan, P. J. Maechling, R. Mayani, W. Chen, R. Ferreira da Silva, M. Livny, and K. Wenger, “Pegasus, a workﬂow management system for science automation,” Future Generation Computer Systems, vol. 46, no. 0, pp. 17–35, 2015. [19] K. Keahey, J. Anderson, Z. Zhen, P. Riteau, P. Ruth, D. Stanzione, M. Cevik, J. Colleran, H. S. Gunawi, C. Hammock et al., “Lessons learned from the chameleon testbed,” in 2020 USENIX Annual Technical Conference (USENIX ATC 20), 2020, pp. 219–233."
2105.00129,"data, publicly available",242,2022-05-17,0,"Our ground truth consists of real Montage and Epigenomics workﬂow instances. These instances are publicly available on the WorkﬂowHub repository [5]. They were obtained based on logs of application executions with the Pegasus Workﬂow Management System [18] on the Chameleon academic cloud testbed [19]. Speciﬁcally, we consider 14 Montage workﬂow instances with between 105 and 9807 tasks, and 25 Epigenomics workﬂow instances with between 75 and 1697 tasks. We generate synthetic workﬂow instances with the same number of tasks as real workﬂow instances, so as to compare synthetic instances to real instances. Both WorkﬂowGenerator and WorkﬂowHub encode application-speciﬁc knowledge to produce synthetic workﬂow instances for any desired number of tasks, n. Instead, WfChef generators rely on training data, i.e., real workﬂow instances. We use a simple “training and testing” approach. That is, for generating a synthetic workﬂow instance with n tasks, we invoke WFCHEFRECIPE with all real workﬂow instances with < n tasks. For instance, say we want to use WfChef to generate an Epigenomics workﬂow with 127 tasks. We have real Epigenomics instances for 75, 121, and 127 tasks. We invoke WFCHEFRECIPE with the 75and 121-tasks instances to generate the recipe. We then invoke WFCHEFGENERATE, passing to it this receipt and asking it to generate a 127-tasks instance."
2105.00129,database,12,2022-05-17,0,1The IEEE Xplore digital database includes 118 articles with both the words
2105.00129,github,12,2022-05-17,0,"[20] “WRENCH Pegasus Simulator,” https://github.com/wrench-project/"
2105.00129,github,16,2022-05-17,0,"[6] “DAGGEN: a synthetic task graph generator,” https://github.com/"
2105.00129,github,6,2022-05-17,2,4https://github.com/wfcommons/workﬂow-schema 5https://github.com/tainagdcoleman/wfchef
2105.00129,"python, github, package",85,2022-05-17,2,"We have implemented our approach in a Python package called wfchef. Speciﬁcally, this package deﬁnes a Recipe class. The constructor for that class takes as input a list of workﬂow instances and implements algorithm WFCHEFRECIPE. The workﬂow instances are provided as ﬁles in the WfCommons JSON format 4. The class has a public method duplicate that implements the WFCHEFGENERATE algorithm, and a private method duplicate_nodes that implements the REPLICATEPOS algorithm. This Python package is available on GitHub5."
2105.00129,"python, package",182,2022-05-17,0,"Approximate Edit Distance (AED) – Given a real workﬂow instance w and a synthetic workﬂow instance w(cid:48), the AED metric is computed as the approximate number of edits (vertex removal, vertex addition, edge removal, and edge addition) necessary so that w = w(cid:48), divided by |w|. Lower values include a higher similarity between w and w(cid:48). We compute this metric via the optimize_graph_edit_distance method from the Python’s NetworkX package. Note that NetworkX also provides a method to compute an exact edit distance, but its complexity is prohibitive for the size of the workﬂow instances we consider. Even though the AED metric can be computed much faster, because it is approximate, we were able to compute it only for workﬂow instances with 865 or fewer tasks for Epigenomics and 750 or fewer tasks for Montage. This is because or RAM footprint issues (despite using a dedicated host with 192 GiB of RAM)."
2105.00775,"code, github, code available",50,2022-04-21,2,"Copyright © 2021 O. Mesnard and L.A. Barba, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Lorena A. Barba (labarba@gwu.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/barbagroup/petibm-rollingpitching.."
2105.00775,"code, open-source, code available, python, github, data",323,2022-05-17,3,"We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results. Admittedly, not everyone has access to an HPC cluster with GPU nodes and with Singularity installed. Lacking those resources, it becomes difficult to fully reproduce our workflow. However, we made the effort to deposit on Zenodo7 the primary data (directly output from our CFD solver) and post-processing scripts needed to reproduce the figures of the present manuscript. Once the Zenodo repository is downloaded, readers should be able to spin up a Docker container and run a Bash script to compute the secondary data and generate the figures, or generate different figures to explore the data in new ways. The Docker images produced and used for this study are stored on DockerHub8, under a basic free subscription. In the event Docker adopts a policy to automatically purge inactive images (those who have not been recently downloaded) from the Hub, the Dockerfiles are version-controlled on the GitHub repository and can be used to rebuild the images. We spent time engineering a transparent and reproducible workflow to produce the artifacts of this replication study. Surely, we cannot assert our steps will be fully reproducible in years from now; the software stack could very well become obsolete with new hardware generations. While the likelihood of the study being reproducible may decrease with the years, the transparency of the steps we took to generate the data shall remain constant."
2105.00775,"code, open-source, code available, python, github, data",342,2022-05-17,3,"production, wake topology, and propulsive performance of a pitching and rolling wing. Although our numerical values do not fully match those from the original study Li and Dong5, we obtain the same trends and thus consider this replication attempt to be successful. A CFD solver typically outputs the solution of primary variables. For example, PetIBM outputs the pressure and velocity fields, as well as the body forces. We often use multiple post-processing scripts to generate the final data and figures reported and analyzed in the manuscript; it involves computing secondary data, such as the vorticity field, the aerodynamic power and forces. If the code is not made available, readers cannot inspect what has been done to produce these data; bugs introduced in these post-processing steps would go undetected. If no code is available, we cannot explain discrepancies observed between our replication and the original study. As Donoho and coworkers14 once said: “The only way we’d ever get to the bottom of such a discrepancy is if we both worked reproducibly and studied detailed differences between code and data.” We made our best efforts to ensure that our replication study is reproducible. Our computational application makes use of fully open-source tools, and we created a GitHub repository6 for this study. The repository contains the source code of the PetIBM application, as well as all input files of the simulations reported here, and pre- and postprocessing Python scripts. We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results."
2105.00775,"code, open-source, github, data, open-source code",327,2022-05-17,2,"Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way. We have reimplemented the three-dimensional rolling and pitching kinematics in an open-source code shared on GitHub1 and prepared extensive reproducibility packages for all results."
2105.00775,"code, open-source, github, data, open-source code",333,2022-05-17,2,"of this field are the Navier-Stokes equations, which are notoriously difficult to solve numerically, with computational experiments often taking a long time, even on parallel compute clusters. Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way."
2105.00775,"code, publicly available, open-source, github, data, data repository",335,2022-05-17,2,"The final product of the original study is a published manuscript in the journal Physics of Fluids. Although the manuscript is well detailed, the code and input data used to produce the computational results were not made publicly available by the authors. In that regard, we consider the study to not be reproducible. Thus, we aim to replicate the scientific findings claimed in the original study with our own research software stack and deliver reproducible results. PetIBM10 is developed in the open under the permissive (non-copyleft) 3-Clause BSD license, version-controlled with Git, and hosted on a public GitHub repository.2 Each major release of the software is archived on the data repository Zenodo. Our implementation of the three-dimensional rolling and pitching wing, which relies on PetIBM, is also open source and available on GitHub3 under the same license. The repository contains all input data and processing scripts that were used to produce the computational results reported in the next section. This allows anyone to inspect the code, to verify the steps that were taken to produce computational results, and to modify and re-use it for other applications. The repository also includes README files to guide readers that may be interested in re-running the analysis. Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others."
2105.00775,"code, publicly available, open-source, github, data, data repository",337,2022-05-17,2,"Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others. We have already used Docker containers in the past to create a reproducible workflow for scientific applications on the public cloud provider Microsoft Azure.13 Here, we aim to adopt a similar workflow on our local HPC cluster. Early in this replication study, we hit a snag: Docker is not available to users on Pegasus. Indeed, Docker is not available at most HPC centers for security reasons. Submitting container-based jobs with Docker implies running a Docker daemon (a background process) that requires root privileges that users do not and should not have on shared production clusters. Thus, we decided to leverage the Singularity container technology to conduct the replication study on Pegasus. Singularity is more recent than Docker, was designed from the ground up to prevent escalation of user privileges, and is compatible with Docker images. Our reproducible workflow starts with creating a Docker image that installs PetIBM and its applications, as well as all their dependencies. We then push the image to a public registry on DockerHub.4 Anyone interested in using the application code can now pull the image from the registry and spin up a Docker container to get a faithfully reproduced computational environment. Next, we use the cloud service Singularity Hub to build a"
2105.00775,"data, code",185,2022-05-17,0,"Abstract This article reports on a full replication study in computational ﬂuid dynamics, using an immersed boundary method to obtain the ﬂow around a pitching and rolling elliptical wing. As in the original study, the computational experiments investigate the wake topology and aerodynamic forces, looking at the effect of: Reynolds number (100–400), Strouhal number (0.4–1.2), aspect ratio, and rolling/pitching phase difference. We also include a grid-independence study (from 5 to 72 million grid cells). The trends in aerodynamic performance and the characteristics of the wake topology were replicated, despite some differences in results. We declare the replication successful, and make fully available all the digital artifacts and workﬂow deﬁnitions, including software build recipes and container images, as well as secondary data and post-processing code. Run times for each computational experiment were between 8.1 and 13.8 hours to complete 5 ﬂapping cycles, using two compute nodes with dual 20-core 3.7GHz Intel Xeon Gold 6148 CPUs and two NVIDIA V100 GPU devices each."
2105.00775,"data, code",24,2022-05-17,0,"Reproducibility is obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis."
2105.00775,"data, code",333,2022-05-17,0,"θ(t) = −Aθ cos(2πf t + ψ) where Aθ is the pitching amplitude and ψ is the phase-difference angle between the pitching and rolling motions. For the present replication study, we use the same wing kinematics and numerically solve the three-dimensional Navier-Stokes equations (velocity/pressure formulation) for , where an incompressible viscous flow. The Reynolds number is defined as Re = U∞c U∞ is the incoming freestream speed and ν is the kinematic viscosity. The convective and diffusion terms of the partial differential equations are time-integrated using second-order accurate Adams-Bashforth and Crank-Nicolson methods, respectively. We enforce a Dirichlet condition (streamwise velocity set to the freestream speed U∞) on all boundaries, except at the outlet where we use a convective boundary condition (to carry vortical structures outside the computational domain). Our code base, PetIBM, solves the incompressible Navier-Stokes equations using a projection method, seen as an approximate block-LU decomposition of the fully discretized equations.6 To compute the flow around a moving object (e.g., a pitching-rolling wing), we use an immersed boundary technique. The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved)."
2105.00775,"data, code",337,2022-05-17,0,"The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved). Our code base includes several implementations of the immersed-boundary projection method;8 we use the formulation of Li et al.9 for all computations of the present study. These methods fall into the category of diffuse-interface methods, as the discrete delta function smears the solution over a few grid cells around the boundary. Each time step, we successively solve three linear systems for an intermediate velocity field, the Lagrangian forces, and the pressure field. The system for the velocity is solved using a stabilized bi-conjugate gradient method (from the PETSc library) with a Jacobi preconditioner and a convergence criterion based on the absolute L2-norm of the residual set to atol = 10−6. We solve the system for the Lagrangian forces with a direct solver (SuperLU_dist library). The pressure Poisson system is solved with a conjugate-gradient method using a classical algebraic multigrid technique (via the NVIDIA AmgX library); here, too, convergence is reached when the absolute L2-norm of the residual is 10−6. To quantify aerodynamic performance of the wing, we report the thrust, lift, and spanwise force coefficients, defined as"
2105.00775,"data, code, data available, code available",122,2022-05-17,0,"The minimum requirement for computational research to be reproducible is to make code and data available to others. Peng2 introduced the concept of a reproducibility spectrum, in which reproducible research is a “minimum standard for judging scientific claims when full independent replication of a study is not possible” or not available. The two extremes on the reproducibility spectrum are “not reproducible” (when a published manuscript is the sole deliverable from a study) and “fully replicated” (the gold standard for a study). This paper addresses reproducibility and replicability in computational fluid dynamics, a mature field and one of the oldest branches of computational science. At the center"
2105.00775,github,10,2022-04-21,2,2PetIBM: github.com/barbagroup/petibm 3PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 4DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,10,2022-04-21,2,6PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 7Repro-packs: doi.org/10.5281/zenodo.4732946 8DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,3,2022-05-17,0,1PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching
2105.00775,open-source,216,2022-05-17,0,"pitching-rolling plates.” In: Physics of Fluids 28.7 (2016), p. 071901. J. B. Perot. “An analysis of the fractional step method.” In: Journal of Computational Physics 108.1 (1993), pp. 51–58. R. Mittal, H. Dong, M. Bozkurttas, F. Najjar, A. Vargas, and A. Von Loebbecke. “A versatile sharp interface immersed boundary method for incompressible ﬂows with complex boundaries.” In: Journal of computational physics 227.10 (2008), pp. 4825–4852. K. Taira and T. Colonius. “The immersed boundary method: a projection approach.” In: Journal of Computational Physics 225.2 (2007), pp. 2118–2137. R.-Y. Li, C.-M. Xie, W.-X. Huang, and C.-X. Xu. “An efﬁcient immersed boundary projection method for ﬂow over complex/moving boundaries.” In: Computers & Fluids 140 (2016), pp. 122–135. P.-Y. Chuang, O. Mesnard, A. Krishnan, and L. A. Barba. “PetIBM: toolbox and applications of the immersedboundary method on distributed-memory architectures.” In: The Journal of Open Source Software 3.25 (May 2018), p. 558."
2105.00775,"python, github",316,2022-05-17,2,"In the original study, the authors reported the results of a grid-independence study to justify the spatial and temporal grid resolutions used for the parametric study. They compared force coefficients, profiles of the velocity components, profiles of the fluctuating kinetic energy, and distances between vortical structures in the near wake, obtained with different grid resolutions. Here, we also report the results of our grid-independence study before moving on to the results of the parametric study. We use the same domain size as in the original study: 30c × 25c × 25c (where c is the chord length of the wing). The root of the wing (around which the plate undergoes the rolling/pitching motion) is located at the center of the computational domain. We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository)."
2105.00775,"python, github",331,2022-05-17,2,"We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository). In the present study, we model the wing with a flat elliptical surface, discretized with Lagrangian markers uniformly distributed on its surface (with a similar resolution as the grid-spacing size of the background Eulerian grid). As in the original study, we consider the case of a circular wing (AR = 1.27) with Reynolds number Re = 200, Strouhal number St = 0.6, and phase-difference angle ψ = 90o, to assess independence in the numerical results. We investigated the effect of the grid-spacing size, the time-step size, and the convergence criterion of the iterative solvers, on the numerical solution. To assess the effect of the grid-spacing size ∆x in the vicinity of the wing on the solution, we computed five flapping cycles on three grids: coarse (∆x = 0.03c), nominal (∆x ="
2105.09146,"data, dataset",113,2022-05-17,0,"The noisy observation data was used to train two models for each noise level: a HNN using the architecture from Section 3 and a baseline SINDy model. After training, each HNN model was used to generate 5000 coordinate predictions. These predictions were then used to ﬁt a SINDy+HNN model for each noise level. In total three models were returned for each noise level: a SINDy Baseline, a HNN, and a SINDy+HNN model. The SINDy+HNN approach is illustrated in Figure 8. For this example, the proposed SINDy+HNN approach to noise regulation is data-efﬁcient requiring only a small data set of 5k observations."
2105.09146,"data, dataset",117,2022-05-17,0,"Neural network performance is highly dependent on the quantity and quality of the available training data. Experimental datasets are unavoidably noisy and scarce in quantity. This can limit a network’s ability to learn features and generalize. For neural networks modeling physical behaviors, this can be particularly problematic as it can lead to physically inconsistent predictions that violate governing laws. The black-box nature of the learned features and relations makes it challenging to assess if the network is accurately learning the underlying physics, constraints, and parameters from data. These issues of network’s interpretability and generalization ability limit their utility to model and simulate physical systems."
2105.09146,"data, dataset",174,2022-05-17,0,"Neural networks often require large amounts of data to generalize and can be ill-suited for modeling small and noisy experimental datasets. Standard network architectures trained on scarce and noisy data will return predictions that violate the underlying physics. In this paper, we present methods for embedding even–odd symmetries and conservation laws in neural networks and propose novel extensions and use cases for physical constraint embedded neural networks. We design an even–odd decomposition architecture for disentangling a neural network parameterized function into its even and odd components and demonstrate that it can accurately infer symmetries without prior knowledge. We highlight the noise resilient properties of physical constraint embedded neural networks and demonstrate their utility as physics-informed noise regulators. Here we employed a conservation of energy constraint embedded network as a physics-informed noise regulator for a symbolic regression task. We showed that our approach returns a symbolic representation of the neural network parameterized function that aligns well with the underlying physics while outperforming a baseline symbolic regression approach."
2105.09146,dataset,163,2022-05-17,0,"Implementation. A standard MLP and an MLP with an even–odd hub layer were implemented in PyTorch following the architecture described in [15]. The standard MLP model consisted of two fully connected layers of 5 neurons each, with sigmoid activations and one fully connected output layer. The MLP with the even–odd hub layer replaced the last hidden layer with an even–odd hub neuron. As in [15] we applied the models to a dataset generated from even (cosine) or odd (sine) functions with normally distributed noise N ∼ (µ = 0, σ = 0.2). The models were trained to minimize the mean squared error (MSE) of x(t) and ˆx(t). This loss function was then modiﬁed to include the symmetry metric term and an additional standard MLP model was trained. Figure 1 compares results for even symmetry function."
2105.09146,"open-source, package, python, data open-source , data",117,2022-05-17,0,"[31] Rick Chartrand. Numerical differentiation of noisy, nonsmooth data. ISRN Applied Mathematics, 2011, 2011. [32] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445–22451, 2019. [33] Brian de Silva, Kathleen Champion, Markus Quade, Jean-Christophe Loiseau, J. Kutz, and Steven Brunton. Pysindy: A python package for the sparse identiﬁcation of nonlinear dynamical systems from data. Journal of Open Source Software, 5(49):2104, 2020."
2105.09146,python,142,2022-05-17,0,"[24] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020."
2105.09146,python,149,2022-05-17,0,"All SINDy models in this work were implemented using the PySINDy python library [33]. For each SINDy implementation, the same feature libraries were provided: a polynomial library containing terms up to the 2nd degree and a Fourier library limited to 1 frequency and containing both sine and cosine terms. The only argument that varied between the SINDy models was the threshold parameter provided to the STLSQ optimizer. Varying this parameter was necessary to ensure model performance and prevent the complete dropout of the terms in one model. For each model, the threshold parameter was selected as the value that dropped out the largest number of terms while maintaining the quality of the ﬁt. Each ﬁt was then integrated with an ODE solver to return the coordinate predictions. The same solver setup as in section 3 was used."
2105.09146,python,220,2022-05-17,0,"Implementation. As in [11] we implemented two networks in PyTorch [22] a Baseline MLP Figure 4(a) and a HNN Figure 4(b). The Baseline MLP directly outputs the time derivatives and served as a point of comparison for the constraint embedded HNN network. The Baseline MLP consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 2 corresponding to ( dq dt ). The HNN consisted of two equation parameterizations, Equation 8 and 9. The former parameterized the Hamiltonian and consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 1 corresponding to Hθ. The latter parameterized the time evolutions and used the python Autograd library [23] to return the partials of Hθ with respect to q and p, ( ∂H ∂q ). After training an ODE solver was used to integrate both the Baseline and HNN models to generate coordinate predictions. The SciPy [24] solve_ivp function was used to integrate the network with a tolerance of 1e-12 and a 4th order Runge-Kutta method was used for the solver [25]."
2105.09869,"code, github, code available",11,2022-04-21,2,1The MATLAB code is available at https://github.com/amasoumi60/Robust Dynamic-Mode-Decomposition.
2105.09869,"data, data available",141,2022-05-17,0,"A wealth of data science methods have been developed by researchers and made available to practitioners. Dynamic mode decomposition (DMD) stands out because of its connection with the Koopman operator theory [3], which reconciles data analysis and the mathematical knowledge of dynamical systems; the reader is referred to [4], [5] for more details. Since the publication of the paper authored by Schmid and Sesterhenn [6], [7], DMD has become the mainstream method for data-driven modeling of dynamical systems, mainly applied to ﬂuid mechanics [3], electric power grids [8], neuroscience [9], ﬁnance [10], climate science [11], and transportation [12], to name a few."
2105.09869,"data, data available",49,2022-05-17,0,"of human activity is a crucial driver for the research and development of data science methods [1], [2]. This fact especially applies to complex dynamical systems for which ﬁrst-principles models are challenging to obtain while a large amount of data are available."
2105.09869,"data, dataset",131,2022-05-17,0,"Abstract—This paper develops a robust dynamic mode decomposition (RDMD) method endowed with statistical and numerical robustness. Statistical robustness ensures estimation efﬁciency at the Gaussian and non-Gaussian probability distributions, including heavy-tailed distributions. The proposed RDMD is statistically robust because the outliers in the data set are ﬂagged via projection statistics and suppressed using a Schweppe-type Huber generalized maximum-likelihood estimator that minimizes a convex Huber cost function. The latter is solved using the iteratively reweighted least-squares algorithm that is known to exhibit a better convergence property and numerical stability than the Newton algorithms. Several numerical simulations using canonical models of dynamical systems demonstrate the excellent performance of the proposed RDMD method. The results reveal it outperforms several other methods proposed in the that literature."
2105.09869,"data, dataset",192,2022-05-17,0,"It turns out that solving the sensitivity of DMD to deviations from the assumptions made about the data set is a challenging task [20] due to the vulnerability of the least-squares estimator to non-Gaussian noise and outliers, which is a great concern to practitioners. This fact motivated several independent investigations to assess the accuracy of the DMD in capturing the underlying system dynamics directly from the data set [21]–[24]. For instance, Dawson et al. [25] and Hemati et al. [26] address, respectively, the bias introduced by Gaussian noise and the bias resultant from asymmetrically processing snapshots. In Section IV of this paper, numerical experiments conﬁrm that the DMD variant proposed by Hemati et al. [26] has excellent performance in the presence of Gaussian noise and has good performance in the presence of non-Gaussian but symmetrically distributed noise. This is achieved thanks to a reformulation of the DMD using a total least-squares estimator [27]; however, this estimator is still vulnerable to outliers."
2105.09869,"data, dataset",224,2022-05-17,0,"The vulnerability of the least-squares estimator to outliers is not directly solvable without data preprocessing; therefore, Askham et al. [28] reformulate DMD as an optimization problem and make use of a least trimmed squares (LTS) estimator, speciﬁcally the trimmed M-estimator introduced by Rousseeuw [29]. To the best of the authors’ knowledge, [28] is the only formulation of DMD that makes use of a robust estimator. In particular, the LTS estimator has a high breakdown point [29]—that is, this estimator is very robust from a statistical standpoint; however, the formulation in [28] lacks a mechanism to identify outliers. Indeed, identifying outliers without access to a system model is challenging but necessary in DMD. The formulation in [28] circumvents this challenge by making a blanket assumption that the time-series data can be represented “by the outer product of a matrix of exponentials, representing Fourier-like time dynamics, and a matrix of coefﬁcients, representing spatial structures.” Consequently, nonexponential dynamics in the data set are, therefore, classiﬁed as outliers. This fact precludes the application of the method proposed in [28] to dynamical systems that present nonexponential dynamics."
2105.09869,"data, dataset",267,2022-05-17,0,"In the classic literature in robust statistics [16], [17], one deﬁnes robustness as insensitivity to deviations from the assumptions. In this sense, the least-squares estimator is not robust. Two cases of deviations from the assumptions are of particular concern. The ﬁrst case arises when the probability distribution of the observations is not Gaussian. The leastsquares estimator quickly loses its statistical efﬁciency (that is, accuracy) when the tails of the probability distribution of the observations become slightly thicker than the Gaussian distribution or when the probability distribution of the observations becomes slightly asymmetric. The second case arises when the probability distribution of the majority of the observations is Gaussian except for a few observations, which may take arbitrary values. In this respect, one deﬁnes an outlier as a data point that violates the underlying assumptions—in other words, it is a data point that is distant from the majority of the point cloud [18]. The least-squares estimator produces strongly biased results in the presence of a single outlier in the data set [19]. Both cases of deviations from the assumptions often occur in practice—for example, when the probability distribution of the observations is not known while being assumed to be Gaussian or when outliers arise because of instrumentation and communications errors or a poor experimental setup. This fact precludes the DMD from being applied to practical settings, especially for control purposes where pre-cleaning the data set is not an option."
2105.09869,"data, dataset",59,2022-05-17,0,"Deﬁnition 2 (Median absolute deviation from median in the case of a univariate data set). Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A very robust estimator of scale [38] is the median absolute deviation from the median, which is deﬁned as"
2105.09869,"data, dataset",86,2022-05-17,0,"Deﬁnition 3 (Median absolute deviation from the median in the case of a multivariate data set). Let a multivariate data set, P ⊆ Rm, be {p1, ..., pN }. The median absolute deviation from the median is deﬁned as (cid:0) (cid:12) (cid:12)pT for k, j = {1, 2, ..., N }, where v is the direction to which the data points are projected."
2105.09869,"data, dataset",87,2022-05-17,0,"Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A measure of the distance between a data point, pk ∈ P, and the center of the data cloud is given by pk−(cid:98)(cid:96) , where (cid:98)(cid:96) denotes an estimator of location, and (cid:98)s denotes an estimator of scale. A classic measure of distance in the univariate case is provided by"
2105.09869,"data, dataset, data available",116,2022-05-17,0,"and the results are depicted in Fig. 7. Note that for the data collected from nonlinearly evolving signals, the approximated DMD modes are reﬂecting the behavior of the most dominant Koopman modes. As shown in Fig. 7, when there is no outlier contamination within the data set, all considered DMD methods calculate approximately the same dominant eigenvalues, and a good response of x2 is reconstructed; however, when there are some outliers among the data set, only N-RDMD can capture the same eigenvalues as found for the outlier-free data. In other words, the process of ﬁnding eigenvalues has been made robust against outliers."
2105.09869,dataset,56,2022-05-17,0,"[26] M. S. Hemati, C. W. Rowley, E. A. Deem, and L. N. Cattafesta, “De-biasing the dynamic mode decomposition for applied Koopman spectral analysis of noisy datasets,” Theoretical and Computational Fluid Dynamics, vol. 31, no. 4, pp. 349–368, 2017."
2105.10037,code,2,2022-05-17,0,A. Pseudo-code
2105.10037,code,68,2022-05-17,0,"∈P Next, the learnt inverse model is used to augment ˆ with agent speciﬁc actions. Finally, these action augmented trajectories are used to learn the ﬁnal policy πTA via behavioral cloning. Note that our correspondence learning framework is agnostic to the imitation from observation algorithm used for learning the agent policy. The pseudo-code for training our framework is presented in Appendix A."
2105.10037,"code, github, code available",73,2022-04-21,2,"In this section, we analyze the efﬁcacy of our proposed method on the xDIO task. We adopt MuJoCo (Todorov et al., 2012) as the experimental test-bed and evaluate on several cross-domain tasks, along with a thorough ablation study of different modules in our overall framework. Implementation details are presented in Appendix B. Code and videos are available at: https://driptarc.github. io/xdio.html."
2105.10037,"data, dataset provided",131,2022-05-17,0,"In this paper, we present a novel framework to tackle the xDIO task by learning a state-map across domains using both local and global alignment. Local alignment is performed via transition distribution matching and cycleconsistency in both the state and latent space, while global alignment is enforced via the idea of temporal position preservation. While previous approaches rely on paired data and expert actions, we provide a general framework that can learn the mapping from unpaired, unaligned demonstrations without expert actions. We demonstrate the efﬁcacy of our approach on multiple cross-domain tasks encompassing dynamics, viewpoint and morphological mismatch. Our future work will concentrate on extending our method for learning correspondence using random trajectories, thus mitigating the need for proxy tasks."
2105.10037,dataset,115,2022-05-17,0,"The objective of xDIO is to learn an optimal policy πTA in in the agent domain, given state-only demonstrations the expert domain. In this paper, we propose to ﬁrst learn a transformation ψ : SE → SA between the domains and then leverage ψ to imitate from the expert demonstrations. Following prior work (Gupta et al., 2017; Liu et al., 2018; Kim et al., 2020), we assume access to a dataset consisting of expert-agent trajectories for M different proxy tasks: M j=1. Proxy tasks encompass simple ) D } primitive skills in both domains and are different from the inference task"
2105.10037,dataset,147,2022-05-17,0,"We use the learned ψ to map the states in the inference task expert demonstrations to the agent domain. Given DM the set of transferred state-only demonstrations ˆ , we can use any imitation from observation algorithm to learn the ﬁnal policy. In this work, we follow the Behavioral Cloning from Observation (BCO) approach proposed in (Torabi et al., 2018). BCO entails learning an inverse dySA × SA → AA to infer missing acnamics model tion information. First, we collect a dataset of state-action A, st+1 triplets by random exploration. The A ) } inverse model is subsequently estimated by Maximum Like. lihood Estimation (MLE) of the observed transitions in P Assuming a Gaussian distribution over actions, this reduces to minimizing an (cid:96)2 loss as follows,"
2105.10037,dataset,63,2022-05-17,0,"Given the alignment dataset D containing trajectories from the M proxy tasks, we ﬁrst pre-train the temporal position M j=1 using Equation 6. This is folestimators lowed by adversarial training of the state maps ψ, φ, where we use separate discriminators on the state space and latent space for each proxy task. The full objective is then:"
2105.10702,"data, data available",114,2022-05-17,0,"Many of these machine learning algorithms are supervised learning approaches that require large amounts of annotated image data for training. Gathering suitable data is especially challenging in the medical domain as it is incredibly time consuming for radiologists to generate ground-truths of the standard and volume required for training a predictive model. An alternative approach is to use past clinical images and corresponding radiological reports available through a hospital’s picture archiving and communication system (PACS); the advantage being that, although this data is largely unstructured (free text), it is available to us in high volumes and removes the need for manual annotation."
2105.10702,database,14,2022-05-17,0,"hierarchical image database. In CVPR, pages 248–255. IEEE, 2009."
2105.10702,database,55,2022-05-17,0,"[13] Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, and Ronald M Summers. Interleaved Text / Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation. Journal of Machine Learning Research (JMLR), 17:1–31, 2016. doi: 10.1109/CVPR.2015.7298712."
2105.10702,dataset,189,2022-05-17,0,"The knee X-ray dataset has been extracted from the PAC system of St Thomas Hospital (part of Guys and St Thomas NHS Foundation Trust) and has been fully anonymised to remove sensitive patient information. It comprises a total of 330 knee X-ray exams collected over the years 2015 and 2016. Each exam consists of a textual report and one or more X-ray images (left/right knees or both, taken from different views: anteroposterior (AP), lateral (L) and skyline (S), and different positions: weight-bearing (WB) and non-weight-bearing(nonWB)). The most common exam consists of both AP and L views of left and right knees separately, making up 42% of total exams. The reports vary in length between 2 and 145 words, with an average of 30 and standard deviation of 18.7; and between 1 and 16 sentences with an average of 2.7 per report. The X-ray images vary in sizes between 420 × 650 × 3 and 3056 × 3056 × 3."
2105.10702,dataset,2,2022-05-17,0,3 Dataset
2105.10702,dataset,93,2022-05-17,0,"We adopt the GoogLeNet [18] CNN architecture, pre-trained on the ImageNet dataset [19], and extract image features of each X-ray image view (V1 - VK) from the last spatial average pooling layer (R1024). The maximum value of each feature is aggregated across the exam images to create a ﬁxed-size input to the RNN of dimension R1024, which is then passed through a fully connected layer in order to reduce the dimension to R256, equal to the RNN input size."
2105.12306,"data, dataset",33,2022-05-17,0,Table 2: Statistics of the used datasets. All the training data are merged to train the REALISE model. The test sets are used separately to evaluate the model performance.
2105.12306,"data, dataset",69,2022-05-17,1,"in 2013, 2014 and 2015 (denoted as SIGHAN13, SIGHAN14 and SIGHAN15). Table 2 shows the data statistics. Originally, the SIGHAN datasets are in the Traditional Chinese. Following previous works (Wang et al., 2019; Cheng et al., 2020; Zhang et al., 2020), we convert them to the Simpliﬁed Chinese using the OpenCC tool2."
2105.12306,dataset,1,2022-05-17,0,Dataset
2105.12306,dataset,14,2022-05-17,0,Table 6: Ablation results of the REALISE model on each SIGHAN dataset.
2105.12306,dataset,28,2022-05-17,0,"Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018. Being negative but constructively: Lessons learnt from creIn ating better visual question answering datasets."
2105.12306,dataset,284,2022-05-17,0,"""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!! ""!#OITAnd the Yankees’ acepitcherThe number ofbabiesbornto womencontinues todecline(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:18415)(cid:12787)(cid:12713)!!!""!#OIT(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:16855)(cid:20043)(cid:7529)(cid:15919)!!! ""!#OITDataset"
2105.12306,dataset,383,2022-05-17,0,"(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10449)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)1.001.001.001.001.000.971.000.991.001.001.001.001.000.200.200.140.340.180.130.370.250.190.310.250.230.170.090.040.130.120.080.060.520.090.040.040.060.080.05(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10450)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)!!! ""!#OIIplan to watchamoviewith mygirlfriendT(cid:10356)(cid:24485)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)0.991.001.001.000.991.000.991.000.991.000.990.260.230.240.170.370.340.210.210.210.300.210.220.620.240.070.180.230.090.060.040.050.05(cid:10356)(cid:8148)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)The affair  also  happened  from  this  point!!!""!#OIT(cid:18619)(cid:7870)(cid:11870)(cid:12065)1.001.001.001.000.600.340.370.520.100.060.090.06(cid:8711)(cid:7870)(cid:11870)(cid:8402)!!! ""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!!"
2105.12306,github,10,2022-05-17,2,1Code and model are available at https://github.
2105.12306,github,3,2022-05-17,0,2https://github.com/BYVoid/OpenCC
2105.12309,"code, github, code available",55,2022-04-21,2,the EKF Algorithm for RexROV becomes as shown in Algorithm (2). The algorithm has been applied to several test courses from [23] in the next section and their results are discussed. The source code for implementation of the EKF algorithm shown below is available in https:// gitlab.engr.illinois.edu/auvsl/submarine
2105.12309,open-source,108,2022-05-17,0,"4x4 state matrix thereby reducing computational cost.The 4 DOF model is proposed in Ref. [19] and was successfully used by Ref. [20] in control development. Since motion predictions are being augmented with sensor readings, it is expected from this approach to work effectively with the problem proposed in this paper. A Robotic Operating System (ROS) Gazebo-based open-source marine vehicle simulator was used in this research [14]. The simulator incorporates the dynamic model by Fossen [14] with a vehicle model based on parameters derived from the works by Berg [17]."
2105.12309,open-source,238,2022-05-17,0,"Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) are used for a wide variety of missions related to exploration and scientiﬁc research. Successful navigation by these systems requires a good localization system. Kalman ﬁlter based localization techniques have been prevalent since the early 1960s and extensive research has been carried out using them, both It has been found that the in development and in design. use of a dynamic model (instead of a kinematic model) in the Kalman ﬁlter can lead to more accurate predictions, as the dynamic model takes the forces acting on the AUV into account. Presented in this paper is a motion-predictive extended Kalman ﬁlter (EKF) for AUVs using a simpliﬁed dynamic model. The dynamic model is derived ﬁrst and then it was simpliﬁed for a RexROV, a type of submarine vehicle used in simple underwater exploration, inspection of subsea structures, pipelines and shipwrecks. The ﬁlter was implemented with a simulated vehicle in an open-source marine vehicle simulator called UUV Simulator and the results were compared with the ground truth. The results show good prediction accuracy for the dynamic ﬁlter, though improvements are needed before the EKF can be used on real time. Some perspective and discussion on practical implementation is presented to show the next steps needed for this concept."
2105.12309,open-source,30,2022-05-17,0,"Experiments were conducted on open source simulation software Gazebo 7. For underwater scenario and sensors simulation, UUVSIM [14] is used in parallel with ROS Kinetic."
2105.12309,package,62,2022-05-17,0,"[14] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “UUV simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in Proceedings of OCEANS 2016 MTS/IEEE Monterey, 19-23 September, 2016, Monterey, CA, USA., IEEE, sep 2016."
2105.15074,"data, data available",212,2022-05-17,0,"This research aimed to evaluate the performance of a classiﬁcation algorithm based on ANN of children with FASD and without the syndrome, using the result of psychometric, DTI, and saccade tests and comparing the accuracy of the model with the SVMR developed by Zhang et al. et al. (2019). Our results suggest that ANN can make a preliminary diagnosis of pathologies reasonably. When numerical data are available. Most of the studies carried out to classify FASD patients using machines learning, study brain images or natural language Fang et al. (2006); Wozniak and Muetzel (2011); Suttie et al. (2018). Only the study conducted by Zhang et al. et al. (2019) uses numerical data from psychometric and eye movement studies and is freely available. Then, the results obtained with the use of ANN were compared with the results obtained by these authors in their study (Fig. 2). The purpose of this study was to evaluate, whether ANN can be used to classify patients with FASD from data obtained non-invasively and that do not require many studies for a preliminary diagnosis."
2105.15074,"data, data available",28,2022-05-17,0,"In this section, we analyzed the data available and evaluated the performance of the ANN method and the conﬁguration of the neural network with reasonable accuracy."
2105.15074,"data, dataset",110,2022-05-17,0,"The data used for testing included psychometric tests. Associated with analyzing social behavior, memory activities, language delay and all altered behavioral factors in FASD children. Also, the data set included other developmental diseases, making the classiﬁcation process more difﬁcult. This paper used two models of dense networks using this type of data. The ﬁrst model archived an accuracy of 75.55%. However, the model did not show signiﬁcant improvements, despite the variation in the layers and neurons in the layers. In the second model, a feature layer was added, achieving an accuracy of 88.46%."
2105.15074,"data, dataset",129,2022-05-17,0,"Once the network conﬁguration was chosen, training and testing or validation behavior are shown in the model precision and loss functions. The loss function with a high result indicates that the neural network has a poor performance and a low result, that it is doing a good job. Fig. 3a shows the accuracy for each of the data set. In terms of the number of that it was aspects found and related to the number of individuals evaluated. We attempted to ﬁnd a model with no signiﬁcant difference, between the labeled data and the prediction. Nonetheless, with the implemented conﬁguration, the training data has increased accuracy. In addition, the validation data are also increasing accuracy."
2105.15074,"data, dataset",147,2022-05-17,0,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model."
2105.15074,"data, dataset",159,2022-05-17,0,"Prenatal alcohol exposure causes brain damage. And the neuropsychological consequences are deep. These deﬁcits in cognitive functions include: difﬁculties in planning, organization, and attention, consequential learning failures, and memory deﬁciencies. Some have speech and/or language difﬁculties, visuospatial functions and spatial memory, that are increased by exposure to prenatal alcohol (Glass et al., 2017; Green, 2007; Mohammad et al., 2020). Those characteristics could be seen in the task and the evaluation. It is a vital diagnosis based on the abnormalities founded and not only in the traditional facial characteristic (Wozniak et al., 2009). The novelty of the Zhang et al. (2019)’s data set is that it includes numeric and image data. This selection observes a deep learning operation in numerical data because most of the studies are based on images."
2105.15074,"data, dataset",253,2022-05-17,0,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model. For the Antisaccade movement, Prosaccade used a dense network with two hidden layers of 128 neurons with ""ReLu"" activation. For Memory-guide saccade, Diffusion tensor imaging (DTI), and Psychometric data, we used four interleaved hidden layers of 64 and 128 neurons with sigmoid, ReLu activation, and Leaky ReLu just for DTI. The models trained for a total of 50 epochs. We used Leaky ReLu just in one case (DTI), which increasing the perfection. In other cases, the results not improved and we discarded to use the optimization function."
2105.15074,"data, dataset, data available",83,2022-05-17,1,"The data used in this research is based on an open-access dataset collected by (Zhang et al., 2019). This research will use psychometric, saccadic eye movement and DTI data. From an open data set collected and analyzed by (Zhang et al., 2019) to address the research question. This data set contains children’s information from 3 to 18 years old. Including subject that are clinically diagnosed with or without FASD."
2105.15074,"data, python, dataset provided",181,2022-05-17,0,"The input data are the features of each test. And the output is binary to classify FASD or control. The amount of data must be large enough to provide training examples. From which a large set of parameters can be drawn. Only a large number of parameters give rise to the wealth of class functions that model implicit knowledge Faust et al. (2018). Unfortunately, there are little data on children with FASD, making the classiﬁcation difﬁcult. The Keras library of TensorFlow was used in Python to implement the algorithm (Ketkar, 2017). We designed various models with dense layer connections, changing the number of neurons in each input layer, hidden layer, and output layer. To compile the model, we used “backpropagation with optimization Adam” and loss “sparse categorical cross-entropy” in the ﬁrst model. And “Binary Crossentropy” in another model, which are ideal conﬁgurations for classiﬁcation of categories in the FASD and non-FASD cases."
2105.15074,github,14,2022-04-21,2,1The algorithms and functions used are available in the GitHub repository https://github.com/vjduarte/ANN_FASD
2105.15074,python,18,2022-05-17,0,"Ketkar, N. (2017). Deep Learning with Python: A Hands-on Introduction. Apress."
2106.03907,code,113,2022-05-17,2,"In this section, we report the empirical performance of the DFPV method. First, we present the results of estimating structural functions; we design two experimental settings for low-dimensional treatments and high-dimensional treatments, respectively. Then, we show the result of applying PCL methods to the bandit oﬀ-policy evaluation problem with confounding. We include the results for problems considered in prior work in Appendix E. The experiments are implemented using PyTorch [27]. The code is included in the supplemental material. All experiments can be run in a few minutes on Intel(R) Xeon(R) CPU E5-2698 v4 2.20GHz."
2106.03907,"data, data available",245,2022-04-21,0,"One common assumption to cope with confounding bias is to assume no unobserved confounders exist [8], or more generally, the ignorable treatment assignment assumption [28], which states that the treatment assignment is independent of the potential outcomes caused by the treatment, given the background data available. Although a number of methods are proposed based on this assumption [7, 9, 36], it can be too restrictive, since it is often diﬃcult to determine how the confounder aﬀects treatment assignments and outcomes. A less restrictive assumption is that we have access to proxy variables, which contain relevant side information on the confounder. In the ﬂight tickets example, we can use the number of views of the ticket reservation page as a proxy variable, which reﬂects peoples’ desire for ﬂights. Note that if we can completely recover the confounder from proxy variables, the ignorable treatment assignment assumption can be satisﬁed. Motivated by this, Lee et al. [14] and Louizos et al. [17] aim to recover the distribution of confounders from proxy variables using modern machine learning techniques such as generative adversarial networks [5] or variational auto-encoders (VAE) [11]. Although these methods exhibit powerful empirical performance, there is little theory that guarantees the correct recovery of the causal eﬀects."
2106.03907,"data, data available",74,2022-05-17,0,"where each element of the matrix B ∈ R10×4096 was generated from Unif(0.0, 1.0) and ﬁxed throughout the experiment. We ﬁxed the shape parameter to heart and used other parameters as the treatment-inducing proxy Z. We sampled another image that shared the same posY as treatment A, which is used as output-inducing proxy W . Details of data generation process can be found in Appendix F.2."
2106.03907,"data, dataset",105,2022-05-17,0,"Here, we describe the data generation process for the dSprites dataset experiment. This is an image dataset parametrized via ﬁve latent variables (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. In this experiment, we ﬁxed the shape parameter to heart, i.e. we only used the heart-shaped images. The other latent parameters take values of scale ∈ [0.5, 1], rotation ∈ [0, 2π], posX ∈ [0, 1], posY ∈ [0, 1]."
2106.03907,"data, dataset",109,2022-05-17,0,"Table 1 shows the result for this dataset. In this setting, the performance of DFPV matches KPV and PMMR. As in the experiment described in the revious section, the setting is low-dimensional (one-dim treatment variable, three-dim treatment-inducing proxy, four-dim outcome-inducing proxy) and the generative model is smooth (the ""ground truth"" being a generalized additive model and a Gaussian mixture model). For these reasons, we might again expect this data to favor kernel methods, such as KPV and PMMR; nonetheless, our method matches them. DFPV again outperforms CEVAE in this setting."
2106.03907,"data, dataset",145,2022-05-17,1,"To test the performance of DFPV in a more realistic setting, we conducted the experiment on the Grade Retention dataset introduced by Deaner [3]. This aims to estimate the eﬀect of grade retention based on the score of math and reading on the long-term cognitive outcomes, in which we use scores in elementary school as a treatment-inducing proxy (Z) and cognitive test scores from Kindergarten as the an outcome-inducing proxy (W). Following Mastouri et al. [18], we generate a synthetic ""ground truth"" by ﬁtting a generalized additive model to learn a structured causal model (SCM), and a Gaussian mixture model to learn unmeasured confounder based on the learned SCM. Note, this is needed since for real-world data there is no measured ground truth."
2106.03907,"data, dataset",274,2022-05-17,0,"We compare the DFPV method to three competing methods, namely KPV [18], PMMR [18], and an autoencoder approach derived from the CEVAE method [17]. In KPV, the bridge function is estimated through the twostage regression as described in Section 2, where feature functions are ﬁxed via their kernel functions. PMMR also models the bridge function using kernel functions, but parameters are learned by moment matching. CEVAE is not a PCL method, however, it represents a state-of-theart approach in correcting for hidden confounders using observed proxies. The causal graph for CEVAE is shown in Figure 2, and CEVAE uses a VAE [11] to recover the distribution of confounder U from the “proxy” Q. We make two modiﬁcations to CEVAE to apply it in our setting. First, we include both the treatment-inducing proxy Z and output-inducing proxy W as Q in CEVAE (we emphasize that this does not follow the causal graph in Figure 2, since there exist arrows from Q to A, Y ). Second, CEVAE is originally used in the setting where Q is conditioned on a particular value, whereas we marginalize Q. See Appendix F.4 for the choice of the network structure and hyper-parameters. We tuned the regularizers λ1, λ2 as discussed in Appendix A, with the data evenly split for Stage 1 and Stage 2. We varied the dataset size and ran 20 simulations for each setting. Results are summarized in Figure 3."
2106.03907,"data, dataset, dataset provided, data available",147,2022-05-17,1,"Experiments for Structural Function We present two structural function estimation experiments. One is a demand design experiment based on a synthetic dataset introduced by Hartford et al. [6], which is a standard benchmark for the instrumental variable regression. Here, we modify the data generating process to provide a benchmark for PCL methods. We consider the problem of predicting sales Y from ticket price P , where these are confounded by a potential demand D ∈ [0, 10]. To correct this confounding bias, we use the fuel price (C1, C2) as the treatment-inducing proxy, which has an impact on price P , and the number of views of the ticket reservation page V as the outcome-inducing proxy. Details of the data generation process can be found in Appendix F.1."
2106.03907,dataset,108,2022-05-17,1,"treatment variables. We test this using the dSprite dataset [19], which is an image dataset described by ﬁve latent parameters (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. Based on this, Xu et al. [35] introduced the causal experiment, where the treatment is each ﬁgure, and the confounder is posY. Inspired by this, we consider the PCL setting that learns the same structural functions with nonlinear confounding, which is not possible to handle in the instrumental variable setting. Speciﬁcally, the"
2106.03907,dataset,14,2022-05-17,0,"From this dataset, we generate the treatment variable A and outcome Y as"
2106.03907,dataset,31,2022-05-17,0,5: 6: until convergence 7: Compute ˆu(θ(t)) from (20) 8: Compute mean feature for W using stage 1 dataset
2106.03907,dataset,39,2022-05-17,0,5: 6: until convergence 7: Compute ˆu(θ(t)) from (7) 8: Compute mean feature for W using stage 1 dataset: µθW ← 1 n (cid:18)
2106.03907,dataset,59,2022-05-17,0,"If ˆRS1(H1) → 0 and Corollary 3. Let Assumption 7 hold and κ1, κ2 = 0. ˆRS2 (H2) → 0 in probability as the dataset size increases, ˆh converges to h∗ in probability with respect to (cid:107) · (cid:107)P(A,W )."
2106.03907,dataset,6,2022-05-17,0,E.2 Experiments using Grade Retention dataset
2106.03907,dataset,62,2022-05-17,0,"Proposition 6. [Theorem 3.3 24, with slight modiﬁcation] Let S be a measurable space and H be a family of functions mapping from S to [0, M ]. Given ﬁxed dataset S = (s1, s2, . . . , sn) ∈ S n, the empirical Rademacher complexity is given by"
2106.03907,dataset,8,2022-05-17,0,Table 1: Results of grade retension dataset
2106.03907,"dataset, github, data https",31,2022-05-17,0,"[19] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. URL https://github.com/deepmind/ dsprites-dataset/."
2106.06064,"code, code available",10,2022-05-17,2,Code to reproduce our experiments is available at https:
2106.06064,data,114,2022-04-21,0,"In this work, we model multivariate time-series as random realizations from a nonlinear state-space model, and target Bayesian inference of the hidden states for probabilistic forecasting. The general framework we propose can be applied to univariate or multivariate forecasting problems, can incorporate additional covariates, can process an observed graph, and can be combined with data-adaptive graph learning procedures. For the concrete example algorithm deployed in experiments, we build the dynamics of the state-space model using graph convolutional recurrent architectures. We develop an inference procedure that employs particle ﬂow, an alternative to particle ﬁlters, that can conduct more effective inference for high-dimensional states."
2106.06064,"data, data https",1,2022-05-17,0,tlc-trip-record-data.page
2106.06064,"data, dataset",102,2022-05-17,0,"Point forecasting results on non-graph datasets : We evaluate our proposed ﬂow-based RNN on the Electricity and Trafﬁc datasets, following the setting described in Appendix C.4 in (Oreshkin et al., 2020). We augment the results table in (Oreshkin et al., 2020) with the results from an FC-GRU (a fully connected GRU encoder-decoder) and GRU+ﬂow. We use a 2 layer GRU with 64 RNN units in both cases. We follow the preprocessing steps in (Oreshkin et al., 2020). In the literature, four different data splits have"
2106.06064,"data, dataset",157,2022-05-17,0,"Spatio-temporal forecasting has numerous applications in analyzing wireless, trafﬁc, and ﬁnancial networks. Many classical statistical models often fall short in handling the complexity and high non-linearity present in time-series data. Recent advances in deep learning allow for better modelling of spatial and temporal dependencies. While most of these models focus on obtaining accurate point forecasts, they do not characterize the prediction uncertainty. In this work, we consider the time-series data as a random realization from a nonlinear state-space model and target Bayesian inference of the hidden states for probabilistic forecasting. We use particle ﬂow as the tool for approximating the posterior distribution of the states, as it is shown to be highly effective in complex, high-dimensional settings. Thorough experimentation on several real world time-series datasets demonstrates that our approach provides better characterization of uncertainty while maintaining comparable accuracy to the state-of-theart point forecasting methods."
2106.06064,"data, dataset",310,2022-05-17,1,"Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets. Some qualitative visualization of the conﬁdence intervals for 15-minute ahead predictions for the PeMSD3, PeMSD4, PeMSD7, and PeMSD8 datasets are shown in Figures 7, 8, 9, and 10 respectively. We observe that the conﬁdence intervals from the proposed algorithm are considerably tighter compared to its competitors in most cases, whereas the coverage of the ground truth is still ensured."
2106.06064,"data, dataset",344,2022-05-17,1,"In Table 1 of the main paper, we report the average MAE of the top 10 algorithms. The detailed comparisons in terms of MAE, MAPE, and RMSE with all the baseline algorithms on the four PeMS datasets are provided in Tables 10, 11, 12, and 13. We observe that statistical models such as HA, ARIMA, and VAR and basic machine learning models such as SVR, FNN, and FC-LSTM show poor predictive performance as they cannot model the complex spatio-temporal patterns present in the real world trafﬁc data well. Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets."
2106.06064,"data, dataset",47,2022-05-17,0,"both for the ground-truth test data, and samples of forecasts, and then computing the (normalized) CRPS on the summed data. The results are summarized in Table 7. We observe that the proposed GRU+ﬂow achieves the lowest CRPSsum for all datasets."
2106.06064,"data, dataset",71,2022-05-17,0,"test split is set at 70/10/20% chronologically and standard normalization of the data is used as in (Li et al., 2018). We use one hour of historical data (P = 12) to predict the trafﬁc for the next hour (Q = 12). Graphs associated with the datasets are constructed using the procedure in (Huang et al., 2020)."
2106.06064,"data, dataset",90,2022-05-17,0,"We address the task of discrete-time multivariate time-series prediction, with the goal of forecasting multiple time-steps ahead. We assume that there is access to a historical dataset for training, but after training the model must perform prediction based on a limited window of historical data. Let yt ∈ RN ×1 be an observed multivariate signal at time t and Zt ∈ RN ×dz be an associated set of covariates. The i-th element of yt is the observation associated with time-series i at time-step t."
2106.06064,"data, dataset, data available",113,2022-05-17,0,"5.2. Inference We assume that a dataset Dtrn is available for training. Although this data may be derived from a single time-series, because our task is to predict yt0+P +1:t0+P +Q using a limited historical window yt0+1:t0+P , we splice the time-series and thus construct multiple training examples, denoted by (y(m) P +1:P +Q). In the training set, all of these observations are available; in the test set yP +1:P +Q are not. In addition, the associated covariates z1:P +Q are known for both training and test sets."
2106.06064,"data, dataset, data available",168,2022-05-17,0,"In Table 6, we observe that the ﬂow based approach performs comparably or better than the state-of-the-art NBEATS algorithm for the Electricity dataset, even with a simple GRU as the state transition function. The better performance of the univariate N-BEATS compared to the multivariate models suggests that most time-series in these datasets do not provide valuable additional information for predicting other datasets. This is in contrast to the graphbased datasets, where the performance of N-BEATS was considerably worse than the multivariate algorithms. The proposed ﬂow-based algorithm achieves prediction performance on the Trafﬁc dataset that is comparable to N-BEATS except for one split with limited training data. Across all datasets and split settings, our ﬂow-based approach signiﬁ cantly outperforms the FC-GRU. The proposed algorithm outperforms TRMF, DeepAR, DeepState and DeepGLO. It outperforms DeepFactors for the Electricity dataset, but is worse for the Trafﬁc dataset (for the same split with limited available training data)."
2106.06064,"data, dataset, publicly available",125,2022-05-17,0,"6.1. Datasets We evaluate our proposed algorithm on four publicly available trafﬁc datasets, namely PeMSD3, PeMSD4, PeMSD7 and PeMSD8. These are obtained from the Caltrans Performance Measurement System (PeMS) (Chen et al., 2000) and have been used in multiple previous works (Yu et al., 2018; Guo et al., 2019; Song et al., 2020; Bai et al., 2020; Huang et al., 2020). Each of these datasets consists of the trafﬁc speed records, collected from loop detectors, and aggregated over 5 minute intervals, resulting in 288 data points per detector per day. In non-graph setting, we use Electricity (Dua"
2106.06064,dataset,1,2022-05-17,0,Dataset
2106.06064,dataset,1,2022-05-17,0,gluon-ts/tree/mv_release/datasets
2106.06064,dataset,10,2022-05-17,0,Table 9. Summary statistics of the multivariate non-graph datasets
2106.06064,dataset,12,2022-05-17,0,Dataset Algorithm 11.41/13.11/14.62/16.27 DeepAR 14.16/15.87/17.59/18.99 DeepFactors GRU+ﬂow 11.23/12.70/13.98/15.25 DCGRU+ﬂow 11.21/12.14/12.87/13.64 AGCGRU+ﬂow 10.53/11.39/12.03/12.47
2106.06064,dataset,127,2022-05-17,0,"Probabilistic forecasting results on non-graph datasets : For comparison with state-of-the-art deep learning based probabilistic forecasting methods on standard non-graph time-series datasets, we evaluate the proposed GRU+ﬂow algorithm following the setting in (Rasul et al., 2021). The results reported in Table 1 of (Rasul et al., 2021) are augmented with the results of the GRU+ﬂow algorithm. We use a 2 layer GRU with 64 RNN units in each case. We follow the preprocessing steps as in (Salinas et al., 2019; Rasul et al., 2021). The evaluation metric is (normalized) CRPSsum (deﬁned in the supplementary material), which is obtained by ﬁrst summing across the different time-series,"
2106.06064,dataset,141,2022-05-17,0,"In this experiment, we compare the proposed state-space model with different learnable noise variance at each node (parameterized by the softplus function in eq. (9) in the main paper with ﬁxed and uniform noise standard deviation γ = 0.01/0.05/0.10 at all nodes. Other hyper-parameters and the training setup remain unchanged. The results in Table 18 demonstrate that the learnable noise variance approach is not particularly beneﬁcial in comparison to a uniform, ﬁxed variance approach in most cases. However, we note that the probabilistic metrics reported in Table 19 are the lowest for the learnable noise variance model in all cases. This suggests that different time-series in these road trafﬁc datasets have different degrees of uncertainty which cannot be effectively modelled by the uniform, ﬁxed noise variance approach."
2106.06064,dataset,15,2022-05-17,0,Table 8. Summary statistics of the PeMS road trafﬁc datasets PeMSD3 PeMSD4 PeMSD7 PeMSD8
2106.06064,dataset,17,2022-05-17,1,We perform experiments on four graph-based and four nongraph based public datasets to evaluate proposed methods.
2106.06064,dataset,177,2022-05-17,0,"posed AGCGRU+ﬂow algorithm achieves on par or better performance with the best-performing spatio-temporal models, such as GWN, GMAN and AGCRN. We present a comparison of the average rankings across datasets in Figure 3. Our proposed method achieves the best average ranking and signiﬁcantly outperforms the baseline methods. Table 3 summarizes the results for probabilistic forecasting. We observe that in most cases, the proposed ﬂow based algorithms outperform the competitors. MQRNN also shows impressive performance in predicting the forecast quantiles, as it is explicitly trained to minimise the quantile losses. In particular, comparison of GRU+ﬂow with the DeepAR model reveals that even without a sophisticated RNN architecture, the particle ﬂow based approach shows better characterization of prediction uncertainty in most cases. Figure 4 provides a qualitative comparison of the uncertainty characterization, showing example conﬁdence intervals for 15-minute ahead prediction for the PeMSD7 dataset. We see that the proposed algorithm provides considerably tighter intervals, while still achieving coverage of the observed values."
2106.06064,dataset,2,2022-05-17,0,Dataset Algorithm
2106.06064,dataset,2,2022-05-17,0,PeMS datasets
2106.06064,dataset,27,2022-05-17,0,"6.2. Preprocessing For the PeMS datasets, missing values are ﬁlled by the last known value in the same series. The training, validation and"
2106.06064,dataset,28,2022-05-17,0,"For the experiments on the PeMS road trafﬁc datasets, we compare the proposed AGCGRU+ﬂow algorithm with four different classes of forecasting techniques, listed as follows:"
2106.06064,dataset,283,2022-05-17,0,"6.4. Hyperparameters and training setup For our model, we use an L = 2 layer AGCGRU (Bai et al., 2020) as the state-transition function. The dimension of the learnable node embedding is de = 10, and the number of RNN units is dx = 64. We treat ρ and σ as ﬁxed hyperparameters and set ρ = 1 and σ = 0 (no process noise). We train for 100 epochs using the Adam optimizer, with a batch size of 64. The initial learning rate is set to 0.01 and we follow a decaying schedule as in (Li et al., 2018). Hyperparameters associated with scheduled sampling (Bengio et al., 2015), gradient clipping, and early stoppng are borrowed from (Li et al., 2018). We set the number of particles Np = 1 during training and Np = 10 for validation and testing. The number of exponentially spaced discrete steps (Li & Coates, 2017) for integrating the ﬂow is Nλ = 29. For each dataset, we conduct two separate experiments minimizing the training MAE (results are used to report MAE, MAPE, RMSE, and P50QL) and the training negative log posterior probability (results are used to report CRPS, P10QL, and P90QL). We also experiment with alternative state transition functions, including the DCGRU (Li et al., 2018) and GRU (Chung et al., 2014). For these, the hyperparameters are ﬁxed to the same values as presented above."
2106.06064,dataset,3,2022-05-17,0,THE PEMS DATASETS
2106.06064,dataset,3,2022-05-17,1,https://archive.ics.uci.edu/ml/datasets/
2106.06064,dataset,32,2022-05-17,0,"Table 25. Execution time, memory consumption (during training) and model size for AGCRN-ensemble, GMAN-ensemble and AGCGRU+ﬂow for the four PeMS datasets. Lower numbers are better."
2106.06064,dataset,36,2022-05-17,0,Table 6. Normalized Deviation on Electricity and Trafﬁc datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better.
2106.06064,dataset,38,2022-05-17,0,Figure 3. Boxplot of ranks of the top 10 algorithms across the four trafﬁc datasets. The means of the ranks are shown by the black triangles; whiskers extend to the minimum and maximum ranks.
2106.06064,dataset,40,2022-05-17,0,Figure 6. Scatter-plots of average MAE at each node for AGCGRU+ﬂow v.s. that of AGCRN on PeMS datasets. The AGCGRU+ﬂow has lower average MAE compared to AGCRN at most of the nodes for all four datasets.
2106.06064,dataset,40,2022-05-17,0,"Table 7. Average CRPSsum for Electricity, Trafﬁc, Taxi, and Wikipedia datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better"
2106.06064,dataset,42,2022-04-21,0,"3) we show that the proposed method provides a superior characterization of the prediction uncertainty compared to existing probabilistic multivariate time-series forecasting methods, both for datasets where a graph is available and for settings where no graph is available."
2106.06064,dataset,42,2022-05-17,0,"Table 10. Average MAE, MAPE and RMSE for PeMSD3 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 11. Average MAE, MAPE and RMSE for PeMSD4 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 12. Average MAE, MAPE and RMSE for PeMSD7 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 13. Average MAE, MAPE and RMSE for PeMSD8 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,49,2022-05-17,0,"Figure 4. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at node 4 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 10. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 1, 17, 95, and 164 of PeMSD8 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 7. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 37, 54, 100, and 187 of PeMSD3 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 8. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 2, 44, 57, and 213 of PeMSD4 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 9. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 43, 108, 163, and 201 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,64,2022-05-17,0,"For spatio-temporal predictions using the graph-based recurrent architectures, this can be done if the graph can be partitioned meaningfully. For non-graph datasets, we can use the cross-correlation among different time-series to group them into several lower-dimensional problems. Alternatively, we can train a univariate model based on all the time-series as in (Rangapuram et al., 2018)."
2106.06064,dataset,7,2022-05-17,0,10. Description and statistics of datasets
2106.06064,dataset,78,2022-05-17,0,"& Graff, 2017) (hourly time-series of the electricity consumption), Trafﬁc (Dua & Graff, 2017) (hourly occupancy rate, of different car lanes in San Francisco), Taxi (Salinas et al., 2019), and Wikipedia (Salinas et al., 2019) (count of clicks to different web links) datasets. The detailed statistics of these datasets are summarized in the supplementary material."
2106.06064,dataset,82,2022-05-17,0,The statistics of the PeMS datasets and the non-graph datasets used in our experiments are summarized in Tables 8 and 9 respectively. The description of the PeMS datasets are provided in Section 6.1 of the main paper. The Electricity dataset contains electricity consumption for 370 clients. The Trafﬁc dataset is composed of 963 time-series of lane occupancy rates. The Taxi dataset contains counts of taxis on different roads and the Wikipedia dataset speciﬁes clicks to web links.
2106.06064,dataset,9,2022-05-17,0,Dataset No. nodes No. time steps Interval
2106.06064,dataset,92,2022-05-17,0,"If our focus is on obtaining a point estimate, then we can perform optimization on the training set with respect to a loss function derived from Mean Absolute Error (MAE) or Mean Square Error (MSE). The point forecast ˆy(m) P +1:P +Q is obtained based on a statistic such as the mean or median of the samples {yj,(m) j=1. The MAE loss function on a dataset indexed by D can then be expressed as:"
2106.06064,dataset,93,2022-05-17,0,"Table 25 summarizes the run time, GPU usage during training, and the size of the learned model for AGCRN-ensemble, GMAN-ensemble, and the proposed AGCGRU+ﬂow for the four PeMS datasets. We observe that if we choose the ensemble size so that the algorithms have an approximately equal execution time, then the model-size of the ensemble algorithms are comparable to our approach as well. However, our method requires more GPU memory compared to the ensembles during training because of the particle ﬂow in the forward pass."
2106.06064,dataset,93,2022-05-17,0,"The novel contributions in this paper are as follows: 1) we propose a graph-aware stochastic recurrent network architecture and inference procedure that combine graph convolutional learning, a probabilistic state-space model, and particle ﬂow; 2) we demonstrate via experiments on graph-based trafﬁc datasets that a speciﬁc instantiation of the proposed framework can provide point forecasts that are as accurate as the state-of-the-art deep learning based spatio-temporal models. The prediction error is also comparable to the existing deep learning based techniques for benchmark non-graph multivariate time-series datasets;"
2106.06064,"dataset, code, publicly available",152,2022-05-17,0,"6.5. Results and Discussion Comparison with baselines : Results for the point forecasting task are summarized in Table 1. We observe that most of the spatio-temporal models perform better than graph agnostic baselines in most cases. Moreover, the pro Some of the recent spatio-temporal models such as (Chen et al., 2020; Zhang et al., 2020; Park et al., 2020) do not have publicly available code. Although the codes for (Wu et al., 2020; Song et al., 2020; Pan et al., 2019) are available, these works use different datasets for evaluation. We could not obtain sensible results from these models for our datasets, even with considerable hyperparameter tuning. The code for (Kurle et al., 2020; de B´ezenac et al., 2020) is not publicly available."
2106.06064,"dataset, used dataset",29,2022-05-17,0,"been used for the Electricity dataset, and three different splits have been used for the Trafﬁc dataset. The evaluation metric is P50QL (Normalized Deviation)."
2106.06064,github,1,2022-05-17,0,//github.com/networkslab/rnn_flow
2106.06064,github,3,2022-05-17,0,https://github.com/mbohlkeschneider/
2106.14178,"data, code, publicly available, code available",122,2022-05-17,2,"In this work, we proposed a novel residual moment loss function to extract location information in medical image segmentation. Motivated by image moments, we explicitly encoded the coordinate information of pixels (or voxels) to the RM loss, which is simple but can capture the target location eﬀectively. In addition, our method is also easy to optimize with high computational eﬃciency. The experimental results demonstrated that our method could be adapted to various data types and network architectures. The method can also be easily embedded into other network training strategies and used in diﬀerent practical problems, which will be investigated in our future research. Source code will be publicly available."
2106.14178,dataset,128,2022-05-17,1,"The LA dataset contains 100 training cases and 54 testing cases. Following the experimental setting in [10], we randomly selected 16 cases for training and 20 cases for testing for a fair comparison with [10]. We cropped all cases centering at the heart region to alleviate the problem of unbalanced categories. All cases are normalized by subtracting the mean and dividing by the standard deviation. We use 3D V-Net [12] as the baseline which is optimized by SGD with the 0.01 learning rate. To prevent overﬁtting, we use dropout during training but turn it oﬀ in the inference stage. Besides, the RM loss weight is α = 0.01 in Eq. 6."
2106.14178,dataset,19,2022-05-17,0,Table 2. Left atrial segmentation accuracy with mean (standard deviation) on the LA MRI dataset.
2106.14178,dataset,2,2022-05-17,0,Dataset Method
2106.14178,dataset,20,2022-05-17,0,Table 1. Optical cup and disk segmentation accuracy with mean (standard deviation) on the GS dataset.
2106.14178,dataset,39,2022-05-17,0,"Fig. 3. Visualization of the left atrial segmentation results on the LA MRI dataset. The ﬁrst column is ground-truth. The second and third columns are the results of baseline and our method, respectively."
2106.14178,dataset,43,2022-05-17,0,"To verify the eﬀectiveness and generalization of our method, we apply the residual moment loss to 2D and 3D neural networks with various public datasets. All our experiments are implemented with the PyTorch (1.3.0) framework [14]."
2106.14178,dataset,45,2022-05-17,1,"We evaluate our method on two datasets, which are the 2D optic cup and disk segmentation dataset: Drishti-GS (GS) [16] and the left atrial (LA) 3D gadolinium-enhanced magnetic resonance imaging (MRI) [19]."
2106.14178,dataset,55,2022-05-17,0,"16. Sivaswamy, J., Krishnadas, S., Joshi, G.D., Jain, M., Tabish, A.U.S.: Drishti-GS: Retinal image dataset for optic nerve head (ONH) segmentation. In: IEEE 11th International Symposium on Biomedical Imaging. pp. 53–56. IEEE (2014)"
2106.14178,dataset,68,2022-05-17,0,"Fig. 2. The optic cup and disk results of the GS dataset. The green and blue lines represent the contours of ground-truth and the segmentation results, respectively. The ﬁrst row is the results of the baseline and the second row is our method, while the ﬁrst two columns are the cup results and the last two columns are the disk results."
2106.14178,dataset,77,2022-05-17,0,"For the GS dataset, we use 50 images for training and 50 for testing. These images are resized to 512 × 512 for computational eﬃciency. We apply the 2D U-Net [15] as the baseline and set the weight of RM loss in Eq. 6 as α = 1. The model is trained by the stochastic gradient descent (SGD) with the learning rate of 0.001 for 2000 iterations."
2106.14178,dataset,87,2022-05-17,0,"Our method can be easily extended to 3D. Table 2 and Fig. 3 show the quantitative and qualitative results of the LA dataset. Here, we compare with some implicit position embedding methods, which are boundary loss [8], Hausdorﬀ distance loss [7] and signed distance function loss [20]. The experimental results of these three comparison methods are directly taken from Ma et al. [10] and our experimental settings followed their work."
2106.14178,"dataset, publicly available",228,2022-05-17,10,"Abstract. Location information is proven to beneﬁt the deep learning models on capturing the manifold structure of target objects, and accordingly boosts the accuracy of medical image segmentation. However, most existing methods encode the location information in an implicit way, e.g., the distance transform maps, which describe the relative distance from each pixel to the contour boundary, for the network to learn. These implicit approaches do not fully exploit the position information (i.e., absolute location) of targets. In this paper, we propose a novel loss function, namely residual moment (RM) loss, to explicitly embed the location information of segmentation targets during the training of deep learning networks. Particularly, motivated by image moments, the segmentation prediction map and ground-truth map are weighted by coordinate information. Then our RM loss encourages the networks to maintain the consistency between the two weighted maps, which promotes the segmentation networks to easily locate the targets and extract manifold-structure-related features. We validate the proposed RM loss by conducting extensive experiments on two publicly available datasets, i.e., 2D optic cup and disk segmentation and 3D left atrial segmentation. The experimental results demonstrate the eﬀectiveness of our RM loss, which signiﬁcantly boosts the accuracy of segmentation networks."
2107.05429,"data, dataset",113,2022-05-17,1,"In order to test the performance under various unknown noise, we also used the test set from WSJ-0 [29] as the test speech. It contains 651 utterances from 8 speakers. There are two noise datasets used for test; one is the music data from MUSAN [30], the other is babble, factory1 and f16 from NOISEX92 [31]. The SNR range of the test noisy speech is the same as the training set. We also evaluated the model on the development test set and blind test set provided by DNS challenge. All the audio used is sampled at 16kHz."
2107.05429,database,51,2022-05-17,0,"[31] A. Varga and H. Steeneken, “Assessment for automatic speech Ii. noisex-92: A database and an experiment to recognition: study the effect of additive noise on speech recognition systems,” Speech Communication, vol. 12, pp. 247–251, 1993."
2107.05429,database,52,2022-05-17,0,"[27] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,” The Journal of the Acoustical Society of America, vol. 133, p. 3591, 2013."
2107.05429,dataset,112,2022-05-17,1,"We trained the DPCRN on the Interspeech 2021 DNS challenge dataset. 60000 clips of reverberant speech (about 500 h) were generated, with 55000 clips for training and 5000 for validation. The noise clips were mainly generated from Audioset [26], DEMAND [27] and Freesound1. In training stage, we randomly split the waves into 5-second segments and convolved them with room impulse responses (RIRs) randomly-selected from openSLR26 and openSLR28 [28]. Then the noisy speech was generated by mixing reverberant speech and noise. The SNR range of the mixture is set between -5 and 5 dB."
2107.05429,dataset,115,2022-05-17,1,"The widespread noise and reverberation may seriously degrade the performance of automatic speech recognition (ASR) systems and decrease speech intelligibility in communication. Speech enhancement aims at separating clean speech from background interference for higher speech intelligibility and perceptual quality. Despite the rapid progress of DNN-based speech enhancement recently, its performance in real applications still faces the challenges such as low signal-to-noise ratio (SNR), high reverberation and far-ﬁeld pickup. The Interspeech 2021 deep noise suppression (DNS) challenge [1] is organized to foster more competitive speech enhancement system in adverse environments, and training datasets and evaluation metrics are provided for such purpose."
2107.05429,dataset,119,2022-05-17,1,"convolutional layers. The CRM is output from the last transposed convolutional layer. We evaluate the DPCRN on the Interspeech 2021 DNS challenge dataset. Experimental results show that the DPCRN outperforms the baseline models, including NSNet2 [17], DTLN [18] and DCCRN [13]. On simulated test datasets, our model achieves competitive results as baseline models and show better performance in the case of low SNR. With only 0.8M parameters, our model achieves an overall MOS of 3.57 according to the ITU-T P.835 [19] subjective evaluation on DNS challenge blind test set, and reaches the third place in the wide band scenario track."
2107.05429,dataset,131,2022-05-17,0,"The performance on simulated WSJ0-MUSAN test set is presented in Table 1. It can be seen that when the SNR is greater than or equal to 0 dB, the performance of DPCRN-1 is slightly weaker than DCCRN, but better than DTLN. It should be noted that DPCRN-1 performs better than DCCRN at lower SNR. Table 2 shows the results on WSJ0-NOISEX92 test set. Under more disruptive noise from NOISEX92, the DPCRN-1 exceeds the baseline models in terms of all three metrics, demonstrating the beneﬁt of the DPRNN module for spectrogram modeling. On both datasets, DPCRN-2 has better performance than DPCRN-1 in terms of PESQ and STOI but its SDR is slightly worse, indicating that including the time-frequency MSE in the"
2107.05429,dataset,3,2022-05-17,0,3.1. Datasets
2107.05429,dataset,68,2022-05-17,0,"[26] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 776–780."
2107.05429,dataset,95,2022-05-17,0,"Inspired by the successful application of DPRNN and CRN, we propose a deep learning-based speech enhancement model in the time-frequency domain, named as DPCRN. It combines the local pattern modeling capability of CNN and the long-term modeling capability of DPRNN. Compared with CRN, DPCRN demonstrates the beneﬁt of RNN for spectrum modeling. With only 0.8M parameters, our model achieves competitive results on various unknown noise datasets. In the future, we will try to reduce the computational complexity of the model for wider band spectrum processing."
2107.09477,"data, dataset, open-source, used dataset",94,2022-05-17,1,"All recognizers were trained with the LibriSpeech dataset [24]. For the multispeaker TTS dataset, we used the “clean” subsets LibriTTS dataset [25], except in the text-based system for task 2, where we merged open-source single-speaker TTS datasets in Finnish [26], German [27], and Mandarin [28], as in [4]. For each of the two tasks, a separate neural vocoder was trained with the training data of the source and target speakers."
2107.09477,"data, dataset, used dataset",115,2022-05-17,1,"We used the VCC2020 dataset [3], which contained two tasks in our evaluation. The data conditions are summarized in Table 1. Both tasks share the same two source English male and female speakers whose data were not used. There were two target male and female speakers of English in task 1 whereas in task 2 there were one male and one female speaker each of Finnish, German, and Mandarin. During conversion, the source speaker’s voice in the source language was converted as if it was uttered by the target speaker while keeping the linguistic contents unchanged. For each target speaker, 70"
2107.09477,"data, open-source",179,2022-05-17,0,"For each system, ﬁve random utterances were chosen for each conversion pair. In the naturalness test, recordings of the target speakers were also included and served as the upper bound. In the similarity test for task 2, following [3], we selected three English recordings and two L2 language recordings as the natural reference for the ﬁve converted utterances. All subjective evaluations were performed using the open-source toolkit [44] that implements the ITU-T Recommendation P.808 [45] for subjective speech quality assessment in a crowd using the Amazon Mechanical Turk (Mturk) and screens the obtained data for unreliable ratings. We recruited more than 100 listeners from the United States and had each sample rated by ﬁve different participants on average. Note that to reduce cost, we eliminated SPT systems that freeze GST in the listening tests, since they yield inferior performance compared with systems that do not freeze GST, as shown in Section 5.1. Audio samples are available online9."
2107.09477,dataset,13,2022-05-17,0,"Speech Datasets for 10 Languages,” 2019, pp. 1566–1570."
2107.09477,dataset,216,2022-05-17,0,"that SPT can be a sub-optimal strategy for prosody modeling in ASR+TTS-based VC. First, the target-speaker-dependent TTS training causes a mismatch between training and conversion, because the speech of the target speaker is used as input to the reference encoder during training but that of the source is used during conversion. A speaker adversarial classiﬁer can alleviate this issue [5, 13] but requires careful hyperparameter tuning. Second, there are scenarios where SPT is not desired, such as emotion VC or accent conversion. In this work, we examine two prosody modeling methods for ASR+TTS-based VC. In addition to SPT, we propose a novel technique, which we refer to as target text prediction (TTP). We borrow the idea from [14] and train a text prediction (TP) module to generate the prosody embedding from the text derived from the source speech. Figure 1b illustrates this process. The TP module is ﬁrst pretrained with a GST-TTS on a multispeaker dataset, and further ﬁne-tuned in a target-speaker-dependent manner. As a result, TTP does not suffer from a mismatch between training and conversion unlike SPT. Our contributions in this work are as follows."
2107.09477,dataset,23,2022-05-17,0,"[27] Munich Artiﬁcial Intelligence Laboratories GmbH, “The MAILABS speech dataset,” 2019, accessed 30 November 2019."
2107.09477,dataset,53,2022-05-17,0,"The ASR and TTS models adopt sequence-to-sequence (seq2seq) structures, which were shown to improve conversion similarity by modeling the long-term dependencies in speech. Note that the two models can be separately trained and thus beneﬁt from advanced techniques and a wide variety of datasets in their own ﬁelds."
2107.09477,dataset,82,2022-05-17,0,"ASR: A multispeaker dataset DASR ensures the speaker independence of the recognizer. TTS: Synthesizer training involves a pretraining and a ﬁne-tuning stage. Pretraining is performed on a multispeaker TTS dataset DTTS, which is followed by ﬁne-tuning on the limited target speaker dataset Dtrg. This is a common practice in building modern neural TTS models, as pretraining ensures stable quality and ﬁne-tuning retains high speaker similarity [21]. Such a training strategy allows for"
2107.09477,github,3,2022-05-17,0,5https://github.com/pytorch/fairseq/tree/master/
2107.09477,github,3,2022-05-17,0,8https://github.com/kan-bayashi/ParallelWaveGAN
2107.09477,github,3,2022-05-17,0,9https://unilight.github.io/Publication-Demos/
2107.09477,github,6,2022-05-17,0,6https://kaldi-asr.org/models/m8 7https://github.com/espnet/espnet/tree/master/
2107.09477,github,8,2022-05-17,0,implementation on ESPnet: https://github.com/espnet/espnet/tree/master/ egs/vcc20
2107.09477,open-source,135,2022-05-17,0,"All synthesizers map their respective inputs to 80-dimensional mel ﬁlterbanks with 1024 FFT points and a 256-point frame shift (16 ms). The x-vector [37] was used as the speaker embedding, and we used the pretrained model provided by Kaldi 6. The average of all x-vectors of the training utterances of each speaker was used during inference. Synthesizers with discrete input including text and VQW2V had a Transformer-TTS architecture [38] with detailed settings [4, 17]. For the BNF-based synthesizer, we adopted the Voice Transformer Network (VTN) [39, 40] and followed the ofﬁcial implementation7. For the neural vocoder, we adopted the Parallel WaveGAN (PWG) [41] and followed the open-source implementation8."
2107.09477,open-source,180,2022-05-17,0,"For task 2, all systems showed performance degradation brought about by SPT, and TTP signiﬁcantly outperformed SPT for all representations. For systems based on frame-level features, TTP could even outperform the baseline and was also comparable to the textbased system. To investigate this gap, we plotted the breakdown per target language in the bottom graphs of Figure 3. We suspect that the relative performance change was again correlated with the text preprocessing, as stated in Section 5.1.2. As in [4], thanks to the open-source community, we utilized G2P tools to convert both English and Mandarin text into phonemes, resulting in a better acoustic model and a larger improvement brought about by TTP. On the other hand, because of the lack of linguistic knowledge, characters were used for Finnish and German, resulting in degradation when combined with TTP. We thus conclude that TTP is an effective method for improving naturalness in task 2, if the input representation is properly processed."
2107.09477,open-source,33,2022-05-17,0,"[44] B. Naderi and R. Cutler, “An Open Source Implementation of ITU-T Recommendation P.808 with Validation,” in Proc. Interspeech, 2020, pp. 2862–2866."
2107.09477,open-source,58,2022-05-17,0,"[30] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, “Espnet-TTS: Uniﬁed, Reproducible, and Integratable Open Source End-toin Proc. ICASSP, 2020, pp. End Text-to-Speech Toolkit,” 7654–7658."
2107.09477,publicly available,108,2022-05-17,0,"The system was implemented using ESPnet, a well-developed opensource end-to-end (E2E) speech processing toolkit [29, 30]. Following [4], the ASR model for the text-based system was based on the Transformer [31–33] with joint CTC/attention loss [34], and a RNNbased language model for decoding4. The ASR model for BNF extraction was based on TDNNF-HMM [35], where we concatenated 40-dimensional MFCCs and 400-dimensional i-vectors as input. For VQW2V, we used the publicly available pretrained model provided by fairseq [36]5, as in [17]."
2108.05075,"code, github",15,2022-05-17,0,"[52] “Code for synthesizing adversarial patch attack,” https://github.com/A-LinCui/"
2108.05075,"code, github",21,2022-05-17,0,"[41] “Code for lgs defense,” https://github.com/metallurk/local_gradients_smoothing. [42] https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial."
2108.05075,"code, github",61,2022-05-17,0,"[44] “Code for patchguard defense,” https://github.com/inspire-group/PatchGuard. [45] X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang, “Interpretable deep learning under fire,” in 29th {USENIX} Security Symposium ({USENIX} Security 20), 2020."
2108.05075,"code, publicly available, github, code available",9,2022-05-17,2,Our code is publicly available at https://github.com/DependableSystemsLab/
2108.05075,"data available, dataset",133,2022-05-17,0,"45%. We do not consider patches of larger size because a 7% patch is already able to achieve very high attack success rates (average 99%) and a larger patch will make the adversarial samples become visually more suspicious. For each patch, we train it for 30 epochs on a training set with 2000 images and evaluate the attack success rate on a separate test set and choose the one with the highest success rate. For the attack evaluation on ImageNette and CelebA, we use the entire test set in each dataset; for ImageNet and Place365, we use 10000 images from the validation set for each. Examples of adversarial samples for each dataset can be found in Fig. 6."
2108.05075,database,43,2022-05-17,0,"[35] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places: A 10 million image database for scene recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017."
2108.05075,database,44,2022-05-17,0,"[32] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE conference on computer vision and pattern recognition."
2108.05075,dataset,1,2022-05-17,0,Dataset
2108.05075,dataset,101,2022-05-17,0,"2.2 Threat Model This work assumes a white-box attacker, who has full knowledge of the victim DNN such as its structure, parameters. We assume however that the attacker has no knowledge of the exact inputs to the DNN, but instead has access to a surrogate dataset, which follows the same distribution as the legitimate inputs. This is similar to the assumptions in universal attack studies, and it is shown that the knowledge of the input distribution often suffices for the attacker to generate universal adversarial perturbations [2, 3, 5]."
2108.05075,dataset,103,2022-05-17,0,"Our evaluation shows that AT’s performance degrades as the number of target classes increases, on both robust accuracy and FP. This is because with more target classes, the learning objective for AT becomes increasingly difficult - this is similar to how common DNNs would yield lower accuracy on a 1000-class dataset than on a simple 10-class dataset. On the other hand, we see that Jujutsu achieves consistently high performance in terms of both robust accuracy and FP across attacks targeting different classes. Further, Jujutsu yields significantly better performance than AT in all cases."
2108.05075,dataset,107,2022-05-17,1,"CelebA. CelebA [34] is a large-scale face dataset with more than 200k celebrity images. We follow the implementation in [36] to first create a 307-class subset from the original dataset, where each class represents a celebrity identity and contains at least 15 images. The resulting dataset contains 4263 training images and 1215 validation images. We use a pre-trained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 30 epoches, and we use the SGD optimizer with an initial learning rate of 0.01 and momentum of 0.9."
2108.05075,dataset,11,2022-05-17,0,ImageNet. ImageNet [32] is 1000-class datasets with high-resolution
2108.05075,dataset,111,2022-05-17,0,"4.1.3 Attack Setup. The attacker’s goal is to synthesize adversarial patches that achieve both high attack success rate and remain stealthy. Therefore, for each dataset, we generate patches of different sizes, occupying 5%, 6% and 7% of the image pixels. We use 𝑥% patch to refer to a patch that occupies 𝑥% of the pixels of the image. We do not consider patches of smaller size because we find that they are unable to universally cause misclassification, e.g., use of a 4% patch on CelebA degraded the attack success rate by more than"
2108.05075,dataset,12,2022-05-17,1,"• Evaluate Jujutsu on 4 datasets (ImageNet, ImageNette, CelebA"
2108.05075,dataset,134,2022-05-17,0,"Benign inputs are the same as the adversarial inputs, except that they do not have the adversarial patch. We consider an image adversarial if and only if the predicted label for it is identical to that of both the hold-out images implanted with the suspicious features. Table 1 shows Jujutsu’s detection performance on all 4 datasets. Detection success recall. Jujutsu is able to consistently detect adversarial samples, with a detection success recall rate of over 93% across patch sizes (in most cases). The detection success recall increases with the size of the patch as a larger patch has a higher attack success rate. On average, Jujutsu can detect around 96% of the adversarial samples on all the datasets."
2108.05075,dataset,139,2022-05-17,1,"4.1.2 Datasets. We evaluate Jujutsu on ImageNet [32], ImageNette [33], CelebA [34] and Place365 [35]. ImageNet is a 1000-class dataset and we use the pre-trained ResNet-50 from the torchvision library. ImageNette is a 10-class subset of ImageNet and we train a ResNet18 on this dataset. CelebA is a facial dataset with diverse celebrity faces. We created a 307-classes subset from the original set and train a ResNet-18 model following [36] to perform identity classification. Place365 is a 365-class dataset containing common natural sceneries (e.g., patio, restaurant) and we use the pre-trained ResNet-50 from [37]. All images are resized to 224*224. We provide more details of the datasets in Appendix A.1."
2108.05075,dataset,160,2022-05-17,0,"Mitigation success recall: While masking alone is able to achieve higher detection recall compared to masking and inpainting when the masking percentage is small, the difference becomes negligible when the masking percentage increases. This is because when the masking percentage is low, the masked images are more likely to have a label different from those of the original images; while the inpainted images are more likely to have the same label as the original image - this is similar to the reason why robust accuracy from masking alone is higher than that from masking and inpainting for 25% masking. However, when the masking percentage increases, both the masked and inpainted images are likely to have labels that are different from that of the original image - thus the difference becomes negligible between both approaches. We see that Jujutsu is highly effective in detecting adversarial samples on all the datasets."
2108.05075,dataset,163,2022-05-17,0,"derivative. The parametric softplus function can be expressed as: 𝑓 (𝑥) = 1 𝛼 log(1 + exp(𝛼𝑥)), where 𝑥 is the original input to the ReLu function, and 𝛼 is the hyper-parameter to control the shape of the curve. We follow Xie et al. [47] to empirically set 𝛼 as 10 in our experiment. Finally, we only use the parametric softplus for backward propagation, and use ReLU for the normal forward pass. To be conservative, we consider the 7% patch, which allows the attacker to inject larger perturbations to evade Jujutsu. We choose 200 samples for training the adversarial patch, 500 steps per sample and 20 epochs in total. For each dataset, we choose 𝛽 ∈ [0.1, 0.5, 1, 5] and choose the one yielding the highest attack success rate."
2108.05075,dataset,207,2022-05-17,0,"(2) Mitigation FPR is the (reduced) FPR from the two-staged combination of detection and mitigation (explained in Section 3.3.3). (3) Mitigation success recall is the detection recall from the combination of detection and mitigation (explained in Section 3.3.3) - we distinguish this from the detection success recall, which is the detection recall from the detection technique alone. Mitigation success recall gives the final amount of adversarial samples detected by Jujutsu. Result. Table 2 shows Jujutsu’s mitigation performance on all 4 datasets. The results are averaged across patches of different sizes (5% to 7%). The detection performance is higher on larger patches as these patches have higher attack success rate (difference between the largest and smallest patch is about 9%) and the mitigation performance is consistent across different patch sizes (differences on robust accuracy and FP are both less than 2%). We consider two mitigation techniques, (1) masking alone, and (2) masking with inpainting (our mitigation technique). We discuss the results in terms of the 3 aforementioned metrics."
2108.05075,dataset,22,2022-05-17,0,Jujutsu is able to detect an average of 95.93% of the adversarial samples with 3.33% FPR on 4 datasets.
2108.05075,dataset,235,2022-05-17,0,"Equation 5 requires several forward and backward passes for calculating the saliency map ˆ𝑀∗ 𝑗 (𝑥), which is much more timeconsuming than the original optimization (Equation 3). Therefore, we reduce the sampling size 𝑛 in Equation 4 from 50 to 5 for faster training. We experimentally verified that the smaller sampling size 𝑛 does not significantly affect the resulting saliency map, and that we can still find all the salient features. Under this setting, it took around 18 days to generate an adversarial patch on Place365 dataset, compared to about 540 days if we had followed our previous setup. Result. We compare the attack success rate of the patches generated from the undefended models and the ones guarded by Jujutsu in Fig. 9. With the protection of Jujutsu, the adaptive attacker who attempts to evade Jujutsu’s detection suffers a significant drop in attack success rate, from 99% to just 4.9% (on average). This is because in Equation 5, the first term aims to increase the influence on the final prediction to manipulate the output label; while the second term reduces the influence on the output. This equation constrains the adaptive attacker, who cannot evade detection without also significantly degrading the attack’s effectiveness."
2108.05075,dataset,30,2022-05-17,0,This section shows Jujutsu’s performance when using different number of hold-out images for attack detection on the ImageNet dataset. The results are presented in Table 9.
2108.05075,dataset,33,2022-05-17,0,"accuracy on ImageNette, as it is a 10-class dataset, and performing correct image classification on this dataset is easier than on the other complicated datasets such as the 1000-class ImageNet."
2108.05075,dataset,36,2022-05-17,0,"When 75% of the perturbations are masked, it is almost infeasible for the attacker to generate a successful adversarial patch, and hence the success rate is near 0% on all datasets."
2108.05075,dataset,45,2022-05-17,0,"We train the rectangular patches on each dataset using a 7% patch (36*96). We change the square bounding box to a rectangular one, which occupies around 20% of pixels as before. Note that we do not"
2108.05075,dataset,48,2022-05-17,1,Place365. Place365 [35] is a large-scale dataset containing 365 unique scene categories. We use the pre-trained ResNet-50 model from [37]. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments.
2108.05075,dataset,49,2022-05-17,0,Detection FPR. Jujutsu yields an average FPR of 3.3% on the 4 datasets. We find that the FPRs on the two object-recognition datasets (ImageNet and ImageNette) are higher than that on the facial and scenery datasets. This is because the salient features in
2108.05075,dataset,56,2022-05-17,0,know the exact width/height ratio of the rectangular shape created by the attacker. Our detection bounding box has a width/height ratio of 6:4. Table 7 shows the results. We find that Jujutsu is able to achieve high detection and mitigation performance on rectangular patches with a very low FPR across different datasets.
2108.05075,dataset,74,2022-05-17,1,"We evaluate Jujutsu on four diverse datasets (ImageNet, ImageNette, CelebA and Place365), and show that Jujutsu achieves superior performance and significantly outperforms existing techniques. We find that Jujutsu can further defend against different variants of the basic attack, including 1) physical-world attack; 2) attacks that target diverse classes; 3) attacks that construct patches in different shapes and 4) adaptive attacks."
2108.05075,dataset,74,2022-05-17,0,"object-recognition datasets might contain the entire object (e.g., a small bird), which can cause the model to continue to assign the same label to the transplanted image. However, for the facial and scenery datasets, the salient features only contain a fraction of the image pixels (e.g., a partial face), which is unlikely to result in the same label on the transplanted image."
2108.05075,dataset,76,2022-05-17,0,"Under the VGGFace2 dataset, Jujutsu achieves a robust accuracy of 37.26%2, which is significantly higher than that of 0.2%3 by Februus. This is because Februus relies on the pre-defined threshold to identify the regions associated with adversarial patch. This method would fail to locate the adversarial patch if the patch’s influence to the prediction is lower than the threshold, and our experiment validates this."
2108.05075,dataset,77,2022-05-17,0,"4.6 RQ5 - Attacks Targeting Different Labels This section evaluates Jujutsu against attacks that target different class labels. For each target label, we need to perform training to generate the universal adversarial patches. Unfortunately, training is highly time-consuming, and hence, for each dataset, we train five 7% patches targeting different labels. Note that training is only needed for creating the adversarial patches, and not for Jujutsu."
2108.05075,dataset,77,2022-05-17,1,"We evaluate Jujutsu on 4 datasets and show that Jujutsu achieves superior detection and mitigation performance, and significantly outperforms existing techniques. We also demonstrate Jujutsu’s effectiveness in defending against different variants of the patch attack, including: 1) physical-world attacks; 2) attacks that use patches in various shapes; 3) attacks that target diverse classes. Finally, Jujutsu can thwart adaptive attacks in fooling the DNNs."
2108.05075,dataset,78,2022-05-17,0,"We also notice that the robust accuracy by Jujutsu on CelebA is lower than that on the other datasets, which is because the inpainting technique needs to synthesize the correct facial features belonging to a particular celebrity’s face to enable correct identity prediction. This is a much more challenging task for image inpainting than for the other three datasets, and hence Jujutsu yields a lower robust accuracy. Jujutsu achieves the highest robust"
2108.05075,dataset,81,2022-05-17,0,"Mitigation FPR: Masking with inpainting achieves low FPR, because the inpainted inputs are more similar to the original benign inputs than the masked inputs (in the latter case many features are simply masked). Therefore predictions on the original and inpainted inputs are more likely to be the same, which is not the case for inputs that are merely masked. We also see that Jujutsu consistently achieves very low FPRs on all the datasets."
2108.05075,dataset,9,2022-05-17,0,Figure 6: Adversarial samples for each dataset.
2108.05075,"dataset, code",151,2022-05-17,0,"Jujutsu has a lower robust accuracy on VGGFace2 than those on the other datasets due to the insufficient performance yielded by the inpainting technique (PICNet [29]). This is because we need to train the PICNet from scratch on VGGFace2, which is very time-consuming as VGGFace2 is a very large dataset. We trained the PICNet on a small subset of the dataset for a week and used it in our evaluation due to time constraint. The performance of Jujutsu can be further improved with more resources to train the PICNet (e.g., increase the size of training set and number of epoches). 3To ensure the code was implemented correctly, we verified that the code was able to reproduce the results reported in the original paper for trojan attack. We then used the code to evaluate against patch attacks."
2108.05075,"dataset, code, github",79,2022-05-17,0,"[48] “Code for strip defense,” https://github.com/garrisongys/STRIP. [49] “Code for februus defense,” https://github.com/AdelaideAuto-IDLab/Februus.git. [50] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “Vggface2: A dataset for recognising faces across pose and age,” in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)."
2108.05075,"dataset, code, github, data https",39,2022-05-17,0,"[37] “Place365 dataset,” https://github.com/CSAILVision/places365. [38] “Code for smoothgrad,” https://github.com/hs2k/pytorch-smoothgrad. [39] “Code for image inpainting technique,” https://github.com/lyndonzheng/"
2108.05075,"dataset, dataset provided",30,2022-05-17,0,A APPENDIX A.1 Dataset Details We provide the details for each of the 4 datasets in our evaluation below. All the images are resized to 224*224.
2108.05075,"dataset, github, data https",12,2022-05-17,0,"[51] “Imagenette dataset,” https://github.com/fastai/imagenette."
2108.05075,"dataset, github, data https",13,2022-05-17,0,"[36] “Celeba dataset,” https://github.com/ndb796/CelebA-HQ-Face-Identity-and Attributes-Recognition-PyTorch."
2108.05075,"dataset, github, data https",53,2022-05-17,0,"[33] “Imagenette dataset,” https://github.com/fastai/imagenette. [34] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of International Conference on Computer Vision (ICCV), December 2015."
2108.05075,"dataset, used dataset",116,2022-05-17,1,"images. We use the pre-trained ResNet-50 model from the torvision library. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments. ImageNette. ImageNette [33] is a 10-class subset of ImageNet dataset, with 9469 training images and 3925 validation images. We used this dataset to compare with adversarial training. We use a pretrained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 20 epoches, and we use the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.0013 and momentum of 0.9."
2108.05075,"python, dataset",192,2022-05-17,0,"4.1.4 Defense Setup. For saliency map generation, we use the implementation from [38]. For image inpainting, we use the implementation from [39] and their pre-trained models for each dataset. The original implementation does not support pixel-wise random masking, and we added this feature. We use cv2.blur from the Python cv2 library with a filter size of 51 to pre-process the saliency map to make it robust to noise. We then identify the point with the highest value in the saliency map as the center of the detection box. The length of the detection box is set to 102, which is around 20% of the pixels in the images. We sample a total of 1000 random images from the test set as the hold-out dataset. For each image, we empirically choose 2 random images from the the hold-out dataset for feature transfer, as we find that this setup balances the detection success recall and FPR. We show in Appendix A.2 Jujutsu’s performance when we use different numbers of random images."
2108.09408,dataset,1,2022-05-17,0,Datasets
2108.09408,dataset,111,2022-05-17,1,"Datasets and implementation details. The model is trained on the DUTSTR with 10553 images. In detail, we trained the model using the SGD optimizer with initial learning rate 3e-5, 0.9 momentum, 5e-4 weight decay, and batch size 16. Because the ResNet-50 parameters are pre-trained on ImageNet, the learning rate of this part is a tenth of the randomly initialized parts which is set as 3e-5. Then, the trained model is tested on ﬁve datasets, including DUTS-TE with 5019 images, DUT-OMROM with 5168 images, HKU-IS with 4447 images, ECSSD with 1000 images and PASCAL-S with 850 images."
2108.09408,dataset,133,2022-05-17,1,"Abstract. Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at diﬀerent scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edgebased U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods."
2108.09408,dataset,334,2022-05-17,0,Datasets mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em Metrics .745 .049 .862 .860 .868 .044 .911 .914 .692 .064 .809 .837 .871 .038 .907 .937 BMPM[30] .751 .059 .839 .861 .889 .059 .893 .914 .713 .062 .814 .846 .874 .045 .888 .931 RAS[3] .785 .057 .834 .867 .914 .040 .910 .929 .747 .063 .815 .850 .893 .036 .895 .939 R3Net[5] PiCANet[13] .749 .051 .867 .852 .885 .044 .917 .910 .710 .065 .835 .834 .870 .039 .908 .934 MLMSNet[26] .799 .045 .856 .882 .914 .038 .911 .925 .735 .056 .817 .846 .892 .034 .901 .945 .777 .051 .854 .869 .906 .042 .912 .920 .736 .066 .824 .853 .882 .037 .903 .940 PAGE[23] .805 .043 .869 .886 .917 .037 .918 .925 .747 .056 .825 .866 .891 .034 .905 .944 CPD[28] .756 .048 .866 .884 .880 .037 .916 .921 .756 .056 .836 .869 .895 .032 .909 .946 BASNet[19] .840 .035 .888 .902 .925 .033 .924 .927 .766 .053 .838 .870 .840 .062 .855 .859 F3Net[24] .799 .040 .879 .881 .910 .042 .917 .921 .739 .055 .832 .858 .885 .032 PoolNet[12] .941 .767 .048 .865 .879 .880 .040 .918 .922 .739 .059 .837 .854 .878 .038 .907 .942 TDBU[22] .815 .039 .875 .891 .920 .041 .918 .927 .755 .052 .818 .867 .898 .031 .918 .948 EGNet[31] .792 .044 .861 .886 .892 .033 .928 .924 .761 .054 .847 .871 .896 .031 .916 .948 U2Net[18] .828 .037 .884 .917 .924 .033 .925 .953 .756 .055 .833 .873 .908 .028 .920 .961 MINet[16] .855 .034 .892 .910 .930 .034 .924 .925 .773 .051 .838 .873 .914 .027 .919 .954 LDF[25] .870 .031 .904 .917 .936 .028 .934 .929 .790 .052 .851 .881 .917 .026 .925 .956 Ours
2108.09408,dataset,35,2022-05-17,0,Table 3. Quantitative comparison with state-of-the-art methods on ﬁve datasets. The best results are highlighted in bold. The best and the second best results are highlighted in red and green respectively.
2108.09408,dataset,46,2022-05-17,0,"• We build an eﬃcient framework to fully combine and fuse edge information, detailed information and semantic clues. Many experiments are conducted to illustrate the validity of our algorithm and this model could surpass most models on four large-scale salient object detection datasets."
2108.09408,dataset,93,2022-05-17,0,"Quantitative comparison. Table. 3 shows the quantitative evaluation results of the SOTA methods mentioned above and our model in terms of mF , M AE, Sm, and Em. The proposed method consistently performs better than all the competitors across four metrics on four datasets. In terms of Em, our method achieves the second best overall performance, which is slightly inferior to MINet. It is worth noting that MEUNet achieves the best performance in terms of the mean F-measure and structure quality evaluation Sm."
2109.03162,python,119,2022-05-17,0,"PYGLAF [2] is implemented in Python and uses CIRCUMSCRIPTINO (http://alviano.com/software/circumscriptino/), a circumscription solver extending the SAT solver GLUCOSE [3]. Linear reductions are used for all semantics [4]. For the ideal extension, the reduction requires the union of all admissible extensions of the input graph; such a set is computed by means of iterative calls to CIRCUMSCRIPTINO. The communication between PYGLAF and CIRCUMSCRIPTINO is handled in the simplest possible way, that is, via stream processing. This design choice is principally motivated by the fact that the communication is often minimal, limited to a single invocation of the circumscription solver."
2109.03162,python,48,2022-05-17,0,"[4] Mario Alviano. Ingredients of the argumentation reasoner pyglaf: Python, circumscription, and glucose to taste. In Marco Maratea and Ivan Serina, editors, RCRA 2017, volume 2011 of CEUR Workshop Proceedings, pages 1–16. CEUR-WS.org, 2017."
2109.03162,python,92,2022-05-17,0,"The PYGLAF reasoner takes advantage of circumscription to solve computational problems of abstract argumentation frameworks. In fact, many of these problems are reduced to circumscription by means of linear encodings, and a few others are solved by means of a sequence of calls to an oracle for circumscription. Within PYGLAF, Python is used to build the encodings and to control the execution of the external circumscription solver, which extends the SAT solver GLUCOSE and implements algorithms taking advantage of unsatisﬁable core analysis and incremental computation."
2109.04359,"data, data available",74,2022-05-17,0,"Operational Modes and Physical Characteristics Each different operational mode causes different types of gears behavior based on wind intensity, torque and vibration frequencies (Table 6). The data available for this analysis based on 10 minutes intervals cannot be used for analysis of dynamic forces for each operational mode; however, can be used for measuring gear ratios, as an indication of gear tooth wear over time."
2109.04359,"data, data https",39,2022-05-17,0,"Yang, W., Court, R., & Jiang, J. (2013). Wind turbine condition monitoring by the approach of SCADA data analysis. Renewable Energy, 53, 365–376. https://doi.org/10.1016/j.renene.2012.11.030"
2109.04359,"data, data https",42,2022-05-17,0,"Liu, X., Lu, S., Ren, Y., & Wu, Z. (2020). Wind Turbine Anomaly Detection Based on SCADA Data Mining. Electronics, 9(5), 751. https://doi.org/10.3390/electronics9050751"
2109.04359,"data, data https",45,2022-05-17,0,"Zaher, A., McArthur, S., Infield, D., & Patel, Y. (2009). Online wind turbine fault detection through automated SCADA data analysis. Wind Energy, 12(6), 574–593. https://doi.org/10.1002/we.319"
2109.04359,"data, data https",47,2022-05-17,0,"Wilkinson, M., Darnell, B., Delft, T., & Harman, K. (2014). Comparison of methods for wind turbine condition monitoring with SCADA data. IET Renewable Power Generation, 8(4), 390–397. https://doi.org/10.1049/iet-rpg.2013.0318"
2109.04359,"data, data https",49,2022-05-17,0,"Du, M., Yi, J., Mazidi, P., Cheng, L., & Guo, J. (2017). A Parameter Selection Method for Wind Turbine Health Management through SCADA Data. Energies, 10(2), 253. https://doi.org/10.3390/en10020253"
2109.04359,"data, data https",51,2022-05-17,0,"Qiu, Y., Chen, L., Feng, Y., & Xu, Y. (2017). An Approach of Quantifying Gear Fatigue Life for Wind Turbine Gearboxes Using Supervisory Control and Data Acquisition Data. Energies, 10(8), 1084. https://doi.org/10.3390/en10081084"
2109.04359,"data, dataset",66,2022-05-17,0,"3. Operational Wind Farm Data The data used in this analysis consist of 5 turbines from EDP Renewables Spain, open data center dedicated to wind energy research. The dataset contains 521, thousand data points of five wind turbines, 2MW production capacity each. The data recordings are for the period of beginning 2016 to end of 2017, 10-minute intervals."
2109.04359,"data, dataset",80,2022-05-17,0,"Clustering Operational Modes Clustering is an unsupervised machine learning technique, used for grouping of data points, which have similar characteristics. For wind turbine analytics the similarities are based on the four identified parameters, which are also closely related to the operational modes of the turbines. The algorithm used in this analysis is Normal Mixture, which is an effective classification algorithm for large datasets with overlapping clusters (Fraley & Raftery, 2007)."
2109.04359,dataset,16,2022-05-17,0,Grid Integration Nacelle Tower Pitch Rotor Meteorological Table 1. Wind turbine components within the dataset
2109.04359,dataset,39,2022-05-17,0,"Using the parameters defined in Figure 1, wind turbine operating modes can be identified using statistical clustering methods. For clustering the wind turbines dataset four parameters illustrated in Figure 1, were selected as the following:"
2109.04359,dataset,55,2022-05-17,1,"9. Acknowledgements Author would like to express gratitude to EDP Renewables Spain, for providing datasets such as wind turbines and solar panels, free of charge available to research community. Also great gratitude to CFREF (Canadian First research Excellent Fund) for providing the opportunity working on green energy technologies."
2109.04359,dataset,76,2022-05-17,0,"As nature of wind is turbulent with rapid fluctuations, a wind turbine may operate in variety of modes within relatively short period of time. Monitoring rotational speed ratio over time, requires consistent operational conditions such as wind speed and torques within the gearbox. This paper also introduces the concept of clustering such as Normal Mixture algorithm for dividing operating datasets into consistent subgroups, which are used for long term monitoring."
2109.04359,dataset,86,2022-05-17,0,"There have been extensive research into gear tooth wear, fatigue, pitting and microcracks conducted with reliability engineers for decades. In order to analyses different types of gear tooth failures, more detailed dataset with higher sampling frequency in addition to oil analysis is required (Zeng et al., 2020). Given the available dataset, analysis of gear ratio change over time is feasible, which is an indication of gear tooth wear (Jiang et al., 1999)."
2109.04359,dataset,94,2022-05-17,0,"Power Curve Analysis Power curves (active power generated versus wind speed) have been used within the wind energy industry for performance measuring and production prediction under different wind regimes (Sohoni et al., 2016). Table 2, illustrates the power curves of the wind turbines within the analysis dataset. Although power curves are similar in shape, they also exhibit differences which analysis of the detailed cause, are out of scope for this paper. As illustrated in Table 2, Turbine T06 exhibits the most clearly"
2109.05366,code,98,2022-05-17,0,"In this work we show that these limitations are not inherent in GPUfs design, and demonstrate how to achieve high performance that is singificantly faster than the original design, and even exceeds the throughput of a commonly-used traditional CUDA-based [8] baseline where files are accessed from the CPU code. The key to our optimization is a GPU-side I/O readahead prefetcher and page cache replacement mechanism. Together they alleviate the GPUfs performance limitations, by adjusting the GPU I/O layer to match the characteristics of the operating system I/O mechanisms and PCIe."
2109.05366,"data, code",154,2022-05-17,0,"I/O prefechers. [15] introduces a new algorithm that makes the minimum number of I/O instructions to service the I/O requests. [11] proposes an I/O signature-based prefetcher which mainly targets to detect the I/O pattern of an application and issues the requests as early as possible. [21] raises the issue of performing more aggressive I/O prefetching in terms of speculation and data size. Our work follows this direction but also considers other factors as well, like the PCIe data transfers, because of the complexity of a heterogeneous CPU-discrete GPU system. [13] pre-executes a fragment of code, via a pre-execution thread that runs at the same time as the main thread, in order to prefetch I/O requests. This solution cannot be efficiently applied for GPUs because it would require code divergence and would raise performance issues."
2109.05366,"data, data available",43,2022-05-17,0,"(6) If the data is not found in the private buffer, then the threadblock issues a read request to the CPU with size equal to GPUfs PAGE_SIZE + PREFETCH_SIZE (PREFETCH_SIZE is static and is defined before execution)."
2109.05366,"data, data available",58,2022-05-17,0,"(4) If there is a page cache miss, the threadblock allocates a new page and searches if the data is available in its private buffer. (5) If the data is in its private buffer, it updates the page cache, returns the data to the user-level buffer and continues execution."
2109.05366,"data, dataset",114,2022-05-17,0,"GPUfs [26] is the recently introduced system infrastructure which allows GPU threads to access files directly from GPU kernels. GPUfs provides standard POSIX-like APIs (e.g., (read()/write()) for GPU threads to perform file I/O, thus reducing the programming complexity, it implements a local page cache in the GPU memory, allows access to very large datasets and enables applications to perform data-driven, e.g., indexed-based, data accesses efficiently. GPUfs passes the I/O requests to the CPU, yet these remain invisible to the programmer, who only uses a standard and convenient I/O abstraction from GPU kernels."
2109.05366,"data, dataset provided",70,2022-05-17,0,We run a simple experiment to evaluate the sequential I/O performance of GPUfs by using it to move 960MB file into GPU memory. The goal is to compare the effective I/O bandwidth that GPUfs can achieve with the bandwidth of reading the same file from the CPU (no data transfer to GPU). We expect that GPUfs would provide full pipe-lining and therefore achieve similar bandwidth.
2109.05366,"data, dataset, data available",71,2022-05-17,0,"Sequential pattern is among the most common. For example, it is employed in deep neural networks [9] inference and training when reading the input dataset [5]. Such sequential access to data is very popular in high performance computing. In addition, I/O benchmark suites for manycore/multi-node architectures [1] consider sequential access as one of the most popular in data-intensive applications."
2109.05366,"data, dataset, used dataset",203,2022-05-17,0,"GPUs are broadly used in I/O-intensive big data applications. Prior works demonstrate the benefits of using GPU-side file system layer, GPUfs, to improve the GPU performance and programmability in such workloads. However, GPUfs fails to provide high performance for a common I/O pattern where a GPU is used to process a whole data set sequentially. In this work, we propose a number of system-level optimizations to improve the performance of GPUfs for such workloads. We perform an in-depth analysis of the interplay between the GPU I/O access pattern, CPU-GPU PCIe transfers and SSD storage, and identify the main bottlenecks. We propose a new GPU I/O readahead prefetcher and a GPU page cache replacement mechanism to resolve them. The GPU I/O readahead prefetcher achieves more than 2× (geometric mean) higher bandwidth in a series of microbenchmarks compared to the original GPUfs. Furthermore, we evaluate the system on 14 applications derived from the RODINIA, PARBOIL and POLYBENCH benchmark suites. Our prefetching mechanism improves their execution time by up to 50% and their I/O bandwidth by 82% compared to the traditional CPU-only data transfer techniques."
2109.05366,database,77,2022-05-17,0,"To evaluate the performance impact of the page size in such applications, we run the Mosaic benchmark [23] which creates an image collage from multiple tiny images fetched at input-dependent location from a large database (19GB). Each tiny image is 4KB. We run the application with GPUfs configured to use 4KB and 64KB pages. We observe that smaller pages result in 45% higher performance compared to 64KB."
2109.1263,"data, data open-source , open-source",328,2022-05-17,0,"includes a huge number of people, responses are commonly very quick. This platform facilitates humans’ fundamental social instincts. Through sharing on Twitter, users can easily express their opinions on everything at any time. Connected friends or followers (on Twitter) immediately obtain the information on the current situations in the lives of people. This, in turn, contributes to another emotion of humans—i.e., the innate need for knowing the current life situations of people. The user interface (UI) of Twitter is not only real-time but also easy to use. It is understood instinctively and naturally -i.e., the Twitter UI has a very intuitive nature. If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes."
2109.1263,"data, data open-source , open-source",346,2022-05-17,0,"If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes. These brands are chosen since they are all located in the USA, and they have a good, appropriate, and active twitter account that can be dug to obtain information, and had many campaigns over the years. Regarding celebrities, almost the same reason was the case, and their reputation and connection to the companies were a major concern. The present study utilized R studio, R language, and Twitter developer API for data mining as it is an open-source instrument, has many useful packages, and has result visualization ability. Appropriate data were gathered by searching hashtags and keywords relating to companies and celebrities. Then, after they were cleansed and made them meaningful, the analysis part was carried out through the two above-mentioned methods."
2109.1263,"data, dataset provided",148,2022-05-17,0,"The current data age is known for the rapid development in the measure of data and information that are electrically collected, stored, provided. A considerable portion of business information is stored in basically-unstructured text documents. According to Merrill Lynch and Gartner, 85% of a corporate datum is captured and stored in an unstructured format. The equivalent study also suggested that the size of such non-structurally-stored information is increasing every 18 months. Since knowledge provides power in the current business world and is derived from information and data, organizations which make eﬀective and eﬃcient use of their text information sources enjoy the crucial knowledge to make better choices, inducing a competitive advantage over the organizations that lag behind. Thus, the need for text analytics and mining becomes part of the all-inclusive view of the current organizations."
2109.1263,"data, dataset provided",162,2022-05-17,0,"Today, as [12] mentioned, ""Big Data” opportunities are a lot and in fact they provide manual ways for illogically dealing with sentiment analysis and add to the need for making robotized devices for the analysis of textually-expressed consumer sentiment. The present study selected Twitter as the case-study social media since it can be seen as an internet-based short message service (SMS) extension. IAs Jack Dorsey, the co-founder and co-creator of Twitter, said: ""...We came across the word ’twitter’, and it was just perfect. The deﬁnition was ’a short burst of inconsequential information,’ and ’chirps from birds’. And that’s exactly what the product was."" Twitter functions as a utility via which individuals can send SMSs throughout the world. It allows for continuously becoming heard and receiving answers. As the audience of Twitter"
2109.1263,"data, package",194,2022-05-17,0,"11- Install Rstem and sentiment packages 12- Make use of the classify_emotion function that returns a class data frame object, consisting of seven columns (including disgust anger, joy, fear, surprise, sadness, and best_ﬁt) and a row for each of the documents 13- Replace such NA values 14- Employ another function, i.e., classify_polarity(), that is provided by the sentiment package, for the classiﬁcation of tweets into two groups, namely neg (negative sentiment) and pos (i.e., positive sentiment) (It should be noted that the overall tweet sentiment is considered as neutral when this ratio is found to be 1) 15- Create consolidated results in a data frame from these two functions 16- Sort and rearrange the data within the frame 17- Generate a single function so that it is used by the tweets of each business and the sentiment can be plotted for each of the businesses 18- Likewise, plot the polarity distribution of the tweets 19- Try to obtain a sense of the tweets’ overall content through the word clouds."
2109.1263,download,108,2022-05-17,0,"8- Download both negative and positive English opinion/sentiment words (nearly 16000 words) 9- Add some industry-speciﬁc and/or particularly emphatic terms, depending on the requirements 10- Generate a function in R that calculates the raw sentiment by the simple matching algorithm by considering the following: # Remove digits, punctuations, and control characters # Convert them all into the lower sentence case # Divide each of the sentences by using space-delimiter words # Obtain the Boolean matches of the words with the negative and positive opinion-lexicon # Obtain the score in the form of the total positive sentiment dedicated from the total negative sentiment"
2110.0282,code,10,2022-05-17,0,E.1. Randomized Powering algorithm. The pseudo-code for estimating
2110.0282,code,72,2022-05-17,0,"E.2. Adaptive rank selection algorithm. The pseudocode for adaptive rank selection by a-priori error estimation is given in Algorithm E.2. The code is structured to reuse use the previously computed Ω and Y , resulting in signiﬁcant computational is estimated from q iterations of the randomized power method E savings. The error (cid:107) (cid:107) U ˆΛU T . on the error matrix A −"
2110.0282,"data, dataset",137,2022-05-17,1,"where H(ˆθ) Rd×d is the Hessian of the loss function at the solution ˆθ, for each datapoint aj. The main computational challenge is computing the inverse Hessian vector product H −1(ˆθ) θl(ˆθ, aj). When n is very large, we can also subsample the data and average (6.1) over the subsample to estimate ALOOCV. Since ALOOCV solves the same problem with several right-hand sides, blocked PCG methods (here, Nystr¨om blocked PCG) are the tool of choice to eﬃciently solve for multiple righthand sides at once. To demonstrate the idea, we perform numerical experiments on ALOOCV for logistic regression. The datasets we use are all from LIBSVM [6]; see Table 5."
2110.0282,"data, dataset",179,2022-05-17,0,"C.3. Kernel Ridge Regression. We converted the binary classiﬁcation problem to a regression problem by constructing the target vector as follows: We assign +1 to the ﬁrst class and -1 to the second class. For multi-class problems, we do one-vs-all classiﬁcation; this formulation leads to multiple right hand sides, so we use block PCG for both methods. We did no data pre-processing except for EMNIST, MiniBooNE, MNIST, and Santander. For EMNIST and MNIST the data matrix was scaled by 255 so that its entries lie in [0, 1], while for MinBooNE and Santander the features were normalized by their z-score. The number of random features, mrf from 103 for j = 1, . . . , 9. For adaptive Nystr¨om PCG we capped the linear grid mrf = j the maximum rank for the preconditioner at (cid:96)max = and used a tolerance of 40 for the ratio ˆλ(cid:96)/nµ on all datasets."
2110.0282,"data, dataset",63,2022-05-17,0,"6.4. Large scale ALOOCV experiments. Table 7 summarizes results for block Nystr¨om PCG and block CG on the larger datasets. When µ = 10−4, block Nystr¨om PCG oﬀers little or no beneﬁt over block CG because the data matrices are very sparse (see Table 5) and the rcv1 problem is well-conditioned (see Table 13)."
2110.0282,"data, dataset",83,2022-05-17,0,"C.1. Ridge regression experiments. Most of the datasets used in our ridge regression experiments are classiﬁcation datasets. We converted them to regression problems by using a one-hot vector encoding. The target vector b was constructed by setting bi = 1 if example i has the ﬁrst label and 0 otherwise. We did no data pre-processing except on CIFAR-10, where we scaled the matrix by 255 so that all entries lie in [0, 1]."
2110.0282,"data, dataset",83,2022-05-17,0,"Table 13: For µ = 10−4 the Hessian is well-conditioned for both datasets, so there is little value to preconditioning. For µ = 10−8, the ill-conditioning of the Hessian increases signiﬁcantly, making preconditioning more valuable. Furthermore, as ALOOCV uses Block PCG on at least several batches of data points, the cost of constructing the preconditioner is negligible compared to the cost of solving the linear systems (see Table 7 in subsection 6.3)."
2110.0282,"data, dataset",87,2022-05-17,0,"6. Applications and experiments. In this section, we study the performance of Nystr¨om PCG on real world data from three diﬀerent applications: ridge regression, kernel ridge regression, and approximate cross-validation. The experiments demonstrate the eﬀectiveness of the preconditioner and our strategies for choosing the rank (cid:96) compared to other algorithms in the literature: on large datasets, we ﬁnd that our method outperforms competitors by a factor of 5–10 (Table 3 and Table 10)."
2110.0282,dataset,1,2022-05-17,0,Dataset
2110.0282,dataset,105,2022-05-17,0,"6.3.2. Experimental overview. We perform two sets of experiments in this section. The ﬁrst set of experiments uses Gisette and SVHN to test the eﬃcacy of Nystr¨om sketch-and-solve. These datasets are small enough that we can factor H(θ) using a direct method. We also compare to block CG and block PCG with the computed Nystr¨om approximation as a preconditioner. To assess the error due to θl(ˆθ, aj). For any putative an inexact solve for datapoint aj, let x(cid:63)(aj) = H −1(θ)"
2110.0282,dataset,11,2022-05-17,0,Table 8: Kernel ridge regression datasets and experimental parameters.
2110.0282,dataset,123,2022-05-17,0,"6.5.3. Experimental results. Tables 9 to 11 summarize the results for the KRR experiments. Table 9 shows that both versions of Nystr¨om PCG deliver better performance than random features preconditioning on all the datasets considered. Nystr¨om PCG also uses less storage. Table 10 shows that Nystro¨om PCG yields better performance than random features PCG on the larger scale datasets when both are restricted to ranks of 1, 000. Table 11 shows the adaptive strategy proposed in subsection 5.4.2 to select (cid:96) works very well. In contrast, it is diﬃcult to choose mrf for random features preconditioning: the authors of [2] provide no guidance except for the polynomial kernel."
2110.0282,dataset,141,2022-05-17,0,"version uses the oracle best value of (cid:96) found by grid search (from the same grid used to select mrf) to minimize the total runtime, and the second is the adaptive algorithm described in subsection 5.4.2. The grid for (cid:96) and mrf is restriced to less than 10, 000 to keep the preconditioners cheap to apply and store. The adaptive algorithm for each dataset was initialized at (cid:96) = 2, 000, which is smaller than 0.05n for all datasets. For 105, we restricted both (cid:96) and mrf to 1, 000, which corresponds the datasets with n to less than 0.01n. We then run both algorithms till they reach the desired tolerance or the maximum number of iterations are reached."
2110.0282,dataset,172,2022-05-17,0,"We now give the details of the random features experiments. For Shuttle-rf we used random features corresponding to a Gaussian kernel with bandwidth parameter σ = 0.75, we set µ = 10−8/n. For smallNORB-rf we used ReLU random features 10−4. For Higgs we normalized the features by their z-score and we with µ = 6 used random features for a Gaussian Kernel with σ = 5 and regularization µ = 10−4. Similarly for YearMSD, we normalized the matrix by their z-score and used random features for a Gaussian kernel with σ = 8 and µ = 10−5. The sketch size for R&T was d, 2d selected from , to prevent the cost of the forming and applying preconditioner } { from becoming prohibitive. We selected the AdaIHS parameter ρ from the same grid used for the regularization path experiments. We also capped the sketch size for AdaIHS for each dataset by the sketch sized used for R&T."
2110.0282,dataset,19,2022-05-17,0,Table 7: ALOOCV: Large datasets. Block Nystr¨om PCG outperforms block CG as µ becomes small.
2110.0282,dataset,31,2022-05-17,0,We run two sets of experiments. For the datasets with n < 105 we run oracle random features PCG against two versions of the Nystr¨om PCG algorithm. The ﬁrst
2110.0282,dataset,32,2022-05-17,0,"Table 6: ALOOCV: Small datasets. The error for a given value of µ is the maximum relative error on 100 randomly sampled datapoints, averaged over 20 trials."
2110.0282,dataset,39,2022-05-17,0,"6.5.1. Background. We brieﬂy review KRR [32]. Given a dataset of inputs R for i = 1, . . . , n and a kernel function R in the associated reproducing kernel Hilbert"
2110.0282,dataset,43,2022-05-17,0,"Table 3: Ridge regression: Nystr¨om PCG versus AdaIHS and R&T PCG. Nystr¨om PCG outperforms AdaIHS and R&T PCG in iteration and runtime complexity for both datasets. Additionally, Nystr¨om PCG requires much less storage."
2110.0282,dataset,54,2022-05-17,0,"For the large scale problems the adaptive algorithm for Nystr¨om PCG was initialized at (cid:96)0 = 500 and is capped at (cid:96)max = 4000. We set the solve tolerances for both algorithms to 10−10. As before, we sample 100 points randomly from each dataset."
2110.0282,dataset,56,2022-05-17,0,"C.2. ALOOCV. The datasets were chosen so that n and d are both large, the challenging regime for ALOOCV. The ﬁrst three datasets are binary classiﬁcation problems, while SVHN has multiple classes. For SVHN we created a binary classiﬁcation problem by looking at the ﬁrst class vs. remaining classes."
2110.0282,dataset,62,2022-05-17,0,"6.2.3. Random features regression. Tables 3 and 4 compare the performance of Nystr¨om PCG, AdaIHS, and R&T PCG for random features regression. Table 3 shows that Nystr¨om PCG performs best on all datasets for all metrics. The most striking feature is the diﬀerence between sketch sizes: AdaIHS and R&T require much"
2110.0282,dataset,62,2022-05-17,0,"The second set of experiments uses the larger datasets real-sim and rcv1.binary and small values of µ, the most challenging setting for ALOOCV. We restrict our comparison to block Nystr¨om PCG versus the block CG algorithm, as Nystr¨om sketchand-solve is so inaccurate in this regime. We employ Algorithm E.2 to construct the preconditioner for block Nystr¨om PCG."
2110.0282,dataset,69,2022-05-17,0,"consider the shuttle-rf dataset (subsection 6.2). The matrix G has dimension 43, 300 × 10, 000, while the preconditioner is based on a Nystr¨om approximation with rank (cid:96) = 800. Figure 1 shows the progress of the residual as a function of the iteration count. Nystr¨om PCG converges to machine precision in 13 iterations, while CG stalls."
2110.0282,dataset,7,2022-05-17,0,Dataset CIFAR-10 Guillermo smallNorb-rf shuttle-rf Higgs-rf YearMSD-rf
2110.0282,dataset,7,2022-05-17,0,Table 2: Ridge regression datasets.
2110.0282,dataset,73,2022-05-17,0,"All datasets either come with speciﬁed test sets, or we create one from a random 80-20 split. The PCG tolerance, σ, and µ were all chosen to achieve good performance on the test sets (see Table 11 below). Both methods were allowed to run for a maximum of 500 iterations. The statistics for each dataset and the experimental parameters are given in Table 8."
2110.0282,dataset,8,2022-05-17,0,Dataset ijcnn1 MNIST Sensorless SensIT MiniBooNE EMNIST-Balanced Santander
2110.0282,dataset,82,2022-05-17,0,"6.5.2. Experimental overview. We use Nystr¨om PCG to solve several KRR problems derived from classiﬁcation problems on real world datasets from [6, 38]. For 2/(2σ2)). We all experiments, we use the Gaussian kernel (cid:107) compare our method to random features PCG, proposed in [2]. We do not compare to vanilla CG as it is much slower than Nystr¨om PCG and random features PCG."
2110.0282,dataset,86,2022-05-17,0,"6.1. Preliminaries. We implemented all experiments in MATLAB R2019a and MATLAB R2021a on a server with 128 Intel Xeon E7-4850 v4 2.10GHz CPU cores 105), every numerical and 1056 GB. Except for the very large scale datasets (n experiment in this section was repeated twenty times; tables report the mean over the twenty runs, and the standard deviation (in parentheses) when it is non-zero. We highlight the best-performing method in a table in bold."
2110.0282,dataset,9,2022-05-17,0,Table 5: ALOOCV datasets and experimental parameters.
2111.13023,data,43,2022-04-21,0,"[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017. 2"
2111.14671,"code, github, code available",16,2022-04-21,2,"∗Equal contribution 2Download instructions, baselines, and code are available at: https://github.com/RolnickLab/"
2111.14671,dataset,29,2022-04-21,0,"In our experiments we focus on pristine shortwave radiation. Our dataset, however, allows the user to choose the desired target variables based on their needs."
2111.14915,data,15,2022-04-21,0,is trained on data from 2000-2010 to predict gentrifying neighborhoods from 2010-2020. The present
2201.02733,"dataset, github",89,2022-04-21,2,"3.2 Training HDFS Once the train sets have been generated, we deploy HDSF available in HDSF’s authors’ Github repository [8]. We trained HDSF with each of the five training sets (one original and four noisy dataset) over 20 steps, and generated models, on which we performed our tests using a separate test set (Figure 2 (b)). Thereafter we collected loss and accuracy scores for each model obtained for the same test set."
2201.02733,github,15,2022-04-21,0,[8] Hamid Karimi. 2020. HDSF Source Repository. https://github.com/hamidkarimi/
2201.08452,github,15,2022-04-21,0,-- repo_link_a nd _S HA https :// github . com / streamich / memfs
2201.08452,github,21,2022-04-21,0,"5.2 Basic usage This tool can either take JavaScript packages speciﬁed as GitHub repository links, or as npm packages."
2201.08452,github,25,2022-04-21,0,""" repo_link "" : "" https :// github . com / streamich / memfs "" , "" repo_commit_S HA "" : REDACTED FOR LENGTH"
2201.08452,"github, repo",11,2022-04-21,0,More examples are included in the npm-ﬁlter GitHub repo Readme.
2201.08452,"github, repo",12,2022-04-21,0,"To run npm-ﬁlter over GitHub repo links, use the following:"
2201.08452,"github, repo",24,2022-04-21,0,• repo_link: a link to a single GitHub repo to be analyzed • repo_link_and_SHA: link to a GitHub repo followed by a
2201.08452,"github, repo",42,2022-04-21,0,"• repo_list_file: a ﬁle containing a list of GitHub repo links to be analyzed. Each line of the input ﬁle must specify one repo link, with an optional whitespace delimited commit SHA to check the repo out at."
