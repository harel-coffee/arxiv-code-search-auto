id,pattern,token_count,update_date,label,para
1710.02907,"data, dataset",280,2022-04-21,0,"Experiment 2: In this set of experiments, we evaluated the ability of the proposed method to compress high dimensional data. For these experiments, the proposed compression method was deployed on each dimension and the performance metrics are averaged over all the dimensions. In this regard, Dataset 2 ( comprising of three medical images and the color version of lena.jpg image) was used to benchmark ZT-based algorithm with those of DCT and FWHT. It can be observed in Figs. 11a and 12a that ZT-based transform have competitive performance on Scan1 data for block sizes 4 128, performance of ZT-based 8, 32 algorithm plummeted in comparison with DCT and FWHT-based approaches. Experimentation with Scan2 image shows that ZT consistently compresses the MRI data better and faster than its counterpart for most of the block sizes considered as shown in Figs. 11b and 12b. With block size of 128 128, the performance of FWHT improves and became comparable with ZT. Better performance in compression and speed of implementation is again observed on the average for ZT-based method using the Scan3 image, and the performance is more pronounced when the block size is 128 128 as shown in Figs. 11c and 12c. Again the proposed algorithm using color version of Lena image was compared with those of DCT and FWHT. It was observed that for most of the block sizes, ZT consistently compresses better and faster than its counterparts as shown in Figs. 11d and 12d. For block size 4 and 8, zipper transform has slightly better compression capability and"
1811.11012,data,195,2022-04-21,0,"This section of the technical report is focused on two similar desktop applications, which will here be referred to as Intersection Viewer (abbreviated IV) Version 1 and Intersection Viewer Version 2. IV v1 (developed by Nick Hodge and J.T. Blevins, with maintenance by Noah Carter) parses SPaT data and displays it in an intuitive graphical user interface. IV v2 (developed by Jacob Hoyos, Matthew Dale, and Noah Carter) goes a step further by not only displaying the GUI but also acting as a server and relaying the SPaT data to a single speciﬁed client machine over a local network. In IV v2, said client then displays the GUI based on the received SPaT data. The intended use of these applications was to serve as a learning experience and a prototype for the application described in section 3 of this report, IV v3. In IV v3 (itself yet another prototype), the client-server relationship was transitioned to the cloud, and made accessible from all personal devices over LTE (and the Internet generally)."
1811.11012,"data, dataset",70,2022-04-21,0,"volunteers’ vehicles were mounted with BSM-broadcasting devices. (See the ﬁnal section of this technical report for a basic analysis of that particular dataset.) One aspect of such data analysis is visualization and simulation. Allowing an analyst to see the data in a helpful and intuitive format enables the analyst to make more informed and strategic decisions [2], [26]."
1912.09582,dataset,13,2022-04-21,0,for small datasets–a case with Dutch book reviews. arXiv preprint 1910.00896.
1912.09582,dataset,15,2022-04-21,1,Table 4: Sentiment Analysis accuracy scores on the 110k Dutch Book Reviews Dataset.
1912.09582,github,26,2022-04-21,0,"2https://deepset.ai/german-bert 3https://github.com/cl-tohoku/bert-japanese 4A monolingual Dutch model has also been made available at http://textdata.nl, but this this model was consistently"
2005.07667,"data, database, dataset",180,2022-04-21,0,"For that purpose, in recent years, with the extensive development of deep learning techniques, especially convolutional and recurrent neural networks, the results are drastically improving. There have been quite a few researches attempting to generate data processing results by directly linking records in the tables to the semantic meaning of the natural language input, such as [6] and [7]. However, these attempts are not scalable to big tables and are not reusable when the database schema is changed. More recent approaches use only the natural language input and the database schema and metadata to generate the queries. We review the most recent approaches in our research. Furthermore, the release of large annotated datasets containing questions and the corresponding database queries has additionally enhanced the ability to use deep learning or supervised techniques to tackle this problem. This has enabled the problem to evolve into a more complex task where the approaches should be domain independent and involving multiple tables with complex queries."
2005.07667,dataset,45,2022-04-21,0,"Dataset ATIS [8] GeoQuery [9] Restaurants [10], [19] Academic [11] Scholar [12] Yelp [13] IMDB [13] WikiSQL [15] Advising [14] Spider [16]"
2005.08622,dataset,90,2022-04-21,0,"In literature, multi-label classiﬁcation is an important ﬁeld in machine learning and it is strongly related to many realworld applications for example, in biomedical images annotation, document categorization and whatever problem which the instances inside the classes are not disjoint but they keep a hierarchical structure. In this work, we have conducted four empirical studies on different datasets to prove by experimental results the effectiveness and robustness of our proposed model, that can be applied as an extension to any Convolutional Neural Network."
2007.12442,"data available, data",74,2022-04-21,0,"Third, edge-based pub/sub middleware usually lacks any mechanism of access control, e.g., the topics are public. An attacker with knowledge of the publish/subscribe topics could inject carefully crafted information while receiving potentially sensitive data sent by the clients (AV-3). By leveraging access control mechanisms, we can also revoke access to byzantine nodes, hence providing a simple defense mechanism against replay attacks towards the broker."
2007.12442,open-source,14,2022-04-21,0,[54] The Eclipse Foundation. Eclipse Mosquitto - An open source MQTT
2008.01391,code,155,2022-04-21,0,"An SMT system with a code-switched parallel corpus was studied by Menacer et al. (2019) and Fadaee and Monz (2018) for Arabic-English language pair. The authors have manually translated or used back translation method to translate foreign words. The identiﬁcation of the language of the word is based on the orthography. Chakravarthi et al. (2018) used the same approach for Dravidian languages; they used the improved MT for creating WordNet, showing improvement in the results. For English-Hindi, Dhar et al. (2018) manually translated the code-switched component and shown improvements. Machine translation of social media was studied by Rijhwani et al. (2016) where they tackle the code-mixing for Hindi-English and Spanish-English. The same approach translated the main language of the sentence using Bing Translate API (Niu et al., 2018)."
2011.09069,data repos,121,2022-04-21,0,"In case of Facebook platform, the coverage levels of the two aggregators are found to be quite similar, though PlumX has an edge over Altmetric.com. Ortega (2020a) has also found that in collecting mentions from Facebook and Mendeley, Altmetric.com has performed poorly as compared to PlumX. One possible reason for Altmetric.com recording slightly lesser Facebook mentions is that Altmetric.com captures only public pages19 and excludes likes, shares, and comments, whereas PlumX includes shares, likes, and comments along with the interactions in user’s closed network. A similar argument is also given by Zahedi & Costas (2018b) behind the low Facebook mentions captured by Altmetric.com."
2011.09069,data repos,4,2022-04-21,0,(c) Mendeley
2101.00522,dataset,104,2022-04-21,0,"Table 3: The percentage of shift in pixel labels during adaptation for the cardiac dataset. A cell (i, j) in the table has three values. The ﬁrst value represents the percentage of pixels labeled i that are labeled j after adaptation. The second value represents the percentage of switching pixels whose true label is i - lower is better. The third value represents the percentage of switching pixels whose true label is j - higher is better. Bolded cells denote label shift where more than 1% of pixels migrate from i to j."
2103.00265,dataset,169,2022-04-21,0,"λ1 . The metric tensor is considered to be poorly conditioned if the condition number has a high value. (Odena et al., 2018) proposed to eschew the issue of computing the complete spectrum, which could be quite challenging, in favor of sampling random directions (essentially sampling small random values for εvk and empirically computing 1.45, and then adding a penalty to encourage these values to fall within a speciﬁc range. In practice they achieved good results by setting λmin to 1.0 and λmax to 20.0. Making λmax too small could have the eﬀect of making it too hard for the model to be responsive to the latent variables and setting λmin to be too large could have the eﬀect of making it impossible for the model to learn to give large regions in the space relatively constant density. In practice these hyperparameters would likely need to be tuned depending on the dataset in accordance with these concerns."
2103.15335,dataset,2,2022-04-21,0,3.1 Datasets
2103.16349,dataset,148,2022-04-21,0,"The very ﬁrst observation is that MLP itself is a also a strong baseline, which outperforms HI and state-of-the-art models across almost all datasets and prediction lengths. Regardless of this point, we take MLP as a basic model, and evaluate the the ensemble of MLP and HI. We operate weighted summation over MLP’s and HI’s outputs to get the ﬁnal prediction. The weights of two models are set as 0.5/0.5. From Table 4 and Table 5, it could be concluded that this hybrid model can obtain better results in many cases, which is especially evidential for the task of univariate forecasting. MLP + HI brings up to 32% relative improvement over HI and 45% relative improvement over MLP on MSE, and 20%, 27% relative improvement on MAE."
2104.02307,dataset,4,2022-04-21,0,Target (dataset)
2104.09994,"data, dataset",170,2022-04-21,0,"sources (network data but also sensor readings, operating system logs and telemetry data) about a network containing several IoT/IIoT devices. In [27], the authors propose a dataset collecting benign and volumetric attacks traﬃc traces for 27 IoT devices. The main purpose of this dataset is to evaluate volumetric attacks perpetrated against a network containing real commercial IoT devices. The dataset proposed in [28] was generated with the traﬃc of 2 home IoT devices under multiple attack scenarios. It also includes simulated Mirai traﬃc appearing to come from the IoT devices. Finally, IoT23 [29] is a dataset consisting of 20 captures that include malware activity as well as 3 captures of benign IoT traﬃc. To conclude, it is worthy to mention that there is a lack of dataset suitable for FL approaches detecting malware in IoT devices. Existing FL-based solutions must consider split centralized datasets in order to apply federated techniques."
2104.09994,database,42,2022-04-21,0,"be used as a decentralized database where each client would share its local model and retrieve the models of other clients when performing the aggregation. Thus, the framework would be totally decentralized without an entity coordinating the generated models."
2104.09994,"publicly available, dataset",7,2022-04-21,0,Table 1 Public IoT network datasets.
2105.04903,dataset,145,2022-04-21,0,"REFERENCES [1] Omar Adjali, Romaric Besançon, Olivier Ferret, Hervé Le Borgne, and Brigitte Grau. 2020. Building a Multimodal Entity Linking Dataset From Tweets. In Proceedings of the 12th Language Resources and Evaluation Conference. 4285–4292. [2] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2020. Open-Domain Question Answering Goes Conversational via Question Rewriting. arXiv preprint arXiv:2010.04898 (2020). [3] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 344–354."
2105.04903,dataset,177,2022-04-21,1,"Generating annotation candidates. We employ a pooling approach to generate an extended set of candidate mentions and entities. Three EL tools were used to annotate the dialogues: TagMe [28], WAT [46], and REL [57]. Each tool was employed in two ways: (i) the turn method, which annotates a single turn, irrespective of the conversation history, and (ii) the history method, which annotates each turn given the conversation history up to that turn. For the CAsT dataset, only user utterances were given to the EL tool, while for other datasets both system and user utterances were considered as conversation history. This is due to relatively long system utterances in the CAsT dataset, which makes infeasible for the EL tools to annotate the whole conversation history. To further improve the recall of our pool, we included the top-10 Wikipedia search results, using mentions as queries sent to the MediaWiki API.4"
2105.04903,dataset,35,2022-04-21,1,"The resources provided in this paper allow for further investigation of entity linking in conversational settings, can be used for evaluation or training of conversational EL systems, and complement existing conversational datasets."
2105.10148,dataset,12,2022-04-21,0,"valid dataset with a ratio of 9:1, train each algorithm on"
2105.10148,dataset,56,2022-04-21,0,"We then evaluate all the algorithms on the hard dataset in both BSuite and DM Control environments. The episodes are generated from a mixture of partially trained policies from a diﬀerent run, and the distribution of states is likely to have a quite diﬀerent coverage from the distribution generated by the target distribution."
2106.07691,"data, dataset",107,2022-04-21,1,"only 6.09% of T5base’s attempts were AP T . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5base would generate better paraphrases, as measured by a higher chance of passing AP T on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table 2, which shows that T5base was able to generate an AP T passing paraphrase for 84.8% of the sentences in TwitterPPDB."
2106.13764,"data, data https",51,2022-04-21,0,"[35] Sybu Data. Sybu javascript blocker – google chrome extension. https: //sybu.co.za/wp/projects/js-blocker/, 2016. Accessed: 2020-05-02. [36] Houssein Djirdeh. Javascript | 2019 | the web almanac by http archive. https://almanac.httparchive.org/en/2019/javascript, 2019. Accessed: 2020-01-2."
2106.13764,database,138,2022-04-21,0,"SlimWeb’s JS classiﬁcation service crawls popular web pages to identify JS elements used in these pages, and then employs the classiﬁer to label these elements and store their categories in a database. It periodically updates and shares the labels with the users’ browser plugins. On the other hand, SlimWeb’s browser plugin is responsible for blocking noncritical JS elements. These elements are identiﬁed based on the labels received from the service. When a web page is requested by the user, the plugin ﬁrst checks if a label is locally available for each JS element, such that non-critical elements are immediately blocked. In the case of a label absence, the plugin considers the corresponding JS element as critical and requests it from the web."
2107.10483,"data, dataset provided",135,2022-04-21,0,"Assumption 4 ENCO relies on neural networks to determine the conditional data distributions ...). Hence, for providing a guarantee, we assume that in the graph learning step the neural p(Xi| networks have been sufﬁciently trained such that they accurately model all possible conditional ...). In practice, the neural networks might have a slight error. However, as long as distribution p(Xi| enough data, network complexity, and training time is provided, it is fair to assume that the difference between the modeled distribution and the true conditional is smaller than an arbitrary constant (cid:15), based on the universal approximation theorem (Hornik et al., 1989). For the limited data setting, see Appendix B.2.3."
2108.02756,dataset,140,2022-04-21,0,"Inspired by the defense mechanism presented in [20], we could use our method to generate boundary samples, which are examples that fall near the decision boundary of a pre-trained classiﬁer. Such examples can be used for adversarial training to enhance robustness or for knowledge distillation (e.g., see [20, 21]). Here, we focus on synthesizing such examples regardless of the application of interest. Unlike [20], we achieve this without the need to train a CGAN with a full dataset, and with the additional ﬂexibility of choosing a desired soft output pd (e.g., uniform over only a subset of the classes). The ﬁrst four synthesized samples in the bottom row of Figure 1 are all examples of near-decision-boundary samples."
2108.02756,dataset,89,2022-04-21,0,"We augment a repeated version of vector z to create a small training dataset and utilize the back-propagation algorithm [14]. Given the two objectives of BOSS, and the utilization of the adjustable parameters of network h, φ, we introduce the surrogate losses Lh(p(g(z ; φ) ; θ), pd) and Lg(g(z ; φ), xd), and use the back-propagation algorithm to optimize φ based on the minimization"
2109.08237,"data, dataset",225,2022-04-21,0,"In the ﬁrst experiment, the DL algorithm was trained on the different datasets. Figure 6 displays an example from the test set, which shows the gold standard images and the DL reconstructions for data undersampled with R = 4. Generally the visual quality of all the images (both gold standard and reconstructed ones) reduces with increased JPEG compression level (left-to-right in Figure 6); this is expected from compressed data. However, the NRMSE metric shows an unexpected effect: it improves with the compression, i.e. the reconstruction error reduces although the image visual quality degrades. The reason for this phenomenon is that in retrospective experiments the reconstruction quality is measured w.r.t. to a ""gold standard"" image that is also processed (see the pipeline in Figure 1c); the error metric is therefore blind to data processing. Strikingly, the NRMSE could show a misleading improvement even when the human eye cannot see any difference, as demonstrated in the left two columns of Figure 6: although the reconstructions from NC and QF = 75 are visually similar, the NRMSE of the latter is lower by 30%. This reﬂects the subtle bias induced by the pipeline of subtle data crime II."
2109.08237,dataset,81,2022-04-21,0,"[21] A. D. Desai, A. M. Schmidt, E. B. Rubin, C. M. Sandino, M. S. Black, V. Mazzoli, K. J. Stevens, R. Boutin, C. Re, G. E. Gold, et al., “SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation,” in Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021."
2109.12907,dataset,10,2022-04-21,0,Table 3: Usage of relations in the dataset.
2110.05428,"data, dataset",151,2022-04-21,0,"Evaluation Metrics To measure the identiﬁability of latent causal variables, we compute Mean Correlation Coefﬁcient (MCC) on the validation dataset, a standard metric in the ICA literature for continuous variables. MCC reaches 1 when latent variables are perfectly identiﬁable up to permutation and componentwise invertible transformation in the noiseless case (we use Pearson correlation and rank correlation for linearly and nonlinearly related latent processes, respectively). To evaluate the recovery performance on causal relations, we use different approaches for (1) linear and (2) nonlinear transitions: (1) the entries of estimated state transition matrices are compared with the true ones after permutation, signs, and scaling are adjusted, and (2) the estimated causal skeleton is compared with the true data structure, and Structural Hamming Distance (SHD) is computed."
2110.05428,dataset,64,2022-04-21,1,"KiTTiMask The KiTTiMask dataset consists of pedestrian segmentation masks sampled from the autonomous driving vision benchmark KiTTi-MOTS. For each given frame, the position (vertical and horizontal) and the scale of the pedestrian masks are set using measured values. The difference in the sample time (e.g., ∆t = 0.15s) generates the sparse Laplacian innovations between frames."
2110.11575,"github, repo",38,2022-04-21,0,"Make sure the target repo (the repo to be analyzed, not the repo of this tool) is on your machine. In this demo, the target repo is downloaded from a GitHub repo:"
2110.11575,repo,67,2022-04-21,0,# make sure [ repo path ] is the target repo path # the [ output path ] can be anywhere you desire git_stats generate -p [ repo path ] -o [ output path ] # e . g . git_stats generate -p / home / user / git - stats / Weasis -o # / home / user / git - stats / Weasis - analytics
2110.15667,database,45,2022-04-21,0,"[50] L. Deng, “The mnist database of handwritten digit images for machine learning research [best of the web],” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012."
2110.15667,dataset,1,2022-04-21,0,Dataset
2110.15667,dataset,33,2022-04-21,0,"[51] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,” arXiv preprint arXiv:1708.07747, 2017."
2111.03516,"data, dataset",34,2022-04-21,0,"Figure 2 shows the F1 comparisons for each of the data augmentation methods (SMOTE, B SMOTE, ADASYN, SVM-SMOTE, SL-SMOTE, SMOTE-RSB, and CFA) on 25 datasets using"
2111.03516,dataset,16,2022-04-21,1,classifier on the ‘PIMA’ dataset we can see that SMOTE-based methods had better performance
2111.03516,dataset,16,2022-04-21,1,§ Pima Indians Diabetes dataset: A binary-class dataset used to predict whether or not a
2111.03516,dataset,18,2022-04-21,0,"next best with 3 datasets. Baseline, B-SMOTE, and ADASYN had the highest AUC-ROC in 2"
2111.03516,dataset,20,2022-04-21,1,"§ Ecoli dataset: This dataset is a multi-class dataset, with 8 classes. The problem is to classify"
2111.03516,dataset,31,2022-04-21,0,"and Multilayer Perceptron (MLP) models. Several alternative ML models were used because dif ferent models find different decision boundaries for a given dataset, differences that could impact"
2111.04287,python,39,2022-04-21,2,"We ﬁll the gap between the rapid development of algorithms on paper and the lack of practice in real world with the introduction of BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms."
2111.13172,"data, data repository",5,2022-04-21,0,UDM: Unified Data Management
2112.03471,dataset,2,2022-04-21,0,4.1.1 Datasets
2201.00732,database,26,2022-04-21,0,"[49] Luca Biferale, Fabio Bonaccorso, Michele Buzzicotti, and Patricio Clark di Leoni, “TURB-Rot. A large database of 3d"
2201.00732,dataset,106,2022-04-21,0,"FIG. 11. (Main panel) Mean deviation between the predicted and true value of Ω for each of the ten classes considered, the errorbars show the root mean square of the diﬀerence between Ωpred and Ωtrue. (Inset) Mean squared diﬀerence between prediction and true Ω normalized to the square of the correct rotation value for each of the ten classes. The average is taken over the validation dataset, while the predictions are obtained as in Fig. 10, by the neural network, DCNN, or as the mean, ΩBay and most likely, Ω∗"
2201.00732,dataset,3,2022-04-21,0,B. Dataset extraction
2202.12674,github,4,2022-04-21,0,3https://github.com/SC-SGS/PLSSVM/blob/v1.0.1/utility scripts/generate
2202.12674,"github, data, data https, python",59,2022-04-21,0,"[18] H. E. L. Cagnin et al., “A portable OpenCL-based IEEE, 2015. [19] S. Raschka et al., “Machine learning in python: Main developments and technology trends in data science, machine learning, and artiﬁcial intelligence,” MDPI, 2020. https://github.com/ “svm-gpu,”"
10.3390_en10040547,"data available, data",161,,0,"The diesel engine data used in the performance estimations of the WHR systems were the exhaust gas temperature, scavenge air temperature, air mass ﬂow rate and jacket water cooling heat rate. The exhaust gas temperature is deﬁned as the temperature after mixing of the turbocharger and power turbine exhaust gas streams, and the scavenge air temperature is deﬁned as the air temperature after the turbocharger compressor. The exhaust gas temperature, scavenge air temperature and air mass ﬂow rate were based on measured values from the actual engine installed on the container vessel, while the jacket water heat amount was computed from the online CEAS engine calculation tool provided by MAN Diesel & Turbo [31]. Figure 3 depicts the variation of the four engine parameters mentioned above. The markers represent data points that were available from measurements or CEAS data. The values were interpolated for the remaining engine loads."
10.3390_en10040547,"data available, dataset, data",10,,0,Table 3. Relevant data available in the datasets.
10.3390_en10040547,dataset,101,,0,"For Dataset 3, the electrical power generation was estimated with discrepancies of 1.0%, 3.1% and 9.8% at main engine loads of 90%, 75% and 50%, respectively; see Figure 4i. The deviations for the HP and LP mass ﬂow rates in Figure 4j were below 4% and 18%, respectively. The HP and IP pressure predictions (Figure 4k) were within 7% and 10% of the measured values, respectively, while the temperature predictions (Figure 4l) were within 4%."
10.3390_en10040547,dataset,108,,0,"The input parameters to the design model, which were not mentioned in the above, were given the assumed values listed in Table 1, and the service steam production was assumed to be zero. The design model was then used to estimate the power output at 90% engine load for Dataset 1 and to calculate the design point parameters used in Equations (2)–(7). For the remaining measured engine load points, the off-design model was used to estimate the variations in steam turbine generator output, steam pressures, steam mass ﬂow rates and steam temperatures."
10.3390_en10040547,dataset,120,,0,"Figure 4. SRC model validation with three datasets (DS); values normalized with measured value at 90% main engine load. (a) DS 1, power; (b) DS 1, ﬂow; (c) DS 1, pressure; (d) DS 1, temperature; (e) DS 2, power; (f) DS 2, ﬂow; (g) DS 2, pressure; (h) DS 2, temperature; (i) DS 3, power; (j) DS 3, ﬂow; (k) DS 3, pressure; (l) DS 3, temperature"
10.3390_en10040547,dataset,242,,0,"The comparison with the measurements in Dataset 2 indicates that the model overestimates the power (see Figure 4e) and HP mass ﬂow rate (see Figure 4f) up to 14.3% and 51.2%, respectively. Note that the high percentage value for the HP mass ﬂow rate prediction was due to the values being relatively small at 50% main engine load. The overestimation of HP pressure was up to 8.2%; see Figure 4g. Accurate estimations were achieved for the LP steam ﬂow (maximum difference: 0.21 kg/s), the pressure between the HP and LP turbines (maximum difference: 0.12 bar) and the temperatures (maximum difference: 6.2 ◦C). This indicated that the assumption of zero service steam production might not be correct for Dataset 2. By taking out saturated service steam from the HP steam drum, the mass ﬂow rate to the HP turbine would drop. As a consequence, the HP steam pressure would drop according to the Stodola constant calculated for the HP turbine; see Equation (6). The drops in HP steam ﬂow and pressure would result in a drop in the SRC turbine power production, thus resulting in a better match between the estimated and measured values. The relative deviations of the temperature predictions in Figure 4h were below 3%."
10.3390_en10040547,"dataset, data",129,,0,"The model of the SRC system was validated by comparing the model outputs with measured data from the dual pressure SRC system on a Maersk Line container vessel. The data were measured during the sea trials. The data contain measurements from three different days collected during a time period of 10–15 min. The values used in the validation were averaged over these time intervals for each of the three days. All three datasets contain measurements at 90%, 75% and 50% main engine load, while Dataset 1 contains an additional measurement at 40% main engine load. The measured data (proprietary information of Maersk), which were used in the validation, are listed in Table 3."
10.3390_en10040547,"dataset, data",238,,0,"Figure 4 shows the comparison of the SRC model output and the measured values, and Table 3 lists the details on the location of mass ﬂow rate, pressure and temperature measurements. In Figure 4a are compared estimated and measured values of the electric power generated in the steam turbine generator for Dataset 1. An exact match between the model and the measured data was obtained at 90% main engine load, since this point was selected as the design point for the SRC model validation. At 75%, 50% and 40% main engine load, the model estimations were within 2.3%, 2.4% and 11.9% of the measured values, respectively. The larger deviation at 40% main engine load could be due to the zero ﬂow of LP steam to the turbine at this load point, which might result in inaccuracies in the turbine efﬁciency function in Equation (5). The maximum relative deviations for the HP and LP mass ﬂow rates (see Figure 4b) were below 4% and 15%, respectively. The relative deviations for the pressure predictions in Figure 4c were below 2% for the HP pressure and below 22% for the IP pressure, while the relative deviations for the temperature predictions displayed in Figure 4d were below 2%."
10.3390_en10040547,open-source,72,,0,"32. Bell, I.H.; Wronski, J.; Quoilin, S.; Lemort, V. Pure and Pseudo-pure Fluid Thermophysical Property Evaluation and the Open-Source Thermophysical Property Library CoolProp. Ind. Eng. Chem. Res. 2014, 53, 2498–2508. International Maritime Organization. Sulfur Oxides (SOx) Regulation 14; Technical Report; International Maritime Organization: London, UK, 2015."
10.3390_en10040547,open-source,83,,0,"The heat inputs to the SRC and ORC units were delivered from the exhaust gases, scavenge air and jacket water, while heat was rejected to the sea water. Constant values of speciﬁc heat capacity were assumed for the exhaust gases and scavenge air, and the jacket water and sea water were modeled using the properties of water. The thermodynamic properties of water and the organic ﬂuids were computed using the open source software Coolprop [32]."
10.3390_en10040547,"used dataset, dataset, data",249,,0,"The ﬁrst step in the validation was to select the 90% main engine load point in Dataset 1 as the design point for the SRC unit. This data point was then used in the design model to deﬁne the design of the SRC unit. Values for the temperature of the exhaust gases and scavenge air were used directly from the dataset along with HP and LP boiler pressures and mass ﬂow rates and the intermediate pressure (IP) between the HP and LP turbines. The exhaust gas mass ﬂow rate was calculated based on ﬂow characteristic curves for the turbochargers and power turbine. The scavenge air mass ﬂow rate was assumed to be equal to that of the exhaust gases, neglecting the mass ﬂow rate of fuel, which is <5% of the exhaust gas mass ﬂow rate. The approach temperature difference of the HP and LP drums was extracted based on the measured drum feed temperatures and pressures. Similarly, the degree of superheating was extracted from the measured HP steam (HP turbine inlet) temperature and pressure. It was assumed that the HP and LP turbines have the same isentropic efﬁciency. The isentropic efﬁciency of the turbines were tuned in order to match the estimated steam turbine generator output (ηg · ˙Wt) with the measured value. In the validation, the constraints listed in Table 1 were not employed."
10.3390_en10070859,"database, data",142,,0,"Production data, such as production rate, wellhead pressure, watercut, and other measurable information on the surface, is regularly observed and collected in a database. This database is used for history-matching to calibrate a more reliable reservoir model. Downhole data can improve results of history-matching, but needs additional investments due to the need for expensive tools and the operational difﬁculties encountered. Nevertheless, to identify each fracture property in unconventional reservoirs with multi-stage fracturing, it is essential to allocate the total production rate to each fracture based on downhole measurements. A PLT is an effective tool when investments have not been made in permanent downhole tools. However, PLT surveys are charged according to the number of surveys conducted; therefore, a minimum number of surveys should be conducted."
10.3390_en10091352,"data available, data",285,,0,"(i.e., the integral form of air pollutants discharged from a wide range of sources), the consumption of fossil fuel energy, and the meteorological variables in Macao on an annual basis. The number of hours with haze increased continuously from 0 to 766 h a year between 1990 and 2007 while the consumption of primary energy (including oil products, natural gas, and aviation kerosene) had increased by 130 percent. Multiple regression analyses have shown that gas oil/diesel, fuel oil, and natural gas consumed locally and the burning of aviation kerosene were signiﬁcantly associated with the number of hours per year with haze in Macao. A correlation analysis between monthly haze data and monthly records of meteorological variables has indicated that the number of hours with haze was strongly, signiﬁcantly associated with high atmospheric pressure that leads to the lower mixed layer height in the past decade. The combustion of oil products produces particulate matter, sulfur oxides, nitrogen oxides, and volatile organic compounds including non-methane hydrocarbons. All of these air pollutants are well-known sources of haze. Besides, around 150 commercial aircrafts, which emit a signiﬁcant amount of particulate matter in low troposphere, take off and land at the Macao International Airport each day [32]. It is possible that there is a signiﬁcant correlation between the number of humid hours (the hours with a relative humidity between 75 and 80 percent) each year/month and the number of haze hours per year/month. However, the hourly relative humidity data were not available from the Bureau’s publications."
10.3390_en10091352,dataset,18,,0,"where x and y are values of the variables, and n is the size of dataset."
10.3390_en10091352,"dataset, data",317,,0,"Energies 2017, 10, 1352 4 of 12 2.3. Association between Variables The association between two variables was characterized by Pearson’s correlation coefficient. The coefficient, r, was calculated as ()()()()===−−−−=niiniiniiiyyxxyyxxr12121 (5) where x and y are values of the variables, and n is the size of dataset. 2.4. Regression Analysis Multiple regression analysis was employed to assess the relationships between the independent variables and the dependent variable. The accuracy of the identified model was determined by the coefficient of determination, r2. The coefficient provides a measure of how well the collected data are predicted by the model [25]. It was calculated by: SSTSSESSTSSRr−==12 (6) where SSR is the sum of squared regression, SST is the sum of squared total, and SSE is the sum of squared error. 3. Results and Analysis 3.1. Haze and Meteorological Conditions Visibility is an integrative parameter that refers to the ability to see a distant object [25]. When air pollutants are emitted from anthropogenic sources, haze is formed under a stable atmospheric inversion layer, adversely affecting the visibility over the city. Haze has been a common phenomenon in winter in China’s cities [3,8,9,12,18,26]. In Macao, haze is defined as the condition under which visibility is equal to, or less than 5 km and relative humidity is less than 80 percent. The condition is recorded by trained meteorologists and confirmed by a forward scatter visibility meter. The Macao Meteorological and Geophysical Bureau keeps daily records of the number of hours with haze. Figure 1a shows the records for the period 1986 to 2016. (a) (b)Figure 1."
10.3390_en10091352,"dataset, data",339,,0,"2.4. Regression Analysis Multiple regression analysis was employed to assess the relationships between the independent variables and the dependent variable. The accuracy of the identified model was determined by the coefficient of determination, r2. The coefficient provides a measure of how well the collected data are predicted by the model [25]. It was calculated by: SSTSSESSTSSRr−==12 (6) where SSR is the sum of squared regression, SST is the sum of squared total, and SSE is the sum of squared error. 3. Results and Analysis 3.1. Haze and Meteorological Conditions Visibility is an integrative parameter that refers to the ability to see a distant object [25]. When air pollutants are emitted from anthropogenic sources, haze is formed under a stable atmospheric inversion layer, adversely affecting the visibility over the city. Haze has been a common phenomenon in winter in China’s cities [3,8,9,12,18,26]. In Macao, haze is defined as the condition under which visibility is equal to, or less than 5 km and relative humidity is less than 80 percent. The condition is recorded by trained meteorologists and confirmed by a forward scatter visibility meter. The Macao Meteorological and Geophysical Bureau keeps daily records of the number of hours with haze. Figure 1a shows the records for the period 1986 to 2016. (a) (b)Figure 1. (a) The number of hours with haze per year for the period 1986 to 2016; (b) The number of hours with haze per month for the period 1986 to 2016. Figure 1a shows that the number of hours with haze increased steadily from 0 in 1990 to 297 in 2002. However, it sharply increased to 694 in 2003. Between 2003 and 2008, the number of hours with 02004006008001986199119962001200620112016Haze (hours/year)YearEnergies 2017, 10, 1352"
10.3390_en10111898,code,69,,0,"19. Nanou, S.I.; Papathanassiou, S.A. Grid Code Compatibility of VSC-HVDC Connected Offshore Wind Turbines Employing Power Synchronization Control. IEEE Trans. Power Syst. 2016, 31, 5042–5050. [CrossRef] 20. Chaudhary, S.K.; Teodorescu, R.; Rodriguez, P.; Kjaer, P.C.; Gole, A.M. Negative sequence current control in"
10.3390_en10122158,"code package, package",215,,0,"the ESP-r simulation program for an analysis of the PV and building energy consumption, and ANSYS Fluent for a computational ﬂuid dynamics (CFD) analysis of the performance of the wind power conversion system [8,45]. ANSYS Fluent is one of the most-used CFD software offering various turbulence models based on the Reynolds-Averaged Navier Stokes (RANS) model. The power production of the wind turbines is estimated by an examination of the wind speed distribution around the applied system through CFD analyses. The ESP-r software package is recognized and used widely in more than 70 countries as an industry standard for the simulations. The authors employed ESP-r 11.1, which considers the energy use of heating, cooling, lighting, and PV energy generation. Thus, ESP-r has been used extensively to assess building energy applications, particularly as a simulation tool to compare various cities [14,29]. In addition, the energy performance of the PV module has been analyzed based on information, such as the open circuit voltage (Voc), short circuit current (Isc), and maximum power point voltage (Vmpp) in the simulation [46]. The information on the solar PV"
10.3390_en10122158,"dataset provided, code package, package, data",310,,0,"Energies 2017, 10, 2158 3 of 20 • Classify the climate data in global cities • Plot an energy potential chart and diagram by the variation of solar irradiation and wind power • Set the building module and BIPvWt system for energy generation and consumption output • Analyze the energy balance as an application of the BIPvWt in specific areas in terms of energy consumption and generation 2.1. Climate Data The climate data was selected from the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) and Energy Efficiency & Renewable Energy (EERE), which have information on more than 2100 city locations [44]. The paper selects 143 representative cities as variables to compare the solar and wind energy potential in the ASHRAE climate data. They are the major cities in Europe and Asia, and in each state of the U.S. Some cities are capitals of each country or state, and the others are selected based on the population and population density. In addition, the U.S. has the most cases compared to other areas because the climate data is well distributed in terms of climate classification and the weather data is relatively convincing. Each zone was classified according to the ASHRAE standard, which ranges from zone 1 (very hot) to 8 (subarctic), and the zone was analyzed using the thermal and humidity criteria. The population and density data in selected cities were well defined and informed (Demographia 2015). Figure 1 presents a histogram of the city lists according to the ASHRAE climate classification. Figure 1. Histogram of 143 cities according to the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) classification. 2.2."
10.3390_en10122158,"dataset provided, code package, package, data",332,,0,"The population and density data in selected cities were well defined and informed (Demographia 2015). Figure 1 presents a histogram of the city lists according to the ASHRAE climate classification. Figure 1. Histogram of 143 cities according to the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) classification. 2.2. Energy Simulation and Input Data Two simulation programs were used to evaluate the energy potential, BIPvWt: the ESP-r simulation program for an analysis of the PV and building energy consumption, and ANSYS Fluent for a computational fluid dynamics (CFD) analysis of the performance of the wind power conversion system [8,45]. ANSYS Fluent is one of the most-used CFD software offering various turbulence models based on the Reynolds-Averaged Navier Stokes (RANS) model. The power production of the wind turbines is estimated by an examination of the wind speed distribution around the applied system through CFD analyses. The ESP-r software package is recognized and used widely in more than 70 countries as an industry standard for the simulations. The authors employed ESP-r 11.1, which considers the energy use of heating, cooling, lighting, and PV energy generation. Thus, ESP-r has been used extensively to assess building energy applications, particularly as a simulation tool to compare various cities [14,29]. In addition, the energy performance of the PV module has been analyzed based on information, such as the open circuit voltage (Voc), short circuit current (Isc), and maximum power point voltage (Vmpp) in the simulation [46]. The information on the solar PV for the energy simulation is based on the data provided by the manufacturer for a silicon-based PV system [47]. 051015202530354012345678Number of CitiesASHRAE ClassificationA: moistB: dryC: marineEnergies 2017, 10, 2158"
10.3390_en10122158,"dataset provided, data",22,,0,for the energy simulation is based on the data provided by the manufacturer for a silicon-based PV system [47].
10.3390_en10122158,"dataset provided, dataset, data",170,,0,"Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin."
10.3390_en10122158,"dataset provided, dataset, data",340,,0,"Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin. Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs.Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.)."
10.3390_en10122158,"dataset provided, dataset, data",350,,0,"Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs.Energies 2017, 10, 2158 7 of 20 Based on the energy potential data set, Table 4 and Figures 5 and 6 provide a statistical summary and a diagram of the distribution of the solar and wind energy potential, respectively. In addition, the basic abbreviated terms are the maximum value (Max), minimum value (Min), average value (AVG), and standard deviation (S.D.). In the case of Wellington, New Zealand, its energy potential has a maximum value (total: 2.25, solar: 0.97, and wind: 3.53), which has common characteristics in a wind dominated region (average wind energy potential in Australia is 1.15 and New Zealand is 2.31). On the other hand, in the case of Chongqing located in China, its potential has a minimum value (total: 0.31, solar: 0.47, and wind: 0.15), which can explain the regional features of a basin. Figure 5. Total energy potential of the solar and wind source in global regions according to the population (see Appendix A for a more comprehensive list of abbreviations). Figure 6. Solar and wind energy potential in global regions (see Appendix A for a more comprehensive list of abbreviations). U48New YorkSan FranciscoU25A28A32A31TokyoSeoulA09A10E14CopenhagenAmsterdamE31E26E24M07M080.000.501.001.502.002.500510152025303540Relative Ratio of Solar and Wind Energy PotentialPopulation (million)U.S.AAsiaEuropeEtcs.U08U64U48New YorkSan FranciscoU22U47U05A28A26A06A40TokyoA17SeoulA25A10E14CopenhagenAmsterdamE13E02E05E21E200.000.200.400.600.801.001.201.401.601.802.000.000.501.001.502.002.503.003.504.00Relative ratio of Solar Energy PotentialRelative ratio of Wind Energy PotentialUSAAsiaEuropeEtcs. Energies 2017, 10, 2158"
10.3390_en10122158,"dataset, data",265,,0,"The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classiﬁcation to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classiﬁcation compared to the wind case. These results can be explained by the characteristics of the ASHRAE standard, which originate from the division of the thermal and humidity criteria [44]. In the wind energy cases, however, the variation is dispersed irregularly in a similar climate or adjacent cities."
10.3390_en10122158,"dataset, data",328,,0,"(cid:2873)°+Power(cid:3101),(cid:2872)(cid:2873)° (1) 3. Results and Discussion The results are divided into two parts: energy potential analysis in multiple urban areas and energy balance evaluation in selected cities. In the first part, as shown in the Appendix A, the energy potential can be compared according to the variation of global cities, which have their own climate patterns. Second, a feasibility test was performed by analyzing the energy consumption and generation in an office module in a specific building. 3.1. Solar and Wind Energy Potential in Urban Area The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classification to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classification compared to the wind case."
10.3390_en10122158,"dataset, data",332,,0,"Energies 2017, 10, 2158 6 of 20 different angles of incidence (0°, 22.5°, and 45°) were considered to reduce the number of analysis cases [56]. The power production of the wind turbines of a BIPvWt system applied to a building envelop was estimated by the annual wind data from selected cities. The wind rose data were fitted using the Weibull distribution, which is a good fit to the measured wind speed data [57], for each wind direction [58,59]. In addition, the applied system was assumed to generate electric power from the approaching wind within an angle of 90°. For example, the system installed in a northward direction is affected by wind from the northeast, north-northeast, north, north-northwest, and northwest azimuths. Finally, the total power production of the applied system installed towards the (cid:2030) direction can be written as the sum of the power converted from five different approaching wind directions as follows: Power(cid:3101),(cid:2930)(cid:2925)(cid:2930)(cid:2911)(cid:2922)=Power(cid:3101),(cid:2879)(cid:2872)(cid:2873)°+Power(cid:3101),(cid:2879)(cid:2870)(cid:2870).(cid:2873)°+Power(cid:3101),(cid:2868)°+Power(cid:3101),(cid:2870)(cid:2870). (cid:2873)°+Power(cid:3101),(cid:2872)(cid:2873)° (1) 3. Results and Discussion The results are divided into two parts: energy potential analysis in multiple urban areas and energy balance evaluation in selected cities. In the first part, as shown in the Appendix A, the energy potential can be compared according to the variation of global cities, which have their own climate patterns."
10.3390_en10122158,"dataset, data",350,,0,"Second, a feasibility test was performed by analyzing the energy consumption and generation in an office module in a specific building. 3.1. Solar and Wind Energy Potential in Urban Area The solar and wind energy generation potential based on the solar irradiation and wind speed and direction were analyzed. A unit, relative ratio (average value: 1.00), was used to compare the renewable energy potential. The climate data of 142 cities were considered to represent the energy generation in a typical major city. For example, the average value was calculated based on the data from 142 targeted cities among a total 1042 locations, which is provided in the weather data set. A relative ratio of 1 means that the city has an average value of the 142 cities. Therefore, the average level of solar irradiation and wind speed in each city was selected as 1. If the value is greater than 1, there is a high potential for energy generation. Conversely, if the value is less than 1, there is a low potential of energy generation. Initially, the solar and wind energy potentials were compared by the ASHRAE international climate classification to determine the regional similarity and difference in each energy potential. As shown in Figure 4, the solar energy potential showed some analogy in the same climate classification compared to the wind case. These results can be explained by the characteristics of the ASHRAE standard, which originate from the division of the thermal and humidity criteria [44]. In the wind energy cases, however, the variation is dispersed irregularly in a similar climate or adjacent cities. (A) (B) Figure 4. Energy potential according to the ASHRAE classification. (A) Solar energy potential; (B) wind energy potential. 024681012012345678Relative Ratio of Solar Energy Potential ASHRAE Classification024681012012345678Relative Ratio of Wind Energy Potential ASHRAE ClassificationEnergies 2017, 10, 2158"
10.3390_en11020419,"dataset provided, data",109,,0,"The Sec SS 3 has installed 4 dry transformers of 1000 kVA each. In their current use, three remain always connected, the forth being in reserve, to be able to serve the hospital in case of failure of one of the other three. According to the data supplied by the manufacturer [20], its short-circuit and no-load power values are: Psc = 9.79 kW and P0 = 2.3 kW. Considering these data, and according to Equation (6), the optimum load index and the maximum efﬁciency will be: Copt = 0.4847 and ηmax = 0.99060."
10.3390_en11020419,"dataset provided, data",321,,0,"Energies 2018, 11, x FOR PEER REVIEW 7 of 13 2.3.2. Secondary Substation Studied The study proposed does not make sense when demand is constant, since in that case there are always a number of connected transformers that optimizes efficiency. Analogously, it also makes no sense if the Sec SS has a single transformer, since in that case there is only one connection option. Therefore, this type of study offers greater possibilities of energy saving with: (1) more variations, both daily and seasonal, in the demand of the Sec SS; and (2) a greater number of transformers that can enter into service. In the studied hospital complex, this occurs in the Sec SS 3. The magnitude of the changes in demand is basically based on the fact that this Sec SS is connected to the central air conditioning services of the hospital, which implies that there are large variations in both hourly and seasonal demand. The variation in hourly demand is also associated with the fact that this Sec SS also provides the consumption of the hot water pumping service of the hospital, and lighting, elevators, medical equipment, etc. of nearby buildings. The differences of hourly consumption, in a same day, can cause that the demand in the hour of greater consumption multiplies by 2.5 the demand of the hour of lower consumption. Regarding the differences in monthly consumption, the demand corresponding to the month of greatest consumption reaches 175% of the energy supplied in the month of least consumption. Analyzing the demand of the Sec SS 3, it has been observed that there is a great difference in the demand between the one that takes place in working days and the one on holidays."
10.3390_en11020419,"dataset provided, data",322,,0,"of nearby buildings. The differences of hourly consumption, in a same day, can cause that the demand in the hour of greater consumption multiplies by 2.5 the demand of the hour of lower consumption. Regarding the differences in monthly consumption, the demand corresponding to the month of greatest consumption reaches 175% of the energy supplied in the month of least consumption. Analyzing the demand of the Sec SS 3, it has been observed that there is a great difference in the demand between the one that takes place in working days and the one on holidays. This is basically due to the existence of consultations and diagnostic services in the hospital, which only operate on weekdays and therefore do not require space heating or air conditioning (supplied from Sec SS 3). Figure 3 shows the monthly and daily energy demands corresponding to the Sec SS 3. This has been carried out by differentiating between working and non-working days. Figure 3. Monthly and daily demands performed on the Sec SS 3. The total demand for the last spring month, June, is higher than total demand of the first spring month, April. However, there are not significant differences in the time distribution in which this demand is performed. Considering the daily demand, it has been observed that there are three annual periods in which the hourly percentages of daily demand are similar. These periods relate to the different climatology, and correspond to: winter (16 December to 15 March), spring-autumn (16 March to 15 June and 16 September to 15 December) and summer (16 June to 15 September). Figure 4 shows the values of the hourly demand percentages with respect to the daily demand in the Sec SS 3."
10.3390_en11020419,"dataset provided, data",324,,0,"Figure 3 shows the monthly and daily energy demands corresponding to the Sec SS 3. This has been carried out by differentiating between working and non-working days. Figure 3. Monthly and daily demands performed on the Sec SS 3. The total demand for the last spring month, June, is higher than total demand of the first spring month, April. However, there are not significant differences in the time distribution in which this demand is performed. Considering the daily demand, it has been observed that there are three annual periods in which the hourly percentages of daily demand are similar. These periods relate to the different climatology, and correspond to: winter (16 December to 15 March), spring-autumn (16 March to 15 June and 16 September to 15 December) and summer (16 June to 15 September). Figure 4 shows the values of the hourly demand percentages with respect to the daily demand in the Sec SS 3. This has been carried out for all three periods of time, considering whether it is a working or non-working day. It is observed that the demand in the non-working days is much more constant than in the working days; this is mainly due to the operation of consultation and diagnostic services. The Sec SS 3 has installed 4 dry transformers of 1000 kVA each. In their current use, three remain always connected, the forth being in reserve, to be able to serve the hospital in case of failure of one of the other three. According to the data supplied by the manufacturer [20], its short-circuit and no-load power values are: Psc = 9.79 kW and P0 = 2.3 kW. Considering these data, and according to Energies 2018, 11, 419"
10.3390_en11020419,download,52,,0,"19. American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE). Advanced Energy Design Guide for Large Hospitals; ASHRAE: Atlanta, GA, USA, 2012; ISBN 978-1-936504-23-7. Available online: https://www.ashrae.org/standards-research--technology/advanced-energy-design-guides/50percent-aedg-free-download (accessed on 27 December 2017)."
10.3390_en11030500,"dataset, data",167,,0,"Currently, taxis in many cities are equipped with vehicle information collection devices such as GPS systems. There were approximately 65,000 taxis in Beijing in 2014, and the proportions of different car models of the current taxi are shown in Figure 1 [27]. The data used in this study come from GPS devices installed in taxis, which send information such as taxi identiﬁcation, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations. An example of the dataset is shown in Table 2."
10.3390_en11030500,"dataset, data",167,,0,"With improved information and communication technologies, as well as location-based services (LBS) such as mobile phone communications, social software, vehicle-carried GPS (Global Position System) positioning terminals, etc., large-scale, high-quality, and consecutive spatiotemporal trajectory data on urban mobility has become an increasingly popular dataset and principal resource. Many researchers employ advanced data mining techniques and big geospatial data, among which taxi GPS data is one of the prevailing resources, to analyze individual travel patterns, the organization and planning of urban public spaces, construction of smart cities, and so forth. At present, studies using taxi GPS data include, but are not limited to, the following aspects: route planning and path-ﬁnding, trafﬁc operational state identiﬁcation, identiﬁcation of origin-destination (OD) and the clustering method, and taxi fuel consumption and emissions estimation. The typical and important current researches are summarized in Table 1."
10.3390_en11030500,"dataset, data",335,,0,"Energies 2018, 11, x 3 of 22 the dynamic spatiotemporal distribution as maps. To verify the accuracy of the results, we creatively convert carbon emissions and fuel consumption into CEPK and FCPOK, respectively, which are more global, standard, and intuitive factors. The dynamic spatiotemporal distribution of carbon emissions and taxi travel patterns on weekdays and weekends are then highlighted. Finally, the limitations of this research and potential future research areas are proposed. 2. Methodology In this section, we formulate the methodological framework used in this study. First, we process the taxi GPS data to enable its direct use in the next step. Second, using the carbon emission calculation model based on a taxi fuel consumption algorithm and emission factors, we calculate the carbon emissions over the whole network. Finally, a visualization method called kernel density analysis is applied to obtain the spatiotemporal dynamic distribution of carbon emissions. 2.1. Description and Data Cleaning of Taxi GPS Data Currently, taxis in many cities are equipped with vehicle information collection devices such as GPS systems. There were approximately 65,000 taxis in Beijing in 2014, and the proportions of different car models of the current taxi are shown in Figure 1 [27]. The data used in this study come from GPS devices installed in taxis, which send information such as taxi identification, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations."
10.3390_en11030500,"dataset, data",364,,0,"The data used in this study come from GPS devices installed in taxis, which send information such as taxi identification, coordinates, time, driving speed, and passenger state to the taxi control center every 30 to 60 s. It covers approximately 30,000 to 40,000 vehicles, including more than 30,000 taxis, 5000 to 8000 sightseeing vehicles, and all car models of the current taxi in Beijing. These taxis account for approximately 50% of all taxis in Beijing and produce approximately 40 million records every day. We choose a typical workday and the weekend to comprehensively analyze temporal variations. An example of the dataset is shown in Table 2 The data are stored in Oracle and, thus, are easily dealt with using the connection between Oracle (Oracle 11g, Oracle, Redwood City, CA, USA) and ArcGIS (ArcGIS 10.3, Esri, Redlands, CA, USA) software. For example, we clean records with invalid locations, those whose coordinates deviate significantly within a few seconds, and those whose speed is always at zero. We delete repetitive records and convert some fields to enable its direct use in the next step, thus reducing the data scale, as well as improving result accuracy and computing efficiency. We then match data points to spatial coordinates using the geographic map in ArcGIS, after which the “Track intervals to line” tool is applied to obtain taxi trajectories with an average speed, which are used later to obtain the fuel consumption, the distance, and driving duration between two adjacent points of the same taxi. Through the reconstructed taxi trajectories shown in Figure 2, we obtain the structure of the road network in Beijing (Figure 3), which corresponds closely to the actual road network. 3%5%11%31%50% Elantra Jetta Sonata Elysee OthersProportion of the car models Figure 1. Proportion of different car models of the current taxi in Beijing. Energies 2018, 11, 500"
10.3390_en11030500,"used dataset, data available, dataset, data",216,,0,"Furthermore, because of their low cost, wide coverage, easy data access, accurate allocation, high continuity, and, most importantly, their feasibility, big taxi GPS data can also identify the trafﬁc state [7,8]. This includes exhaustive analyses of spatiotemporal congestion patterns on urban roads [9] and measurements of trafﬁc jam indicators [10], allowing for a better understanding of the operational states of road networks. Moreover, by proposing different and effective anomaly detection methods, some studies have used the taxi dataset to detect anomalous trafﬁc events, which occur when the corresponding indicators deviate signiﬁcantly from the expected values [11], and to detect anomalous routes such as fraudulent taxi travel patterns or trafﬁc accidents, as well as identifying the parts of the trajectories responsible for the anomalies and ranking them with an ongoing anomaly score [12]. Other studies have monitored unexpected behaviors, such as vehicle breakdowns, or one-time events like large sporting events, fairs, and conventions, which exhibit the largest statistically signiﬁcant departure from expected behavior [13]. Additional research has estimated the average relationship between travel time and driving speed [14–16]."
10.3390_en11040947,"data https, data available, data",14,,0,Data. Available online: www.csir.co.za (accessed on 22 July 2016).
10.3390_en11071817,benchmark,91,,0,"A real option model of renewable energy generation investment under the current benchmark electricity price system was constructed. The numerical simulation and sensitivity test of the model were carried out by the Monte-Carlo method. This paper discussed the feasibility of investing in wind power projects under the current net electricity price level in China, analyzed the investment opportunity of the investment in the renewable energy project of wind power generation and revealed the change law of the value of the carbon price gap with the related parameters."
10.3390_en11071817,case study data,4,,0,4. Case Study
10.3390_en11071817,database,16,,0,27. Polaris Power Network Database. 2017. Available online: http://news.bjx.com.cn/html/20170420/821314.
10.3390_en11071817,database,18,,0,28. Carbon Emissions Trading Database. 2017. Available online: http://www.tanpaifang.com/ (accessed on
10.3390_en11071832,"data https, data available, data",18,,0,16. Ensol. Technical Data of the Flat Solar Collectors. Available online: http://ensol.pl/new_ensol/wp-content/
10.3390_en11082032,"dataset provided, data",45,,0,Author Contributions: Y.Y. conceived this study. Y.L. designed the methodology and performed the experiments. M.C. carried out the data analysis. X.W. provided guidance for this study. All authors contributed to the writing of this paper.
10.3390_en11113125,benchmark,71,,0,"Cost minimization is the main objective for which the hybrid heuristic technique is designed to optimize the DSM using the RTP tariff. Figure 7a elucidates EC of all OTIs. The ﬁgures clearly demonstrate that benchmark schemes outperformed in terms of EC; however, the proposed hybrid algorithm outperformed by sacriﬁcing UC with affordable WT. EC values for single homes using RTP are shown in Table 6."
10.3390_en11113125,benchmark,89,,0,"The performance of the proposed hybrid algorithm is evaluated using a CPP price tariff. Our proposed hybrid algorithm outperformed as compared to benchmark schemes. Algorithm is envisioned to evade peak formation in any obvious slots of working hours. Therefore, price reduction happens. Our proposed and implemented technique performed fabulously in the case of different power consumption patterns. Figure 4 shows the behavior of load using CPP with four different OTIs. However, total load should be equal before and after scheduling."
10.3390_en12030483,benchmark,46,,0,"IEEE Trans. Power Deliv. 2014, 29, 2018–2031. [CrossRef] Strunz, K. Benchmark Systems for Network Integration of Renewable and Distributed Energy Resources; Technical Report 575, CIGRÉ TF C6.04.02; CIGRÉ: Paris, France, 2014."
10.3390_en12030483,retrieve,56,,0,"The operating principle of such a method is the following. At every time instant tn power and ROCOF measurements are retrieved, and the inertia constant is computed using (17). Then, the residual of N consecutive inertia estimates with respect to the current inertia estimation H(tn) is computed"
10.3390_en12050875,"database, data",105,,1,"The case study used for testing the methodology was chosen from the commercial reference buildings database [26] of the US Department of Energy (DOE). A secondary school located in San Francisco (California) and constructed after the year of 1980 was selected. Data about the energy load demands (whose hourly values are shown in Figure 4) were calculated by means of EnergyPlus simulation software [27] and then imported and processed in MATLAB. Hourly temperatures of the typical meteorological year of San Francisco, which are shown in Figure 5, were considered."
10.3390_en12050902,dataset,154,,0,"The input dataset was generated and normalized between −1 and 1 with the Min-Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration. If this error presented an increasing behavior in a pre-determined number of veriﬁcations (20 iterations), the training was ﬁnalized. The intent of this technique was to ﬁnd the exact moment when the FFNN started to lose the ability to generalize."
10.3390_en12050902,dataset,345,,0,"SOC Imc bat < E(t) < 0 E(t) < Imc bat SC Li‐ion SC Li‐ion SOCbat > SOCbat max & SOCsc = SOCsc max Safety condition SOCbat < SOCbat max & SOCsc = SOCsc max 0 −E(t) × Imax 0 Imc bat × Imax SOCbat < SOCbat max & SOCsc ≠ SOCsc max 0 −E(t) × Imax −(E(t) − Imc bat) × Imax Imc bat × Imax SOCbat > SOCbat max & SOCsc ≠ SOCsc max −E(t) × Imax 0 −E(t) × Imax 0 The input dataset was generated and normalized between −1 and 1 with the Min‐Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration. If this error presented an increasing behavior in a pre‐determined number of verifications (20 iterations), the training was finalized. The intent of this technique was to find the exact moment when the FFNN started to lose the ability to generalize. Figure 8. Multi‐Layer Feed‐Forward Neural Network architecture. 3.2. Control Scheme The implemented control scheme can operate in two different modes, i.e., with closed‐loop current control or closed‐loop voltage control, as shown in Figure 9. The operating modes were selected before starting the system. Output LayerInput LayerHidden Layerb11b12b13b1JW11,1W1i,jSOC(t)batSOC(t)scf1f1f1f1f2f2W21,1W2j,2b22b21E(t) I(t)batI(t)SCEnergies 2019, 12, 902"
10.3390_en12050902,dataset,355,,0,"Energies 2018, 11, x FOR PEER REVIEW  11 of 26  Table 2. Rules for ESS discharge mode. SOC 0 < E(t) < Imd bat E(t) > Imd bat SC Li‐ion SC Li‐ion SOCbat < SOCbat min & SOCsc < SOCsc min Safety condition SOCbat < SOCbat min & SOCsc > SOCsc min E(t) × Imax 0 E(t) × Imax 0 SOCbat > SOCbat min & SOCsc > SOCsc min 0 E(t) × Imax (E(t) − Imax bat) × Imd bat Imd bat × Imax SOCbat > SOCbat min & SOCsc < SOCsc min 0 E(t) × Imax 0 Imd bat × Imax Table 3. Rules for ESS charge mode. SOC Imc bat < E(t) < 0 E(t) < Imc bat SC Li‐ion SC Li‐ion SOCbat > SOCbat max & SOCsc = SOCsc max Safety condition SOCbat < SOCbat max & SOCsc = SOCsc max 0 −E(t) × Imax 0 Imc bat × Imax SOCbat < SOCbat max & SOCsc ≠ SOCsc max 0 −E(t) × Imax −(E(t) − Imc bat) × Imax Imc bat × Imax SOCbat > SOCbat max & SOCsc ≠ SOCsc max −E(t) × Imax 0 −E(t) × Imax 0 The input dataset was generated and normalized between −1 and 1 with the Min‐Max normalization method, ensuring that the mean of the dataset was 0 and the standard deviation was 1. Subsequently, the input dataset was divided into two sets: the training set and the validation set, corresponding to 75% and 25% of the input dataset, respectively. There were two kinds of stopping criteria: the maximum number of iterations or the test using the validation set. The test using the validation set was based on the calculation of the MSE of the validation set in each iteration."
10.3390_en12050902,"dataset, data",255,,0,"where i represents the number of inputs; xi the input data vector; j represents the number of neurons in the hidden layer (in this case 7 neurons); f 1 and f 2 represent the activation functions; Iw represents the weights of the connection between the input layer and the hidden layer; Lw represents the weights of the connection between the hidden layer and the output layer; and b1 and b2 represents the bias of the neurons of the respective layers. The activation function, which works as a threshold, must be chosen appropriately for each layer from the several commonly used functions, such as: the Heaviside function, the symmetric saturating linear function, the sigmoid function, the Gaussian function, the hyperbolic tangent function, and the spline function. In this study was used the symmetric saturating linear function in both layers (output and hidden layers). One of the most important features of an ANN is the training method, which can be divided in two fundamental classes: supervised training method and unsupervised training methods [49]. The implemented training approach was an ofﬂine supervised method, wherein the Levenberg-Marquardt backpropagation algorithm was used to optimize the FFNN internal states (weights and bias). In order to increase ANN learning accuracy and improve their capacity to generalize, input datasets were created with artiﬁcial data, taking into account several rules."
10.3390_en12061114,"data available, data, dataset provided, publicly available, data https",22,,1,20. Building data public open system. Available online: http://open.eais.go.kr (accessed on 23 January 2019).
10.3390_en12061114,"database, publicly available, data",78,,1,"The Korean government provides two separate databases through the publicly available building data system: a building energy database and an architectural database [20]. The building energy database records building addresses and monthly electric and gas energy consumption. Meanwhile, the architectural database records building addresses, building names, building usage, total ﬂoor area, number of ﬂoors, number of elevators, number of parking lots, and year built."
10.3390_en12071342,code,89,,0,"where Tf is the ﬁlter time constant and T represents the sampling period. A high Tf results in higher smoothing proﬁles, whereas a low one will allow higher order frequencies to pass. The ﬁlter is tuned based on the irradiance pattern, which may not ensure an RR of 10% all the time, as the grid code demand. In [18], the output power of a 150 kW PV system is smoothed by an LPF with time constant of 120-seconds."
10.3390_en12071342,code,96,,0,"Different methods for PV generation ramp-rate calculation have been reported in the literature. The two most common are the difference between two end points or the difference between the maximum and minimum points of the same considered interval [5]. In addition, depending on the grid code, the RR calculation may be presented on instantaneous values or per minute values. In this work, the RR calculation is considered to be the difference between two end points of a given 60-second interval as presented in Equation (6):"
10.3390_en12071342,code,281,,0,"It is estimated that PV energy has surpassed the 400 GWp worldwide capacity at the end of 2017 [1]. This represents less than two percent of the worldwide electricity demand, but when compared to the ambition of China alone, of 1300 GW of solar capacity by the year of 2055 [2], illustrates what is yet to come for PV energy systems. However, the increased penetration of solar energy brings new challenges for grid operators, one of which concerns the short-term variability of solar irradiance [3]. This causes high variations in the injected power that can cause serious grid stability issues. To mitigate this problem, power ramp-rate limitation measures have been included in the electrical grid codes of many countries [4]. Generally, these RR limitations are deﬁned on a second or minute time frame or even in both. In addition, the maximum allowable RR can be deﬁned as a percentage of the plant capacity or as a deﬁned set of power. Some examples are the grid code of Ireland (EirGrid), which states a positive ramp up to 30 MW/minute and Hawaii (HECO), ± 2 MW/minute [5]. For Germany [6] and Puerto Rico (PREPA) [5], a maximum ramp-rate of 10%/minute of the rated PV power is considered. Other grid codes also quantify the maximum allowable ramp-rate in the order of seconds. For example, in Denmark [7], a maximum power ramp-rate of 100 kW/s is required."
10.3390_en12071342,code,346,,0,"20 January 2018. Available online: https://press.trendforce.com/press/20170914-2962.html (accessed on 8 April 2019). Yang, X.J. ; Hu, H.; Tan, T.; Li, J. China’s renewable energy goals by 2050. Environ. Dev. 2016, 20, 83–90. [CrossRef] Yang, Y.; Enjeti, P.; Blaabjerg, F.; Wang, H. Wide-scale adoption of photovoltaic energy: Grid code modiﬁcations are explored in the distribution grid. IEEE Ind. Appl. Mag. 2015, 21, 21–31. [CrossRef] Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants with a Power Output Above 11 kW; Energinet.dk: Denmark, 2015; pp. 1–96. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/ Regulations-for-grid-connection (accessed on 8 April 2019). Booth, S.; Gevorgian, V. Review of PREPA Technical Requirements for Interconnecting Wind and Solar Generation; National Renewable Energy Laboratory: Golden, CO, USA, 2013. BDEW Bundesverband der Energie- und Wasserwirtschaft e.V. Technische Richtlinie, Erzeugungsanlagen Am Mittelspannungsnetz; BDEW: Germany, June 2008; p. 138. Available online: https://www.bonn-netz.de/ Einspeisung/Vertraege/Stromeinspeisevertraege/Anlage-3-Technische-Richtlinien-Erzeugungsanlagenam-Mittelspannungsnetz.pdf (accessed on 8 April 2019). Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants Above 11 kW; Energinet.dk: Denmark, 2016; pp. 1–108. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/Regulations-forgrid-connection (accessed on 8 April 2019). Sangwongwanich, A.; Yang, Y.; Blaabjerg, F. A cost-effective power ramp-rate control strategy for single-phase two-stage grid-connected photovoltaic systems. In Proceedings of the 2016 IEEE Energy Conversion Congress and Exposition (ECCE), Milwaukee, WI, USA, 18–22 September 2016. Omran, W.A. ; Kazerani, M.; Salama, M.M.A. Investigation of Methods for Reduction of Power Fluctuations Generated From Large Grid-Connected Photovoltaic Systems. IEEE Trans. Energy Convers. 2011, 26, 318–327. [CrossRef]"
10.3390_en12071342,code,349,,0,"Tsao, R. Strong Chinese Market to Push Annual Global Photovoltaic Demand Above 100 Gigawatts for 2017. 20 January 2018. Available online: https://press.trendforce.com/press/20170914-2962.html (accessed on 8 April 2019). Yang, X.J. ; Hu, H.; Tan, T.; Li, J. China’s renewable energy goals by 2050. Environ. Dev. 2016, 20, 83–90. [CrossRef] Yang, Y.; Enjeti, P.; Blaabjerg, F.; Wang, H. Wide-scale adoption of photovoltaic energy: Grid code modiﬁcations are explored in the distribution grid. IEEE Ind. Appl. Mag. 2015, 21, 21–31. [CrossRef] Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants with a Power Output Above 11 kW; Energinet.dk: Denmark, 2015; pp. 1–96. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/ Regulations-for-grid-connection (accessed on 8 April 2019). Booth, S.; Gevorgian, V. Review of PREPA Technical Requirements for Interconnecting Wind and Solar Generation; National Renewable Energy Laboratory: Golden, CO, USA, 2013. BDEW Bundesverband der Energie- und Wasserwirtschaft e.V. Technische Richtlinie, Erzeugungsanlagen Am Mittelspannungsnetz; BDEW: Germany, June 2008; p. 138. Available online: https://www.bonn-netz.de/ Einspeisung/Vertraege/Stromeinspeisevertraege/Anlage-3-Technische-Richtlinien-Erzeugungsanlagenam-Mittelspannungsnetz.pdf (accessed on 8 April 2019). Energinet.dk. Technical Regulation 3.2.2 for PV Power Plants Above 11 kW; Energinet.dk: Denmark, 2016; pp. 1–108. Available online: https://en.energinet.dk/Electricity/Rules-and-Regulations/Regulations-forgrid-connection (accessed on 8 April 2019). Sangwongwanich, A.; Yang, Y.; Blaabjerg, F. A cost-effective power ramp-rate control strategy for single-phase two-stage grid-connected photovoltaic systems. In Proceedings of the 2016 IEEE Energy Conversion Congress and Exposition (ECCE), Milwaukee, WI, USA, 18–22 September 2016. Omran, W.A. ; Kazerani, M.; Salama, M.M.A. Investigation of Methods for Reduction of Power Fluctuations Generated From Large Grid-Connected Photovoltaic Systems."
10.3390_en12081458,benchmark,19,,0,54. CIGRE. CIGRE Task Force C6.04: Benchmark Systems for Network Integration of Renewable and Distributed Energy
10.3390_en12081458,code,108,,0,"Diﬀerent reactive power control methods including PF(P), constant PF and constant Q control are tested. The conﬁgurations of the methods are based on Technical Regulation 3.2.1 (TR 3.2.1) and Technical Regulation 3.2.2 (TR 3.2.2), the Danish grid code for power plants up to and including 11 kW and PV power plants above 11 kW, respectively. The demonstration of the methods is shown in Figure 2 [34,35]. The PV inverters usually operate with two types of PF, i.e., lagging and leading PF. From the generator convention, they are deﬁned as"
10.3390_en12081458,code,188,,0,"Diﬀerent reactive power control methods have diﬀerent eﬀects on these problems. With the PF(P) control, the power loss and the overload problem in the transformers are the lowest among all the three cases. From this aspect, the PF(P) control shows the best performance in all three case studies. With the constant PF control, the lowest power loss of the entire grid occurs if the PV systems operate with a PF of 0.9 (leading), which is not required by the current Danish grid code for small scale residential PV plants, i.e., TR 3.2.1. Judging from the yearly average power loss condition, it can be beneﬁcial to operate PV systems with leading PF, especially in the grid with many cables. In terms of the worst overvoltage situation, constant PF control results in the lowest maximum bus voltage. With a constant Q control applied to large-scale PV power plants, the grid has the highest loss and experiences the most severe overvoltage problem among all three cases."
10.3390_en12081458,code,230,,0,"Abstract: Driven by the Energy Strategy 2050 of Denmark, renewable energy sources (RESs) are increasingly integrated into the Danish power grid. Solar photovoltaic (PV) plants play an important role in this process. This paper conducted a study to investigate the impacts of residential solar PV integration in the distribution grid on voltage security and grid loss based on the 10 kV distribution grid in Bornholm. Three case studies are performed to test three diﬀerent reactive power control methods, i.e., PF(P), constant PF and constant Q, at diﬀerent penetration levels. The assessment of the impacts of PV integration and diﬀerent control methods are done in the DIgSILENT PowerFactory. It was found that PV integration can contribute to reducing the loss of the system, increased overvoltage in buses and overload in transformers, and 40% penetration at the low voltage is considered to be an optimal level based on the result. PF(P) control gives the best performance among all three methods under the current grid codes. With constant PF control, it was found that the system loss can be signiﬁcantly reduced if the PV systems operate with a power factor of 0.9 leading, which is not the norm of the current Danish grid code."
10.3390_en12081458,dataset,130,,0,"The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reﬂect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figures 6 and 9 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand."
10.3390_en12081458,dataset,335,,0,"By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9."
10.3390_en12081458,dataset,337,,0,"Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with"
10.3390_en12081458,dataset,341,,0,"In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants."
10.3390_en12081458,dataset,342,,0,"Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor."
10.3390_en12081458,dataset,345,,0,"The results in Figure 9 reflect the worst situation in this case. According to the complete result dataset, only very few (2 to 3) transformers are in the extremely severe overloading state (over 200%) for a very short period. A similar situation can also be found in the next case with a constant Q control in the next section. By comparing Figure 9 and Figure 6 from the previous case, the overloading situations in these two cases are close to each other at the same penetration level. The overload problem usually takes place when there is a big mismatch between the PV generation and the load demand. Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, 1458"
10.3390_en12081458,dataset,346,,0,"Combining the results in Figure 7 to 9, because of the lowest power loss of the entire grid, the PF setpoint of 0.9 (leading) is considered to be an optimal operation point. 3.3. Constant Q Control Because the constant Q support is not required for residential PV systems in the small scale [35], in this case, the constant Q control is only implemented to the medium-scale PV systems and the two large centralized PV power plants. The rest, namely, the small-scale residential PV systems, are set to constant PF control with a unity power factor. In this way, the impact of a constant Q control can be assessed more intuitively. From the correlation between active, reactive and apparent power, i.e., S=√𝑃2+𝑄2, the maximum reactive power, in theory, can be provided by a PV system without affecting the normal generation of approximately 70% of its apparent power. The Q setpoint, in this case, ranges from 10% to 70% of the apparent power, with an increment of 20%. In this case, only the operation within the first quadrant, i.e., exporting P and Q, is tested. Energies 2019, 12, x FOR PEER REVIEW 10 of 16  (a) (b) Figure 8. The worst overvoltage situations of buses at different PF setpoints (with constant PF): (a) Yearly maximum number of overvoltage buses; (b) Yearly maximum bus voltages. (a) (b) Figure 9. The worst overload situations of transformers at different PF setpoints (with constant PF): (a) Yearly maximum number of overloaded transformers; (b) Yearly maximum loading rate of the transformer. The overloading situations of the transformers are presented in Figure 9. The results in Figure 9 reflect the worst situation in this case."
10.3390_en12091599,"data available, data",83,,0,"The proposed method employs the RANSAC algorithm to estimate the parameters for the ﬁtting function. In contrast, the classic parameter estimation algorithm least-squares method is based on the smoothness assumption and cannot detect and eliminate the abnormal data. However, the smoothness assumption is not available in most cases, including 3D point cloud data with noise that cannot be compensated. Thus, the RANSAC algorithm is a key component in the process of axis acquisition."
10.3390_en12091599,"database, data",329,,0,"Riveiro, B.; DeJong, M.J.; Conde, B. Automated processing of large point clouds for structural health monitoring of masonry arch bridges. Autom. Constr. 2016, 72, 258–268. [CrossRef] Díaz-Vilariño, L.; González-Jorge, H.; Bueno, M. Automatic classiﬁcation of urban pavements using mobile LiDAR data and roughness descriptors. Constr. Build. Mater. 2016, 102, 208–215. [CrossRef] Vazaios, I.; Vlachopoulos, N.; Diederichs, M.S. Integration of Lidar-based structural input and discrete fracture network generation for underground applications. Geotech. Geol. Eng. 2017, 35, 2227–2251. [CrossRef] Palmer, D.; Koumpli, E.; Cole, I. A GIS-Based Method for Identiﬁcation of Wide Area Rooftop Suitability for Minimum Size PV Systems Using LiDAR Data and Photogrammetry. Energies 2018, 12, 3506. [CrossRef] Le Clainche, S.; Lorente, L.; Vega, J.; Vega Jose, M. Wind Predictions Upstream Wind Turbines from a LiDAR Database. Energies 2018, 3, 543. [CrossRef] Yan, Y.; Tan, Z.; Su, N. Building Extraction Based on an Optimized Stacked Sparse Autoencoder of Structure and Training Samples Using LIDAR DSM and Optical Images. Sensors 2017, 17, 1957. [CrossRef] [PubMed] Bosché, F.; Ahmed, M.; Turkan, Y. The value of integrating Scan-to-BIM and Scan-vs-BIM techniques for construction monitoring using laser scanning and BIM: The case of cylindrical MEP components. Autom. Constr. 2015, 49, 201–213. [CrossRef] Argüelles-Fraga, R.; Ordóñez, C.; García-Cortés, S. Measurement planning for circular cross-section tunnels using terrestrial laser scanning. Autom. Constr. 2013, 31, 1–9. [CrossRef]"
10.3390_en12091599,dataset,161,,0,"25. Chen, S.; Walske, M.L.; Davies, I.J. Rapid mapping and analysing rock mass discontinuities with 3D terrestrial laser scanning in the underground excavation. Int. J. Rock Mech. Min. Sci. 2018, 110, 28–35. [CrossRef] Sánchez-Rodríguez, A.; Riveiro, B.; Soilán, M. Automated detection and decomposition of railway tunnels from Mobile Laser Scanning Datasets. Autom. Constr. 2018, 96, 171–179. [CrossRef] Fumarola, M.; Poelman, R. Generating virtual environments of real world facilities: Discussing four different approaches. Autom. Constr. 2011, 20, 263–269. [CrossRef] Fischler, M.A.; Bolles, R.C. Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and automated cartography. Commun. ACM 1981, 24, 381–395. [CrossRef]"
10.3390_en12091616,benchmark,141,,0,"The abovementioned control strategies have been numerically simulated by MATLAB/Simulink tool. The main system parameters used in the simulation are listed in Table 1. The sampling frequency remains the same as 20 kHz. For simplicity, the conventional DPC method and conventional MPC-based DPC methods without one-step-delay compensation are denoted as “CDPC” and “CMPC-I”, which are used as a benchmark for comparison. The conventional MPC with only one-step-delay compensation and stability enhancement is denoted as “CMPC-II”, the conventional MPC with only proposed mutual inﬂuence elimination is denoted as “MMPC-I”, and the proposed multi-functional MPC with both steady-state and dynamic performance improvement as “MMPC-II”. For convenience, the power ﬂow direction from the AC side to the DC load is supposed as positive."
10.3390_en12091616,benchmark,338,,0,"Energies 2019, 12, x FOR PEER REVIEW 8 of 17 The abovementioned control strategies have been numerically simulated by MATLAB/Simulink tool. The main system parameters used in the simulation are listed in Table 1. The sampling frequency remains the same as 20 kHz. For simplicity, the conventional DPC method and conventional MPC-based DPC methods without one-step-delay compensation are denoted as “CDPC” and “CMPC-I”, which are used as a benchmark for comparison. The conventional MPC with only one-step-delay compensation and stability enhancement is denoted as “CMPC-II”, the conventional MPC with only proposed mutual influence elimination is denoted as “MMPC-I”, and the proposed multi-functional MPC with both steady-state and dynamic performance improvement as “MMPC-II”. For convenience, the power flow direction from the AC side to the DC load is supposed as positive. To analyze both the steady and dynamic-state performances for each of the control strategies, the P*steps up from 0 kW to 4 kW at 0 s while the Q* remains at 0 kVar. After that, the active power decreases to −5 kW at 0.02 s, the reactive power boosts to 3 kVar at 0.04 s. At 0.06 s, the active power boosts from −5 kW to 7 kW, while the reactive power reduces to −4 kVar at 0.08 s. At 0.1 s, the active power decreases from 7 kW to 0 kW. 5.1. Steady-State Performance Comparison To compare the steady-state performance, the AC side input current and reactive power in the simulation are presented from 0 s to 0.04 s to indicate the detailed power ripples. As we can see from Figure 5, both the active and reactive powers track their reference values with high accuracy. From Figure 5a, we can observe that the power ripple reduces and the line currents are more sinusoidal with CMPC-I method."
10.3390_en12091616,benchmark,340,,0,"For convenience, the power flow direction from the AC side to the DC load is supposed as positive. To analyze both the steady and dynamic-state performances for each of the control strategies, the P*steps up from 0 kW to 4 kW at 0 s while the Q* remains at 0 kVar. After that, the active power decreases to −5 kW at 0.02 s, the reactive power boosts to 3 kVar at 0.04 s. At 0.06 s, the active power boosts from −5 kW to 7 kW, while the reactive power reduces to −4 kVar at 0.08 s. At 0.1 s, the active power decreases from 7 kW to 0 kW. 5.1. Steady-State Performance Comparison To compare the steady-state performance, the AC side input current and reactive power in the simulation are presented from 0 s to 0.04 s to indicate the detailed power ripples. As we can see from Figure 5, both the active and reactive powers track their reference values with high accuracy. From Figure 5a, we can observe that the power ripple reduces and the line currents are more sinusoidal with CMPC-I method. By introducing the delay compensation, the performance is further improved with CMPC-II method. (a) (b) (c) Figure 5. From top to bottom. (a) CDPC three-phase currents, CDPC reactive powers, CMPC-I three-phase currents, CMPC-I reactive powers; (b) CMPC-II three-phase currents, CMPC-II reactive powers, MMPC-I three-phase currents, MMPC-I reactive powers; (c) MMPC-II three-phase currents, MMPC-II reactive powers. While with MMPC-II control, the ripples of active and reactive powers in the steady-state are improved in comparison with CMPC-I and MMPC-I method, also it is almost the same as CMPC-II control, as shown in Figure 5c, which aligns well with theoretical analyses. Energies 2019, 12, 1616"
10.3390_en12091616,code,256,,0,"By selecting the voltage space vector which achieves the lowest cost function value of (17) after evaluation of all the voltage vectors, four separate optimization problems are solved comprehensively. In (17), each term has a corresponding weighting factor. In actual application, ﬁrstly, the selection of these weighting factors is through trial and error by simulation. Then, it could be implemented in the experiment and make adjustment accordingly. Finally, the selected weighting factors would be implemented in practical application. Each weighting factor value would have an inﬂuence on the weighting factor selection of the others, and the weighting factor values are in relation with system conﬁgurations, thus it is quite a complex task for the mathematical derivation and veriﬁcation about the calculation of each weighting factor, which is out of scope of the main target of this paper, it will be researched in the future work. By selecting proper weighting factors, good dynamic and steady-state performance can be balanced in a systematic way. It is obvious that the additional terms would increase the computational burden compared with the conventional methods due to more complex cost function, especially in less capable hardware system. However, it could be solved by using general methods like machine code optimization once the control strategy has been programmed. In actual application with 20k sampling frequency, the increased computational burden does not aﬀect the control implementation."
10.3390_en12091789,python,191,,0,"In this study, the test MG system has a PV, a WT, a CDG, a BESS, and load demand, as shown in Figure 1. The MG is interconnected with a CBESS and the utility grid. The system can be operated in both grid-connected and islanded modes. The analysis is conducted for a 24-hour scheduling horizon (T = 24 h) and each time interval is set to be 1 hour. The MILP-based model for MG is implemented in Python integrated with CPLEX 12.6 [26]. The Q-learning-based model for CBESS is also implemented in Python. The market price signals, load proﬁle, and the total output of RDGs are shown in Figure 5a,b, respectively. The information of the CDG unit, BESS, and CBESS are tabulated in Table 1. The operation bounds of BESS and CBESS were chosen as [0%, 100%], same as [27]. The detailed numerical results are shown in the following sections for both grid-connected and islanded modes."
10.3390_en12091789,"python, data",341,,0,"Energies 2019, 12, 1789 9 of 17 In this paper, a Q-learning-based operation strategy for CBESS is proposed for the optimal operation of CBESS. To show the effectiveness of the Q-learning-based operation, the resultsof Q-learning-based operation methods are comparedwith the results of the centralized operationmethod. The detailed numerical results are presented in the following section. 3. Numerical Results 3.1. Input Data In this study, the test MG system has a PV, a WT, a CDG, a BESS, and load demand, as shown in Figure 1. The MG is interconnected with a CBESS and the utility grid. The system can be operated in both grid-connected and islanded modes. The analysis is conducted for a24-hour scheduling horizon (T = 24 h) and each time interval is set to be 1 hour. The MILP-based model for MG isimplemented in Python integrated withCPLEX 12.6[26]. The Q-learning-basedmodel for CBESS is also implemented in Python. The market price signals, load profile, and the total output of RDGs are shown in Figures 5a,b, respectively. The information of the CDG unit, BESS, and CBESS are tabulated in Table 1. The operation bounds of BESS and CBESS were chosenas [0%, 100%], sameas [27]. The detailed numerical resultsare shown in the following sections for both grid-connected and islanded modes. (a)(b)Figure 5. Input data: (a) market price signals and load profile; (b) output power of the renewable distributed generator (RDG). Table 1. The detail parameters of BESS, CBESS, and controllable distributed generator (CDG). Parameters BESS CBESSParametersCDGMax. CapP(kWh)200 300 Max. maxP(kWh)500 InitialCapiniPSoC⋅(kWh)50 150 Min. minP(kWh)0 Min."
10.3390_en12091789,"python, data",350,,0,"The detailed numerical resultsare shown in the following sections for both grid-connected and islanded modes. (a)(b)Figure 5. Input data: (a) market price signals and load profile; (b) output power of the renewable distributed generator (RDG). Table 1. The detail parameters of BESS, CBESS, and controllable distributed generator (CDG). Parameters BESS CBESSParametersCDGMax. CapP(kWh)200 300 Max. maxP(kWh)500 InitialCapiniPSoC⋅(kWh)50 150 Min. minP(kWh)0 Min. minCapPSoC⋅(kWh)0 0 Cost/kWh CDGC(KRW)136 Char. Loss L+(%) 5 3 Start-up costSUC(KRW) 200 Dis. LossL−(%) 5 3 Shut-down cost SDC(KRW)100 3.2. Operation of the System in Grid-Connected Mode This section presents the operation of the MG and CBESS in grid-connected mode. The MG-EMS performs optimization to minimize the total operation cost of the MG. The amount of buying/selling power is determined based on the amount of surplus/shortage power in the MG system, as shown in Figure 6a. The buying/selling power of the MG is traded with two external systems, i.e., CBESS and the utility grid. It can be observed from Figure 6b that the CBESS always decides to importpower from cheaper resources. During intervals 3, 4, 6, the generation cost of CDG is less than the buying prices from the utility grid. Therefore, CBESS decides to charge surplus power from MG instead ofbuying from the utility grid. Figure 6c shows the buying power ofthe MG system. The MG decides to import power from the external systems for minimizing the total operation cost. At intervals 2 and 5, MGimports power from the utility grid to fulfill load amount with cheaper price compared with 1201401601802004006008001471013161922kWhInterval (hour)LoadPrbuyPrsellwon/kWh05101520251357911131517192123kWhInterval (hour)PVWTEnergies 2019, 12, 1789"
10.3390_en12132496,"database, data available, open-source, data",115,,1,"upon the type of ecosystem, as measured by the Köppen climate classiﬁcation, which has been recently updated [30]. At present, climatic data is available for most of the climates; according to the present analysis, the test reference year came from several open source databases [31], with the only exception of Zurich, whose TRY belongs of the Meteonorm database [20]. Among the diﬀerent ways to deﬁne the weather conditions, the Köppen–Geiger scale and the degree days (DDs) have been used [28]. The European map of the Köppen–Geiger climate classiﬁcation is shown in Figure 1."
10.3390_en12132522,database,5,,0,3.1. Establish Feature Database
10.3390_en12132522,database,5,,0,4.2. Establish Feature Database
10.3390_en12132522,database,38,,0,"(b) Establish a feature database. Extract features by Spearman correlation coefﬁcient and GBDT relative importance, and choose the feature set that minimizes the average error of the model as the ﬁnal feature database."
10.3390_en12132522,database,75,,0,"So far, we can jointly determine the contribution of each feature through the analysis of GBDT relative importance and Spearman correlation coefﬁcient. Next, we need to further ﬁlter features according to the prediction performance of different feature combinations on GBDT. Here, we use mean squared error (MSE) to evaluate. The ultimate feature database is the feature set that minimizes MSE. The formula is as follows:"
10.3390_en12132522,"database, data",45,,0,"Line loss prediction modelPrediction and analysisEstabish GBDT line loss prediction modelValidate and evaluate the modelPredict line loss rate Select electrical featuresEstablish feature databaseCompare algorithms and analyze the error Data PreprocessingFeature database in LV distribution network Intergrate and standardize data Energies 2019, 12, 2522"
10.3390_en12132522,"database, dataset, data",162,,0,"The aims of this paper are to distinguish key features that signiﬁcantly inﬂuence line loss rate, and predict line loss rate under the condition that outliers exist. Consequently, we propose a gradient boosting decision tree (GBDT)-based approach to calculate line loss rate in the LV distribution network. First, we select the features by correlation analysis and construct the corresponding database. Second, considering the great difference in grid structure and the numerical dispersion of line loss rate, we use density-based spatial clustering of applications with noise (DBSCAN) to classify the LV distribution network. Finally, we establish GBDT prediction models for each area, assess the prediction results, and revise the outliers. Rationality and effectiveness have been veriﬁed through the analysis of the data set in a city and the comparison among other algorithms. What is more, the prediction accuracy can be signiﬁcantly improved."
10.3390_en12132522,"dataset, data",23,,0,(a) Input data set. Set the scan radius (cid:101) and the minimum number of samples in the neighborhood
10.3390_en12132522,"dataset, data",27,,0,"(c) Form a data set of core object. For Xi, ﬁnd all neighbor points within (cid:101) distance. Points with a"
10.3390_en12132522,"dataset, data",62,,0,"(d) For those core points that are not already assigned to a cluster, create a new cluster. Recursively ﬁnd all its density connected points and assign them to the same cluster as the core point. (e) Iterate through the unvisited points in data set. Points that do not belong to any cluster will be"
10.3390_en12132522,"dataset, data",114,,0,"Since GBDT is a serial computing model, it is challenging to carry out parallel computing. Therefore, in this paper, we normalize GBDT with a subsample, which ranges from 0 to 1. This approach is called stochastic gradient boosting tree (SGBT) [46]. At each round of iteration, a subsample of training data is drawn at random from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to ﬁt the base learner. In this way, the GBDT model can be partially paralleled, thereby reducing model overﬁtting to a certain extent."
10.3390_en12132522,"dataset, data",130,,0,"In this paper, we propose a GBDT-based approach to predict line loss rate in the LV distribution network. Effectiveness has been demonstrated by the analysis and veriﬁcation in the data set of the LV distribution network. First of all, the strength of our approach lies in its high accuracy of predicting line loss rate. In addition, paralleling in a subsample fashion can reduce overﬁtting to some extent, which overcomes the shortcoming of the original GBDT. Moreover, its good robustness to the outliers and missing values, as well as its partially paralleled design makes it possible to perform better in the process of predicting line loss rate, compared with SVR and RF. We believe that GBDT is of high"
10.3390_en12132522,"dataset, data",156,,0,"Although these algorithms are good at handling various nonlinear problems, in many cases SVR is sensitive to outliers and missing values, while GBDT and RF are not. Both GBDT and RF are ensemble learning methods and predict by combining the outputs from individual trees, yet on our dataset, GBDT proved to be more effective than RF. The reason is that, for data including categorical variables with different numbers of levels (e.g., the total length of the line ranges from hundreds to thousands), random forests are more biased in favor of those attributes with more levels, compared with GBDT. Therefore, the variable importance scores from random forest are not reliable for this type of data, hence the prediction accuracy of RF would be severely affected. In summary, the overall prediction results of GBDT are better than SVR and RF."
10.3390_en12132522,"dataset, data",170,,0,"Classiﬁcation and regression trees (CART) is one of the most well-established machine learning techniques, ﬁrst introduced by Breiman in 1984 [21]. CART is a typical binary tree, its essence is to divide the feature space into two parts and split the scalar attribute and the continuous attribute [22–26]. The CART algorithm consists of the following two steps: (1) Generate decision tree. This is done via training data set, build nodes from top to bottom. In order to make the resulting child nodes as pure as possible, split each node according to the best attribute. For the classiﬁcation problem, use GINI value as the basis for splitting node; for the regression problem, use the smallest variance instead. (2) Pruning. A technique that improves predictive accuracy by reducing overﬁtting and includes pre-pruning and post-pruning. Pre-pruning is to terminate the growth of the decision tree in advance"
10.3390_en12132522,"dataset, data",194,,0,"Abstract: Line loss rate plays an essential role in evaluating the economic operation of power systems. However, in a low voltage (LV) distribution network, calculating line loss rate has become more cumbersome due to poor conﬁguration of the measuring and detecting device, the difﬁculty in collecting operational data, and the excessive number of components and nodes. Most previous studies mainly focused on the approaches to calculate or predict line loss rate, but rarely involve the evaluation of the prediction results. In this paper, we propose an approach based on a gradient boosting decision tree (GBDT), to predict line loss rate. GBDT inherits the advantages of both statistical models and AI approaches, and can identify the complex and nonlinear relationship while computing the relative importance among variables. An empirical study on a data set in a city demonstrates that our proposed approach performs well in predicting line loss rate, given a large number of unlabeled examples. Experiments and analysis also conﬁrmed the effectiveness of our proposed approach in anomaly detection and practical project management."
10.3390_en12142675,"data https, data available, data",52,,0,"Environmental Commissioner of Ontario. Surplus Baseload Electricity Generation in Ontario. 2017. Available online: https://eco.on.ca/blog/surplus-baseload-electricity-generation-in-ontario (accessed on 6 April 2018). Power Imports and Exports Data. IESO, 2017. Available online: http://www.ieso.ca/en/power-data/supplyoverview/imports-and-exports (accessed on 5 April 2018)."
10.3390_en12142675,"data https, data available, data",71,,0,"for Ontario. Int. J. Hydrogen Energy 2012, 37, 7343–7354. [CrossRef] IESO. 2017 Electricity Data, Independent Electricity System Operator. 2018. Available online: http: //www.ieso.ca/corporate-ieso/media/year-end-data (accessed on 9 May 2018). Independent Electricity System Operator, Hourly Ontario Energy Price Ieso. 2012. Available online: https://www.ieso.ca/imoweb/marketdata/hoep.asp (accessed on 16 May 2018)."
10.3390_en12173287,"data available, data",45,,0,"from the oﬃcial annual reports of the Consorzio Italiano Biogas (CIB), the Gestore Servizi Energetici (GSE) and the data available through the European Project ISAAC (Increasing Social Awareness and ACceptance of biogas and biomethane) [23–25]."
10.3390_en13010140,"case study data, data",185,,0,"To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS. This case is used as a reference while comparing the total power losses reduction in all considered cases. Then Case-I considers only PVs in place. After that, Case-II includes solar PVs and an aggregated BESS. Finally, Case-III comprises solar PVs and distributed BESSs. A detailed description of each case is presented as per the following subsections:"
10.3390_en13010140,"case study data, data",337,,0,"Energies 2020, 13, x FOR PEER REVIEW 8 of 16 operation is the operation that will do this task as shown in (13) and (14), where γ and σ are random numbers between 0 and 1. ()mdmmnXXXXγ=+− (11) ()ndnnmXXXXγ=+− (12) maxmin()mdmdXXXXσ=+− (13) maxmin()ndndXXXXσ=+−. (14) Several GA approaches have been utilized in recent studies such as [7,15,29]. It indicates and highlights the high effectiveness of the GA compared to other stochastic approaches that are based on the error percentage of the solution and the processing duration. Figure 5 illustrates the utilization of GA to find the optimal allocation of BESS. DN DataBESS DataGA and Load FlowOptimal Allocation of BESS Figure 5. Illustration of GA utilization to find BESS optimal allocation. In this study, the DN data and the BESSs data are fed as inputs to the GA. The input data has been processed through many iterations and GA internal operations. The resultant of the GA processing is the optimal locations of the BESSs in the DN which assure the minimum average power losses. 4. Results and Discussions To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS."
10.3390_en13010140,"case study data, data",341,,0,"DN DataBESS DataGA and Load FlowOptimal Allocation of BESS Figure 5. Illustration of GA utilization to find BESS optimal allocation. In this study, the DN data and the BESSs data are fed as inputs to the GA. The input data has been processed through many iterations and GA internal operations. The resultant of the GA processing is the optimal locations of the BESSs in the DN which assure the minimum average power losses. 4. Results and Discussions To validate the proposed technique in this paper, the IEEE 33-Bus system has been utilized as a case study and incorporated the provided data in [33]. The system is linked with the utility grid via a transmission line with limited capacity. The presented work contains four cases that have been simulated using MATLAB software. The focus of this work was the total power losses reduction in DN. Starting with the reference case study, which is used to compare the improvements before and after the addition of the solar PV systems, an aggregated BESS or distributed BESSs. Hence, the reference case demonstrates the DN with no solar PV systems and BESS. This case is used as a reference while comparing the total power losses reduction in all considered cases. Then Case-I considers only PVs in place. After that, Case-II includes solar PVs and an aggregated BESS. Finally, Case-III comprises solar PVs and distributed BESSs. A detailed description of each case is presented as per the following subsections: 4.1. Distribution System with Solar PVs: Case-I Six solar PVs, each with a size of 1.6 kW, are added arbitrary in the distribution network, as shown in Figure 6. This case demonstrates DN with a high level of PV penetration. The PVs are installed on buses 3, 8, 14, 25, 30, and 31. Energies 2020, 13, 140"
10.3390_en13010197,code,21,,0,"32. DL/T 596—1996 Prevenive Test Code for Electric Power Equipment; China Electric Power Press: Beijing, China,"
10.3390_en13010197,database,12,,0,3. Weight Determination Based on Fuzzy Iteration and Expert Weighted Database
10.3390_en13010197,"database, data",112,,0,"After the key state quantity of the power distribution switch is selected, it is necessary to perform reasonable weight allocation for each state quantity to perform comprehensive evaluation of the state of the distribution network equipment. In this paper, we use the eclectic fuzzy decision-making and multi-level fuzzy comprehensive evaluation model to analyze the previous data of the distribution transformer; continuously update the weight ratio of the evaluation set through the weight inverse operation; reduce the inﬂuence of subjective factors brought by the expert review opinions; and improve the data, the reliability of the analysis and ultimately the establishment of a weight expert database."
10.3390_en13010197,"database, data",156,,0,"In this paper, based on the large amount of operational information generated during the operation of the distribution network equipment, a state evaluation model of the distribution network equipment integrating multi-source information is established. The model comprehensively considers the critical state quantities of the distribution network equipment, and based on the fuzzy iterative method of big data and the establishment of the weight expert database, weights the multi-source information and reasonably evaluates the equipment status. Finally, taking the distribution transformer as an example, the evaluation results of the fusion of multi-source information proposed in this paper are proved to be more comprehensive. The method proposed in this paper can accurately judge the running status of the power distribution equipment based on various types of information, and provide a reference for the subsequent power marketing evaluation of the user equipment state, which is more instructive."
10.3390_en13010197,"database, data",267,,0,"The fuzzy positive ideal and the fuzzy negative ideal are determined by analyzing a large number of transformers of the same type. This paper adopts a combination of eclectic fuzzy decision-making and multilevel fuzzy comprehensive evaluation, and the weights of quantitative indicators and qualitative indicators can be obtained by performing repeated fuzzy iterations. The weight ratio of the evaluation set is constantly updated through the weight inverse operation, reducing the inﬂuence of subjective factors brought by the expert review opinions and avoiding errors caused by data redundancy or errors or omissions, which improving the reliability of data analysis and promoting the establishment of the weight expert database, ﬁnally. The distances between the evaluated object and the fuzzy positive and negative ideals are determined, and the membership degree belonging to the fuzzy positive ideal is calculated. The greater the membership degree, the better the state of the transformer. The smaller the membership degree, the worse the state of the transformer, so it is necessary for the operation and maintenance personnel to pay attention to it and arrange the maintenance work in good time. The comparison and analysis of the relevant data in Tables 6 and 8 show that the ﬁnal score of each transformer in Table 8 can truly reﬂect the actual operation status of the transformer in Table 6, which provides quantitative parameters for the evaluation of distribution equipment, and which is beneﬁcial to the further focus and ﬁeld evaluation of the equipment, providing help for maintenance and operation."
10.3390_en13010197,"database, data",305,,0,"Energies 2020, 13, x FOR PEER REVIEW 7 of 15 improve the data, the reliability of the analysis and ultimately the establishment of a weight expert database. 3.1. Compromising Fuzzy Decision Weight Solving Process The flow chart of the compromise fuzzy decision [34] is shown in Figure 1. The basic principle is the virtual fuzzy positive ideal and the fuzzy negative ideal. Then, the Euclidean distance method is used to determine the distance between the candidate object and the fuzzy positive and negative ideals, and the membership degree belonging to the fuzzy positive ideal is calculated to determine the selection scheme. The greater the degree of membership, the better the solution and the priority. Triangle fuzzy number expression of indicator dataNormalization of fuzzy indicator matrixConstruct a fuzzy decision matrix, that is to perform weightingDetermine fuzzy positive ideals and fuzzy negative idealsMake fuzzy preference decision to obtain the membership degree of the evaluation objectInversely calculate counterweights by frequency statistics, or determine counterweights by fuzzy analytic hierarchy processCompare the inverse weight with the fuzzy weight and calculate the approximate rateApproximation rate is greater than expectedOutput the membership of the evaluation objectInversely calculate counterweights by frequency statistics, or determine counterweights by fuzzy analytic hierarchy processMake the inverse weights the construction weightsReconstruct fuzzy decision matrixendApproximation rate is smaller than expected Figure 1. A flow chart of eclectic fuzzy decision-making model. The basic solution steps for the compromising fuzzy decision are as follows: Step 1: The indicator data is transformed into a triangle fuzzy number representation. Let()FR be the overall fuzzy set on R, set ()MFR∈. The membership function Mμ of M is expressed as Energies 2020, 13, 197"
10.3390_en13020429,code,105,,0,"Simulink 1N). This is mostly due to the lower running time of the plant (9.42% lower). On the other hand, the ORC unit works most of the time at operating conditions having higher electrical eﬃciencies and as a result the mean electric eﬃciency is 3% higher (6.65% compared to 6.44% of the Simulink 1N). The CPU time in Table 3 is measured on a workstation equipped with 32 GB of RAM and the Intel Xeon E5530 @ 2.4 Ghz processor whilst the code is able to use a single thread only."
10.3390_en13020429,code,137,,0,". where the source term Qloss included in the advection Equation (1) can be positive in the case of heat losses from the internal ﬂuid to the ambient and vice versa. Considering that the pipelines under investigation are referred to a microscale CHP plant, a detailed solution of the equation as for DH networks is not necessary whilst the robustness and the velocity of the solving code are preferred. Hence, all the terms of the second member of Equation (1) can be deleted and the source term excluded, with good approximation. Indeed, Van der Heijde et al. [5] have shown that the diﬀusivity term can be neglected, while the pressure diﬀerence and the wall friction are not relevant in comparison with the"
10.3390_en13020505,code,156,,0,"Battery companies have the code of quality assurance of their products to ensure that the battery can operates normally for a certain number of cycles. In this paper, throughput capacity of energy is used as a life standard to evaluate whether the ESS should be retired. According to the battery energy, the number of life cycles, the cut-oﬀ capacity and the depth of discharge (DOD) speciﬁed in the technical agreement, the total throughput capacity of energy of the LTO battery can be calculated by Equation (18). When the cumulative energy throughput of the battery in actual use reaches this value, it can be judged that the battery should be out of service, as shown in Equation (19). At the same time, the decline of the battery life can also be roughly judged based on the used energy throughput."
10.3390_en13040898,code,190,,0,"Supplementary Materials: The following are available online at http://www.mdpi.com/1996-1073/13/4/898/s1, Figure S1: Analysis of Desmodesmus sp. nl3 S516 intron. (a) Phylogenetic analysis of S516 group I introns of IE and IC1 classes amongst microalgae based on the P3–P8 conserved structures, (b) pairwise analysis of group I introns of the Scenedesmaceae family. Figure S2: Amino acid sequences of homing endonucleases (HE) of Naegleria jamiesoni (Njam, AAB71747.1), Porphyra umbilicalis (Pumb, AAV35433), Allovahlkampﬁa spelaea (Aspe, ABD62811), Coemansia mojavensis (Cmoj, BAB87243), Naegleria philippinensis (Nphil, CAJ44447), Desmodesmus opoliensis GS2j (DopS2j, AB917110), Sclerotinia sclerotiorium (Sscl, XP_001587714). Color code (according Clustal X color scheme): blue: hydrophobic residues; red: positively charged residues; magenta: negatively charged residues; green: polar residues; cyan: aromatic residues, orange: glycine residue, yellow: proline residue. See [21] for detailed analysis of HE."
10.3390_en13040898,database,49,,0,The overlapping partial sequences between two consecutive sequences were assembled using NCBI Blast Tool (Standard nucleotide blast) to obtain a complete 18S rDNA–ITS1-5.8S-ITS2 sequence which was compared with sequences on NCBI database for hits. The sequence of nl3 is deposited in GenBank (MN746324).
10.3390_en13040898,database,116,,0,"For identiﬁcation, sequence of 18S rDNA excluding the introns and complete ITS1-5.8S-ITS2 region of isolate nl3 was searched against non-redundant nucleotide database for homologous sequences using online blast program (BLASTN) (http://www.ncbi.nlm.nih.gov/BLAST/). Clustal X2.1 software was then employed for the automatic multiple alignments of homologous sequences and studied sequence. A preliminary analysis with Paup was carried out [22]. Phylogenetic trees were generated using maximum likelihood (ML, GTR, G + I:4 model) with TREEFINDER [23], distance (neighbour-joining, K2 model) and maximum parsimony (MP) with MEGA7 [24]), with 1000 bootstraps."
10.3390_en13040898,database,315,,0,"A PCR fragment was obtained using the primers NS1 and ITS4 (Table 1) for both strains named nl3 and nl6. These primers, originally described to amplify the 18S-ITS1-5.8S-ITS2 rDNA region in fungi [25], proved also to work well for land plants [26]. The PCR fragment has a size of around 3500 bp for nl3 and 2600 bp for nl6. PCR products were sequenced using the primers listed in Table 1 in both forward and reverse orientations and blasted against the sequences deposited on NCBI database. The BLAST search results showed for nl6 a 100% identity with N. salina (D12, accession number JX185299.1), while nl3 proved to be closer although not identical to Desmodesmus sp. GM4i (AB917136.1). The sequence of Desmodesmus sp. nl3 is deposited in GenBank (MN746324). Desmodesmus sp. nl3 and Desmodesmus sp. GM4i, both have an intron inserted at the same position in the 18S rDNA, S516 (referring to the insertion in the rDNA of E. coli), although the nl3 is longer in size (754 pb versus 404 bp). The presence of S516 intron in Desmodesmus GM4i has already been reported [27]. Therefore, we analyzed the secondary structure of the nl3 S516 intron. It comprises the 9 typical stem-loop structures of group I introns, with an additional loop of 347 bp at the level of the P9.3 branch (Figure 1). Overall, the deduced structure and sequence variations strongly indicate that the intron belongs to the E class [28], probably of the E2 type. Another group I intron, S1046, class C, is also present in the 18S rDNA of the strain nl3."
10.3390_en13040898,dataset,17,,0,"Datasets. Mol. Biol. Evol. 2016, 33, 1870–1874. [CrossRef]"
10.3390_en13040937,"data available, data",135,,0,"Considering the areas obtained for each created scenario with the suggested zones for the cultivation of energy crops and the implementation of microalgae crops, and bearing in mind the realistic yield data found in the literature, it is possible to estimate the production values theoretically for each species and scenario. From this starting point, Table 6 presents the value of the areas obtained (according to ArcGIS software), as well as the percentage of these areas out of the whole Portugal mainland area of 89,015 km2, the minimum and maximum productivity (according to Table 1) and the estimated minimum and maximum production for each crop. It is important to specify that the calculated productivity values are overestimated, considering a productivity of 100%."
10.3390_en13040937,"data available, data",174,,0,"In order to obtain the appropriate areas for the crops implementation, it was necessary to compile as many data or factors as possible, according to information available from various sources, mainly on official websites of Portuguese and European institutions. The administrative maps of the territory and those related to land use and occupation provided by DGT have been considered; environmental aspects such as temperature, precipitation, sunshine and frost provided by APA, including the map created with CO2 production in each municipality; various ecological factors of soil and subsoil and the edapho-morphological aptitude for agriculture and forestry of the Ecological Planning, Investigation and Cartography - EPIC WebGIS platform (ISA-UL); protected areas and soils susceptible to desertification from ICNF; contaminated soils based on the mining areas managed by EDM and the capacity and treatment applied in the wastewater treatment plants (WWTPs) in mainland Portugal, according to the EEA platform, each factor being considered a spatial thematic layer."
10.3390_en13040937,"data available, data",204,,0,"Based on data found in the literature, the saline soils are considered marginal, therefore, areas with moderate and high concentrations of saline elements are inadequate for the implementation of food crops [7]. The term salinization refers to areas with low precipitation and high evapotranspiration that causes salt accumulation making it impossible to wash on the soil surface. These areas can be found in the coastal part of the territory [8]. Much of the marginal land could be used for agriculture due to the quality and type of soil, however, many of them, are found in high zones, with high slopes, hard-to-reach areas or abandoned land, that are no longer used for this purpose [8] and now are considered suitable for other purposes such as the implementation of energy crops. For these reasons, in this study, we consider as marginal lands the pasture areas such as natural herbaceous vegetation, areas with dense, light dense undergrowth, dense and dense sclerophyte vegetation, other woody formations and, lastly, areas related to uncovered spaces or with sparse vegetation [44]."
10.3390_en13040937,"data available, download, data",337,,0,"To select the energy crops to implement in the mainland Portugal; To search which types of soils are of interest and present a low ILUC (indirect land-use change) risk; To search and download all colletected maps finded in shapefile or raster format from official websites of Portuguese Institutions like Agência Portuguesa do Ambiente (APA), Instituto Superior de Agronomia da Universidade de Lisboa (ISA-UL), Instituto da Conservação da Natureza e das Florestas (ICNF), Empresa de Desenvolvimento Mineiro (EDM) and European Institutions too as the European Environment Agency (EEA); To create the georeferenced databases on ArcGIS, an ArcMAP document (tool of ArcGIS software) has to be created for each chosen culture, introducing only the selected maps for specif properties and/or attributes of interest such as temperature, precipitation, frost, land steepness, soil texture, soil pH, soil thickness, presence of physical obstacles, ecological soil value, current permeability, natural and semi-natural vegetation with conservation value, soil-morphological aptitude to irrigated agriculture and silviculture, soil susceptibility to desertification, protected areas, land use and land cover (COS 2010 and COS 2015), corine land cover (CLC 2012), contaminated soils, wastewater treatment plant capacity, CO2 production in the energy and industrial sectors in mainland Portugal, among others. Bearing in mind the characteristics of growth and adaptation of each culture combined with the intersection of all maps, output data have been obtained suggesting available and appropriate areas for the cultivation of each culture. The productivity forecasting and predicted bioenergy generation are presented and critically discussed; Lastly, the publication on Laboratório Nacional de Energia e Geologia–LNEG´s spatial data infrastructure, i.e., institutional geoportal of energy and geology, to access all the created maps and related information. Figure 1 represents a summary of the applied methodology."
10.3390_en13040937,database,51,,0,"The database for georeferenced mapping of the mainland territory, to evaluate areas of potential interest for the cultivation of energy crops, microalgae, as well as to map the cultivated agricultural/silvicultural species (including their residues), was created with ArcGIS software, a tool for GIS."
10.3390_en13040937,"database, data",232,,0,"Abstract: The main objective of the Portuguese project “CONVERTE-Biomass Potential for Energy” is to support the transition to a low-carbon economy, identifying biomass typologies in mainland Portugal, namely agri-forest waste, energy crops and microalgae. Therefore, the aim was to design and construct a georeferenced (mapping) database for mainland Portugal, to identify land availability for the implementation of energy crops and microalgae cultures, and to locate agricultural and forestry production areas (including their residues) with potential for sustainable exploitation for energy. The ArcGIS software was used as a Geographic Information System (GIS) tool, introducing the data corresponding to the type of soil, water needs and edaphoclimatic conditions in shapefile and raster data type, to assess the areas for the implantation of the biomass of interest. After analysing the data of interest in each map in ArcGIS, the intersection of all maps is presented, suggesting adequate areas and predicting biomass productions for the implementation of each culture in mainland Portugal. Under the conditions of the study, cardoon (72 kha, 1085 kt), paulownia (81 kha, 26 kt) and microalgae (29 kha, 1616 kt) presented the greater viability to be exploited as biomass to energy in degraded and marginal soils."
10.3390_en13051203,dataset,244,,0,"high-quality scenarios. Step 0: initialization. Randomly select S H, and set the estimated social welfare as ˆΨs = 0, ∀s ∈ S H. Initialize the training dataset as empty. Step 1: social welfare calculation. Solve the GEP problem using Algorithm 1, and use its optimal solution (x∗, p∗) and Equation (18) to calculate the actual social welfare for the set of high-quality scenarios S H. Checkpoint: The algorithm ﬁnishes if the error between actual and estimated social welfare is small enough and proceeds to Step 2 otherwise. Step 2: social welfare estimation. Add new results from Step 1 to the training dataset; obtain updated regression parameters for Model in Equation (20), then use the updated model to estimate social welfare values for the whole set of scenarios S W. Step 3: scenario selection. Update the set of high-quality scenarios S H by minimizing the Kantorovich distance from S W, DKan(S H, S W), which was deﬁned in Equation (19). Various heuristic algorithms, such as the golden section search method [49], can be used in this step. As proven by Dupaˇcová et al. [17], the probabilities of high-quality scenarios are given by ps = ∑ pi, ∀s ∈ S H, where"
10.3390_en13051203,"dataset provided, data",155,,0,"We use a linear regression model to provide a computationally efﬁcient estimation of social welfare. Conceptually, high-quality scenarios could be selected by ﬁrst calculating the social welfare for all scenarios and then selecting a subset to minimize the probabilistic distance between the distributions of social welfare resulting from the high-quality and whole set of scenarios. However, this approach requires solving the GEP problem millions of times using the time consuming Algorithm 1. Alternatively, our strategy is to train a regression model to estimate social welfare and select the high-quality scenarios based on the estimated rather than actual social welfare values. If trained efﬁciently, the regression model requires only a small set of training data to provide reasonably accurate estimation; thus, we only need to use Algorithm 1 to calculate the actual social welfare for a small number of scenarios to produce the training data."
10.3390_en13051203,"used dataset, dataset",208,,0,"We used the same dataset for the U.S. Eastern and Western Interconnections as used in [50] with some modiﬁcations. The dataset contained 169 buses, 730 transmission lines, 1640 existing generators, and 1568 candidate generators, representing the transmission infrastructures of the North American power grid. The locations of the 169 buses are shown in Figure 3. Demand for each year was divided into 19 load blocks. There were 60 generation technologies and fuel types, including coal, gas, oil, nuclear, hydro, geothermal, biomass, wind, and solar. Approximately 30% of existing generation capacity was renewable, and this ratio was required to increase by 1% each year, so that it would reach 45% by the end of year 15. One million demand and fuel cost elements of the scenarios were randomly generated with an average 1% annual growth rate for both. All algorithms were implemented in MATLAB [51], and to solve the models, we used MATLAB and the TOMLAB interface [52] to call CPLEX V.12 [53], used as the mixed integer linear programming solver."
10.3390_en13051246,"code, data",231,,0,"Abstract: Rooftop gardens ona building have proved to be a good way to improve its storm water management, but many other beneﬁts can be obtained from the installation of these systems, such as reduction of energy consumption, decrease of the heat stress, abatement on CO2 emissions, etc. In this paper, the eﬀect from the presence of these rooftop gardens on abuilding’s energy consumption has been investigated by experimental campaigns using a green roof ona public building in a Mediterranean location in Spain. The obtained results demonstrate a substantial improvement by the installation of the green roof onthe building’s cooling energy demand for a standard summer day, in the order of 30%, and a reduction, about 15%, in the heating energy demand for a winter day. Thus, given the longer duration of the summer conditions along the year, a noticeable reduction on energy demand could be obtained. Simulation analysis, using commercial software TRNSYS code, previously calibrated using experimental data for typical summer and winter days, allows for the extrapolation to the entire year of these results deducing noticeable improvement in energy eﬃciency, in the order of 19%, but with an increase of 6% in the peak power during the winter period."
10.3390_en13051246,"dataset provided, data",57,,0,"A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at diﬀerent layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools."
10.3390_en13051246,"dataset provided, data",154,,0,"Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, 1246"
10.3390_en13051246,"dataset provided, data",326,,0,"The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, 1246"
10.3390_en13051246,"dataset provided, data",345,,0,"Energies 2020, 13, x FOR PEER REVIEW 4 of 15  Figure 3. Initial roof structure. The green roof was built over the present “inverted roof”. It was decided to remove the layer of gravel and a water retention layer was added below the growing medium (separated with a filter fabric layer). This storage layer increases the capacity of the roof for retaining water after a rain episode and significantly reduces the amount of runoff generated. Figure 4 displays the green roof structure, which includes the following layers: growth medium (80 mm thickness), permeable textile layer (2 mm), drainage layer (water storage layer, 30 mm), geotextile layer/root barrier layer (3 mm), XPS insulation (40 mm), waterproofing membrane (5 mm) and a concrete hollow block (300 mm). Figure 4. Green roof structure. The growth medium is a mixture of conventional gardening organic substrate (40%), volcanic lava rocks (40%) and silica sand (20%). In the upper part of the green roof, there are plants covering almost the entire area with a height in the range of 50 to 150 mm. These plants are genus sedum (a mixture of sedum album AH, sedum floriferum, sedum sediform, sedum reflexum, sedum spurium, sedum moranense and sedum acre). Figure 5 displays the plan view of the building, denoting the roof area where the green roof was installed by the dotted line. The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof."
10.3390_en13051246,"dataset provided, data",350,,0,"Figure 4. Green roof structure. The growth medium is a mixture of conventional gardening organic substrate (40%), volcanic lava rocks (40%) and silica sand (20%). In the upper part of the green roof, there are plants covering almost the entire area with a height in the range of 50 to 150 mm. These plants are genus sedum (a mixture of sedum album AH, sedum floriferum, sedum sediform, sedum reflexum, sedum spurium, sedum moranense and sedum acre). Figure 5 displays the plan view of the building, denoting the roof area where the green roof was installed by the dotted line. The building area under controlled conditions with green and conventional rooftop was 280 m2. Figure 6 shows the green roof already installed. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7. Monitoring system scheme. A set of six type T thermocouples, whose positions and missions are detailed in Table 1, enables to determine the evolution of the temperature at different layers of the roof. These temperature measurements allow identifying similar ambient temperature conditions in the registered data sets and provide the data required for the simulation tools. Specifications of the thermocouple sensors were: Probe PT100 RS PRO M16, PT100, +100 to + 450 °C, diameter 6mm, Connection head, Class B 4 Stainless Steel. Energies 2020, 13, x FOR PEER REVIEW 5 of 15  Figure 5. General view of building roof (dotted line indicating green roof affected area). Figure 6. General view of greenroof. The monitoring system used for these experiments is presented in Figure 7. Figure 7."
10.3390_en13081967,"dataset, data",41,,0,"48. Borradaile, G.J. Statistics of Earth Science Data; Springer: Berlin/Heidelberg, Germany, 2003. 49. Chuchro, M.; Danek, M. Selection of optimal gridded dataset for application in Polish Sudetes Mountains."
10.3390_en13092250,"data available, data",343,,0,"Therefore, instead of assuming the concept of future data, it will be useful to implement an algorithm that recognizes the distribution of the features of arriving data. Besides, in some applications of prosumers, the robustness of the methods is essential to differentiate outliers from concept drifts. However, in this study, the agent trusts the external information he receives from measurement systems and weather information services. In the data stream, there could be different kinds of drifts mixed and outliers data samples. Another important concern with the concept drift in the speciﬁc context of the prosumer agent is the concurrence of the drift types. Thus the agent could be facing sudden drifts, gradual drifts, and incremental drifts within one timeframe [27]. For that reason, adaptation algorithms that were made to solve problems related to speciﬁc cases of drift are not the best option for the agent. Here, we test some of those algorithms to validate this afﬁrmation. It is impractical for a prosumer agent to have different models trained with different data sets and ensemble the forecasts. Therefore, considering the available mechanisms to update the agents knowledge, when using a single model, the only strategy is to adapt the parameters. Nevertheless, in addition to parameters’ adaptation, it is also possible to combine the models by weighting them as in ensemble learning [28]. The choice of the method should be made by taking into account the residential agent’s restrictions related to the processing time and hardware limitation. Normally, the models of three main groups are used to provide information for other processing systems, such as a Home Energy Management Systems (HEMS), with a practical objective, like either minimizing energy cost or maximizing comfort. These systems usually provide results in ﬁve to ﬁfteen minutes intervals, thus limiting convenient exploitation of ensemble learning."
10.3390_en13092250,"data available, data",347,,0,"The appearance of gradual drifts makes it impractical to assume that the concept of future data is always closer to the latest data. Therefore, instead of assuming the concept of future data, it will be useful to implement an algorithm that recognizes the distribution of the features of arriving data. Besides, in some applications of prosumers, the robustness of the methods is essential to differentiate outliers from concept drifts. However, in this study, the agent trusts the external information he receives from measurement systems and weather information services. In the data stream, there could be different kinds of drifts mixed and outliers data samples. Another important concern with the concept drift in the speciﬁc context of the prosumer agent is the concurrence of the drift types. Thus the agent could be facing sudden drifts, gradual drifts, and incremental drifts within one timeframe [27]. For that reason, adaptation algorithms that were made to solve problems related to speciﬁc cases of drift are not the best option for the agent. Here, we test some of those algorithms to validate this afﬁrmation. It is impractical for a prosumer agent to have different models trained with different data sets and ensemble the forecasts. Therefore, considering the available mechanisms to update the agents knowledge, when using a single model, the only strategy is to adapt the parameters. Nevertheless, in addition to parameters’ adaptation, it is also possible to combine the models by weighting them as in ensemble learning [28]. The choice of the method should be made by taking into account the residential agent’s restrictions related to the processing time and hardware limitation. Normally, the models of three main groups are used to provide information for other processing systems, such as a Home Energy Management Systems (HEMS), with a practical objective, like either minimizing energy cost or maximizing comfort."
10.3390_en13092250,"database, data",144,,0,"Afterwards, the models were adapted during the selected period with the techniques presented earlier. The average RMSE (and NRMS) for each case is presented in Table 1. The results obtained with the proposed method are systematically better than the most straightforward adaptation by sliding window, which means that implementing the method gives more reliable information to the prosumer than not doing so, even though the error reduction may be small. Other algorithms will occasionally perform better for adapting speciﬁc models. For example, the golden ratio method reduces the error when adapting the power generation model, but it is not suitable to adapt the thermal load model. Furthermore, the proposed method forgets less data, thus making it possible for the prosumer to use a single database for all three models."
10.3390_en13092250,"database, data",196,,0,"Generally, for a residential prosumer agent, it is possible to distinguish two environments that are labeled as local and external. The former refers to the behind-the-meter resources [7], while the latter describes situations where the prosumer agent can interact with other agents and information services. In most cases, the external environment only collects data of either weather variables or their forecast, but in a decentralized management scheme, it is possible to consider the external environment as a multi-agent system (MAS) [8]. The agent is able to perceive the local environment by observing the power consumption data of different appliances. In fact, it constructs a time-series database by accumulating new information from a data stream [9]. However, this process is problematic since agents have limitations on memory and processing time [10]. In addition, the data stream can drift over time, thus causing previously trained data models of appliances to lose accuracy [11]. Therefore, model adaptation on the basis of recent data is essential [12]."
10.3390_en13092250,dataset,27,,0,"24. Moreno-Torres, J.G.; Raeder, T.; Alaiz-Rodríguez, R.; Chawla, N.V.; Herrera, F. A unifying view on dataset"
10.3390_en13092250,"dataset, data",206,,0,"method [33]. The measurement of ﬁt, in this case, is the RMSE because it gives more weight to bigger deviations; thus, it is better to identify the appearance of concept drifts [36]. It is relevant to mention that the threshold to accept the results of the cross-validation depends on the nature of the target variable of each model [37]. The test data set will be the closest batch to future features. Now, to identify that batch, the distance will be measured as in the FISH method [34] as a combined distance in time and space of the samples. If the result of the cross-validation test is not good enough, then the model will be retrained only with the closest N batches. The parameter N needs to be tuned according to the model to avoid convergence problems in training but knowing that, when a concept drift appears, it is safer to train with a small amount of data to ensure that all samples correspond to the new concept. Figure 3 summarizes the proposed algorithm with the procedure for when new data arrives."
10.3390_en13092250,open-source,15,,0,"J. Open Source Softw. 2018, 3, 884. [CrossRef]"
10.3390_en13092250,"open-source, data",176,,0,"Germany, 2001. Odell, J.; Giorgini, P.; Müller, J. Agent-Oriented Software Engineering V; Springer: Berlin/Heidelberg, Germany, 2004. Damisa, U.; Nwulu, N.I.; Sun, Y. Microgrid energy and reserve management incorporating prosumer behind-the-meter resources. IET Renew. Power Gener. 2018, 12, 910–919. [CrossRef] Zhang, Y.; Huang, T.; Bompard, E.F. Big data analytics in smart grids: A review. Energy Inform. 2018, 1, 1–24. [CrossRef] Khamphanchai, W.; Saha, A.; Rathinavel, K.; Kuzlu, M.; Pipattanasomporn, M.; Rahman, S.; Akyol, B.; Haack, J. Conceptual Architecture of Building Energy Management Open Source Software (BEMOSS). In Proceedings of the 2014 5th IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe), Istanbul, Turkey, 12–15 October 2014; pp. 1–6. [CrossRef]"
10.3390_en13092250,python,65,,0,"38. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Müller, A.; Nothman, J.; Louppe, G.; et al. Scikit-learn: Machine Learning in Python. Psychol. Sci. 2012, 25, 1682–1690. [CrossRef]"
10.3390_en13092250,"python, package",25,,0,"39. Holmgren, W.; Hansen, C.; Mikofski, M. pvlib python: A python package for modeling solar energy systems."
10.3390_en13092375,"code package, package",79,,0,"The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power ﬂow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China."
10.3390_en13092375,"code package, package",290,,0,"Energies 2020, 13, x FOR PEER REVIEW 11 of 19 5. Case Analysis The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power flow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China. 5.1. Test System Description The simplified diagram of the large-scale wind power base sending-side system in Gansu Province is shown in Figure 8. The receiving-side system is equivalent to an infinity bus system. Hexi Substation is the border node in the sending-side that the transmission channel connects to. Four thermal generation plants are involved; the capacities of the units in each plant are shown in Table 1; and the relative parameters of each kind of unit are shown in Table 2. The electrical distance parameters that describe the grid topology are shown in Table 3 (the resistances of the lines are small and ignored). The base power is 𝑆(cid:3003)=100 𝑀𝑉𝐴 in the system. While calculating the transient, stability-constrained TTC of the transmission channel, the transient models of the involved electronic components are used, and the relative parameters and the TTC calculation process are shown in the Appendix. Figure 8. Simplified diagram of the large-scale wind power base sending-side system in Gansu Province. Table 1. Capacities of units in each thermal generation plant."
10.3390_en13092375,"code package, package",339,,0,"Case Analysis The proposed optimal day-ahead thermal generation scheduling method to enhance TTC was applied to the large-scale wind power base sending-side system in Gansu Province in China. The optimization models were solved using the above algorithms in MATLAB. TTC was calculated using the transient, stability-constrained continuation power flow method [23], using the Power System Analysis Software Package (PSASP), which is widely used for power system calculations and simulations in China. 5.1. Test System Description The simplified diagram of the large-scale wind power base sending-side system in Gansu Province is shown in Figure 8. The receiving-side system is equivalent to an infinity bus system. Hexi Substation is the border node in the sending-side that the transmission channel connects to. Four thermal generation plants are involved; the capacities of the units in each plant are shown in Table 1; and the relative parameters of each kind of unit are shown in Table 2. The electrical distance parameters that describe the grid topology are shown in Table 3 (the resistances of the lines are small and ignored). The base power is 𝑆(cid:3003)=100 𝑀𝑉𝐴 in the system. While calculating the transient, stability-constrained TTC of the transmission channel, the transient models of the involved electronic components are used, and the relative parameters and the TTC calculation process are shown in the Appendix. Figure 8. Simplified diagram of the large-scale wind power base sending-side system in Gansu Province. Table 1. Capacities of units in each thermal generation plant. Thermal Generation Plant Unit Number Unit Capacity /(MW) JC Unit 1 500 Unit 2 500 Unit 3 500 Unit 4 300 Unit 5 200 ZY Unit 1 200 Unit 2 100 Unit 3 100 JQ Unit 1 500 Unit 2 300 Unit 3 200 BLS Unit 1 200 Unit 2 100 Unit 3 100 Energies 2020, 13, 2375"
10.3390_en13112825,"data https, data",162,,0,"A preliminary achievement regarding the universality objective is reported in Reference [52] where a probabilistic data association ﬁlter [56,57] was employed to associate the online measurements from batteries to their model parameters, thus, resulting in a chemistry-adaptive BFG. Further research needs to be done on this topic so that reliable algorithms can be developed to extend adaptivity for load-range, size, temperature, nominal voltage and age as well. This would require large computing power that the traditional battery management systems are not allocated for, for example, portable electronics. Cloud computing [58] allows one to outsource intense computing to external sources; that is, by combining information fusion with cloud computing, a greater deal of universality can be achieved in battery management systems, paving the way for optimal battery reuse (see Section 2.5) and reduced electronic clutter in households and work places."
10.3390_en13112825,"data https, data available, data",29,,0,"57. Bar-Shalom, Y.; Willett, P.K.; Tian, X. Tracking and Data Fusion; YBS Publishing: 2011. Available online: http:"
10.3390_en13143528,"data available, data",97,,0,"In this study, initially, to remove the eﬀect of bad data (e.g., null values, bad log measurements) available data were processed. Afterward, data from wells 2 and 3 were selected as the training (25596 samples), test (5484 samples), and validation set (5484 samples). Then, a three-layer feed forward neural network (Figure 2) with sigmoid hidden neurons and linear output neurons was used to build the model. The network was trained with the Levenberg–Marquardt backpropagation algorithm."
10.3390_en13143528,"data available, data",138,,0,"Geomechanics has shown its potential to cover a broad range of work during the lifespan of a ﬁeld from exploration to production and then abandonment [1,2]. By examining the available data and comprehending the key issues observed while drilling previous wells, we can predict the optimum way of drilling and ﬁnding proper casing shoe locations in the development phase. By coupling ﬂuid-ﬂow with the stress–strain regime in the ﬁeld, compaction and subsidence analyses can be performed [3,4]. The basic approach to geomechanics analysis is to process available data for predicting rock elastic properties, in-situ stresses, and pore pressures. Shear velocity (Vs) is a kind of data that is not always available for wells, especially old wells, and needs to be predicted."
10.3390_en13143528,"data available, data",342,,0,"Energies 2020, 13, 3528 4 of 16  some weight functions and then are linked to the output layer that corresponds to the results we are looking for. In this study, initially, to remove the effect of bad data (e.g., null values, bad log measurements) available data were processed. Afterward, data from wells 2 and 3 were selected as the training (25596 samples), test (5484 samples), and validation set (5484 samples). Then, a three‐layer feed forward neural network (Figure 2) with sigmoid hidden neurons and linear output neurons was used to build the model. The network was trained with the Levenberg–Marquardt backpropagation algorithm. Figure 2. Neural network model including 3 hidden layers. 3 inputs are DTC, GR, RHOZ, and the single output is DTS. After gathering all required data, by assuming elastic isotropy and using the following equations, elastic moduli such as dynamic Young’s modulus and Poisson’s ratio were calculated [40], 𝜈(cid:3404)12(cid:4666)𝐷𝑇𝑆DTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (7) 𝐸(cid:3005)(cid:3404)𝑅𝐻𝑂𝐵(cid:4666)1DTS(cid:4667)(cid:2870)3(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)4(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (8) where ν and ED represent Poisson’s ratio and dynamic Young’s modulus. However, Equation (8) will provide a dynamic elastic modulus which needs to be converted to static modulus using available correlations such as the equation presented by Eissa and Kazi [41]."
10.3390_en13143528,"data available, data",349,,0,"Figure 2. Neural network model including 3 hidden layers. 3 inputs are DTC, GR, RHOZ, and the single output is DTS. After gathering all required data, by assuming elastic isotropy and using the following equations, elastic moduli such as dynamic Young’s modulus and Poisson’s ratio were calculated [40], 𝜈(cid:3404)12(cid:4666)𝐷𝑇𝑆DTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (7) 𝐸(cid:3005)(cid:3404)𝑅𝐻𝑂𝐵(cid:4666)1DTS(cid:4667)(cid:2870)3(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)4(cid:4666)DTSDTC(cid:3415)(cid:4667)(cid:2870)(cid:3398)1, (8) where ν and ED represent Poisson’s ratio and dynamic Young’s modulus. However, Equation (8) will provide a dynamic elastic modulus which needs to be converted to static modulus using available correlations such as the equation presented by Eissa and Kazi [41]. The reason for this conversion is due to the difference in the measurement condition [42,43]. Uniaxial compressive strength (UCS) and friction angle (φ) were also estimated based on Plumb’s correlation [44]. Calculated rock properties for Well‐1 are presented in Figure 3. The next step in analyzing wellbore stability is predicting pore pressure. In this study, MDT (modular formation dynamics tester) data in the reservoir section, and Eaton method in the clay‐rich intervals were used to predict pore pressure and interpolated for other sections as well [45]. In order to find the clay‐rich intervals, the cross plot of the Gamma ray versus density log, as in Figure 4, was used."
10.3390_en13143528,"data available, data",361,,0,"In this study, MDT (modular formation dynamics tester) data in the reservoir section, and Eaton method in the clay‐rich intervals were used to predict pore pressure and interpolated for other sections as well [45]. In order to find the clay‐rich intervals, the cross plot of the Gamma ray versus density log, as in Figure 4, was used. The pore pressure profile was then calibrated by used mud weight and observed kicks while drilling. The Eaton equation is presented below [46], 𝑃(cid:3043)(cid:3034)(cid:3404)𝑂𝐵𝐺(cid:3398)(cid:4666)𝑂𝐵𝐺(cid:3398)𝑃(cid:3043)(cid:3041)(cid:4667)(cid:4666)(cid:3005)(cid:3021)(cid:3004)(cid:3041)(cid:2888)(cid:2904)(cid:2887) (cid:4667)(cid:2871), (9) where Ppg is pore pressure gradient, OBG is overburden gradient, Ppn is normal pore pressure gradient, and DTCn is the transient time of compressional wave in the normally pressured zone. Vertical stress (σv) was then calculated based on the weight of overburden [47], 𝜎(cid:3049)(cid:3404)𝑔(cid:1516)RHOB 𝑑𝑧(cid:3053)(cid:2880)(cid:3269)(cid:3271)(cid:3253)(cid:2868), (10) where g is the gravitational acceleration, RHOB is bulk density log, and z refers to depth. Principal horizontal stresses should be predicted using indirect methods such as poroelastic horizontal strain method [48], by considering the tectonic strains: 𝜎(cid:3035)(cid:3404)𝜐1(cid:3398)𝜐𝜎(cid:3049)(cid:3397)1(cid:3398)2𝜐1(cid:3398)𝜐𝛼𝑃(cid:3043)(cid:3397)𝐸1(cid:3398)𝜐(cid:2870)𝜀(cid:3051)(cid:3397)𝜐𝐸1(cid:3398)𝜐(cid:2870)𝜀(cid:3052), (11) Energies 2020, 13, 3528"
10.3390_en13143600,"dataset provided, data",338,,0,"The energy saving potential was analyzed from climate characteristics, natural lighting, and natural ventilation [11]. It was indicated that more than 40% of the total energy consumption was expended by air conditioning systems for airport terminals in China [12]. Energy consumption data of 29 airport terminals in Greece were obtained; comparison of energy consumption levels was conducted between airport terminals and other types of public buildings. The energy conservation potential was analyzed on three airport terminals, among them, in detail, around the premise that indoor environmental quality was met [13]. The energy saving potential of the public buildings were mainly studied on the HVAC system [14,15] and lighting system [16,17]. Energy saving eﬀect under diﬀerent forms of air conditioning systems, lighting power, and heat transfer coeﬃcient of the envelope was studied by simulation, based on energy consumption data of public buildings in Tianjin [18]. According to the benchmarking index system established by H. Li, et al. [19], the energy saving measures were developed mainly from reasonable ventilation cooling load and improvement of air conditioning system energy performance. The vehicle exhaust and the building operation are the main sources of CO2 emissions in coach stations. Research of the vehicle emissions in transportation terminals was most concentrated on pollutant emissions of vehicle exhaust and its negative eﬀects on the indoor environment of the terminals [20,21]. According to research on a coach station with 30,000 daily passengers in cold region of China, annual pollutant emissions (CO and PM2.5) from vehicle exhaust were 0.43t/a in the coach station [22]. In a coach station, in a hot summer and cold winter region in China, the CO2 emissions from building operations accounted for 47.25% of the total CO2 emissions [23]. It is indicated that"
10.3390_en13143600,"dataset provided, data",339,,0,"Most research on the energy consumption of public buildings was based on detailed ﬁeld investigations on energy consumption of each energy-using sector. The annual total energy consumption of airport terminals and railway stations was 206.3 and 115.7 kWh/(m2·a), respectively [8]. The sub-item annual power consumption of eight railway stations in diﬀerent climate regions in China were obtained, results showed that the HVAC system was the largest energy-using sector. The energy saving potential was analyzed from climate characteristics, natural lighting, and natural ventilation [11]. It was indicated that more than 40% of the total energy consumption was expended by air conditioning systems for airport terminals in China [12]. Energy consumption data of 29 airport terminals in Greece were obtained; comparison of energy consumption levels was conducted between airport terminals and other types of public buildings. The energy conservation potential was analyzed on three airport terminals, among them, in detail, around the premise that indoor environmental quality was met [13]. The energy saving potential of the public buildings were mainly studied on the HVAC system [14,15] and lighting system [16,17]. Energy saving eﬀect under diﬀerent forms of air conditioning systems, lighting power, and heat transfer coeﬃcient of the envelope was studied by simulation, based on energy consumption data of public buildings in Tianjin [18]. According to the benchmarking index system established by H. Li, et al. [19], the energy saving measures were developed mainly from reasonable ventilation cooling load and improvement of air conditioning system energy performance. The vehicle exhaust and the building operation are the main sources of CO2 emissions in coach stations. Research of the vehicle emissions in transportation terminals was most concentrated on pollutant emissions of vehicle exhaust and its negative eﬀects on the indoor environment of the terminals [20,21]."
10.3390_en13143695,"dataset, data",43,,0,where x1 and x2 represent the data set of temperature and irradiance. While the expression for reference value of peak power voltage given by ANN is represented as general expression in Equation (3) which can also be written as:
10.3390_en13143695,"dataset, data",117,,0,"After training, ANN is tested by giving different inputs and then the model predictive output of an ANN is obtained which is compared with an already available model that was obtained by multiple linear regression of the data set. In this comparison, as shown in Figure 6, irradiance is kept constant at 1000 W/m2 and only temperature is varied for testing the trained ANN model with the one obtained by multiple linear regression algorithm, therefore in Figure 6, only x1 is represented on the x-axis while x2, which is irradiance, is taken as constant for computation of peak power voltage which is the output y of both algorithms."
10.3390_en13164055,code,63,,0,"In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental eﬀects of the vortex on the energy absorption capability of the hydro-turbine. This multi-purpose code solves three-dimensional RANS equations for steady and turbulent ﬂuid ﬂow, especially in the ﬁeld of turbomachinery simulations and analyses."
10.3390_en13164055,code,324,,0,"In this study, four values of tip-clearance size, namely, δ = 0%, 0.25%, 0.5%, and 0.75% (corresponding to actual sizes of 0.0, 0.95, 1.9, and 2.85 mm, respectively) were considered to analyze the tip-clearance effects. For each TCS case, a series of monitoring points were set along the blade tip, as shown in Figure 2. The points, PS1–PS11, were located on the pressure side of the tip region, from the leading to the trailing edge of the runner blade. In the same way, there were corresponding points named SS1–SS11 at the blade-tip suction side. Similar monitoring points were also placed at blade spans 0.8 and 0.6 (80% and 60% of blade height) to collect pressure statistic at these positions to examine pressure fluctuations. Figure 2. Monitoring points at blade tip. 3. Numerical Method 3.1. Turbulence Model and Boundary Conditions In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental effects of the vortex on the energy absorption capability of the hydro-turbine. This multi-purpose code solves three-dimensional RANS equations for steady and turbulent fluid flow, especially in the field of turbomachinery simulations and analyses. The shear-stress transport k–ω turbulence model was developed by Menter to solve the complex flows by combining the advantages of the k–ε model in the far-field and of the k–ω model in viscous sub-layer at near-wall regions [21]. It was chosen as the turbulence model for simulations because of its good capability in estimating vortex appearance and flow separation with adverse pressure gradients near the complex geometry surfaces. The validation of the SST turbulence model can be Energies 2020, 13, 4055"
10.3390_en13164055,code,343,,0,"Energies 2020, 13, x FOR PEER REVIEW 4 of 18 The pressure coefficient is a dimensionless number describing the relative pressure throughout a flow field, and is defined as 𝐶𝑃=𝑃𝑙𝑜𝑐𝑎𝑙0.5𝜌𝑈𝑡𝑖𝑝2 (3) where Plocal, local static pressure; and Utip, blade tip velocity in the design condition. Hydraulic power, output power, and hydraulic efficiency are calculated using the following equations: 𝑃ℎ=𝜌𝑔𝑄𝐻 (4) 𝑃𝑚𝑒𝑐=𝑇𝜔 (5) 𝜂ℎ=𝑃𝑚𝑒𝑐𝑃ℎ (6) where Ph, hydraulic power; Pmec, mechanical power generated in turbine shaft; ρ, water density; ηh, hydraulic efficiency; Q, flow rate; H, effective head; T, torque on runner blade; and ω, rotational speed. In this study, four values of tip-clearance size, namely, δ = 0%, 0.25%, 0.5%, and 0.75% (corresponding to actual sizes of 0.0, 0.95, 1.9, and 2.85 mm, respectively) were considered to analyze the tip-clearance effects. For each TCS case, a series of monitoring points were set along the blade tip, as shown in Figure 2. The points, PS1–PS11, were located on the pressure side of the tip region, from the leading to the trailing edge of the runner blade. In the same way, there were corresponding points named SS1–SS11 at the blade-tip suction side. Similar monitoring points were also placed at blade spans 0.8 and 0.6 (80% and 60% of blade height) to collect pressure statistic at these positions to examine pressure fluctuations. Figure 2. Monitoring points at blade tip. 3. Numerical Method 3.1. Turbulence Model and Boundary Conditions In the current study, CFD simulations were performed with commercial code ANSYS-CFX 17.0 to predict the formation of the tip-leakage vortex and evaluate the detrimental effects of the vortex on the energy absorption capability of the hydro-turbine."
10.3390_en13164055,"dataset provided, data",107,,0,"Nevertheless, the aforementioned literature mainly focused on the research objects of axial pumps, mixed-ﬂow pumps, and gas turbines, while tip clearance in hydro-turbines had not been considered. In this study, numerical simulations applying the shear-stress transport (SST) k–ω turbulence model were conducted to demonstrate TLV formation under design and oﬀ-design conditions, and evaluate the inﬂuence of diﬀerent tip-clearance sizes on the energy performance and stability of a propeller turbine. Computational results were compared and validated by experimental data provided in the references to ensure the precision of the numerical method. The distribution of velocity vectors and"
10.3390_en13164055,package,86,,0,"The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The reﬁned mesh was implemented at the blade’s hub and shroud to achieve higher mesh density. Meanwhile, all wall surfaces (including blades and pipes) adopted 10-layer inﬂation to ensure accurate predictions of the"
10.3390_en13164055,package,336,,0,"Figure 3 shows three components of the computational domains in simulations, namely, the suction, runner, and discharge domains. Since the four runner blades were the same as each other in all aspects, one-fourth of the simulation domain (90° periodicity) was utilized instead of the full-size domain in order to reduce calculation time and computational resources. At the inlet of the suction domain, total pressure corresponding to the designed gross head was applied with a turbulence intensity of 5%. Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains. At first, the steady-state simulations were carried out to assess the impact of tip-clearance size on turbine performance; then, results were taken as initial suggestions for the transient simulations to monitor pressure fluctuations at the tip-gap region of the turbine. The time step for transient simulations was 1.1 × 10−3 s corresponding to 3o when the turbine rotates at the design speed. The convergence criterion was set as 10–5 for the root-mean-square residual value. Figure 3. Computational domains. 3.2. Mesh Strategy and Grid Independent Analysis The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The refined mesh was implemented at the blade’s hub and shroud to achieve higher mesh density."
10.3390_en13164055,package,341,,0,"Energies 2020, 13, x FOR PEER REVIEW 5 of 18 found in research performed by Zhang et al. [10], Bardina et al. [22], and Chen et al. [23]. Moreover, a number of studies involving the flow in pump-jet propulsors, mixed-flow pumps, and propeller turbines have confirmed the reliability of the SST k-ω turbulence model with favorable results [13,14,16]. In this turbulence model, the eddy viscosity is defined as the function of the turbulence kinetic energy-k and turbulence frequency-ω: 𝜇𝑡=𝜌𝛼1𝑘𝑚𝑎𝑥⁡(𝑆𝐹2,𝛼1𝜔) (7) 𝐹2=𝑡𝑎𝑛ℎ[{𝑚𝑎𝑥(2√𝑘𝛽′𝜔𝑦;500𝜇𝜌𝑦2𝜔)}2] (8) where α1 and β’, model constants: α1 = 5/9, β’ = 0.09 [24]; F2, blending function in boundary layer flow; and S, an invariant measure of the strain rate. Figure 3 shows three components of the computational domains in simulations, namely, the suction, runner, and discharge domains. Since the four runner blades were the same as each other in all aspects, one-fourth of the simulation domain (90° periodicity) was utilized instead of the full-size domain in order to reduce calculation time and computational resources. At the inlet of the suction domain, total pressure corresponding to the designed gross head was applied with a turbulence intensity of 5%. Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains."
10.3390_en13164055,package,347,,0,"Meanwhile, the flow rate was specified at the outlet of the discharge domain. At all wall boundaries, smooth and no-slip wall conditions were enabled. The rotational periodicity interface option was put into service for the couples of the side surfaces in each domain. As for the interfaces between stationary and rotating domains, the multi-reference frame approach was applied to steady-state calculations and the sliding mesh method was adopted in transient calculations to exchange the flow field information between domains. At first, the steady-state simulations were carried out to assess the impact of tip-clearance size on turbine performance; then, results were taken as initial suggestions for the transient simulations to monitor pressure fluctuations at the tip-gap region of the turbine. The time step for transient simulations was 1.1 × 10−3 s corresponding to 3o when the turbine rotates at the design speed. The convergence criterion was set as 10–5 for the root-mean-square residual value. Figure 3. Computational domains. 3.2. Mesh Strategy and Grid Independent Analysis The reliability of the numerical calculations is related to the quality of the computational gird. In the current study, all domains were computationally discretized on the basis of structured hexahedral elements using the TurboGrid package in ANSYS Workbench, as shown in Figure 4. The refined mesh was implemented at the blade’s hub and shroud to achieve higher mesh density. Meanwhile, all wall surfaces (including blades and pipes) adopted 10-layer inflation to ensure accurate predictions of the flow field at near-wall regions. The local grid at the tip-gap region was also generated with 30 layers due to the complicated flow pattern in this area, giving rise to better observation of the vortex structure. The Y+ value for the wall grid of the turbine blade was around 1, satisfying the demand of SST k–ω turbulence model to precisely predict flow behavior in the viscous sub-layer [21]. Energies 2020, 13, 4055"
10.3390_en13184709,"data available, data",336,,0,"Energies 2020, 13, x FOR PEER REVIEW 10 of 14 Moreover, the effects of the initial maximum capillary pressure and the viscosity of the non-wetting phase on water imbibition are studied in this work, and the corresponding results are shown in Figures 11 and 12. According to the figures, when the initial capillary pressure increases and the non-wetting phase viscosity decreases, the corresponding imbibition rates increase. However, the final imbibition recovery factor tends to be similar, which is because the water imbibition process is a capillary-dominated flow and the recovery factor is controlled by capillarity, which is affected by pore shape, aspect ratio, contact angle, etc. Therefore, the viscous forces in the reservoir conditions increase the imbibition rates but not the final recovery factor. Figure 11. Spontaneous imbibition with different initial maximum capillary pressures. Figure 12. Spontaneous imbibition with different non-wetting phase viscosities. To further investigate the capillarity effect on the water imbibition of Barnett shale, the following sensitivity studies are conducted: the determination of the pore throat aspect ratio, contact angle and shape factor of the cross-area. The corresponding results for the water imbibition recovery factor with respect to dimensionless time are shown in Figures 13–15, respectively. A higher aspect ratio tends to increase the percentage of the non-wetting phase trapped by the snap-off effect, which increases the residual non-wetting phase saturation and hence the final recovery factor (c.f. Figure 13). When the contact angle and cross-area shape factor decrease, leading to an increase of capillary pressure, the imbibition rate increases at the beginning. However, the final recovery factor decreases slightly (c.f. Figures 13 and 14) because snap-off tends to occur more frequently with a lower contact angle and shape factor, as shown in Equation (13). Energies 2020, 13, 4709"
10.3390_en13184709,"data available, data",341,,0,"Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures."
10.3390_en13184709,"data available, data",341,,0,"Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]."
10.3390_en13184709,"data available, data",344,,0,"Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8."
10.3390_en13184709,"data available, data",347,,0,"Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, 4709"
10.3390_en13184709,"data available, data",347,,0,"th in cm [49]; we follow the same units here. According to the figure, with the increasing proportion of oil-wet pores, the imbibition rate and recovery factor decrease dramatically. The results indicate that wettability dominates the water imbibition characteristics. Figure 6. Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process."
10.3390_en13184709,"data available, data",348,,0,"Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability. The corresponding imbibition recovery with respect to dimensionless time is shown in Figure 9. With the existence of micro-fractures, the imbibition rate increases significantly while the final recovery remains similar. The micro-factures contribute to the conductance of the pore network instead of the pore volume, and the increase of conductance increases the imbibition rate but not the final recovery, as the final recovery is controlled by the capillarity of the matrix pores and throats. Figure 8. Schematic of micro-fracture distributions in the pore network. Figure 9. Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10."
10.3390_en13184709,"data available, data",349,,0,"Energies 2020, 13, x FOR PEER REVIEW 8 of 14  Figure 5. Water saturation profiles of the water imbibition process using the proposed dynamic pore network model with an average water saturation equal to 0.2239. Generally, a mixed wettability is assumed for shale and tight formations. Here, we considered mixed wet conditions for Barnett shale. We assigned three types of mixed wet conditions—30% oil-wet, 50% oil-wet and 70% oil-wet—as shown in Figure 6. The corresponding spontaneous water imbibition-induced oil recoveries are shown in Figure 7 for dimensionless time 𝑡(cid:3005)=𝐶𝑡(cid:3495)(cid:3038)(cid:3109)(cid:3097)(cid:3091)(cid:3298)(cid:2869)(cid:3013)(cid:3118) , where 𝐶 is the unit conversion factor, which is equal to 0.018849 if 𝑡 is in minutes, 𝑘 in md, 𝜙 in decimal, 𝜎 in 𝑚𝑁/𝑚, 𝜇(cid:3050) in cp, and L is a characteristic length in cm [49]; we follow the same units here. According to the figure, with the increasing proportion of oil-wet pores, the imbibition rate and recovery factor decrease dramatically. The results indicate that wettability dominates the water imbibition characteristics. Figure 6. Three types of mixed wet conditions. Figure 7. Spontaneous imbibition recovery with respect to dimensionless time for different wettability distributions. Energies 2020, 13, x FOR PEER REVIEW 9 of 14 The existence of micro-fractures is considered in this work to investigate their effect on water imbibition, as natural and hydraulic fractures are presented within shale and tight formations. Four lines of micro-fractures are assigned within the Barnett shale pore network, and the pore radius is set as 20 nanometers (c.f. Figure 8), which is 3.23 times the mean radius of the Barnett shale matrix; therefore, the micro-fracture permeability is approximately 10 times that of the matrix permeability."
10.3390_en13184709,"data available, data",350,,0,"Water imbibition recovery factor with and without micro-fractures. Based on the above analysis, the simulated imbibition recovery factor with respect to dimensionless time is compared with the laboratory experimental work of Barnett shale samples by Morsy and Sheng [16]. The corresponding results are shown in Figure 10. Both the effects of wettability and micro-fractures are considered to fit the experimental data. Mixed wettability for Barnett shale is found within the fitting process. Since micro-fractures are observed in the imbibition experiments, the characteristic length is reduced significantly. In the fitting results, the effective characteristic length of Barnett shale samples is approximately several millimeters, and the fraction of oil-wet pores ranges from 70% to 50%. Figure 10. Imbibition recovery factors of Barnett shale: pore network modeling and experiments [16]. Energies 2020, 13, x FOR PEER REVIEW 10 of 14 Moreover, the effects of the initial maximum capillary pressure and the viscosity of the non-wetting phase on water imbibition are studied in this work, and the corresponding results are shown in Figures 11 and 12. According to the figures, when the initial capillary pressure increases and the non-wetting phase viscosity decreases, the corresponding imbibition rates increase. However, the final imbibition recovery factor tends to be similar, which is because the water imbibition process is a capillary-dominated flow and the recovery factor is controlled by capillarity, which is affected by pore shape, aspect ratio, contact angle, etc. Therefore, the viscous forces in the reservoir conditions increase the imbibition rates but not the final recovery factor. Figure 11. Spontaneous imbibition with different initial maximum capillary pressures. Figure 12. Spontaneous imbibition with different non-wetting phase viscosities. To further investigate the capillarity effect on the water imbibition of Barnett shale, the following sensitivity studies are conducted: the determination of the pore throat aspect ratio, contact angle and shape factor of the cross-area."
10.3390_en13195050,"data available, data",128,,0,"Behavioral validation aims to compare the model-generated behavior to that of a real system. During the construction of the baseline model, our model used a speciﬁc case: China’s data obtained from available references or interviews about the real system [41]. Submodels were adopted in the LCT model, such as environmental sensitivity and feedback between supply and demand. The adopted submodels of the existing models can serve as structural validation for ABM, and Denmark can be used as one of representatives of the LCT model where the environmental sensitivity and feedback between supply and demand has been performed [1]. Therefore, we compared our simulation data with the real change that occurred in Denmark."
10.3390_en13195065,"dataset, data",293,,0,"particles therein. Wang et al. [4], Liu, et al. [5], Oda et al. [13], Mitchell et al. [14] analyzed changes to the permeability during crack propagation in granite, where seepage only occurs after the onset of cracking. Alkan [15] explored the corresponding relationship between change of permeability in salt rock and acoustic emission information, ﬁnding that the acoustic emission event concentration occurred in the area of variation of permeability. Nara et al. [16] investigated changes in wave velocity during permeability evolution in basalt. Moreover, Pereira et al. [17] researched the eﬀect of fracture distribution on permeability. They all found that the permeability of intact rocks was very low and increased only when cracks appeared. Liu et al. [18] studied the correspondence between creep deformation of mudstone and pore pressure and found the permeability evolution could be determined by the microstructural evolution of rock. Li et al. [19] found that axial strain (in a rock specimen) has a functional relationship with permeability. Pradip et al. [20] and Legrand et al. [21] investigated the rate of convergence under stable seepage in crushed rock and pressure-drop data during seepage and used a capillary model to predict the experimental data expressed in terms of a pore friction factor. In the test results of rock failure under hydraulic coupling, there are few complete datasets relating to volumetric strain and circumferential strain and the correlations of coeﬃcient of permeability with hydraulic gradient and volumetric strain remain unclear."
10.3390_en13215563,"data available, data",87,,0,"This paper pays attention to the moving target surveillance by a group of UAVs. A practical application of the considered scenario is that, in wireless sensor networks, the sensor nodes collect data from the environment. UAVs function as data sinks to collect the sensory data from sensor nodes [8]. In general, the number of available UAVs is smaller than that of the sensor nodes. Thus, the UAVs carry out a periodical surveillance of the sensor nodes."
10.3390_en13225874,"case study data, data",295,,0,"Abstract: Annual mean wind speed distribution models for power generation based on regional wind resource maps are limited by spatial and temporal resolutions. These models, in general, do not consider the impact of local terrain and atmospheric circulations. In this study, long-term ﬁve-year wind data at three sites on the North, East, and West of the Baltimore metropolitan area, Maryland, USA are statistically analyzed. The Weibull probability density function was deﬁned based on the observatory data. Despite seasonal and spatial variability in the wind resource, the annual mean wind speed for all sites is around 3 m/s, suggesting the region is not suitable for large-scale power generation. However, it does display a wind power capacity that might allow for non-grid connected small-scale wind turbine applications. Technical and economic performance evaluations of more than 150 conventional small-scale wind turbines showed that an annual capacity factor and electricity production of 11% and 1990 kWh, respectively, are achievable. It results in a payback period of 13 years. Government incentives can improve the economic feasibility and attractiveness of investments in small wind turbines. To reduce the payback period lower than 10 years, modern/unconventional wind harvesting technologies are found to be an appealing option in this region. Key contributions of this work are (1) highlighting the need for studying the urban physics rather than just the regional wind resource maps for wind development projects in the build-environment, (2) illustrating the implementation of this approach in a real case study of Maryland, and (3) utilizing techno-economic data to determine suitable wind harnessing solutions for the studied sites."
10.3390_en13225874,"dataset, data",323,,0,"US Department of Energy (DOE); Energy Information Administration (EIA). Annual Energy Outlook 2050. 2020. Available online: https://www.eia.gov/outlooks/aeo/pdf/AEO2020%20Full%20Report.pdf (accessed on 15 July 2020). US Department of Energy (DOE). Wind Vision: A New Era for Wind Power in the United States. 2020. Available online: http://energy.gov/eere/wind/wind-vision (accessed on 15 July 2020). Goudarzi, N.; Zhu, W.D. A review on the development of the wind turbine generators across the world. Int. J. Dyn. Control 2013, 1, 192–202. [CrossRef] Lee, J.A.; Doubrawa, P.; Xue, L.; Newman, A.J.; Daxl, C.; Scott, G. Wind resource assessment for Alaska’s oﬀshore region: Validation of a 14-year high-resolution WRF data set. Energies 2019, 12, 2780. [CrossRef] Dorrell, J.; Lee, K. The cost of wind: Negative economic eﬀects of global wind energy development. Energies 2020, 13, 3667. [CrossRef] Poore, R.; Lettenmaier, T. Alternative Design Study Report: WindPACT Advanced Wind Turbine Drive Train Designs Study; NREL/SR-500-33196; National Renewable Energy Laboratory (NREL): Golden, CO, USA, 2003. Polinder, H.; Van Der Pijl, F.F.A.; De Vilder, G.-J.; Tavner, P.J. Comparison of Direct-Drive and Geared Generator Concepts for Wind Turbines. IEEE Trans. Energy Convers. 2006, 21, 725–733. [CrossRef] Goudarzi, N.; Zhu, W.D. Oﬀshore and onshore wind energy conversion: The potential of a novel multiple-generator drivetrain. Key Eng. Mater. 2013, 569–570, 644–651. [CrossRef]"
10.3390_en13236216,"dataset provided, data",268,,0,"In order to get information about possible modiﬁcations of the hard carbon layers arrangement induced by the diﬀerent binders used for electrodes formulation, Raman spectra was performed on the CGDHC-based electrodes prepared with CMC, Alg, PAA, and PVDF binders, respectively labeled as CGDHC-CMC, CGDHC-Alg, CGDHC-PAA, and CGDHC-PVDF. This information can be obtained by calculating the ratio between the ID and IG bands. The comparison of Raman spectra of electrodes with diﬀerent binders for LIBs and NIBs is shown in Figure S3, while the calculated ID /IG and La are presented in Table S1. As expected, all the Raman spectra present the same characteristic peaks −1 (G-band) [8], together with already observed for the powder at ~1345 cm −1, correlated to 2D and D+G bands [31]. Commonly, two broad peaks in the range of 2650–2950 cm the graphitization degree of hard carbon is bound to conductivity [23], while porosity and defects enable surface storage processes (thus relevant for NIBs) [33]. However, in the present case the data provide evidence, for all electrodes, of similar peak shapes and only small variations of the intensity ratio between D and G bands. This behavior may be attributed to only minor interactions of the active CGDHC and of the conductive additive with the functional groups. Therefore, we may exclude a relevant role of the binder in modifying structural or conduction properties of the active materials."
10.3390_en14010108,code,28,,0,Funding: This work was supported by the funding program PIACERI 2020-22 of the University of Catania (project code 22722132140; principal investigator M.V.).
10.3390_en14010123,"dataset, data",46,,0,"Step 2. Identiﬁcation of the SARIMA model, all its seasonal and non-seasonal parameters. In this step autocorrelation (ACF) and partial autocorrelation (PACF) are examined to determine the best combinational order of the SARIMA model for each data set."
10.3390_en14010123,"dataset, data",183,,0,"The high frequency components are identified from IMF 1 to IMF 4, and the low frequency components are given from IMF 5 to IMF 9. The last component is the residual showing the trend of the original time series. The first model implemented in this paper is a SARIMA without autoregressive coefficients, p and P both equal 0 (no significant positive spikes in ACF and PACF plots). The ACF and PACF plots are used as a starting point to determine the best SARIMA parameters. According to the most used notation the model belongs to the SARIMA (0,1,1) × (0,1,1)12 type. Since the time series data of AQI have seasonal and non-seasonal trends, a non-seasonal and seasonal first order differencing are employed. In this model D of 1 calculate a first order seasonal difference and a Q = 1 use a first order errors in the model. Also, the value m = 12 means that the data is monthly and suggests a yearly seasonal cycle."
10.3390_en14010123,"dataset, data",333,,0,"Energies 2020, 13, x FOR PEER REVIEW 13 of 28  (a) (b) Figure 6. (a) AQI from 2015 to 2019, (b) AQI for 2020 for months from January to October. In general, there are two steps to AQI forecast used in this paper: Step 1. Decompose the original time series of the AQI into components (IMFs) and residual (residual), from 2015 to 2019. Step 2. Identification of the SARIMA model, all its seasonal and non-seasonal parameters. In this step autocorrelation (ACF) and partial autocorrelation (PACF) are examined to determine the best combinational order of the SARIMA model for each data set. In the first step, nine IMFs are obtained, and one residue, shown in Figure 7. The high frequency components are identified from IMF 1 to IMF 4, and the low frequency components are given from IMF 5 to IMF 9. The last component is the residual showing the trend of the original time series. The first model implemented in this paper is a SARIMA without autoregressive coefficients, p and P both equal 0 (no significant positive spikes in ACF and PACF plots). The ACF and PACF plots are used as a starting point to determine the best SARIMA parameters. According to the most used notation the model belongs to the SARIMA (0,1,1) × (0,1,1)12 type. Since the time series data of AQI have seasonal and non-seasonal trends, a non-seasonal and seasonal first order differencing are employed. In this model D of 1 calculate a first order seasonal difference and a Q = 1 use a first order errors in the model. Also, the value m = 12 means that the data is monthly and suggests a yearly seasonal cycle."
10.3390_en14010123,"dataset, data",428,,0,"The model can be analytically formulated as follows: (1−(cid:1828))(1−(cid:1828)(cid:2869)(cid:2870))(cid:1850)(cid:3047)=(cid:1853)+(1+(cid:2004)(cid:1828))(1+(cid:1990)(cid:1828)(cid:2869)(cid:2870))(cid:2013)(cid:3047) ⋮ (cid:1850)(cid:3047)=(cid:1853)+(cid:1850)(cid:3047)(cid:2879)(cid:2869)+(cid:1850)(cid:3047)(cid:2879)(cid:2869)(cid:2870)−(cid:1850)(cid:3047)(cid:2879)(cid:2869)(cid:2871)+(cid:2013)(cid:3047)+(cid:2004)(cid:2013)(cid:3047)(cid:2879)(cid:2869)+(cid:1990)(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2870)+(cid:2004)(cid:1990)(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2871) (17)It was found that the model fitted the data well and the stochastic seasonal fluctuation was successfully modelled. The AIC and BIC values of all selected models are shown in Table 6. Obviously from Table 6 can be seen that the model SARIMA (0,1,1) × (0,1,1)12 has the smallest value of AIC and BIC. Once the model is established, the parameters can be estimated using statistical techniques, such as least square estimation method. According to specific case in this paper and the estimation of the model parameters the equation is: (cid:1850)(cid:3047)=(cid:2013)(cid:3047)+0.8509(cid:2013)(cid:3047)(cid:2879)(cid:2869)−0.9321(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2870)−0.8323(cid:2013)(cid:3047)(cid:2879)(cid:2869)(cid:2871). Table 6. AIC and BIC values for optimal model selection. Models AIC BIC SARIMA(0,1,1) × (0,1,1)12 4.90 3.92 SARIMA(0,0,1) × (0,1,1)12 4.91 3.95 SARIMA(1,0,1) × (0,1,1)12 5.01 4.0 SARIMA(1,1,1) × (0,1,1)12 5.09 4.02 SARIMA(0,0,0) × (0,1,1)12 4.92 3.98 Energies 2021, 14, 123"
10.3390_en14010123,"publicly available, data",134,,0,"From the public service PVGIS the data on how many kWh in a given month at the target location with the given orientation and slope of the module can produce 1 kWp of photovoltaic modules are obtained. The required number of modules is determined by dividing the obtained required power by the power of one module. Also, the area of a photovoltaic system is simply obtained by multiplying the number of modules by the area of one module. The capacity of a solar prosumers is designed in such a way that its own production satisﬁes the customer’s needs for electricity in times of more expensive tariffs. The solar prosumer is connected to the installation of the building and is in parallel with the public distribution network."
10.3390_en14020484,package,165,,0,"The main application of the driver board is to drive the SM switching devices. In this context, an isolated driver with two complementary channels in a single package is used (SI824x from Silicon Labs [42]). This driver is speciﬁcally targeted to drive complementary switching devices (as in the case of half-bridge power converter developed). In addition, its main feature is the integrated deadtime generator between the high-side/low-side drivers that allow highly precise control for achieving optimal total harmonics distortion (THD). Another driver, HCPL-3120, is employed to drive an additional IGBT switch for the purpose of overvoltage protection, as explained in Section 3.3.2. Figure 8 shows the driver circuit board hardware. The pulse width modulation (PWM) input signals are supplied from the main central control unit and the outputs of this board are connected to the gate-emitter of each IGBT that composes the SM."
10.3390_en14020484,"publicly available, data",113,,0,"41. Barros, L.A.M.; Tanta, M.; Martins, A.P.; Afonso, J.L.; Pinto, J.G. STATCOM Evaluation in Electriﬁed Railway Using V/V and Scott Power Transformers. In Proceedings of the Sustainable Energy for Smart Cities, SESC 2019, Braga, Portugal, 4–6 December 2019; Afonso, J.L., Monteiro, V., Pinto, J.G., Eds.; Springer International Publishing: Cham, Switzerland, 2020; pp. 18–32. Silicon Labs Si824x—Class D Audio Driver with Precision Dead-Time Generator. Available online: https://www.silabs.com/ documents/public/data-sheets/Si824x.pdf (accessed on 9 October 2018)."
10.3390_en14030737,code,23,,0,11. European Commission. Commission Regulation (EU) 2016/1388 of 17 August 2016 Establishing a Network Code on Demand Connection;
10.3390_en14030737,code,52,,0,"10. European Commission. Commission Regulation (EU) 2016/1447 of 26 August 2016 Establishing a Network Code on Requirements for Grid Connection of High Voltage Direct Current Systems and Direct Current-Connected Power Park Modules (Text with EEA Relevance); European Commission: Brussels, Belgium, 2016."
10.3390_en14030737,code,74,,0,"In this paper, the deﬁnition used is the one from Network Code on Requirements for Grid Connection of Generators (NC RfG), which is in line with the deﬁnition from [18]: “synthetic inertia means the facility provided by a power park module or HVDC system to replace the effect of inertia of a synchronous power-generating module to a prescribed level of performance” [9]."
10.3390_en14030737,code,77,,0,"ISBN 978-83-01-20006-0. ENTSO-E. Need for Synthetic Inertia (SI) for Frequency Regulation: ENTSO-E Guidance Document for National Implementation for Network Codes on Grid Connection; ENTSO-E: Brussels, Belgium, 2018. European Commission. Commission Regulation (EU) 2016/631 of 14 April 2016 Establishing a Network Code on Requirements for Grid Connection of Generators (Text with EEA Relevance); European Commission: Brussels, Belgium, 2016."
10.3390_en14030737,"data available, data",229,,0,"In the presented case, time series data for 24 h period in 15 min interval is available. These data represent forecasted system load and generation. The results of a set of load ﬂow calculations are presented in Figure 7. Left-hand side plot shows a scenario in which only synchronous generation is present. The plot shows generation (black line) and system load (blue dashed line) patterns and calculated system kinetic energy Esys (red thick line). Note the reduced value of Esys at night indicating temporary tripping of generators due to low demand night period. Expected worst-case scenario RoCoF can also be noticed (red line) and should it exceed the threshold level necessary actions could be taken, as described in Step 4. Right-hand side plot of Figure 7 is constructed for a scenario in which a part of synchronous generation is superseded by wind generation, whose total power output is marked with green line. Calculated system inertia is, thus, lower and resulting RoCoF is lower. In this scenario, generation is also switched off during the night because of wind generation supplying low system demand. Note that there are many intervals for which RoCoF is below the threshold of −1 Hz/s, thus synthetic inertia is introduced."
10.3390_en14030737,"dataset provided, data",305,,0,"A modiﬁed version of the IEEE 14-bus system shown in Figure 4 is used for the purpose of providing a proof of concept. The modiﬁcation regarded ratings of the machines and their type are presented in Table 1. The example is based on data from Scenario 2, in which the largest generation trip was 117 MW (G3). Data for this power ﬂow are provided in Table 1, whereas the calculations are conﬁrmed by the plots in Figure 5. Based on Equation (6), the resulting RoCoFmax for this ∆P is equal to 1.92 Hz/s, whereas the goal is to limit the RoCoF to RoCoFlim = 1.0 Hz/s. According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic inertia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13). By using this relationship, the main time constant TSI can be adjusted to satisfy the requirement of the speed of response of the SI controller. Finally, based on Equations (13) and (14), gain KSI can be calculated to match the required amount of energy ∆Eτ and, in this example, is equal to 61.49 MWs/Hz."
10.3390_en14030737,"dataset provided, data",340,,0,"The results also con-firm that for the time period of interest, i.e., up to 1 s after the disturbance, the frequency response can be considered linear, which is the main assumption for parameterisation of the controller. The last plot in Figure 5 shows energy delivered to the power system by BESS during this disturbance. One second after the outage, it reached the value of 56.2, which fulfils the control objective expressed by Equation (14). Figure 4. Single line diagram of the modified 14-bus IEEE system with BESS added to bus 12; the BESS model is a generic current injection model with WECC BESS Control System for RMS simulation [33] (left) and equivalent block diagram for this model in a lumped form (right). Energies 2020, 13, x FOR PEER REVIEW 8 of 17 Table 1. Operating point of generators in the modified 14-bus IEEE system. Gen 1 Gen 2 Gen 3 Gen 6 Gen 8 Type Synchronous Synchronous Synchronous Wind Wind Rating [MVA]/Load [MW] 200/50 220/30 160/117 72/49 100/77 H [s] 3.2 4.0 2.0 0 0 Figure 5. Response to generation trip of 1. purely inertial system (grey line); 2. inertial system with synthetic inertia (SI support exhibiting linear decline of frequency in the first 0.5 s (red line); 3. same as no. 2 but with governor action (dashed line). 4. System Operation Planning Methodology with Focus on RoCoF As explained in Section 1, the assessment of inertia adequacy to support the required RoCoF throughout the whole planning period can be based on Equation (1) only if syn-chronous generators are the only type of generation supplying the system with power. Then, it is a matter of summing their respective kinetic energy and identifying the largest possible trip in the power system."
10.3390_en14030737,"dataset provided, data",343,,0,"Energies 2020, 13, x FOR PEER REVIEW 7 of 17 size of the disturbance but shortly after is influenced by instantaneous power delivered by the SI (which is the controller’s output). Thus, the required energy E500ms can be deliv-ered only if RoCoF is equal to RoCoFlim, which can happen only if Eτ is equal to E500ms. In practice, it is impossible to keep this equilibrium during the dynamic process, thus small deviations should be expected. 3.3. Synthetic Inertia Concept Verification Based on Simple Theoretical Model of a Power System A modified version of the IEEE 14-bus system shown in Figure 4 is used for the pur-pose of providing a proof of concept. The modification regarded ratings of the machines and their type are presented in Table 1. The example is based on data from Scenario 2, in which the largest generation trip was 117 MW (G3). Data for this power flow are provided in Table 1, whereas the calculations are confirmed by the plots in Figure 5. Based on Equation (6), the resulting RoCoFmax for this ΔP is equal to 1.92 Hz/s, whereas the goal is to limit the RoCoF to RoCoFlim = 1.0 Hz/s. According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic iner-tia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13)."
10.3390_en14030737,"dataset provided, data",345,,0,"Figure 4. Single line diagram of the modified 14-bus IEEE system with BESS added to bus 12; the BESS model is a generic current injection model with WECC BESS Control System for RMS simulation [33] (left) and equivalent block diagram for this model in a lumped form (right). Energies 2020, 13, x FOR PEER REVIEW 8 of 17 Table 1. Operating point of generators in the modified 14-bus IEEE system. Gen 1 Gen 2 Gen 3 Gen 6 Gen 8 Type Synchronous Synchronous Synchronous Wind Wind Rating [MVA]/Load [MW] 200/50 220/30 160/117 72/49 100/77 H [s] 3.2 4.0 2.0 0 0 Figure 5. Response to generation trip of 1. purely inertial system (grey line); 2. inertial system with synthetic inertia (SI support exhibiting linear decline of frequency in the first 0.5 s (red line); 3. same as no. 2 but with governor action (dashed line). 4. System Operation Planning Methodology with Focus on RoCoF As explained in Section 1, the assessment of inertia adequacy to support the required RoCoF throughout the whole planning period can be based on Equation (1) only if syn-chronous generators are the only type of generation supplying the system with power. Then, it is a matter of summing their respective kinetic energy and identifying the largest possible trip in the power system. However, this simple methodology does not enable us to include other inertia-providing resources in the calculation. For instance, virtual inertia provided by wind turbines or synthetic inertia from BESS with different energy to power ratios and control principles would be very difficult to be accurately represented in this equation. Therefore, there is a need for frequency stability assessment methodology that takes into account non-synchronous contributors to system inertia and frequency support. This methodology is explained below. It consists of five steps depicted in"
10.3390_en14030737,"dataset provided, data",352,,0,"According to Equation (10), this task entails delivering additional energy E1s from BESS equal to 56.2 MWs within the time of contribution of synthetic iner-tia, which in this case, is assumed to be one second. As a rule of thumb, the controller measurement time constant, Tm, is selected to be equal to 20 ms. An inherent feature of the controller described by Equation (12) with Tm = 0.02 s is that after time τ = TSI, it will deliver 61% of total energy, and after time τ = 2 TSI, 86% of energy, which is a direct consequence of Equation (13). By using this relationship, the main time constant TSI can be adjusted to satisfy the requirement of the speed of response of the SI controller. Finally, based on Equations (13) and (14), gain KSI can be calculated to match the required amount of energy ∆𝐸𝜏 and, in this example, is equal to 61.49 MWs/Hz. Figure 5 shows the result of G3 outage for three configurations of frequency control in the power system. The theoretical case with only inertial response is shown in red; the case with added SI contribution tuned according to the rules described above is shown in thick grey, whereas the same case with primary frequency control active is marked with a dashed line. Synthetic inertia brings considerable improvement to the frequency decline process: the frequency does not drop under the assumed level of 49.0 Hz during the first second, indicating that the average RoCoF is not lower than −1 Hz/s. The results also con-firm that for the time period of interest, i.e., up to 1 s after the disturbance, the frequency response can be considered linear, which is the main assumption for parameterisation of the controller. The last plot in Figure 5 shows energy delivered to the power system by BESS during this disturbance."
10.3390_en14030774,"code, data available, code available, data",332,,0,"The PSO engine de-termined DER PF settings by evaluating circuit performance to minimize the risk of volt-age or protection violations while also maximizing economic value. The formulation op-timization problem was designed to acquire the voltage regulation values of operating DERs off the unity PF value, as shown in Equation (5). min 𝑃𝐹[𝜔0∙𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)+𝜔1∙𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒)+𝜔2∙𝐶(𝑷𝑭)] (5) where: 𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)=1 if any |V | > 𝑉𝑙𝑖𝑚 (6) 𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒) is the standard deviation of 𝑽−𝑽𝑏𝑎𝑠𝑒 (7) 𝐶(𝑃𝐹)=∑1−|𝑷𝑭| (8) where, the variable 𝑽 represents the vector of bus voltages of the distribution system, while the variable 𝑽𝑏𝑎𝑠𝑒 is a vector the nominal voltages at each one of the buses. The variable PF is a vector representing the PFs of each one of the DERs in the system. For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values. The varia-ble 𝑉𝑙𝑖𝑚 represents the maximum and minimum voltage limit. These values were selected to comply with voltage range A of ANSI C84.1, which specifies voltage limits of ±0.05 pu. During the execution of the optimization algorithm, calculated solutions that deviate from the specified 𝑉𝑙𝑖𝑚 would be discouraged. Equation (8) is implemented to prevent calcu-lated values that deviate from unity PF, due to active power curtailment during high irra-diance condition. For the experiments performed in this paper, the weights used for vari-ables 𝜔0, 𝜔1 and 𝜔2 were 1.0, 2.0 and 0.05, respectively. The PSO algorithm was config-ured so that it would not execute if all the bus voltages of the distribution feeder were within the selected nominal voltage threshold of 0.20%."
10.3390_en14030774,"code, data available, code available, data",333,,0,"For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values. The varia-ble 𝑉𝑙𝑖𝑚 represents the maximum and minimum voltage limit. These values were selected to comply with voltage range A of ANSI C84.1, which specifies voltage limits of ±0.05 pu. During the execution of the optimization algorithm, calculated solutions that deviate from the specified 𝑉𝑙𝑖𝑚 would be discouraged. Equation (8) is implemented to prevent calcu-lated values that deviate from unity PF, due to active power curtailment during high irra-diance condition. For the experiments performed in this paper, the weights used for vari-ables 𝜔0, 𝜔1 and 𝜔2 were 1.0, 2.0 and 0.05, respectively. The PSO algorithm was config-ured so that it would not execute if all the bus voltages of the distribution feeder were within the selected nominal voltage threshold of 0.20%. Alternatively, bus voltage values outside the ANSI C84.1 Range A would execute the PSO algorithm. To reduce DER com-munication traffic, if the new calculated PF did not have an effect on the objective function, specified by a threshold value of 1×10−7, the DERs PF value would not be adjusted. Connected Energy Software, Cloud ApplicationParticle Swarm OptimizationWinIGS State EstimatorWinIGS Section 2 PDC Data CaptureWinIGS Section 1 PDC Data Capture PMU IEDs (metered locations)PMU IEDs (metered locations)State Estimation Solution (C37.118)Connected Energy DER Communication ModuleIEEE C37.118PV Production ForecastsCalculate P and Q for Loads for OpenDSSInstantiate OpenDSSInitial set of DER reactive power settingsRun OpenDSS over time horizonOptimal DER reactive power settingsUpdate DER power factors using PSOCalculate objective functionPV Production DatabaseWinIGS Section 3 PDC Data CapturePMU IEDs (metered locations)DERsIEDs Figure 2. Block Diagram of the Information Flow in the PSO Optimization Method. Energies 2021, 14, 774"
10.3390_en14030774,"code, data available, code available, data",350,,0,"Communications to/from the Connected Energy system used the DNP3 Application Note AN2013-001 information model to change the grid-support functions [92,93]. PF functions were used to change the active and reactive power behaviors of the PV systems. Data Bus (DBus) is a TCP/IP protocol developed by EPRI used to enable commu Energies 2021, 14, x FOR PEER REVIEW 7 of 20  nute over a 15-min horizon using 3 periods with a 5-min step size. The forecast PV pro-duction for each of the epochs was calculated using the forecasting code. In cases where there was no PV production data available, scaled surrogate PV system forecasts were used. The p_mult and q_mult values persisted for the entire time-domain simulation. PSO was selected to locate the optimal DER PF settings because the fitness landscape was non-convex due to the voltage regulators and other binary components. The PSO engine de-termined DER PF settings by evaluating circuit performance to minimize the risk of volt-age or protection violations while also maximizing economic value. The formulation op-timization problem was designed to acquire the voltage regulation values of operating DERs off the unity PF value, as shown in Equation (5). min 𝑃𝐹[𝜔0∙𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)+𝜔1∙𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒)+𝜔2∙𝐶(𝑷𝑭)] (5) where: 𝛿𝑣𝑖𝑜𝑙𝑎𝑡𝑖𝑜𝑛(𝑽)=1 if any |V | > 𝑉𝑙𝑖𝑚 (6) 𝜎(𝑽−𝑽𝑏𝑎𝑠𝑒) is the standard deviation of 𝑽−𝑽𝑏𝑎𝑠𝑒 (7) 𝐶(𝑃𝐹)=∑1−|𝑷𝑭| (8) where, the variable 𝑽 represents the vector of bus voltages of the distribution system, while the variable 𝑽𝑏𝑎𝑠𝑒 is a vector the nominal voltages at each one of the buses. The variable PF is a vector representing the PFs of each one of the DERs in the system. For the experiment, the objective function shown in Equation (5) was minimized when the bus voltages of the distribution system were at the desired 𝑽𝑏𝑎𝑠𝑒 and 𝑷𝑭 values."
10.3390_en14030774,"data available, data, python, code, code available",251,,0,"The optimization engine used the PSO method to determine the optimal PFs of controllable DER systems. This method determined the optimal PV PF setpoints and associated optimal power ﬂow (OPF) by wrapping an OpenDSS time series simulation of the reduced-order feeder model inside a PSO. The active and reactive components of the loads in the OpenDSS model were populated using live state estimation results. PV forecasts for each of the PV systems were populated in the OpenDSS model to optimize operations over a future time horizon. The OpenDSS load data was changed by the WinIGS state estimation solution and the PV production was populated by the PV forecasts. A simpliﬁed representation of the PSO approach is shown in Figure 2. A Python interface was created to capture the state estimation IEEE C37.118 data streams from WinIGS. These phasor data for each of the buses and PV systems were used to calculate the active and reactive power levels for the dynamic loads in the OpenDSS model. Then using the communication interface to OpenDSS, the active and reactive multipliers, p_mult and q_mult, were updated in the OpenDSS environment. The optimization was completed every minute over a 15-min horizon using 3 periods with a 5-min step size. The forecast PV production for each of the epochs was calculated using the forecasting code. In cases where there was no PV production data available, scaled surrogate PV system forecasts"
10.3390_en14030774,github,44,,2,"Supplementary Materials: The following are available online at https://ieeexplore.ieee.org/document/ 8478426. The anonymized, reduced-order OpenDSS and Opal-RT feeder model, and all portions of the non-proprietary ProDROMOS codebases are included in the project GitHub repository: https://github.com/sunspec/prodromos."
10.3390_en14041018,code,22,,0,"code for the simulation of ﬁxed-bed burners. Energy Convers. Manag. 2015, 105, 30–44. [CrossRef]"
10.3390_en14041018,code,29,,0,"Figure 4. Reaction rate as a function of initial particle radius for particles simulated with the semi-2D code with ρ = 700 kg/m3, AR = 2."
10.3390_en14041018,"data available, data",17,,0,Data Availability Statement: The data presented in this study are available in the Appendix A.
10.3390_en14041018,database,23,,0,"27. Chase, M.W., Jr. “Thermophysical Properties of Fluid Systems” in NIST Chemistry WebBook, NIST Standard Reference Database"
10.3390_en14041018,"dataset, data",176,,0,"The cross validation is made to ensure that the presented model is robust. It can be done in many ways; here, the random subset method [36] is used with 6 splits and 20 iterations, thus on average 17% of the data set is removed in each iteration. All models have been made with two PLS components. When doing the cross validation, preferably the obtained RMSECV values should be low, and ExpVar values high. The values in Tables 3 and 4 are the basis for choosing the relevant preprocessings for a model. Due to the lower RMSECV(A) and RMSECV(E) and higher ExpVar(X) (compared to the number of input data in X) and ExpVar(Y) values, model 2 has been chosen both for A and E. If models appear equally good or almost equally good based on RMSECV and ExpVar data, the simpler model is usually preferred within PLS."
10.3390_en14051297,"data https, data",46,,0,"a CAMEO Chemicals (https://cameochemicals.noaa.gov, accessed on 5 February 2021). b ChemAxon (http://www.chemicalize.org, accessed on 2 January 2021). c Hazardous Substances Data Bank (http://pubchem.ncbi.nih.gov, accessed on 2 January 2021)."
10.3390_en14051321,"data available, data",19,,0,Data Availability Statement: The data presented in this study are available on request from the corresponding author.
10.3390_en14051387,"benchmark, data",197,,0,"MPC simulations were carried out over 10 days, with a focus on the last 48-hour period, with outdoor temperature conditions ranging from −20 ◦C to 3 ◦C (Figure 9). A business as usual (BAU) indoor temperature setpoint proﬁle, with a nighttime setpoint of 18 ◦C and a daytime setpoint of 22 ◦C, was created as a benchmark, which is shown in the top graph of Figure 10 as a dashed black line. For the MPC studies, the zone temperature setpoint was constrained by a lower bound TSP,min (17 ◦C at night and 19 ◦C during the day) and an upper bound TSP,max (24 ◦C at all times) in order to maintain a level of thermal comfort for the zone occupants. To demonstrate the methodology, typical known occupancy schedules for the warehouse (7 a.m.–6 p.m.) and weather data were used for this MPC study; however, an existing weather forecast tool such as CanMETEO [45] could be incorporated into eventual implementation within the building automation system in a real building."
10.3390_en14051387,code,18,,0,order. Easy and quick modiﬁcations of the model are thus possible without rewriting the simulation code.
10.3390_en14051387,code,73,,0,The simulation code of the models were developed using MATLAB and can be conveniently modiﬁed to adjust the model order. The model order was automatically adjusted from very simple (1 node) to more detailed by deﬁning the two-dimensional grid of rows and columns of brick nodes. The number of rows multiplied by the number of columns was thus the total number of brick capacitance nodes for that given model
10.3390_en14051387,"data available, data",214,,0,"Figure 2 depicts how heat is delivered to the warehouse zone by the HVAC system [39]. Sensors measuring air temperatures are located throughout the HVAC system, and four thermocouples are embedded in the ETS, which measure brick temperatures. During operation of the ETS, a portion of supply air is directed to the thermal storage device to provide the zone with increased heat from stored heat energy. The zone is modeled using a 1R1C explicit ﬁnite difference modeling approach (Figure 3), with a heating coil model with proportional-integral control. The zone model was originally developed for a previous study [39] and was calibrated using several days of measured data from the real building. Statistical indices were found for a Root-Mean-Square Error (RMSE) of 0.75 ◦C and a Mean Absolute Error (MAE) of 0.66 ◦C. This simple model was used for demonstration of the overall methodology presented in this study and was sufﬁcient for shorter control horizons, such as the ones evaluated here (1 or 2 days). C is the equivalent thermal capacitance of the zone, Tair is the effective operative temperature of the zone, R is the equivalent resistance"
10.3390_en14051387,"data available, data",269,,0,"Typically, in MPC, the optimal control problem is solved at each deﬁned control step by looking ahead at forecast weather and occupancy schedules over the prediction horizon, PH. The prediction horizon is a time period where we have reasonably reliable information, ranging from a few hours to a couple of days. Using data available from the prediction horizon period, an optimization routine is solved and an optimal sequence of control moves is identiﬁed through the implementation of MPC. The identiﬁed schedules and control moves are applied to the building over a “control horizon”, which can be the same length or be shorter than the prediction horizon. Once the current control horizon has ended, the optimization exercise is performed again for the following prediction horizon. This process is repeated until the end of the simulation time (e.g., one day or one year). For the sake of simplicity, in this particular case, the prediction and control horizon are the same length of time and MPC was only initiated with a notiﬁcation signal: either 30 h for a 12-hour ahead notiﬁcation at 6 p.m. or 22 h for a 4-hour ahead notiﬁcation at 2 a.m. (for a DR event at 6 a.m.). Further studies could be carried out on performance related to the control horizon length; however, Date et al. [44] found that longer control horizons (12 h or more) were favourable for a similar building in a similar climate."
10.3390_en14051470,code,86,,0,"A Bosch, solenoid, common rail type injector is installed in the combustion chamber. The injector nozzle originally had 5 holes, with an included angle of 124◦, hole diameter of 0.21 mm (nozzle code DSLA124P 1659). To avoid spray-spray interaction and spray window interaction and to allow fuel injection spray studies (not conducted in this work), the fuel injector is modiﬁed so only one hole was left open, with the remainder laser welded closed."
10.3390_en14071925,"data available, data",10,,0,Data Availability Statement: Data available in this manuscript.
10.3390_en14092648,"data available, data, download, publicly available, data https, dataset",69,,1,Data Availability Statement: Publicly available datasets were analyzed in this study. This data can be found here: https://projects.worldbank.org (accessed on 31 August 2020); https://data. worldbank.org/indicator/NY.GDP.DEFL.ZS (accessed on 23 September 2020); https://irena.org/ Statistics/Download-Data (accessed on 3 April 2021); https://edgar.jrc.ec.europa.eu/overview.php? v=booklet2020 (accessed on 3 April 2021).
10.3390_en14092709,benchmark,24,,0,"32. Lottersberger, F.; Hafner, N.; Jodin, D. Efﬁciency indicators for benchmark and improvement of energy efﬁciency on automated"
10.3390_en14092709,"data available, data",110,,0,"The technical parameters of devices are gathered in Table 3. These are average values characteristic not of speciﬁc producers but of broader classes of devices for initial selection in most practical applications. The values result from the technical speciﬁcations of devices in their standard conﬁguration, obtained by a broad overview of the equipment manufacturers’ offerings. The prices can differ according to the conﬁguration of the equipment and the purchase contract conditions. The prices were subjectively determined on the basis of practice and equipment offers and can be considered reliable for Central Europe in the year 2020, but no ofﬁcial data are available."
10.3390_en14092709,"data available, data",222,,0,"n = annual cost of human labor in variant n (EUR); n = annual cost of transport equipment maintenance in variant n (EUR); n = annual cost of energy consumption of equipment and MHSs in variant n (EUR); n = annual cost of warehouse control system maintenance in variant n (EUR); n = annual cost of rack system maintenance in variant n (EUR). Table 10 presents the expenditure on warehouse equipment and infrastructure. Expenditure for transport devices is a product of the number of devices in variants (Table A7) and the unit price (Table 3). The expenditure on buildings is a function of the assumed cost of erecting 1 m2 of an industrial hall of a given height and the areas of functional zones different in subsequent variants. Construction costs obtained from warehouse realizations in central Poland (suburban industrial areas) were averaged. The average cost of erecting 1 m2 of an industrial hall of up to 10 m in height is EUR 347.83; for between 10 to 20 m in height, it is EUR 543.48; and for above 20 m, it is EUR 739.13. No ofﬁcial data are available, however."
10.3390_en14092709,database,138,,0,"43–53. [CrossRef] Jacyna, M.; Wasiak, M.; Bobi ´nski, A. SIMMAG3D as a tool for designing of storage facilities in 3D. Arch. Transp. 2017, 42, 25–38. [CrossRef] Jachimowski, R.; Goł˛ebiowski, P.; Izdebski, M.; Pyza, D.; Szczepa ´nski, E. Designing and efﬁciency of database for simulation of processes in systems. Case study for the simulation of warehouse processes. Arch. Transp. 2017, 41, 31–42. [CrossRef] Szczepa ´nski, E.; Jachimowski, R.; Izdebski, M.; Jacyna-Gołda, I. Warehouse location problem in supply chain designing: A simulation analysis. Arch. Transp. 2019, 50, 101–110. [CrossRef]"
10.3390_en14102800,benchmark,28,,0,"49. Udomsilp, D.; Lenser, C.; Guillon, O.; Menzler, N.H. Performance Benchmark of Planar Solid Oxide Cells Based on Material"
10.3390_en14102800,open-source,193,,0,"The simulation is conducted in MFiX 19.2.2 (Multiphase Flow with Interphase eXchange, MFiX), a free open-source multiphase ﬂow resolver, which is a powerful tool to simulate the gasiﬁcation of solid fuel and allows users to develop model conveniently [43]. TFM is the most mature model in MFiX and can calculate dense, reactive multiphase ﬂow by treating both solids and gases as interpenetrating continua. TFM is adopted to describe the gas–solid ﬂow and the governing equations are solved using ﬁnite volume method. The implicit Euler scheme with the ﬁrst-order accurate is employed for temporal discretization and Superbee, a second-order accurate scheme, is used for spatial discretization. The automatic time-step adjustment is utilized to accelerate calculation. A grid size smaller than 8dp (dp stands for the diameter of solid particle) is thought to be ﬁne enough [42,44]; about 4dp is adopted here to achieve a satisﬁed accuracy and grid independency. The electrochemical reactions occur in SOFC are treated as reactive boundary conditions in MFiX and the SOFCs are operating under ﬁxed current densities."
10.3390_en14123501,"data https, data",6,,0,"Martinopoulos, G. Bin Weather Data"
10.3390_en14123501,database,96,,0,"26. Kneifel, J.; O’Rear, E.; Webb, D.; O’Fallon, C. An exploration of the relationship between improvements in energy efﬁciency and life-cycle energy and carbon emissions using the BIRDS low-energy residential database. Energy Build. 2018, 160, 19–33. [CrossRef] [PubMed] Said, S.A.M.; Habib, M.A.; Iqbal, M.O. Database for building energy prediction in Saudi Arabia. Energy Convers. Manag. 2003, 44, 191–201. [CrossRef]"
10.3390_en14123501,"dataset, data",84,,0,"The results are presented in diagrams and tables. Due to space limitations, only parts of them are presented in this manuscript; more analytical results are included elsewhere [14]. Scope of this work is to provide a data set, based on recent historical climatic records, which is useful for performing energy analysis of HVAC systems with simpliﬁed multiple measure methods such as the classical bin method [15,16], the modiﬁed bin method [17]"
10.3390_en14123501,"dataset, data",103,,0,"Table 2 contains the data for the cooling period, Table 3 contains the data for the heating period and Table 4 the data for the transient months. The period of the day was divided into six 4-h shifts for a better part load energy analysis. The results are derived from monthly data [14]. These datasets can be used for the estimation of energy requirements and fuel consumption according to the bin and modiﬁed bin method [15,17], and may serve for updating the bin data of the Warmer zone of EN14825 [16]."
10.3390_en14123501,"dataset, data",113,,0,"In the present work, a 30-year long (1983–2012) time series of hourly outdoor air temperature and relative humidity records from the meteorological station of NOA in Athens, Greece was used to produce speciﬁc data for the application of bin methods. To that end, the outdoor air temperature and relative humidity hourly measurements were analyzed and the frequency of outdoor air dry-bulb temperature at intervals of 2 ◦C, as well as the corresponding mean coincident humidity ratio or wet-bulb temperature values, was calculated. The main goal was to provide an up-to-date bin data set for the city of Athens, based on recent records."
10.3390_en14123501,"dataset, data",301,,0,"Energies 2021, 14, x FOR PEER REVIEW 17 of 21  Table 6. Heat transfer coefficients U (W/m2K) and shading coefficients SC of windows. Window Types (Layer/Frame) Room/Space U (W/m2K) (Summer) U (W/m2K) (Win-ter) Shading Co-Efficient SC Double, metal Floors’ offices/corri-dors 3.3 3 0.85 Single, metal Ground floor of-fices/rooms 4.6 4.7 1 Single, metal Staircases 6 6.4 1 The operating hours of the building are from 9:00 a.m. to 20:00 p.m., with an internal temperature of 20 °C in the heating season and 26 °C in the cooling season. Temperatures of 15 and 30 °C were assumed for the rest of the day. For the heating and cooling periods, the temperature of the stairwells and the entrance area was set at 15 and 30 °C in each case. Throughout the year, the ventilation rate was set at 1 and 0 ACH in operation and non-operation periods, respectively. The office building was simulated using the 3 dec-ades (1983–1992, 1993–2002 and 2003–2012) temperature bin data of the city of Athens. Resulting Impact of Climate Change on the Energy Demands for Heating and Cooling It is obvious that there is a noticeable increase in the total energy requirements of the office building for cooling and dehumidification during the period from May to October, as highlighted in Figure 10 and in Table 5. There is an increase of approximately 10% per decade (11.4% in the second decade and 8.2% in the third), while overall cooling loads increased by 20.6%. Figure 10. Total (sensible and latent) monthly energy requirements for cooling."
10.3390_en14123501,"dataset, data",331,,0,"The total increase in the energy requirements of the building for both heating and cooling from the first to the last decade is 14.5% (Table 7). Table 7. Latent and sensible loads for the 1983–2013 period. Time Period Cooling Loads Heating Loads Sensible Latent Total (kWh) Sensible Latent Total (kWh) 1983–1992 94.8% 5.2% 413,009 86.7% 13.3% 60,804 1993–2002 95.0% 5.0% 460,184 87.0% 13.8% 50,403 2003–2012 91.8% 8.2% 497,963 83.7% 16.3% 44,665 The trend of cooling loads is increasing in all summer months, especially in June, July and August, and is decreasing in all winter months (Figure 10). The decrease in the heat-ing loads cannot offset the increase in cooling loads during the specified period, so overall energy requirements are increased. This is also reflected in the building’s specific energy requirements for heating and cooling. 5. Conclusions In the present work, a 30-year long (1983–2012) time series of hourly outdoor air tem-perature and relative humidity records from the meteorological station of NOA in Athens, Greece was used to produce specific data for the application of bin methods. To that end, the outdoor air temperature and relative humidity hourly measurements were analyzed and the frequency of outdoor air dry-bulb temperature at intervals of 2 °C, as well as the corresponding mean coincident humidity ratio or wet-bulb temperature values, was cal-culated. The main goal was to provide an up-to-date bin data set for the city of Athens, based on recent records. Some conclusions concerning the behavior of the calculated climate parameters dur-ing the selected time period and the impact of changes observed on local climate are dis-cussed as well. From the results it is obvious that the annual average dry-bulb temperature Energies 2021, 14, 3501"
10.3390_en14123501,"dataset, data",335,,0,"Figure 10. Total (sensible and latent) monthly energy requirements for cooling. On the other hand, there is a noticeable decrease in the total energy requirements of the office building for heating and humidification during winter (from November to April), with a decrease of approximately 14.2% per decade (17.1% in the second decade and 11.4% in the third) and an overall heating load decrease of 26.5%, as evidenced in Figure 11 and in Table 5. It should be pointed out that Figures 10 and 11 present the sensible and latent energy requirement of the building, the performance coefficients of the primary equipment not being accounted for. Energies 2021, 14, x FOR PEER REVIEW 18 of 21  Figure 11. Total (sensible and latent) monthly energy requirements for heating. In particular, the increase in sensible loads accounts for 11.3% over the period con-sidered, while in contrast, the increase in latent loads totals 62.6% in the three decades. The total increase in the energy requirements of the building for both heating and cooling from the first to the last decade is 14.5% (Table 7). Table 7. Latent and sensible loads for the 1983–2013 period. Time Period Cooling Loads Heating Loads Sensible Latent Total (kWh) Sensible Latent Total (kWh) 1983–1992 94.8% 5.2% 413,009 86.7% 13.3% 60,804 1993–2002 95.0% 5.0% 460,184 87.0% 13.8% 50,403 2003–2012 91.8% 8.2% 497,963 83.7% 16.3% 44,665 The trend of cooling loads is increasing in all summer months, especially in June, July and August, and is decreasing in all winter months (Figure 10). The decrease in the heat-ing loads cannot offset the increase in cooling loads during the specified period, so overall energy requirements are increased."
10.3390_en14123501,publicly available,16,,0,"publicly available sensors. Urban Clim. 2019, 28, 100464. [CrossRef]"
10.3390_en14123548,benchmark,172,,0,"This paper proposes a novel approach for solving the problems in allocating the DG and capacitor simultaneously in RDS with various load models. The objective of the system is to reduce the power loss and to enhance the VSI of the system. The Bat algorithm is used for sizing and locating both capacitor and DG. Linear changing of feeder loads is performed ranging from 0.5 (light load) to 1.6 (peak load) with a step size of value 0.01, where at each step, sizing is performed using CFT whose formulation is performed in the form of a simple quadrature equation. The present approach is supportive in the selection of a particular size of capacitor and DG based on distribution network operators (DNOs) and their load steps. The standard IEEE 33-bus test system and 69-bus benchmark test system are used to test the effectiveness and feasibility. Comparison of the simulated results is performed with that of other heuristic-based algorithms."
10.3390_en14133765,"data, database, retrieve, data https, dataset",23,,0,Data Availability Statement: All data retrieved from Eurostat datasets: https://ec.europa.eu/ eurostat/data/database (accessed on 5 January 2021).
10.3390_en14133765,"database, data",76,,0,"15. Eurostat Database. Available online: https://ec.europa.eu/eurostat/data/database (accessed on 10 May 2021). 16. Hache, E. Do renewable energies improve energy security in the long run? Int. Econ. 2018, 156, 127–135. [CrossRef] 17. Valentine, S.V. Emerging symbiosis: Renewable energy and energy security. Renew. Sustain. Energy Rev. 2011, 15, 4572–4578."
10.3390_en14133765,"dataset, data",91,,0,"The determined eigenvalues of the correlation matrix (Table 8) reﬂect the signiﬁcance of the principal components in describing the information resources of the input variables (percentage share in the variability of the data set). The Kaiser criterion was used to determine the optimal number of the principal components. Based on this criterion, such number of principal components was adopted to explain as much variation as at least one original diagnostic variable. Thus, for the indicators from 2008, these are two principal"
10.3390_en14133765,"dataset, data",183,,0,"In multivariate analyses of the energy market, including renewable energy, the research subject is oftentimes characterized by a large number of mutually correlated variables (factors). Such a large number of these variables makes it possible to describe the studied phenomena in a very accurate, and at the same time credible, manner. However, this often causes difﬁculties in interpretation. Therefore, in order to ﬁnd signiﬁcant relationships between the variables describing the research object, it is necessary to use methods reducing the dimension of the data space while maintaining as much of its variance as possible [53]. The PCA is one of such methods. It is a statistical measure in which a data set is reduced by creating a new space where the initial factors account for the most variability. The reduction of the number of dimensions is obtained by transforming variables into uncorrelated principal components, which are ordered in a descending manner by the size of the described variance of the community [54]:"
10.3390_en14133765,package,335,,0,"In addition, by the end of 2020, at least 10% of ﬁnal energy consumption in transport was to come from renewable energy sources. At the end of 2014, the European Council adopted and then, in December 2018, revised targets under which it undertook to achieve by 2030 a reduction of at least 40% in greenhouse gas emissions versus 1990 levels and an increase to 32% of the share of renewable energy in all sources of energy consumed throughout the European community [11]. Other breakthrough solutions were proposed by the European Commission and presented at the UN COP25 climate summit in Madrid (Spain) [12]. They include a new, pioneering European climate strategy called the European Green Deal [13]. According to this strategy, the EU’s economy is to become a ""zero-emission"", i.e., climate-neutral economy by 2050. In turn, by 2030, carbon dioxide (CO2) emissions are to be reduced by 50% (plans assume even a 55% decrease) versus its emissions in 1990 [13]. These assumptions are more ambitious than those adopted at the UN climate summit COP24, which took place in 2018 in Katowice (Poland) [14]. Currently, the European Commission has announced that it will push for a higher target of reducing CO2 emissions by 2030 versus 1990. When compared to the previously adopted target, it will be raised from 40% to 55%. This target will enable an increase in the production and consumption of energy from RES. The latest main objectives of the EU’s energy policy until 2050 (the European Green Deal) are aimed at achieving a situation in which the EU economy becomes environmentally neutral. It will result in the elimination of fossil fuels as energy sources."
10.3390_en14133765,package,344,,0,"In general, when considering the volume of energy production from RES in total (as Mtoe of production), Germany is the undisputed leader in this respect among all the EU countries. Germany is responsible for nearly 20% of this production in the entire EU (in 2008, it was about 19%, and in 2018—nearly 20%). However, in the presented research, the authors used, among others, indicators that are related to the implementation of the EU climate and energy package until 2020 in terms of RES, as well as indicators showing the growth of RES production capacity or the volume of energy production from RES in relation to the wealth of a given country. This approach to the analysis showed that the situation in Germany in terms of RES is not as good as compared to the leading countries. Over the last 10 years, Germany has not changed its position in the RES development ranking (Table 10) and still ranks 12th. The level of its development is described as average low (Figure 6). Within 10 years, between 2008 and 2010, the share of RES in total energy consumption increased from 10.01% to 16.5%, but Germany has not yet achieved its 2020 target. In fact, many less prosperous countries have already achieved this goal (e.g., Bulgaria, Estonia, Cyprus, Czech Republic). In order to switch to RES, Germany must pay at least €160 billion over ﬁve years [71], which Kay Scheller says is extremely disproportionate to the outcomes. Additionally, Germany has introduced a number of incentive mechanisms to promote RES: a tariff and low-interest loans for investments in new power plants, as well as several programs for the development of the heating and cooling sector. In transport, subsidy systems and ﬁscal regulation are used [61]."
10.3390_en14133765,package,348,,0,"The current state of RES development in Germany needs to be looked at a bit more critically. In general, when considering the volume of energy production from RES in total (as Mtoe of production), Germany is the undisputed leader in this respect among all the EU countries. Germany is responsible for nearly 20% of this production in the entire EU (in 2008, it was about 19%, and in 2018—nearly 20%). However, in the presented research, the authors used, among others, indicators that are related to the implementation of the EU climate and energy package until 2020 in terms of RES, as well as indicators showing the growth of RES production capacity or the volume of energy production from RES in relation to the wealth of a given country. This approach to the analysis showed that the situation in Germany in terms of RES is not as good as compared to the leading countries. Over the last 10 years, Germany has not changed its position in the RES development ranking (Table 10) and still ranks 12th. The level of its development is described as average low (Figure 6). Within 10 years, between 2008 and 2010, the share of RES in total energy consumption increased from 10.01% to 16.5%, but Germany has not yet achieved its 2020 target. In fact, many less prosperous countries have already achieved this goal (e.g., Bulgaria, Estonia, Cyprus, Czech Republic). In order to switch to RES, Germany must pay at least €160 billion over ﬁve years [71], which Kay Scheller says is extremely disproportionate to the outcomes. Additionally, Germany has introduced a number of incentive mechanisms to promote RES: a tariff and low-interest loans for investments in new power plants, as well as several programs for the development of the heating and cooling sector."
10.3390_en14133765,package,350,,0,"The unquestionable leader in the energy transition process is the European Union (EU). Here, the contribution of RES to savings related to the import of fossil fuels in 2015 amounted to approximately EUR 15 billion. It is estimated that in 2030, it should be as much as EUR 58 billion [7]. This is due to the energy policy pursued by the EU. As early as in 2001, the EU adopted Directive 2001/77/EC [8], which requires each Member State to take appropriate measures to achieve a speciﬁc indicative target of RES-generated electricity. At that time, the target was to achieve renewable energy consumption at the level of 12% with an indicative 22% share of electricity produced from RES by 2010. The next measure was Directive 2003/30/EC [9], aimed at promoting the use of biofuels in transport and other renewable fuels (5.75% share of biofuels in transport fuel consumption). Pursuant to the next Directive 2009/28/EC [10], the EU countries were supposed to increase the share of energy obtained from RES in their total energy consumption. The European climate and energy package contained objectives that were to be achieved by 2020. These targets included a 20% reduction in EU greenhouse gas emissions below levels reported in 1990, increasing the share of energy produced from RES to 20% and improving energy efﬁciency by 20%. In addition, by the end of 2020, at least 10% of ﬁnal energy consumption in transport was to come from renewable energy sources. At the end of 2014, the European Council adopted and then, in December 2018, revised targets under which it undertook to achieve by 2030 a reduction of at least 40% in greenhouse gas emissions versus 1990 levels and an increase to 32% of the share of renewable energy in all sources of energy consumed throughout the European community [11]."
10.3390_en14133765,"package, data",326,,0,"2. Mohsin, M.; Abbas, Q.; Zhang, J.; Ikram, M.; Iqbal, N. Integrated effect of energy consumption, economic development, and population growth on CO2 based environmental degradation: A case of transport sector. Environ. Sci. Pollut. Res. 2019, 26, 32824–32835. [CrossRef] Kanagawa, M.; Nakata, T. Assessment of access to electricity and the socio-economic impacts in rural areas of developing countries. Energy Policy 2008, 36, 2016–2029. [CrossRef] Gonzalez-Salazar, M.A. ; Venturin, M.; Poganietz, W.R.; Finkenrath, M.; Leal, M.R.L.V. Combining an accelerated deployment of bioenergy and land use strategies: Review and insights for a post-conﬂict scenario in Colombia. Renew. Sustain. Energ. Rev. 2017, 73, 159–177. [CrossRef] EU Energy in Figures. Statistical Pocketbook. 2018. Available online: https://op.europa.eu/en/publication-detail/-/publication/ 99fc30eb-c06d-11e8-9893-01aa75ed71a1 (accessed on 13 October 2020). Sorin, G.A. ; Anca, E. The effect of ﬁnancial development on renewable energy consumption. A panel data approach. Renew. Energy 2020, 147, 330–338. Study on Technical Assistance in Realisation of the 2016 Report on Renewable Energy. In Preparation of the Renewable Energy Package for the Period 2020–2030 in the European Union, Freiburg, Germany. 22 February 2017. Available online: https://ec.europa.eu/energy/sites/ener/ﬁles/documents/res-study_ﬁnal_report_170227.pdf (accessed on 13 October 2020). Directive 2001/77/EC of the European Parliament and of the Council of 27 September 2001 on the Promotion of Electricity Produced from Renewable Energy Sources in the Internal Electricity Market; Ofﬁcial Journal of the European Union location L 283. 27 October 2001. Available online: https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A32001L0077 (accessed on 13 October 2020)."
10.3390_en14133765,"package, data",342,,0,"Environ. Sci. Pollut. Res. 2019, 26, 32824–32835. [CrossRef] Kanagawa, M.; Nakata, T. Assessment of access to electricity and the socio-economic impacts in rural areas of developing countries. Energy Policy 2008, 36, 2016–2029. [CrossRef] Gonzalez-Salazar, M.A. ; Venturin, M.; Poganietz, W.R.; Finkenrath, M.; Leal, M.R.L.V. Combining an accelerated deployment of bioenergy and land use strategies: Review and insights for a post-conﬂict scenario in Colombia. Renew. Sustain. Energ. Rev. 2017, 73, 159–177. [CrossRef] EU Energy in Figures. Statistical Pocketbook. 2018. Available online: https://op.europa.eu/en/publication-detail/-/publication/ 99fc30eb-c06d-11e8-9893-01aa75ed71a1 (accessed on 13 October 2020). Sorin, G.A. ; Anca, E. The effect of ﬁnancial development on renewable energy consumption. A panel data approach. Renew. Energy 2020, 147, 330–338. Study on Technical Assistance in Realisation of the 2016 Report on Renewable Energy. In Preparation of the Renewable Energy Package for the Period 2020–2030 in the European Union, Freiburg, Germany. 22 February 2017. Available online: https://ec.europa.eu/energy/sites/ener/ﬁles/documents/res-study_ﬁnal_report_170227.pdf (accessed on 13 October 2020). Directive 2001/77/EC of the European Parliament and of the Council of 27 September 2001 on the Promotion of Electricity Produced from Renewable Energy Sources in the Internal Electricity Market; Ofﬁcial Journal of the European Union location L 283. 27 October 2001. Available online: https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A32001L0077 (accessed on 13 October 2020). Directive 2003/30/EC of the European Parliament and of the Council of 8 May 2003 on the Promotion of the Use of Biofuels or Other Renewable Fuels for Transport; Ofﬁcial Journal of the European Union location L 283. 17 May 2003. Available online: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32003L0030&from=en (accessed on 13 October 2020)."
10.3390_en14133970,"code, code available, github",41,,2,"The code and documentation of TESS are available at https://github.com/slacgismo/ TESS, accessed on 29 June 2021. The current setup reﬂects the speciﬁcation of TESS for our ﬁeld deployment in Colorado, as described in Section 4."
10.3390_en14133970,"code, code available, github",224,,2,"Residential distribution systems are a potential source of system ﬂexibility, particularly in demand. This potential can be realized by an approach called transactive systems (TS), which coordinates residential distributed energy resources (DER) through a market-based mechanism. In this work, we present Transactive Energy Service System (TESS), a modularized platform for the implementation of TS, which enables the deployment of adjusted market mechanisms, economic bidding, and the potential entry of third parties. (The code and documentation of TESS are available at https://github.com/slacgismo/TESS, accessed on 29 June 2021). TESS thereby opens up current integrated closed-system TS, allows for the better adaptation of TS to power systems with high shares of renewable energies, and lays the foundations for a smart grid with a variety of stakeholders. Furthermore, we describe TESS as we have modiﬁed it for a ﬁeld implementation within the service territory of Holy Cross Energy, an electric cooperative serving Eagle, Pitkin, Garﬁeld, Mesa, and Gunnison counties in Colorado. Importantly, our speciﬁcation addresses challenges of implementing TS in existing electric retail systems, for instance, the design of bidding strategies when a (non-transactive) tariff system is already in place."
10.3390_en14144178,"data available, data, database, publicly available, data https",98,,1,"Data Availability Statement: The software used (MERGER), available on the IS-EPOS platform from https://tcs.ah-epos.eu/ (accessed on 9 July 2021), provides graphic outputs that coincide with those shown in the article. Extreme Weather parameters can be requested from the Met Ofﬁce: UK Climate Projections (UKCP) (http://datapoint.metofﬁce.gov.uk/public/data/, accessed on 9 July 2021). Data used for the PGA Probability simulation are available online at British Geological Survey earthquake database search (http://quakes.bgs.ac.uk/earthquakes/dataSearch.html, accessed on 9 July 2021)."
10.3390_en14144357,provide implementation,152,,0,"Energy access has increased signiﬁcantly in Maputo in recent decades as a result of the government’s effort to provide energy to all citizens through implementation of energy policies, an expansion of the electricity grid, and an increase of generation capacity. The trade of power generation technologies has found a good market in Maputo and, thus, solar PV system components are of easy access. However, even with this market opening, there is still a lack of knowledge regarding the potential of solar systems and its installation and operation procedure that could spread the application of solar technology. The Maputo province in Mozambique has high global irradiation when compared with other good locations in Europe and Asia. Maputo’s global horizontal irradiation is about 1785 kWh/m2/year and its solar power potential is estimated to be 400 MW [38]."
10.3390_en14154436,code,86,,0,"Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor between ﬁnal electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical building code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]."
10.3390_en14154436,code,177,,0,"estimated parameter bypass damper compressor condenser technical building code desiccant regenerative indirect evaporative cooler direct expansion desiccant wheel exhaust air electric energy consumption (cooling period) (kWh m−2 year−1) evaporator factor fan heat transfer coefﬁcient (W m−2 K−1) heating coil heating, ventilating and air conditioning indirect evaporative cooler insulation (m2 K W−1) number of parameters metabolic rate (W m−2) mixing box mass ﬂow (kg s−1) nearly zero energy buildings outdoor air static pressure (Pa) predicted percentage dissatisﬁed (%) predicted mean vote (-) capacity (kW) return air regenerative indirect evaporative cooler area (m2) supply air dry bulb temperature (◦C) heat transfer coefﬁcient (W m−2 K−1) air velocity (m s−1) expansion valve; valve volumetric air ﬂow rate (m3 h−1) effective mechanical power (W m−2) electric power consumption (kW) thermal comfort indicator (%) air quality indicator (%) input variable estimated variable"
10.3390_en14154436,code,334,,0,"Energies 2021, 14, 4436 13 of 27  III <1350 IV >1350 For this case, the weighting factor values of air quality, wfAQ, for each category were obtained with the ΔCO2 value in real time and the limit ΔCO2 value of each category, see Equation (10). wfAQ = ΔCO2,actual/ΔCO2,limit (10)The sum of the product of this factor and the step time was performed to each cooling period of the climate zones. The categories I and II were considered favorable, as well as for the favorable thermal comfort conditions. 2.5.3. Energy Consumption and CO2 Emission The energy consumption of the air-cooling systems was obtained as the sum of the electric consumption of each HVAC element, i.e., compressor, fans and pumps. The time period used to integrate this consumption was the cooling period, see Table 9. EEC = ⅀ Electric Energy Consumptionelement × Time step/Sclassroom (11)In Equation (11), the energy-consuming elements of the DX system were the com-pressor, the exhaust fan, the condenser fan and the process fan. Regarding the RIEC and DRIEC systems, the elements that consume energy were the pumps, the exhaust fan and the process fan. The regeneration fan was also considered for the DRIEC system. Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor be-tween final electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical build-ing code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]. Table 13. CO2 emission factor for each climate zone."
10.3390_en14154436,code,336,,0,"Regarding the RIEC and DRIEC systems, the elements that consume energy were the pumps, the exhaust fan and the process fan. The regeneration fan was also considered for the DRIEC system. Regarding environmental impact, CO2 emissions emitted by each air-cooling system during the cooling period in each climate zone were calculated. The emission factor be-tween final electricity consumption and CO2 emissions were different for each country, as shown in Table 13. The CO2 emission factor for Spain was taken from the technical build-ing code, CTE 2019 [42]. The CO2 emission factors for the rest of the countries were taken from the 2020 carbon footprint document [43]. Table 13. CO2 emission factor for each climate zone. Climate Zone Country Factor (kgCO2 kWh−1) Lampedusa Italy 0.466 Seville Spain 0.331 Thessaloniki Greece 0.577 Zagreb Croatia 0.273 The CO2 emissions results [kgCO2 m−2 year−1], for each system and each climate zone throughout cooling period, were calculated as the product of the total energy consump-tion [kWh m−2 year−1] and the respective CO2 emission factor [kgCO2 kWh−1]. 3. Results and Analysis Daily and annual analysis were carried out for the three air-cooling systems. The daily analysis was performed for the climate zone of Lampedusa. Two summer days were selected: a typical summer day and a severe summer day. Then, the influence of climatic severity in the thermal comfort, air quality and energy consumption criteria were studied to understand the annual results in each climate zone. 3.1. Thermal Comfort 3.1.1. Daily Analysis of the Air-Cooling Systems The daily results of the air-cooling systems DX, RIEC and DRIEC are represented in Figures 5–7, respectively. For each air-cooling system, air temperatures, air humidity ratio and PPD values over typical and severe summer days are shown. Energies 2021, 14, 4436"
10.3390_en14154436,"data https, data available, data",49,,0,"40. Weather Data. Trnsys 17, vol. 8. Available online: http://www.trnsys.com/ (accessed on 5 December 2020). 41. Comité técnico ISO/TC 159 UNE-EN ISO 7730. Ergonomía del ambiente térmico. 2006. Available online: https://www.une.org/"
10.3390_en14154436,"database, data",54,,0,"The three air-cooling systems were simulated for the representative cities of the hotdry, warm-humid, warm-dry and mixed-humid climate zones. The energy simulations were carried out using the Meteonorm software database [40]. The average values of climate data of the four climate zones are shown in Table 10."
10.3390_en14154507,database,67,,0,"27. Mazzeo, D.; Matera, N.; De Luca, P.; Baglivo, C.; Congedo, P.M.; Oliveti, G. A literature review and statistical analysis of photovoltaic-wind hybrid renewable system research by considering the most relevant 550 articles: An upgradable matrix literature database. J. Clean. Prod. 2021, 295, 126070. [CrossRef]"
10.3390_en14154507,database,260,,0,"the pipes are buried, it should be noted that a greater depth allows the system to be less affected by external temperature ﬂuctuations. In horizontal systems, to avoid interaction between the pipes, it is recommended that the pipes be spaced at least 1.5 m and buried at a depth of 1.2 m to 1.8 m [33]. There are clear advantages in using the earth-to-air heat exchangers in buildings located in hot climates during the summer season, already with short pipes, while in winter the beneﬁts of the system are just for few hours during the day. In the paper [34], it was investigated how geometric conﬁguration, soil thermal conductivity, heat transfer ﬂuid velocity, and installation depth affect the behavior of earthto-air heat exchangers, using CFD simulations. In addition, the paper [35] shows the impact of the unsaturated ground on the performance of the ground heat exchanger as the depth varies through numerical simulation. The results showed that the performance under unsaturated soil conditions decreased up to 40% compared to that under fully saturated ground conditions. In this regard, the study [36] has given a complete database on transient ground temperatures, which was acquired by surveying a case study located in southern Brazil. Regarding outdoor climate conditions, the study [37] considers different climate zones in China, showing higher energy savings of the system application in warm climates."
10.3390_en8031685,"data available, data",167,,0,"This study first imported wind speed data measured by a tidal station (in Miaoli, Taiwan) and a buoy (in Tainan, Taiwan) into the WAsP simulation software to estimate the high altitude wind speeds for the two areas. Then, a Lidar system was set up near the tidal stations and buoys, and high-altitude wind speeds measured by the Lidar system were compared with the WAsP-estimated high altitude wind speed. The two data sources were found to be rather consistent, and regression analysis R-squared values are in the range of 0.8–1. Long-term wind speed data observed by buoys and tidal stations in other areas were also imported into WAsP to create a forecast of wind speed at 50–200 m on the west cost of Taiwan (Hsinchu, Miaoli, Taichung, ChangHua, Yunlin, Chiayi and Tainan).  At the same time, WAsP Engineering 3.0 was used to analyze extreme wind speed."
10.3390_en8031685,dataset,22,,0,"on buildings in urbanized area using 3-D GIS and Lidar datasets. Build. Environ. 2013, 59,  528–535."
10.3390_en8031685,download,53,,0,"Copyright of Energies (19961073) is the property of MDPI Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use."
10.3390_en8053640,download,53,,0,"Copyright of Energies (19961073) is the property of MDPI Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use."
10.3390_en9100838,database,17,,0,56. Phyllis2—Database for Biomass and Waste. Available online: https://www.ecn.nl/phyllis2/ (accessed on
10.3390_en9100838,database,29,,0,"Table 2. Elemental analysis of the lipid extracted algae feedstock for the developed study cases, from the Energy Centre of Netherlands Phyllis2 database [56]."
10.3390_en9100838,"database, code package, package",151,,0,"The complete plant layouts were simulated on the commercial software Aspen Plus™. The thermodynamic property package Peng Robinson Wong Sandler (PRWS) was selected for units operating near to and beyond supercritical processing, namely within blocks A2 and A3. The PRWS was selected due to its reported higher accuracy for phase equilibria estimations for asymmetric and/or polar-nonpolar and/or supercritical mixtures as is the case with biomass SCWR [41]. While the Predictive Soave Redlich Kwong (PSRK) was used for subcritical to ambient processing units. The thermo-physical properties of solid content in LEA were deﬁned as a non-conventional compound and estimated based on software built-in coal database correlations HCOALGEN and DCOALIGT in Aspen Plus™. The gross and net heating values of 22.95 and 21.23 MJ/kg on dry ash free basis are similar to those reported in literature elsewhere [20,56]."
10.3390_en9100838,"database, code package, package",305,,0,"Energies 2016, 9, 10 of 26 Table 2. Elemental analysis of the lipid extracted algae feedstock for the developed study cases, from the Energy Centre of Netherlands Phyllis2 database [56]. Proximate Analysis, wt. % (as Received Basis) Ultimate Analysis, wt. % (Dry Ash Free Basis) Ash content 4.59 Carbon 50.99 Moisture 5.62 Hydrogen 7.44 Volatiles 75.34 Oxygen 33.61 Fixed carbon 14.45 Sulfur 0.48  Nitrogen 7.48 3. Developed Conceptual Plant Designs A simplified block representation of the envisaged BioSNG and hydrogen production pathways is shown in Figure 1. From the left, at the LEA feedstock entry point, the block diagram illustrates the sequential common upstream processing steps for both production pathways, which principally convert the solid organic content into dry sweet syngas. At first, solid valorization takes place under SCWR conditions within blocks A1–3, shown in light green. This is followed by the extraction of the SCWG product water content and acid gases (CO2 and H2S) removal, in blocks B1 and B2 respectively, shown in shades of blue. The directly produced SCWG product is a mixture of H2, CH4 and COx, excluding water, whose concentrations depend on the operating conditions of the SCWG reactor. A parametric analysis for a similar LEA feedstock was reported in our earlier publication [20]; where the theoretical maximum or equilibrium limit for CH4 purity in dry syngas at supercritical processing of 400 °C, 250 bar and 15 wt. % solid content was only 51.5 mol%. Meanwhile for hydrogen, the upper limit at 600 °C, 5 wt. % solid content and a similar pressure was 59.3 mol%."
10.3390_en9100838,"database, code package, package",346,,0,"From the left, at the LEA feedstock entry point, the block diagram illustrates the sequential common upstream processing steps for both production pathways, which principally convert the solid organic content into dry sweet syngas. At first, solid valorization takes place under SCWR conditions within blocks A1–3, shown in light green. This is followed by the extraction of the SCWG product water content and acid gases (CO2 and H2S) removal, in blocks B1 and B2 respectively, shown in shades of blue. The directly produced SCWG product is a mixture of H2, CH4 and COx, excluding water, whose concentrations depend on the operating conditions of the SCWG reactor. A parametric analysis for a similar LEA feedstock was reported in our earlier publication [20]; where the theoretical maximum or equilibrium limit for CH4 purity in dry syngas at supercritical processing of 400 °C, 250 bar and 15 wt. % solid content was only 51.5 mol%. Meanwhile for hydrogen, the upper limit at 600 °C, 5 wt. % solid content and a similar pressure was 59.3 mol%. As a result, downstream blocks, C1–2 and D1–2, were designed for further syngas upgrading to meet the desirable properties for the two final products; Finnish grid quality BioSNG injection and 99.99% purity hydrogen as a minimum requirement for chemical industrial purposes or fuel cell power generation systems [35,37]. Figure 1. Simplified block diagram for the envisaged lipid extracted algae hydrothermal treatment to Bio- synthetic natural gas and hydrogen production pathways. The complete plant layouts were simulated on the commercial software Aspen Plus™. The thermodynamic property package Peng Robinson Wong Sandler (PRWS) was selected for units operating near to and beyond supercritical processing, namely within blocks A2 and A3. The PRWS was selected due to its reported higher accuracy for phase equilibria estimations for asymmetric Energies 2016, 9, 838"
10.3390_en9100838,"database, data",126,,0,"In this present study, we examine the conversion of lipid extracted algae (LEA), post biodiesel production, for an integrated onsite CHP conﬁguration along with synthetic gas, either BioSNG or hydrogen, for offsite power and chemicals production. The conceptual bio-reﬁnery, envisages a poly-generative energy structure, with a lower carbon footprint than a whole algae stand-alone hydrothermal plant as reported in literature [54]. The elemental analysis for the plant’s solid feedstock used in this study is provided in Table 2, and is based on averaged data reported in the Phyllis2 database of the Energy Centre of Netherlands (ECN) for a broad range of lipid extraction methods [56]."
10.3390_en9100838,dataset,230,,0,"As expected for the lower temperature processing conditions, the model CH4 molar prediction showed better conformity with the higher carbon conversion datasets D-1 and D-2 than that of D-3 and D-4. The predicted H2 concentration was clearly distorted by the CH4 favorable catalytic activity of ruthenium, a similar conclusion was found when model results were compared to the work of Haiduc et al. [43]. Another reason behind the model distortion compared to D-3 and D-4 was the detected molar composition of heavier hydrocarbons C+ in the experimental runs, which indicates incomplete conversion into the expected equilibria gases. A limited carbon gasiﬁcation rate was reported, as low as 45% in D-3, something that is expected at such lower temperature conditions and shorter residence times [6]. When analyzing D-3 and D-4, both cases with increased solid throughput, a condition needed to favor CH4 production, longer residence times that enable complete decomposition of solid organics into equilibrium gases is needed. To illustrate further, a look at the total carbon gas yields (not shown in the table), the model (~90% CGE) with a 0.29 g/gfeed compared favorable to D-1 (93% CGE) with 0.26 g/gfeed than D-3 (45% CGE) with 0.09 g/gfeed."
10.3390_en9100838,"dataset, data",297,,0,"For the higher temperature conditions, Chakinala et al. [52] performed a parametric analysis with a capillary tube reactor setup. The model validation for H2 production across varied processing conditions was shown in our earlier publication [20]. Chakinala et al. [52] investigated the effect of catalyst nature on the product as shown in the variation of CGE and product gas composition within data sets D-5 to D-8. The higher reported CGE values lead to better conformity by the model data set-M. D-7 with excess loading of a ruthenium based catalysts 2 g/gfeed, recorded complete carbon partitioning into gas phase and had an almost identical representation of H2 and CH4 molar compositions to that of the model. The reported C+ (heavier hydrocarbon) composition, despite minimal compared to that of D-5, D-6 and D-8, showed that gas reforming equilibrium was still not achieved. Hydrocarbon reforming and gas phase reactions such as steam reforming and water gas shift activity are reported to take place after a much longer residence time than that reportedly set in the current experimental setup [4,5,7,11]. As such, it could be concluded that the modelling approach is capable of illustrating reasonably accurate predictions to the product gas nature as well as reactor yields from an algal slurry, for appropriate reactor setup conditions, that enable complete carbon conversion and gas formation equilibria. On a more conservative interpretation of the model ﬁndings, the thermo-chemical processing limit for chemical fuel production could be estimated. Similar approaches have been adapted in literature and were also validated with other available experimental data sets for a wide variety of biomass [31,35,36,46]."
10.3390_en9100838,package,2,,0,Ideal package
10.3390_en9100838,package,189,,0,"The principal challenge to ensure the development of a reliable reactor or process model remains the computation of the speciﬁc thermodynamic properties of the highly asymmetric and multi-dimensional slurry mixtures. The parametrization of the SCWR phase equilibria interactions for the various processing components of solid and ﬂuid phases, polar and non-polar in nature, within super- and sub-critical conditions remains an area under development [41]. As such, predictive empirical equations of states (EOS) have garnered signiﬁcant attention over recent years for SCWG assessment studies. The advantage of empirical EOS in general compared to their activity coefﬁcient counterparts is the ability to predict phase equilibria at elevated pressures where inﬁnite dilution in a single phase is experienced [41]. Some of the EOS adapted in literature are the Peng-Robinson (PR) [24,26,28,31,32,35], Soave Redlich Kwong (SRK) [36,38], Duan [27,40], Statistical Association Fluids Theory (SAFT) [25], Virial EOS [30] and the original ideal package [33]. Some authors employed"
10.36001_phmconf.2015.v7i1.2716,benchmark,6,,0,AIRTOBS Aircraft Technology and Operations Benchmark
10.36001_phmconf.2015.v7i1.2716,benchmark,33,,0,"At DLR, lifecycle cost-benefit model AIRTOBS (Aircraft Technology and Operations Benchmark System) was developed to enable a holistic economic assessment of aircraft technologies1 already in a conceptual design phase."
10.36001_phmconf.2015.v7i1.2764,benchmark,132,,0,"Diagnostic and prognostic results of RS-based, LS-based, and VLS-based algorithms are compared in Table 1 with the same benchmark. Compared with RS-based prognosis with a horizon of 525 cycles and small number (20) of particles at the 472th cycle, the LS-based and VLS-based prognosis have a horizon of 24 and 50 Lebesgue states, and can afford 500 particles. The computation time for LS-based and VLS-based prognosis are only 0.41% and 1.36% of the RSbased prognosis, respectively. Compared to LS-based prognosis, VLS-based prognosis is a little computational expensive. The reason is that VLS-based prognosis keeps a closer monitoring on the SOH after the 472nd cycles by shrinking the Lebesgue state length dynamically to accommodate an accelerated degradation speed."
10.36001_phmconf.2017.v9i1.2456,benchmark,194,,0,"For rolling element bearing, when its outer race, inner race, rolling element or cage has fault, periodic impact is produced exciting the whole structure’s high frequency resonance. As a result, amplitude modulation occurs at the associated bearing pass frequencies such as ball pass frequency outer race (BPFO), ball pass frequency inner race (BPFI), ball spin frequency (BSF), fundamental train frequency (FTF) as shown in Figure 1. These bearing pass frequencies calculated from Eq. (3-6). The raw signal often contains little information about bearing faults. Over many years envelope analysis has been established as benchmark method for a bearing diagnostics. Envelope mechanism for extracting the periodic excitation of the resonance from vibration signals by demodulating the vibration signals at the resonances. It is realized by applying Hilbert transform and the construction of the analytic signal. Finally, the frequency spectrum of envelope signal is used for feature extraction. The details of the method can be found in reference (Randall & Antoni, 2011)."
10.36001_phmconf.2018.v10i1.465,benchmark,21,,0,"The algorithm based on the wheelset dynamics is invariable to track gauge, despite passing the quality test with a benchmark"
10.36001_phmconf.2018.v10i1.564,benchmark,7,,0,4. MARKOV CHAIN MONTE CARLO BENCHMARK
10.36001_phmconf.2018.v10i1.564,benchmark,11,,0,5. SMC STATIC PARAMETER ESTIMATION AND COM PARISON WITH BENCHMARK
10.36001_phmconf.2018.v10i1.564,benchmark,21,,0,Figure 3. Benchmark multi-dimensional parameter chain sampled via MCMC and the kernel density estimates of the associated marginal PDFs.
10.36001_phmconf.2018.v10i1.564,benchmark,38,,0,"When distributing the workload of the SMC sampler on a high-performance computing platform, a theoretical maxi Qualitatively, the SMC results can be compared to the benchmark through examination of the joint sample plots in Fig 8"
10.36001_phmconf.2018.v10i1.564,benchmark,42,,0,"Figure 5. Joint sample points generated from (a) the MCMC benchmark and (b) the SMC samplers (number of processors used, P = 1, 000; the dotted lines represent the estimated parameter means."
10.36001_phmconf.2018.v10i1.564,benchmark,44,,0,"verse solution to Equation 2). In this study, MCMC sampling served as a benchmark with which to evaluate the performance of the proposed parallel SMC implementation. The comparison was focused on both total simulation time and posterior sample statistics."
10.36001_phmconf.2018.v10i1.564,benchmark,57,,0,"Quantitatively, the SMC-based estimates of mean and variance are displayed in Table 5, and the percent relative difference (PRD) of those estimates with respect to the MCMC benchmark are shown in Table 6 under the “1,000 particles” heading. The PRDs for mean and variance were calculated according to the equations"
10.36001_phmconf.2018.v10i1.564,benchmark,66,,0,"The goal of the present study was to use the parallel implementation of the sequential Monte Carlo (SMC) methodology presented in Section 2, Algorithm 1, to produce results similar to the benchmark of Section 4. The expected outcome was a parameter estimation process that could match the performance of an MCMC benchmark while expending a fraction of the computation time."
10.36001_phmconf.2018.v10i1.564,benchmark,116,,0,"Using N = 4, 000 particles (i.e., the same number of samples in the thinned set of MCMC benchmark samples) and P = 4, 000 processors, the weighted samples shown in Figure 6 were obtained. Compared to the 1,000 particle system, the variance of the weights in the 4,000 particle system was reduced,7 which was expected due to the asymptotic convergence properties of SMC estimators as the number of particles N → ∞. However, the PRDs in Table 6 are largely unaffected. Again, it is important to note that the true posterior distribution is unknown and the MCMC and SMC results are approximations."
10.36001_phmconf.2018.v10i1.564,benchmark,123,,0,"2 (cid:12) SMC − σ2 (cid:12)σ2 MCMC µSMC + µMCMC where, µ and σ2 represent an estimated mean and variance, respectively, and the subscripts indicate whether the estimator was built using SMC or MCMC sampling. As shown, all of the PRDs are less than 5%, indicating good agreement. It should be noted that, although serving as the benchmark, both methods are approximations of an unknown quantity, and there is no guarantee that the MCMC estimator is any more correct than the SMC estimator in this case. Instead, the focus here was on verifying the SMC approach and comparing performance with a commonly used method."
10.36001_phmconf.2018.v10i1.564,benchmark,123,,0,"Autocorrelation of the sampled parameters in the present benchmark study was in fact high, as shown in Figure 2 (a). This is typically a sign of sub-optimal mixing. To reduce this autocorrelation and to simulate a sub-sampling for prognostic purposes, the chains were thinned by discarding all but every 10th sample. Parameter estimation results for the thinned chain are shown in Figure 3. The resulting autocorrelation of the chain after thinning is shown to have been reduced signiﬁcantly in Figure 2 (b). The thinned chains will be used to compare this benchmark with the SMC results in Section 5. Statistics obtained using the thinned chains are shown in Table 3."
10.36001_phmconf.2018.v10i1.564,benchmark,139,,0,"By assuming an average surrogate model evaluation time of 0.5 seconds, a theoretical scaling curve was be generated. The time to complete an SMC sampling procedure is approximately equal to the product of the evaluation time and the total number of non-parallel model evaluations, as deﬁned by the SMC tuning parameters. The scaling behavior of the parallel algorithm presented in this study was examined by repeating the SMC-based parameter estimation process using the tuning parameters in Table 4 and varying the number of processors used, P. The theoretical scaling and true scaling results are shown in Figure 4. At the maximum number of processors, P = 1, 000, the SMC sampling took 32 seconds, three orders of magnitude less than the MCMC benchmark time (52,524 seconds)."
10.36001_phmconf.2018.v10i1.564,benchmark,157,,0,"The work presented here focuses on the efﬁciency and performance of a generalized parallel SMC sampler for static parameter estimation in the context of non-deterministic prognostics. In particular, two critical beneﬁts provided by SMC to the PHM ﬁeld are discussed. First, a potential path toward real-time, high-ﬁdelity prognostics using a combination of surrogate modeling and a parallel SMC sampler is demonstrated. Second, the use of SMC samplers to enable tractable parameter estimation for full-ﬁdelity (i.e., non-surrogateassisted) damage models is discussed. Both of these topics are explored in the context of fatigue crack growth in a geometrically complex metallic specimen. Three introductory sections precede the results and conclusions. These sections include discussion of (i) the relevant SMC theory, (ii) the case study used to demonstrate the proposed method, and (iii) an MCMC-based benchmark for method evaluation."
10.36001_phmconf.2019.v11i1.783,benchmark,158,,0,"Battery’s lifetime-conscious PMSs have been under consistent development to achieve the above-mentioned goals in real-time. Methods of PMSs can be categorized into optimization-based and rule-based ones (Silvas et al., 2017). Optimization-based algorithms are sophisticatedly formulated to deﬁne optimal control policies for speciﬁc driving cycles. Such algorithms, i.a. dynamic programming or genetic algorithms, are time-intensive and hence are typically used as benchmark solutions to deﬁne optimized trajectories for battery’s state-of-charge SoCb, that achieves minimal health degradation for the considered driving cycle (Ettihir et al., 2016). On the other hand, rule-based algorithms are deﬁned as simple if-else rules and hence are suitable for realtime applications. In such algorithms, experience-based or manufacturer-given knowledge is used to derive the control rules, i.e. charging/discharging boundaries and power assistance/recuperation limits of the battery (Wang et al., 2019)."
10.36001_phmconf.2019.v11i1.815,benchmark,26,,0,The interacting quadruple tank is a well-known benchmark multi-input multi-output (MIMO) system that consists of four interconnected tanks with two pumps and a reservoir
10.36001_phmconf.2021.v13i1.2983,benchmark,34,,0,"Sv¨ard, C., & Nyberg, M. (2011). Automated design of an fdi-system for the wind turbine benchmark. IFAC Proceedings Volumes, 44(1), 8307–8315."
10.36001_phmconf.2021.v13i1.3059,"benchmark, data",197,,0,"In the automated feature engineering approach, we use the automatic feature extraction frameworks: Time Series Feature Extraction based on Scalable Hypothesis tests (TSFRESH) (Christ et al., 2018) and Time Series Feature Extraction Library (TSFEL) (Barandas et al., 2020). TSFRESH can produce up to 111 statistical features, while TSFEL can produce up to 60 statistical features. Some of these features can result in NaNs or inﬁnite values, which are replaced by the average and extreme values respectively. Also, if all the values for a calculated feature result in inﬁnite, the entire feature is In addition, we implement the pattern discovset to zero. ery strategy described by Peng et al. (2017) on their active learning framework for time series classiﬁcation, known as ACTS. We use this implementation as a benchmark to our proposed framework. ACTS is based on shapelet discovery, which identiﬁes discriminative patterns in the time series. The splits generated during pattern discovery are used to estimate the data entropy of the pattern, which in turn is used"
10.36001_phmconf.2017.v9i1.2450,case study data,7,,0,3. WIND TURBINE BEARING CASE STUDY
10.36001_phmconf.2017.v9i1.2450,case study data,8,,0,4. COAL PLANT OPERATIONAL ANALYSIS CASE STUDY
10.36001_phmconf.2020.v12i1.1162,case study data,4,,0,5. CASE STUDY
10.36001_phmconf.2017.v9i1.2427,"case study data, data",63,,0,"To validate the suggested methodology, a case study on vibration data from the same type of gearbox but for three different wind turbines is presented. The wind turbines considered in this study are rated at between 1.5 and 3 MW. The vibration data acquisition system consists of eleven accelerometers and a tachometer on the high speed shaft. The generator"
10.36001_phmconf.2017.v9i1.2427,"case study data, data",74,,0,"through a case study using real wind turbine vibration data. The case study validated the model performance and effectiveness. According to the results, SAP,SER, narrowband kurtosis and a decision tree algorithm can be used to correctly classify healthy or faulty data in discrete time ranges. These indicators have some dependence on the loading conditions and therefore the torque is taken into consideration in the model training."
10.36001_phmconf.2017.v9i1.2427,"case study data, data",198,,0,"This paper aims to present the development of a framework for monitoring of wind turbine gearboxes and prognosis of gear fracture faults, using vibration data and machine learning techniques. The proposed methodology analyses gear vibration signals in the order domain, using a shaft tachometer pulse. Indicators that represent the health state of the gear are algorithmically extracted. Those indicators are used as features to train diagnostic models that predict the health status of the gear. The efﬁcacy of the proposed methodology is demonstrated with a case study using real wind turbine vibration data. Data is collected for a wind turbine at various time steps prior to failure and according to the maintenance reports there is enough data to form a healthy baseline. The data is classiﬁed according to the time before failure that the signal was collected.The learning algorithms used are discussed and their results are compared. The case study results indicate that this data driven model can lay the groundwork for a robust framework for the early detection of emerging gear tooth fracture faults. This can lead to minimisation of wind turbine downtime and revenue increase."
10.36001_phmconf.2015.v7i1.2676,"case study data, dataset provided, data",211,,0,"This article provides a method to characterize the evolution in time of criminal risk in a speciﬁc area. A case study with real data that includes location of public services and criminal incidents is also presented. The novel methodology for quantifying risk of criminal events uses a particle-based empirical representation. Two different stages are distinguished in this method: off-line and on-line. The former considers the location of services and 1870 crimes that occurred during a time period of 6 months, yielding a probabilistic characterization of the risk using prior knowledge of historical crimes in the area. The on-line stage approximates the posterior spatial risk distribution using a sequence of 185 new crimes. This task is done by sequentially updating the location of samples (particles), using concepts of importance sampling and resampling. In addition, a strategy for criminal risk prediction is presented. For this prediction strategy, a GMM is ﬁtted using historical registers of recent criminal activity. This GMM is used to simulate 185 future crime events that help to explore the evolution in time of particle positions. Each of these simulations has an associated time of occurrence, modeled by an exponential distribution."
10.36001_phmconf.2015.v7i1.2564,code,128,,0,"The basic task that the considered diagnostic component SDC has to tackle is to ensure that some deﬁned safety requirements are met even if the control software CS would show some hazardous behavior. The latter could result either from some fault(s) in the software design like forgotten corner cases, faults in the implementation, some memory corruption (e.g. due to (Kim et al., 2014)) or other hardware faults, compiler errors, and many other issues. Certainly software designers add assertions and other sanity checks to their control software, possibly complemented with some elaborate code that aims at keeping the control software in a safe state (e.g. fault tolerant control concepts)."
10.36001_phmconf.2015.v7i1.2655,code,122,,0,"In literature, publications specific on feature engineering are very sparse as stated by Brownlee (2014) that “feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books”. Recently there are a few attempts on developing feature engineering tools that aim for facilitating the feature engineering task. For example, Anderson et al. (2014) proposed a feature engineering development environment that allows the user to write feature engineering code and evaluate the effectiveness of the engineered features.  Heimerl et al. (2012) developed FeatureForge tool that uses interactive visualization for supporting feature engineering for natural language processing."
10.36001_phmconf.2015.v7i1.2716,code,4,,0,Task Code BSI CHK
10.36001_phmconf.2015.v7i1.2716,code,11,,0,Table 2. Task code groups and potential PHM impact.
10.36001_phmconf.2015.v7i1.2716,code,18,,0,Task code  Group (TCG) TCG 1 TCG 2 TCG 3 TCG 4 TCG 5 TCG 0
10.36001_phmconf.2015.v7i1.2716,code,47,,0,"represent the maintenance man-hours and task code groups shown in Table 5 over the lifecycle of 25 years. The short interval tasks are characterized by intervals between 80 and 1000 FH. The intervals of the medium interval tasks range from 4,000 to 14,000 FH."
10.36001_phmconf.2015.v7i1.2716,code,73,,0,"The potential impact of PHM on preventive, scheduled maintenance tasks depends on its task code. Scheduled maintenance tasks can be assigned to a variety of different task codes (Airbus, 2007) as listed in Table 1. While tasks with some task codes could become obsolete if a PHM system is used, prognostics have no influence on other the scheduled maintenance tasks scheduled program (MPD)."
10.36001_phmconf.2015.v7i1.2716,code,125,,0,"For the sake of simplification and generalization, the task codes are summarized to six task code groups (TCG) within the model as shown in Table 2. TCG 1 to 3 reflect tasks, which are potentially redundant (obsolete), if a PHM system covers the contained tasks. It is assumed that the prognostic system is able to automatically carry out a certain fraction of the check- or inspection-tasks in a continuous or noncontinuous manner. The fraction of tasks covered by a PHM system can be adjusted with the task redundancy parameter PTR. The parameter PTR implies that it is possible to eliminate the corresponding scheduled maintenance task from the MPD under consideration of certification requirements."
10.36001_phmconf.2015.v7i1.2716,code,141,,0,"In addition to routine activities, scheduled checks also comprise large amounts of non-routine tasks. Detected findings result in non-routine activities (i.e. repairs or replacements of the respective items), when the degradation may reach a critical state prior to the next preventive inspection. It is assumed that a certain part of these nonroutine tasks can be conducted at a later time, the respective items are subject to a CBM strategy (and monitored by PHM). These tasks are summarized in TCG 5. The last task code group (TCG 0) includes non-routine (e.g. findings that are critical for flight safety and thus have to be repaired immediately) and other tasks (e.g. cabin refurbishments and paintings) to which a PHM system has no influence."
10.36001_phmconf.2015.v7i1.2758,code,114,,0,"𝛤 1 + 1 The Weibull random function was then called to simulate the time of failure of 5 components. Then, using these failure times, an estimate of Weibull parameters, a 0.1 and 0.9 confidence of the Weibull was estimated using Cohen’s method (Cohen, 1965). Example Matlab© code can be found in the appendix. Figure 1 shows the simulated example of a component that will fail after 4.42 years, where the experiment has run 3.19 years. The RUL is 1.43 years, with a lower limit (e.g. confidence of the RUL) of 0.48 years to 2.28 years."
10.36001_phmconf.2015.v7i1.2768,code,36,,0,"a. IVHM Internal to the UAS The products (e.g. sensors, code) and services (e.g. automated response to a leak) that are to be implemented internally to the UAS."
10.36001_phmconf.2016.v8i1.2505,code,219,,0,"Arc failure mode establishes a current path in the air, and this current path might be established due to any discontinuity in the current carrying conductors or insulation breakdown in adjacent current carrying conductors (Alam, Khan, Johnson, & Flicker, 2013). Any type of arc fault is harmful for the PV system, and may introduce fire that may result in insulation burn-out and fire hazards in presence of any flammable substances in the vicinity of the PV plant (Johnson, Schoenwald, Kuszmaul, Strauch, & Bower, 2011). National Electrical Code® (NEC)-2011 requires a series arc-fault protection device in a PV system if the DC operating voltage is equal to or higher than 80V. These devices are called as arc-fault circuit interrupters (AFCIs) (Schimpf & Norum, 2009). The causes of arc faults depend on their types, whether they are series or parallel. Series arc fault reasons include degradation in solder joints, wiring or connections inside the junction box, loosening of screws, and increased operating temperature that may result in thermal stress, leading to accelerated aging or complete disconnection (Hastings, Juds, Luebke, & Pahl, 2011;"
10.36001_phmconf.2016.v8i1.2521,code,162,,0,"FEA-based methods traditionally compute the crack driving forces at each growth step by either an explicit representation where the crack surface and component assembly is remeshed at each step, or through the use of enriched elements (e.g., XFEM). In either case, the system is solved and the displacements are used to compute the crack driving forces. As discussed, the ﬁnite element solution is typically the most computationally intensive part of this process. Therefore, the proposed crack growth model replaces this process with a surrogate modeling approach. Here, a large number, Φ, of crack growth simulations are run using a FE-based, high-ﬁdelity fracture mechanics code a priori, or without knowledge of the true crack path. Each of these simulations are then broken down by growth step, meaning that each explicitly mod Figure 1. General illustration of the fatigue crack growth model geometrical framework."
10.36001_phmconf.2016.v8i1.2542,code,31,,0,International Organization for Standardization (2012). ISO 230-1 - test code for machine tools − part 1: Geometric accuracy of machines operating under no-load or quasistatic conditions.
10.36001_phmconf.2016.v8i1.2569,code,92,,0,"Testing the network follows a straightforward approach. The test samples are preprocessed and normalized into subpatterns the same way as the training set. The stored weights of the SOM module and the link weights, obtained during training of the network, are used. The input is then propagated through the network and output is determined. A slightly different code is used for testing the network where the weights of the SOM and link weights are loaded from the already stored values obtained from the training process."
10.36001_phmconf.2017.v9i1.2306,code,21,,0,(2012). Code manual volume IV: Models and correlations (Computer software manual No. INEEL-EXT-98-00834).
10.36001_phmconf.2017.v9i1.2306,code,26,,0,"RELAP5-3D-I. (2012). Code manual volume I: Code structure, system models, and solution methods [Computer software manual]."
10.36001_phmconf.2017.v9i1.2306,code,45,,0,"Luchinsky, D. G., Smelyanskiy, V. N., & Brown, B. (2014c). Physics based model for cryogenic chilldown and loading. part iv: Code structure (Technical Publication No. NASA/TP-2014-218399). NASA, ARC."
10.36001_phmconf.2017.v9i1.2307,code,96,,0,"On the other hand, we see that a large number of phases must be accommodated on relatively short distance along the descending path. This fact can be used to substantially reduce the dimensionality of the problem by approximating segments connecting neighboring transition points with low degree polynomials (in particular, straight lines). This assumption also agrees with the results of the optimization obtained using GPOPS code, see Figure 2 and compare with results of (de Jong, 2014; de Jong et al., 2015, 2017)."
10.36001_phmconf.2017.v9i1.2437,code,4,,0,4.1. Host Code
10.36001_phmconf.2017.v9i1.2437,code,4,,0,4.2. Device Code
10.36001_phmconf.2017.v9i1.2437,code,17,,0,"# Host code, operates on CPU int main(int argc, char* argv)"
10.36001_phmconf.2017.v9i1.2437,code,26,,0,Pseudo code of the device code (kernel) and the host code (main CPU program) can be found in the following subsections.
10.36001_phmconf.2017.v9i1.2437,code,36,,0,"Once all the threads have run, a memory copy operation makes the results available on the CPU memory. The host code aggregates the results and calculates the probability of end of discharge state."
10.36001_phmconf.2017.v9i1.2437,code,80,,0,"The massively parallel nature of the GPU and the accessible and well-documented CUDA API enables developers to quickly and easily decide which portions of their code should be processed by the CPU and which should be processed by the GPU. However, not all code or algorithms receive a performance boost when implemented on a GPU. Factors such as memory access, direct human interaction, or a serial nature can reduce the potential for GPU-based acceleration."
10.36001_phmconf.2017.v9i1.2437,code,91,,0,"The device code operates on the GPU in parallel. Once the GPU kernel is called by the main program, this code is executed in parallel by the threads within the blocks. The kernel function operates on a single sample, i.e., it runs a single realization of the future system evolution. In this case, it simulates up to a ﬁnite time horizon of 5,000 seconds. It saves whether this particular thread hit end of discharge, and if so, at what time."
10.36001_phmconf.2017.v9i1.2437,code,168,,0,"The host code operates on the CPU and is the main body of the application. Within the host code, parameters for the simulation are deﬁned, memory is allocated on the CPU memory and the GPU memory, and a GPU accelerated Mersenne Twister pseudo random number generator is quickly implemented to create random samples for the input of each Monte Carlo realization. Within the host code the GPU kernel is called specifying both the number of blocks to use and the number of threads per block. Additionally, pointers to the random number array and an empty results array are passed to the kernel during the call. While the GPU executes the kernel, each operation of the kernel operates on each thread of each block speciﬁed. During this time the CPU is free to continue executing the host code. In this case, we synchronize before continuing to ensure that all threads have completed before processing their results."
10.36001_phmconf.2017.v9i1.2437,code,173,,0,"of the EOD algorithm on the GPU following the approach outlined in the previous section. To produce an estimation of the time until EOD, a Monte Carlo simulation of the battery state is performed. With a time step of 1 second the battery state is updated and checked against the low voltage threshold until a time horizon of 5,000 seconds or the end of discharge is reached. The implementation of this algorithm on the GPU required the creation of a kernel with instruction for the threads and state estimation and threshold checking functions to be compiled for the GPU in support of the kernel. Additionally, main code operating on the CPU was used to allocate memory, measure kernel compilation time, synchronize thread processing, and perform the ﬁnal calculation of the EOD using the results of each thread. This code was developed in such a way that the number of samples in the simulation could be manually increased or decreased before the simulation."
10.36001_phmconf.2017.v9i1.2450,code,128,,0,"1. Plant Name (Categorical, 93 levels, not used) 2. Unit ID (Categorical, 152 levels, not used) 3. Primary Fuel Code (Categorical, with 4 levels) 4. Heat Rate, in BTU/kW-Hr. both as continuous measurements and binned into two categories 5. Total CO2 emissions (tons), both as continuous measurements and binned into two categories 6. Total SO2 emissions (lbs.), both as continuous measurements and binned into two categories  7. Total NOx emissions (lbs.), both as continuous measurements and binned into two categories 8. Number of major load swings, both as continuous measurements and binned into two categories"
10.36001_phmconf.2017.v9i1.2476,code,85,,0,"dimensional (3D) representation of the PD activity. A typical PRPD result is shown in Figure 4. In this pattern, the PD amplitude is plotted against the position with respect to voltage phase angle and a color code is used to show the pulse count. PRPD measurements are carried out as needed to improve the recognition of the active PD sources previously detected with than 5000 measurement files have been recorded using the PRPD instrument on about 100 hydrogenerators."
10.36001_phmconf.2017.v9i1.2485,code,31,,0,International Organization for Standardization (2012). ISO 230-1 - test code for machine tools − part 1: Geometric accuracy of machines operating under no-load or quasistatic conditions.
10.36001_phmconf.2018.v10i1.492,code,22,,0,"The following pseudo code dynamically adapts the sojourn time of each (past, present and future) hidden state when the"
10.36001_phmconf.2018.v10i1.492,code,58,,0," The ratios between the training and testing sojourn times of hidden state i and i+1 should be constant. last assumption, which To demonstrate dynamical updates the sojourn times of the future hidden states based on the current and past hidden states’ sojourn time adaptation, the following flowcharts and pseudo code are presented."
10.36001_phmconf.2018.v10i1.564,code,154,,0,"The damage model used in this work was based on the fracture simulation software FRANC3D (“FRANC3D Reference Manual, Version 7”, 2016). The code inserts and grows cracks within an existing ﬁnite element model through a local/global remeshing scheme. Although it enables arbitrary, three-dimensional crack growth, FRANC3D is currently limited to linear elastic fracture mechanics (LEFM). Once a crack is inserted, the new mesh is passed to any one of a variety of ﬁnite element codes that FRANC3D is compatible with including commercial codes such as ANSYS and ABAQUS and government codes such as SIERRA Mechanics (Sierra Solid Mechanics Team, 2011) and ScIFEN2 (Warner, Bomarito, Heber, & Hochhalter, 2016). ScIFEN, which is a parallel ﬁnite element code developed at NASA Langley Research Center, was used in this work."
10.36001_phmconf.2018.v10i1.585,code,152,,0,"James Eno received his Bachelor of Applied Science in Electrical Engineering from the University of Waterloo in Ontario.  His initial employment with Philips Electronics, Inc. the located development of linear and digital video circuitry for the commercial TV market. Jim found opportunities with Harris Corporation where he wrote embedded firmware for base station repeaters and portable/mobile radio products for Public Safety communications. He enriched the feature set and reliability of radio communications meeting the high demand of Police, Fire and Ambulance services worldwide.  He also developed the hardware, firmware and PLD logic for an Adaptive Differential Pulse Code Modulation (ADPCM) codec to support the coupling of legacy analog base stations with the system’s digital infrastructure. Jim joined Luna in 2016 as an electrical engineer and is actively designing embedded firmware to drive a variety of sophisticated health monitoring technologies."
10.36001_phmconf.2019.v11i1.790,code,2,,0,PSF Code
10.36001_phmconf.2019.v11i1.790,code,83,,0,"Technician gives up searching prior to ﬁnding appropriate problem-code. (RB) (Related task: 5.2.3) Depending on how much time is available to perform this mapping interation (PSF H) and how easy it is to search or discover classiﬁcations due to the software design (PSF L), the technician may simply give up the search for ”appropriate” classiﬁcations, and default back to the closest thing he is familiar or comfortable using."
10.36001_phmconf.2019.v11i1.790,code,91,,0,"DB schema does not contain appropriate problem code to represent the MWO features (RB) (Related task: 5.2.2) In this case, the perception that the schema is missing a problemcode is either correct, in which case it is important that the communication between management and the shop-ﬂoor is always up-to-date (PSF K), or it is wrong, and the technician may need to be more familiar with the detailed, technical behavior of the system in question (PSF G)."
10.36001_phmconf.2019.v11i1.790,code,176,,0,"PTA CodeDescriptionUnsafe ActsUA CodeError ClassificationPSF CodesPSF CategoryTask 3: Distingish MWO by relevant symptoms/featuresTask 3.1Recall symptoms in issues/MWOTechnician did not observe all symptoms relevant to the MWOUA-1SBASocialBTechnologicalTechnician does not remember significant symptoms UA-2SBCOrganizationTask 3.2Identify causal and functional linksTechnician is unaware of relevant system architecture or functionality at the requisite level. UA-3KBDPersonalEOrganizationFOrganizationTask 3.4Classify WO by relevant symptoms/featuresMWO has features and/or symptoms completely unlike previous experienceUA-4KBDPersonalGTechnologicalSelected MWO features are not relevant and/or MWO type is incorrectly identifiedUA-5RBHOrganizationIPersonalGTechnologicalTask 5: Organize symptoms into DB Schema, for future useTask 5.2: Categorize knowledge into controlled vocabularyTask 5.2.1Translate symtpom featuresTechnician is unaware of relevant system architecture or functionality at the requisite level. UA-3KBDPersonalEOrganizationFOrganizationTechnician misunderstands target use-case and functionality of a controlled-vocabulary CMMSUA-6KBJPersonalKSocialTask 5.2.2Categorize symtpoms into DB schemaDB schema does not contain appropriate problem code to represent the MWO featuresUA-7RBGTechnologicalKSocialTechnician does not applyappropriate problem code (unfamiliar with DB classification schema)UA-8KBFOrganizationLOrganizationTask 5.2.3Locate relevant categories in DB interfaceTechnician gives up searching prior to finding appropriate problem-code. UA-9RBHOrganizationLOrganizationANNUAL CONFERENCE OF THE PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2019"
10.36001_phmconf.2019.v11i1.815,code,17,,0,"Color Code: Highest AUC, 2nd Highest AUC, 3rd Highest AUC, 4th Highest AUC"
10.36001_phmconf.2019.v11i1.815,code,42,,0,"P: Precision, R: Recall (or TPR), MDR: Missed Detection Rate (= 1 – TPR) Color Code: Best value of metric, 2nd best value of metric, 3rd best value of metric"
10.36001_phmconf.2019.v11i1.857,code,65,,0,"Figure 5B lays out prescriptive active diagnosis procedures for mismatch in altitude measurement among the associated sensors. The “Altitude Check” active diagnosis procedure indicates those tests whose status could be evaluated to prune the fault set. While the model does not include the procedural code for executing the active diagnosis procedure, it captures the operational constraints under which the procedures"
10.36001_phmconf.2019.v11i1.857,code,95,,0,"For each possible contingency action A (e.g., divert to emergency airport, pull parachute) we deﬁne a schema, which is a recursive piece of code. A schema represents a piece of a contingency plan with “holes”, which need to be ﬁlled by calculations, constraint reasoning, or recursive invocation of other schemas. (Figure 1). The goal of the UAS mission is to fully execute the full remaining ﬂight plan G = (cid:104)wi, ...w⊥(cid:105),"
10.36001_phmconf.2019.v11i1.857,code,103,,0,"Dr. Johann Schumann received his PhD (1991) and German habilitation degree (2000) in Computer Science from the Technische Universit¨at M¨unchen (TUM) in Germany. He is engaged in research on system and software health management, autonomy for UAV, V&V of advanced air trafﬁc control systems, and the automatic generation of reliable code. Dr. Schumann is author of a book on theorem proving in software engineering and has published numerous articles on automated deduction, automatic program generation, V&V of safety-critical systems, and neural network oriented topics."
10.36001_phmconf.2019.v11i1.857,code,133,,0,"Apart from the D-matrix, the FML generates Prolog code to set up the interface between DR and DM (Figure 6B). It generates Prolog rules that aid DM to understand the DR output (the diagnostic hypothesis for the current set of faults). Furthermore, FML generates Prolog code to capture model information that allows DM to set up the functions and variables impacted, the possible active diagnosis procedures that could be executed to reﬁne the diagnosis output, possible local recovery actions to arrest or mitigate a fault and resulting impact on the system performance. DM uses these rules at every stage to plan the next course of action, an active diagnosis procedure, a recovery procedure, or updates to the mission."
10.36001_phmconf.2019.v11i1.876,code,13,,0,1D CNN layer. The implementation code is provided in the appendix.
10.36001_phmconf.2020.v12i1.1131,code,10,,0,The batch algorithm pseudo-code is detailed in algorithm 2.
10.36001_phmconf.2020.v12i1.1131,code,29,,0,"Tmin; iterations Output: SOM code vectors {mk} Initialize SOM parameters {mk} ; for n = 1, . . . , iterations do"
10.36001_phmconf.2020.v12i1.1205,code,131,,0,"Figure 3 shows the calculated health indices for the generator bearing temperature h(tw) in degrees Celsius in each available time window. The color code denotes the selected threshold, in this case parameterized by α = 0.0001 in all panels. All red colored points in the plots indicate a detected faulty behavior. For this turbine we had access to “true labels” from the operator, indicating the onset of two faults, followed by their actual detection time by the staff on site. The ﬁrst one (f1) started showing up 9.12.2017 and lead to a complete turbine stoppage on the 30.1.2018. The second fault (f2) started on the 20.3.2018, causing a stoppage on the 5.4.2018."
10.36001_phmconf.2021.v13i1.2978,code,68,,0,"There are two limitations in this paper. The first one is that the simulation code currently can only consider the lifetime of the weakest tooth. All other teeth’s lifetime is not considered, yet. Also, choosing the wrong tooth as the weakest tooth does not result in a penalty. An update to address both limitations is currently in the works."
10.36001_phmconf.2021.v13i1.3030,code,99,,0,"In Figure 12 the stop codes for each ﬂight taken by the UAV is shown. The majority of the time the UAV successfully reaches all waypoints of the trajectory. A few times around the 60th ﬂight the stop code is low SOC, meaning that the ﬂight had to be stopped because the UAV’s battery SOC went below the minimum SOC threshold (a parameter set in the initialize workspace block described above). Then around the 75th ﬂight the same stop code was repeatedly triggered, and ultimately the sample was terminated."
10.36001_phmconf.2021.v13i1.3054,code,8,,0,• Cloud-assisted model training and code generation for
10.36001_phmconf.2021.v13i1.3054,code,26,,0,"acquisition, processing, model training&testing and actual device deployment. It also generates C code for various type of devices including MCUs."
10.36001_phmconf.2021.v13i1.3054,code,88,,0,"On the MCU side, we make use of the Edge Impulse cloud platform (Louis Moreau, Mihajlo Raljic, 2021) to train a crack detection model. This model training is using transfer learning based on a pre-trained Mobilenet (Howard et al., 2017) for computer vision tasks. The training process takes less than 10 minutes with 6000 training images and 2000 testing images with a resolution of 32 ∗ 32. The cloud platform also generated C code for MCU deployments.1"
10.36001_phmconf.2018.v10i1.564,"code, benchmark",175,,0,"To facilitate a fair comparison of computation time, both the parallel SMC code and the MCMC code were implemented on the Pleiades supercomputer, which is housed at the NASA Advanced Supercomputing (NAS) facility at Ames Research Center. All sampling procedures were executed on Sandy Bridge nodes with two eight-core, 2.6 GHz Intel Xeon E52670 processors. The three primary tuning parameters of the SMC sampler were set as shown in Table 4 such that the total number of model evaluations required, N × T × Z = 50, 000 evaluations. This was equivalent to the minimum number of model evaluations required for the MCMC benchmark in the ideal case (i.e., assuming perfect mixing and that all proposed samples were accepted). This assumption was conservative in favor of MCMC. As was the case for the MCMC benchmark, it was assumed that the measurement error (cid:15) ∼ N (0, νavg) with νavg = 4.83 × 10−4."
10.36001_phmconf.2019.v11i1.787,"code, code available",40,,0,"The detailed algorithm of VMD in (Dragomiretskiy & Zosso, 2014) and its MATLAB code is available at (Zosso, 2013). The readers are advised to refer these for more details on this method."
10.36001_phmconf.2021.v13i1.3054,"code, code available, github",60,,2,The remainder of the paper is organized as follows: Section 2 describes the potential techniques for more efﬁcient cooperation between cloud and TinyML models in PHM applications; Section 3 provides empirical experiment design and evaluation results of applications on hardware platforms; and Section 4 concludes the paper. Code available at: https://github.com/dustinjoe/TinyML -with-Cloud-for-PHM.
10.36001_phmconf.2015.v7i1.2655,"code, data",71,,0,"To prevent autoencoders from learn the identity function that has zero reconstruction errors for all inputs, but does not capture the structure of the data-generating distribution, it is important that certain regularization is needed in the training criterion or the parametrization. A particular form of regularization consists in constraining the code to have a low dimension, and this is what the classical auto-encoder or PCA do."
10.36001_phmconf.2015.v7i1.2760,"code, data",91,,0,"Generally speaking, because the radix-2 Fast Fourier Transform (FFT) is easy to code and has less computation burden than an arbitrary length discrete Fourier transform, the TSA is implemented based on the radix-2 length. That is, the number of measured data points in one revolution is resampled to the next larger power of 2. The average number of points in one revolution is then a function of the acquisition system sample rate (sr) divided by the shaft rate (Hz):"
10.36001_phmconf.2017.v9i1.2437,"code, data",81,,0,"execution of the GPU code is complete, the data in the results array can be copied to the CPU memory from the GPU memory. GPU memory can be freed once this is done. Following this processing ﬂow requires knowledge of the data to be copied and the result. Additionally, the overhead incurred by the copying or repeated copying of data between the CPU memory and the GPU memory can signiﬁcantly reduce the beneﬁt of parallelization."
10.36001_phmconf.2017.v9i1.2437,"code, data",125,,0,"Initial work involving GPUs for general purpose computing required the phrasing of computational problems in the form of graphics language calculations (Owens et al., 2007). As GPU technology advanced, Nvidia released the Compute Uniﬁed Device Architecture (CUDA) which provided developers with an application programming interface to the computational units of the GPU. With access to the full computational power of the GPU, software developers were able to instantiate their code on hundreds of cores running thousands of threads for massively parallel programming (Luebke, 2008). Recently, GPUs have seen extensive use in the training of neural networks, virtual reality and augmented reality, and advanced image and data processing methods."
10.36001_phmconf.2018.v10i1.503,"code, data",67,,0,"En route flight plans from the FAA traffic data feed currently do not include taxi routes. However, realistic prediction of the traffic for prognostics requires aircraft taxi plans, as well as the runways from which the aircraft will be operating. NATS provides a set of functions that can be used to create interactive code for taxi way design and for the selection of"
10.36001_phmconf.2019.v11i1.790,"code, data",89,,0,"Technician does not apply appropriate problem code (unfamiliar with DB classiﬁcation schema) (KB) (Related task: 5.2.2) It may be that the technician selects a problem code incorrectly, creating much less-useful data-set for the future. This error is occurring during an active search, making sch an error very knowledge-intensive. This is largely impacted by the training received by the technician (PSF F) and by the intuitive communication of meaning via the GUI tool (PSF L)"
10.36001_phmconf.2019.v11i1.842,"code, data",40,,0,"Löhr, Andreas and Matthias Buderath. 2014. “Evolving the Data Management Backbone : Binary OSA-CBM and in Code Generation for OSA-EAI.” Pp. 1–10 THE CONFERENCE EUROPEAN PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2014. PHM."
10.36001_phmconf.2021.v13i1.3030,"code, data",82,,0,"4. DATA MANAGEMENT DESIGN PATTERN Object oriented design methodology is perhaps the most successful approach to planning and designing software. Advantages include the ability to reuse code, improved maintainability, reduce mistakes, and improve testing or debugging tasks (among many others). We propose an object oriented design pattern for asset, process and data management to record and organize high ﬁdelity simulation data for the development, testing, and evaluation of PHM applications."
10.36001_phmconf.2021.v13i1.3030,"code, data",107,,0,"All data that is generated is inherently organized correctly when this framework is used in conjunction with a simulation environment, regardless of whether it is a UAV, car, spaceIn this manner the entire process of craft, or other system. data generation, storage, retrieval, and analysis among ﬂights with different components can be executed with the same code. It is left to the individual practitioner to implement the dynamical models of their system, this framework handles everything else. We apply this framework to a UAV system in an urban environment as discussed in the following section."
10.36001_phmconf.2021.v13i1.3030,"code, data",181,,0,"8. CONCLUSION & FUTURE WORK In this paper, an asset, process, and data management framework for the research and development of PHM applications has been proposed. This work was motivated by the lack of a comprehensive simulation environment and data management architecture that addresses requirements speciﬁc to PHM research. Therefore, a requirements analysis into the speciﬁc needs of PHM research and development was done and a new framework has been developed from the ground up. The framework was demonstrated with an end-to-end simulation environment implemented in MATLAB®. Using this framework, simulation changes are easily tracked, generated data is inherently organized, and data integrity is guaranteed. Collaboration is also facilitated when different researchers are using the same framework, making it easier to share code, reproduce results, and build off others work. We demonstrate the use of this framework with a data generation experiment and are left with a couple of different open ended questions that, with this simulation architecture will soon be studied."
10.36001_phmconf.2021.v13i1.3054,"code, data",139,,0,"Recently, there has been a growing trend towards minimizing manual code development (Broll & Whitaker, 2017) but rather have users utilize simple logic or drag-and-drop functional blocks to realize the desired features. To design a cyberphysical system with smarter decision making, a development platform purporting to minimizing coding efforts should be able to combine IoT sensing endpoints with hardware-targeted embedded machine learning model deployment. Furthermore, this platform should be able to automatically generate an optimized model that provides optimal on-device inference performance on custom hardware platforms. PHM applications are typical cyber-physical system applications that would involve a holistic workﬂow that requires both data collection and data processing. As a result, we can expect more end-to-end code generation and deployment workﬂows for PHM applications involving embedded machine learning."
10.36001_phmconf.2017.v9i1.2437,"code, data available, data",91,,0,"Additionally, the GPU processing ﬂow should be followed to supply the threads with data to process and available memory to write the result. The GPU processing ﬂow relies on the allocation of memory on the GPU prior to execution of code on the GPU. This means that GPU memory must be allocated for both the data to be copied to the GPU and the resulting data before processing can begin. After allocation, data can be copied from the CPU memory to the GPU memory. Once"
10.36001_phmconf.2019.v11i1.857,"code, database",121,,0,"If the considered contingency action A is applicable in the current state under the current constraints, it is provisionally selected. Then, the implications of A on the aircraft state Y and the future ﬂight-plan G1 are calculated by the function outcome. Depending on the contingency action that might be G1 = G.rest() if we can ﬂy to the next way-point, or G1 might be set to the route to a suitable emergency airport. The code for outcome can include queries to aircraft and failure models, queries to the operational database (e.g., to obtain a route to an emergency landing spot), as well as calls to other schemas."
10.36001_phmconf.2021.v13i1.3030,"code, database, data",58,,0,"These results were generated using the framework described in this paper via standard SQL queries to the database. The common interface provided by this framework ensures the different components’ data is automatically registered with the database and allows for repeatability among experiments and reuse of code, regardless of the vehicle under study or application."
10.36001_phmconf.2021.v13i1.3030,"code, database, data",183,,0,"First, a trajectory is loaded from the database, then the ﬂight is simulated within the simulation environment. The three sources of data described above, telemetry, summary, and degradation, are stored in the database. Next, the stop codes received from the simulation are checked in a logic block that informs us when the UAV has violated a system performance parameter threshold. These stop codes are stored in the stop code tb and are related to the stop code ﬁeld of the flight summary tb . Only valid stop codes are allowed to be entered as ﬂight summary data. This is another of many constraints in place to ensure that data integrity is maintained. The decision to not ﬂy again is made when the stop code is not arrival success for a count of 10 times (does not have to be sequential). This allows us to also catch data from failed ﬂights during the stages of degradation that result in violating a system performance parameter. If the UAV does"
10.36001_phmconf.2018.v10i1.585,"code, dataset provided, data",48,,0,in LabView generated code. A video camera was mounted to the test block/ as sensor data and system status were recorded (Figure 5). This video data was later used to provide a ground truth for comparison with the algorithm predictions generated throughout testing.
10.36001_phmconf.2021.v13i1.3030,"code, dataset, retrieve, source code, data",193,,0,"2. MOTIVATION The origin of this work came from the need to generate data for deep learning and reinforcement learning based prognostics applications. We realized that the creation and curation of the data used to generate machine learning models was a bigger challenge than anticipated, and, this especially applies to the evaluation of PHM technology in general. Since data is generated from the simulations, it is important to account for how the data is generated, stored, and later retrieved. Many system-level prognosics experiments utilize some form of Monte Carlo simulation and take a considerable amount of time to ﬁnish. The amount of data can be quite substantial and a lot of time is spent organizing it after-the-fact. Often times the source code contains changes that are not reﬂected in the accompanying dataset, making it difﬁcult to reproduce results. Different component models or degradation processes could be used in different experiments and these differences manifest in the resultant data. When this type of metadata is missing or the dataset lacks an accurate description, validation exercises are typically unsuccessful."
10.36001_phmconf.2021.v13i1.3030,"code, github, code package, python, package, code available, database",58,,2,"1Data constraints set in the table deﬁnitions are checked and enforced by the database, not user code. 2A complete simulation package that can serve as a stand alone application or as an example to build one using this framework can be found at https://github.com/darrahts/uavTestbed. 3an API in Python is currently under development."
10.36001_phmconf.2021.v13i1.3055,"code, github, data available, code available, data https, data",15,,3,1The code to generate events using C-MAPSS data is available at: https://github.com/Mahbubul-Alam-PhD/PHM-Society-2021
10.36001_phmconf.2020.v12i1.1131,"code, github, dataset, open-source, code open-source, code available, data",56,,2,"distributed, data-parallel implementation of batch SOM using the map-reduce paradigm and the Apache Spark framework (Apache Spark, 2014). This allows to leverage the production cluster to train SOM models on very large datasets of several million ﬂights. The code is open-source and available online at https://github.com/FlorentF9/sparkml -som."
10.36001_phmconf.2018.v10i1.564,"code, open-source, open-source code",108,,0,Uncertainty propagation was not explicitly demonstrated in this work. Future work should demonstrate methods for propagating parameter uncertainty obtained from SMC static sampling. Some methods worth investigating include: (i) Monte Carlo sampling using a brute force approach where the number of particles is set equal to the desired number of samples and (ii) smart Monte Carlo methods such as stochastic reduced order modeling (SROM). Efforts will also be made to release an open source version of the parallel SMC code used in this study and to implement some of the other self-tuning methods available in the literature.
10.36001_phmconf.2015.v7i1.2564,"code, source code",260,,0,"A method central to our approach is fault injection. Software fault injection (Voas & McGraw, 1999) is a common technique used in software testing as a modality to verify the application’s robustness and also tolerance of selected faults. This technique assumes the availability of a selection of operators that inject faults into the application. Among fault injection techniques commonly used for software applications, mutation testing is the oldest one, being introduced for the ﬁrst time in 1971 (DeMillo, Lipton, & Sayward, 1978). Different types of software systems may use the beneﬁts of mutation testing, since it can be successfully applied to different levels of testing and for different programming environments. In the context of mutation testing, we evaluate the quality of a test suite, i.e., a set of test cases, by injecting small software changes, i.e., mutations, at source code or byte code level, and then we verify whether there is some anomalous response. That is, the generated test suite is run against all the mutants generated (the altered software versions), and we investigate whether there is some test case in the suite s.t. the output differs for the original program and a mutant. The mutation score (i.e., the percentage of mutants for which the test suite offers such a test case) serves as metric for assessing a test suite’s quality."
10.36001_phmconf.2015.v7i1.2564,"code, source code",270,,0,"There are two major drawbacks with mutation testing. For there are time complexity issues in respect of the one, ressources needed to run all the tests for all the mutants, speciﬁcally if we include many mutation operators. The other issue is related to interpreting the mutation score. That is, since, most likely, the functional equivalence between a mutant and its original program is not investigated beforehand, we run into the problem of identifying those mutants for which the test suite does not offer a killing test case, but which are functionally (semantically) equivalent to the original program (and thus represent an alternative correct implementation). If this cumbersome process is not executed, such equivalent mutants result in a lower mutation score, which has to be taken into account. There are many tools available for mutation testing, e.g.: FIAT (Fault Injection-based Automated Testing) (Segall et al., 1988), PROTEUM (Delamaro, 1993; Agrawal et al., 1989; Ghosh, 2000) tools for C source code mutation testing, MUJAVA (Ma, Offutt, & Kwon, 2006) a Java based mutation tool, and SQLMutation (Tuya, Su´arezCabal, & la Riva, 2007) and JDAMA (Zhou & Frankl, 2009) for SQL. In this paper we assume that there is such a mutation testing tool for the desired development environment, that in turn allows us to inject faults into the control software."
10.36001_phmconf.2015.v7i1.2596,data,94,,0,"the training data sets which contain run-to-failure temperature measurements. The health index of the test bearing at the current time is computed using Eq. (6), with mEOL obtained from the training data sets. To obtain the time to end of life, tEOL of the test bearing, a polynomial curve of order 2 is ﬁtted to the calculated health index and extrapolated to the point where the health index is zero. The RUL can then be calculated as shown in Figure 4. The performance of"
10.36001_phmconf.2015.v7i1.2596,data,135,,0,"where RULi is the RUL estimated by method i and wi is the weight of method i, which can be taken as the mean score of each method in Table 7. For the case of simple mean, wi = 1. Table 8 shows the effect of combining all or a number of the methods described using weighted mean and simple mean. All possible combinations of the algorithms were evaluated and a combination of three methods (2-3-4) yielded the best results. This shows that an ensemble of a number of algorithms, especially with data fusion from different sensors yields a more robust prognostic approach. Combination of all temperature-based methods yields predictions within a 10% error bound, with all the results being early predictions."
10.36001_phmconf.2017.v9i1.2426,data,116,,0,"For the small number of ﬁlters in RPFB (N = 1, 3), the error bars for the performance of RPFB are large, as expected. As the number of features N are increased, not only the average error of RPFB decreases, but also its variance, which is due to the random choice of ﬁlters, decreases too. RPFB outperforms the window-based approach for most values of the number of features. This synthetic example is reassuring as it shows that RPFB is effective in modeling time series data with an analytical model (ARFIMA) without actually trying to directly estimate the parameters of the model."
10.36001_phmconf.2017.v9i1.2456,data,44,,0,This paper is organized as follows: Section 2 introduces the theoretical background of signal processing technique. Section 3 presents the proposal methodology. Section 4 suggest the result of application to IMS bearing data set. Section 5 gives the conclusion.
10.36001_phmconf.2018.v10i1.490,data,2,,0,Data Challenge
10.36001_phmconf.2018.v10i1.490,data,9,,0,Algorithm 3 Procedure APHM for PHM 2016 Data Challenge
10.36001_phmconf.2018.v10i1.490,data,10,,1,4.1. Prognostic Task in the PHM 2016 Data Challenge
10.36001_phmconf.2018.v10i1.490,data,15,,0,1: Input data: training data {(X tr i ∈ R.
10.36001_phmconf.2018.v10i1.490,data,28,,0,"After describing APHM and explaining the HP-conﬁg cPHM, we now deﬁne the hyper-parameter optimization involved in the Data Challenge task. We aim to solve the optimization"
10.36001_phmconf.2018.v10i1.490,data,59,,0,"Finally, we note that the ML model MRF output by procedure APHM only provide prediction for a block, but not for a timeseries sensor data X in general. Nevertheless, we can readily use BLOCKDEC in conjunction with MRF for predicting the removal rate for a time-series sensor data, as illustrated in Algorithm 5."
10.36001_phmconf.2018.v10i1.490,data,59,,0,"https://www.phmsociety.org/events/ conference/phm/16/data-challenge. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & de Freitas, N. (2016, Jan). Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1), 148-175."
10.36001_phmconf.2018.v10i1.490,data,67,,1,"In conclusion, we have introduced hyper-parameter optimization and its applications to automatically ﬁnd a good hyperparameter conﬁguration in machine learning tasks for machine health prognostics. We evaluate hyper-parameter optimization algorithms on the PHM 2016 Data Challenge, which demonstrate promising results. A future direction is to seek a way to automate the process of feature selection and feature engineering by a similar idea."
10.36001_phmconf.2018.v10i1.490,data,92,,0,"A brief description of the task. The PHM 2016 data challenge task involves the investigation of a wafer ChemicalMechanical Planarization (CMP) tool that removes material from the surface of the wafer through a polishing process. The goal of the task is to predict the average polishing removal rate, based on the sensor data collected during the polishing process. The challenge task (PHM, 2016) provides a collection of time-series-label pairs, where each pair records the sensor data recorded during a polish process."
10.36001_phmconf.2018.v10i1.490,data,97,,1,"In this Section, we provide the experiment results on hyperparameter optimization on the PHM 2016 Data Challenge task. From the results, we see that Bayesian optimization algorithms are able to identify better HP-conﬁgs than the default HP-conﬁg set by a human agent, which can be found under the “Human” column in Table 2. Here, the default hyperparameters for pre-processing the data are chosen based on a few trial and errors. The default hyper-parameters for the random forest regression model follows (Pedregosa et al., 2011)."
10.36001_phmconf.2019.v11i1.785,data,35,,0,"Si X.S. (2015). An Adaptive Prognostic Approach via to Nonlinear Degradation Modeling: Application Industrial Battery Data. Electronics. vol. 62(8), pp. 5082-5096."
10.36001_phmconf.2019.v11i1.785,data,39,,0,"Chen, C. & Pecht, M. (2012). Prognostics of Lithium-Ion Batteries Using Model Based and Data-Driven Methods. Proceedings of the Prognostics & System Health Management Conference. Beijing, China, May 23–25."
10.36001_phmconf.2019.v11i1.785,data,39,,0,"Eleftheroglou, N. & Loutas, T. (2016). Fatigue damage diagnostics and prognostics of composites utilizing structural health monitoring data and stochastic processes. Structural Health Monitoring. vol. 15, pp. 473-488."
10.36001_phmconf.2019.v11i1.785,data,41,,0,"Eleftheroglou, N., Zarouchas, D., Loutas, TH., Alderliesten, R. & Benedictus, R. (2018). Structural health monitoring data fusion for in-situ life prognosis of composite structures. Reliability Engineering and 40–54 System"
10.36001_phmconf.2019.v11i1.785,data,45,,0,"Cadini, F., Sbarufatti,C., Cancelliere, F. & Giglio, M. (2019). State-of-life prognosis and diagnosis of lithiumion batteries by data-driven particle filters. Applied Energy. vol. 235 (1), pp. 661-672."
10.36001_phmconf.2019.v11i1.785,data,52,,0,"This measure maximizes the probability Pr(Qt = i|y1:t, 𝐌∗) of being at the health state i at the time point t given the monitoring data up to time t. With M*={ζ, θ*} a specific model topology is denoted."
10.36001_phmconf.2019.v11i1.785,data,73,,0,"Prognostic measures can be defined based on the θ* parameters and the testing data. In other words, conditional to the testing data and the complete model M*={ζ, θ*}, prognostics tries to estimate the probability of being in health states 1,…,N-1 at a specific time points in future i.e. the conditional reliability function. Conditional reliability function,"
10.36001_phmconf.2019.v11i1.785,data,93,,0,"In the present study, the NHHSMM considers the discharge process in Li-Po UAVs batteries as a stochastic hidden process, which can be estimated from observations (monitoring data) i.e. discharge voltage data of ten actual indoor flights due to the ease of its measurement. The rest of the article is structured as follows. In section 2, a datadriven SOH/RUL methodology is presented. Section 3 summarizes the results of SOH diagnosis and RUL prognosis. The paper closes with the conclusions drawn from this study."
10.36001_phmconf.2019.v11i1.785,data,150,,0,"In this paper, the challenging problem of estimating a Li-Po battery’s state of health (SOH) and estimating the remaining useful flight time of a UAV are addressed. We propose the NHHSMM, which is a data-driven model, to tackle the aforementioned problems. We employ the voltage of the battery cells, as a feature, in order to train the NHHSMM and estimate the battery’s SOH and RUL. Besides the mean RUL estimations, upper and lower bounds quantify the uncertainty associated with each point prediction. Detailed results are presented for two different batteries and the robustness regarding the diagnostics/prognostics is verified even in an outlier flight with a rapidly discharging battery. The same methodology can be easily expanded in the diagnostics and prognostics of other power units as well e.g. in electric cars etc."
10.36001_phmconf.2019.v11i1.788,data,4,,0,Table II Data description
10.36001_phmconf.2019.v11i1.788,data,12,,0,Fig. 5 Sensor selection for train set FD001 in CMAPSS data
10.36001_phmconf.2019.v11i1.788,data,20,,0,M1: Direct RUL prediction model: the model predicts the RUL directly based on the machine operation data;
10.36001_phmconf.2019.v11i1.788,data,21,,0,Fig. 7 gives two examples of typical prediction trajectories for training data. These two units are randomly selected from
10.36001_phmconf.2019.v11i1.788,data,23,,0,"Jain, A. K., & Lad, B. K. (2016). Data driven models for speed milling cutters. Performability"
10.36001_phmconf.2019.v11i1.788,data,24,,0,1) Lack of uncertainty descriptions; 2) Need abundant data to make predictions;  3) Less robust in some cases;
10.36001_phmconf.2019.v11i1.788,data,28,,0,2) The algorithm requires large amount data (or data with enough diversity) to make accurate predictions; 3) Searching complexity increase quadratically as the
10.36001_phmconf.2019.v11i1.788,data,30,,0,"5) Comparing with the SSM and random-coefficient methods, the proposed method does not require HI to be directly observed or to be estimated from multivariate temporal data."
10.36001_phmconf.2019.v11i1.788,data,31,,0,1) Need assumptions about degradation trend; 2) Need to estimate HI from the observable data; 3) The uncertainty of HI estimation is difficult to quantify;
10.36001_phmconf.2019.v11i1.788,data,39,,0,"Tseng, S. T., Hamada, M., & Chiao, C. H. (1995). Using Degradation Data to Improve Fluorescent Lamp Reliability. Journal of Quality Technology, 27(4), 363-369."
10.36001_phmconf.2019.v11i1.788,data,40,,0,0()exp(||)=−−EoLEoLstdhsmeanhhEoLh8………………Find the Best MatchSliding Window Historical Data  Test Data  Kernel Two Sample TestRUL Prediction  Discard(Multivariate Temporal Sequence)YesNo𝑡𝑚𝑎𝑡𝑐ℎ𝑘𝑡𝐸𝑜𝐿𝑘Predicted RULRecent Observationstimetime   ANNUAL CONFERENCE OF THE PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2019
10.36001_phmconf.2019.v11i1.788,data,42,,0,"Lu, J. C., Park, J., & Yang, Q. (1997). Statistical inference of a time-to-failure distribution derived from linear degradation data. Technometrics, 39(4), 391-400. doi:Doi 10.2307/1271503"
10.36001_phmconf.2019.v11i1.788,data,258,,0,"in Fig. 7, the prediction results limits the 𝑅𝑒𝑎𝑟𝑙𝑦 = 125. This is because the RUL prediction is only meaningful after the incipient signature of degradation is detected, which widely referred as starting point of RUL prediction, as in Fig. 1. In the literature, 𝑅𝑒𝑎𝑟𝑙𝑦 for CMAPSS data is normally set to 125(Li, Ding, & Sun, 2018), 135(Ramasso, 2014) or not applied(C. Zhang, Lim, Qin, & Tan, 2017). In Table III, DCNN, LSTM, DBN, MODBNE are NN based regressors. RF and GB are ensemble regressors. SVM and LASSO are sparse regularized regression techniques. RULCLIPPER is a similarity-based RUL prediction method. The prediction accuracy for DCNN (Deep Convoluted Neural Networks) and LSTM (Long Short Term Memory neural network) are the accuracy for reported RULCLIPPER is reported in (Ramasso, 2014) and the accuracies for MODBNE (Multi-Objective Deep Belief Networks Ensemble), DBN (Deep Belief Networks), RF(Random Forest), GB(Gradient Boosting), SVM(Support Vector Machine), Lasso are reported in (C. Zhang et al., 2017). In this investigation, the studies that belong to the family of RCM, SPM and SSM are not benchmarked, since these methods regard the current degradation path as independent from the historical records."
10.36001_phmconf.2019.v11i1.816,data,8,,0,Figure 1. Big Data – Big Models
10.36001_phmconf.2019.v11i1.816,data,12,,0,Data Set 1 2 3 4 5 6 7 8 9 10
10.36001_phmconf.2019.v11i1.816,data,13,,0,Data Description set 1 2 3 4 5 6 7 8 9 10
10.36001_phmconf.2019.v11i1.816,data,20,,0,"Data sets derived from the PubChem Bioassay data set (Dheeru & Karra Taniskidou, 2017). These highly"
10.36001_phmconf.2019.v11i1.816,data,28,,0,imbalanced bioassay data sets are from the differing types of screening that can be performed using HTS technology. These data sets were created from 12 bioassays.
10.36001_phmconf.2019.v11i1.816,data,33,,0,"A rewarding attribute based on validation MPCD. In highly unbalanced data structures, this measure of classiﬁcation performance tends to reﬂect the model’s capacity in detecting the minority class."
10.36001_phmconf.2019.v11i1.816,data,34,,0,"A rewarding attribute based on the validation CEE. It is a relative measure that describes how well a candidate model ﬁts the validation data, where smaller values describe more robust predictions."
10.36001_phmconf.2019.v11i1.816,data,36,,0,"Escobar, C. A., & Morales-Menendez, R. (2017). Machine Learning and Pattern Recognition Techniques for Information Extraction to Improve Production Control and Design Decisions. In P. perner advances in data"
10.36001_phmconf.2019.v11i1.816,data,38,,0,UMW LSW AID373* AID604AID644* AID746AID1284* Statlog (class 1) Statlog (class 2) Credit Card Fraud Occupancy Detection HTRU2 T=Training set V=Validation set *Subsets of PubChem Bioassay Data †highly unbalanced ††ultra unbalanced
10.36001_phmconf.2019.v11i1.816,data,53,,0,"Based on empirical results, proposed criterion shows better performance and stability at solving the posed tradeoff between these three competing attribute than conventional model selection criteria when dealing with highly/ultra unbalanced data structures. As it selected structures with high detection ability, avoided overcomplexity and never selected a myope solution."
10.36001_phmconf.2019.v11i1.816,data,54,,0,Figure 3 shows how BM is applied to process data to monitor and detect those very few DPMO that are generated by the manufacturing process. Where the predominant goal for a classiﬁer is detection (β = 0) with a small as possible false alarm rate – FP (α).
10.36001_phmconf.2019.v11i1.816,data,60,,0,"Abell, J. A., Chakraborty, D., Escobar, C. A., Im, K. H., Wegner, D. M., & Wincek, M. A. (2017). Big Data Driven Manufacturing — Process-Monitoringfor-Quality Philosophy. ASME J of Manufacturing Science and Eng on Data Science-Enhanced Manufacturing, 139(10)."
10.36001_phmconf.2019.v11i1.816,data,63,,0,"The fundamental principle of BM learning paradigm, is that none of the models developed using process (empirical) data is the true model that generates the observed data. Based on this premise, proposed criterion’s objective is not to search for the true model, but to efﬁciently solve the posed tradeoff between these three competing attributes."
10.36001_phmconf.2019.v11i1.816,data,66,,0,"A 3D MS criterion (3D−N N ) is presented which is aimed at analyzing highly/ultra unbalanced data structures. Due to the importance of detecting rare quality events generated by manufacturing systems, proposed criterion is mainly driven by detection ability. It uses three of the most important attributes of an ANN structure – prediction (rewarding attribute), ﬁt (re 3"
10.36001_phmconf.2019.v11i1.816,data,72,,0,"Proposed criterion shows competitive performance at solving the posed tradeoff between the three competing attributes. The AIC, BIC and validation CEE raise a red ﬂag when dealing with highly/ultra unbalanced data structures, as they selected myope structures – a solution that fails to capture the pattern (e.g., M P CD = 0.3078, M P CD = 0.3338, M P CD = 0.4000 respectively)."
10.36001_phmconf.2019.v11i1.816,data,82,,0,"In today’s high conformance manufacturing environment, data sets for binary classiﬁcation of quality (good,bad) tend to be highly/ultra (bad class count < 1%) unbalanced. Therefore, detecting these few Defects Per Million of Opportunities (DPMO) is one of the main challenges addressed by PMQ. Therefore, in manufacturing modeling, functional refers to a parsimonious (Burnham & Anderson, 2003) classiﬁer with high detection ability."
10.36001_phmconf.2019.v11i1.816,data,118,,0,"Process Monitoring for Quality (PMQ) is a big data-driven quality philosophy aimed at defect detection through binary classiﬁcation (Abell et al., 2017). It is founded on Big Models (BM), a modeling paradigm based on optimization, machine learning and statistics, Fig. 1. It includes a learning component that requires many models to be created to ﬁnd the ﬁnal model (classiﬁer) (Escobar, Abell, Hern´andez-de Men´endez, & Morales-Menendez, 2018). Since many Candidates Models (CM) are created, Model Selection (MS) is one of the main challenges. The concept of using three attributes to evaluate"
10.36001_phmconf.2019.v11i1.869,data,8,,0,4.2. Anomaly detection in historical flight data
10.36001_phmconf.2019.v11i1.869,data,20,,0,"Monroe, G. A., Freeman, K., & Jones, K. L. (2012). IT Data"
10.36001_phmconf.2019.v11i1.869,data,31,,0,"Qin, S. J. (2012). Survey on data-driven industrial process monitoring and diagnosis. Annual Reviews in control, 220-234. 36(2), doi:10.1016/j.arcontrol.2012.09.004"
10.36001_phmconf.2019.v11i1.869,data,33,,0,"Hou, Z.-S., & Wang, Z. (2013). From model-based control to data-driven control: Survey, classification and perspective. Information Sciences, 235, 3-35. doi:10.1016/j.ins.2012.07.014"
10.36001_phmconf.2019.v11i1.869,data,38,,0,"where 1 and 0 indicate the anomaly and normal flag. In particular, the anomalies are flagged when these off-nominal errors are continued for at least 5 seconds to exclude data acquisition error in this analysis."
10.36001_phmconf.2019.v11i1.869,data,41,,0,"Fujimaki, R., Yairi, T., & Machida, K. (2005). An approach to spacecraft anomaly detection problem using kernel the Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining."
10.36001_phmconf.2019.v11i1.869,data,42,,0,"Li, G., Rai, A., Lee, H., & Chattopadhyay, A. (2018). Operational Anomaly Detection in Flight Data Using a Multivariate Gaussian Mixture Model. Paper presented at the PHM Society Conference."
10.36001_phmconf.2019.v11i1.869,data,48,,0,"Mounce, S. R., Mounce, R. B., & Boxall, J. B. (2011). Novelty detection for time series data analysis in water distribution systems using support vector machines. Journal of hydroinformatics, 13(4), 672686. doi:10.2166/hydro.2010.144"
10.36001_phmconf.2019.v11i1.869,data,57,,0,"Das, S., Matthews, B. L., Srivastava, A. N., & Oza, N. C. (2010). Multiple kernel learning for heterogeneous anomaly detection: algorithm and aviation safety case study. Paper presented at the Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining."
10.36001_phmconf.2019.v11i1.869,data,57,,0,"International Air Transport Association.  Jung, I.-S., Berges, M., Garrett Jr, J. H., & Poczos, B. (2015). Exploration and evaluation of AR, MPCA and KL anomaly detection techniques to embankment dam piezometer data. Advanced Engineering Informatics, 29(4), 902-917. doi:10.1016/j.aei.2015.10.002"
10.36001_phmconf.2019.v11i1.869,data,73,,0,"Even though many approaches have been proposed, the development of a robust monitoring framework is required to mitigate drawbacks in current methods: (i) lack of the integrated aircraft health monitoring which can monitor the flight performance associated with aircraft subsystems; (ii) demonstration of the developed method to commercial flight data such as onboard sensor data in aircraft to validate detection ability in realistic flight situation."
10.36001_phmconf.2019.v11i1.869,data,316,,0,"as principal component analysis (PCA), support vector machine (SVM), and neural networks (NN) (Fujimaki, Yairi, & Machida, 2005; Jung, Berges, Garrett Jr, & Poczos, 2015; Kromanis & Kripakaran, 2013; Lee, Li, Rai, & Chattopadhyay, 2019; Mounce, Mounce, & Boxall, 2011; Tayarani-Bathaie, Vanini, & Khorasani, 2014; Vanini, Khorasani, & Meskin, 2014). Fujimaki et al. (2005) proposed the monitoring framework using kernel principal component analysis (KPCA) to monitor engine performance of spacecraft by analyzing deviations of eigenvectors in feature space. Kromanis and Kripakaran (2013) utilized the support vector regression (SVR) model with moving fast Fourier transform (MFFT) to investigate the behavior of large structures under abnormal scenarios. Tayarani-Bathaie et al. (2014) proposed multiple dynamic neural networks (DNN) for aircraft turbine FDI by embedding memory term to enhance estimation accuracy in a nonlinear aircraft system, where classical NN structures cannot represent system behavior due to complexity in the aircraft structure. The developed framework was performed on engine performance data from the numerical model and showed detection capability with a coupled response under a fault situation. Furthermore, outlier detection algorithms are also utilized for anomaly detection, which identifies anomalies in flight performance as outliers (Das, Matthews, Srivastava, & Oza, 2010; Li, Rai, Lee, & Chattopadhyay, 2018). Das et al. (2010) developed multiple kernel anomaly detection (MKAD) algorithm using one class SVM model to capture anomalies in discrete and continuous flight features induced by offnominal flight operations such as go-around operation."
10.36001_phmconf.2020.v12i1.1288,data,2,,0,from data-driven
10.36001_phmconf.2020.v12i1.1288,data,11,,0,"2) Estimate 𝜎𝑏, 𝜎𝑐 based on historical data."
10.36001_phmconf.2020.v12i1.1288,data,19,,0,"Breslow, N. E. (1975). Analysis of Survival Data under the International / Revue Internationale de"
10.36001_phmconf.2020.v12i1.1288,data,53,,0,"3) For each testing unit with partial degradation data, initialize the filtering process by using entries in {𝚯𝑘}𝑘=1,...,𝑀. At the end of the filtering process, 𝑀 different filtering results at current time 𝑡 are obtained, which is denoted as  {𝚯𝑘"
10.36001_phmconf.2020.v12i1.1288,data,82,,0,"Wang, P., & Coit, D. W. (2007). Reliability and degradation modeling with random or uncertain failure threshold. Paper presented at the 2007 Annual Reliability and Maintainability Symposium. Xu, Y., Hou, W., Li, W., & Zheng, N. (2018). Aero-Engine Gas-path Fault Diagnosis Based on Spatial Structural Characteristics of QAR Data. Paper presented at the 2018 Annual Reliability and Maintainability Symposium (RAMS)."
10.36001_phmconf.2020.v12i1.1288,data,99,,0,"The application of the RBPF includes the filtering process and the prediction process. The objective of performing filtering on the partial degradation is to estimate the unknown model parameters 𝚯 = {𝑏, 𝑐, 𝑥, 𝜎𝑢, 𝜎𝑦} . Based on the estimated 𝚯𝑡 at current time 𝑡, the prediction can be obtained by extrapolating the state equation (3rd equation) in Eq.(2). 𝜎𝑏, 𝜎𝑐 , which are standard deviation of 𝑏 and 𝑐 , are estimated based on the historical Run-to-Failure (R2F) data."
10.36001_phmconf.2020.v12i1.1288,data,102,,0,"Jia, X., Cai, H., Hsu, Y., Li, W., Feng, J., & Lee, J. (2019). A Novel Similarity-based Method for Remaining Useful Life Prediction Using Kernel Two Sample Test. Paper presented at the Proceedings of the Annual Conference of the PHM Society. Jia, X., Huang, B., Feng, J., Cai, H., & Lee, J. (2018). A Review of PHM Data Competitions from 2008 to the PHM Society 2017. Paper presented at Conference."
10.36001_phmconf.2020.v12i1.1288,data,115,,0,"The MK test further evaluates the presence of a gradual trend in the sensory data throughout the machine degradation. If the MK test is passed, there is a trend in the sensory data. In Figure 2(a), the pass rate of the MK test is utilized as an indicator to show the trend-ability of the data, which indicates that sensors 2, 3, 4, 11, 17 are trend-able. Therefore, in this study, the HI estimation is primarily obtained by performing PCA on the sensor 2, 3, 4, 11, and 17(Jia et al., 2019)."
10.36001_phmconf.2015.v7i1.2710,"data available, case study data, data",111,,0,"In this paper, the sub-systems of the Boeing 787 will be used as a case study of how this high value parametric data and derived diagnostic and prognostic information can be used to enhance commercial aircraft maintenance practice. The Boeing 787 is outfitted with a modern computerized maintenance system that records key data for each of these aircraft systems. The available on-board data is transformed by ASHM into actionable component and system fleet information to guide fleet troubleshooting, opportunistic maintenance, and logistics. In addition to the Boeing 787, the ASHM software is also being used to support the Airbus A320 and A380 platforms."
10.36001_phmconf.2015.v7i1.2564,"data available, data",75,,0,"be available to us as a white box, but rather a box with subboxes, interfaces, and intellectual property cores in varying shades of grey. Furthermore, an easy integration into existing and proven development concepts is essential in order for an approach to be attractive enough for actual deployment. In order to accommodate such concerns, we make the following assumptions in respect of the data available to us."
10.36001_phmconf.2015.v7i1.2597,"data available, data",118,,0,"The inﬂuences of ﬂuting on the degradation of a bearing is rarely discovered as mentioned in section 1. Especially a convenient magnitude of the applied current through the bearing is hardly discussed or the recommended values vary in recent researches. Similarly to section 4 the inﬂuences of ﬂuting on the frequency spectrum of the current signal and the generated cases of damage are presented ﬁrst. An approach for the extraction of a health indicator representing the current state of the bearing by means of the PCA is described afterwards. Since the application of ﬂuting in this test bench is new, only three run to failure data sets are available for the comparison."
10.36001_phmconf.2015.v7i1.2646,"data available, data",4,,0,4.1. Data-Driven Approaches
10.36001_phmconf.2015.v7i1.2655,"data available, data",100,,0,"Inspired by the success of deep learning in many other domains, in this paper we explore how deep learning can in general and combustor benefit PHM applications anomaly detection applications in particular. Broadly speaking deep learning has two types: supervised and unsupervised. Unsupervised feature learning, i.e., using unlabeled data to learn features, is the key idea behind the self-taught learning framework (Raina et al., 2007). Unsupervised feature learning is well suited for machinery anomaly detection since for PHM applications abundant unlabeled data are available and easily accessible, while"
10.36001_phmconf.2015.v7i1.2656,"data available, data",157,,0,"Aside from degradation/failure modes, the availability of sensors in a system also affects decision-making. The assumption of what measurements are available both on-line and those taken during maintenance events has serious impact on the ability to effectively develop a data-mining algorithm. The sensor data available will change the degree to which Type II and III prognostic information can be discovered for the nuclear power plant, or more specifically, investigation. With the system or component under commercially lifecycle effective signals, prognostics models have already been developed for pumps (Jeffries 2014), motors (Nam 2013), and other systems such as heat exchangers (Welz 2014). The practical development of these models is limited by the current availability of plant data. The coupling algorithm will be designed to increase the availability of necessary prognostic data and consequently the ability to develop these prognostic models."
10.36001_phmconf.2015.v7i1.2656,"data available, data",176,,0,"Prognostic models are typically divided into three different types depending on the failure data available. Type I prognostics is based on past failure time distributions and is often referred to as traditional reliability analysis. This type of prognostic model only utilizes past failure times and does not require any additional failure data. Therefore, it can be conducted before operation of additional cycles. During operation, as stressor information such as operating condition or load is obtained, the model transitions to a Type II prognostic model. In parallel to the Type II models, anomaly detection can be conducted on the failure data. When specific signals such as temperature or pressure are tracked over lifetime and show an increase in damage to the system, the Type I or II model transitions to a Type III prognostic model. Type III models use the tracked degradation across multiple signals to measure the overall system health. The transitions between prognostic model types can be seen in Figure 2."
10.36001_phmconf.2015.v7i1.2710,"data available, data",84,,0,"thresholds, the reported parameters against processes computes estimated or expected values for some key parameters, and serves the report data and the processed results as part of a fleet view available to airline, maintenance, and engineering users. The application allows for the creation, integration, and execution of custom analytic modules that extract enhanced diagnostic and prognostic information from the raw report data. This information is also made available for visualization, trending, and alerting."
10.36001_phmconf.2015.v7i1.2710,"data available, data",86,,0,"establishing how the report can be modified to obtain the highest value condition information. The reports were designed to provide the most important system condition information as understood at the time of implementation. On a new aircraft like the Boeing 787®, this means that these decisions were made based upon a theoretical understanding of the system or using the available test data. An examination of field data can identify opportunities to improve the available system configuration information based upon actual usage."
10.36001_phmconf.2015.v7i1.2711,"data available, data",156,,0,"Figure 6 shows the progression of the features vs the speed for each wheel wear level. The colors in figure 6 were chose in accordance with the colors of the wheel profiles in figure 3: green stands for the no wear wheel profile, blue for the medium wear wheel profile and red for the full wear wheel profile. As presented in table 1, due to the experimental nature of the field data, the data sets for each fault were not always recorded at the exactly same speeds. Hence, the features are also only available at the same speeds (as in table 1). Since for comparison purposes the speed has to be the same for each fault, only three speed levels (65, 60 and 50 Mph), at which data was available for the three faults, were selected for analysis."
10.36001_phmconf.2015.v7i1.2716,"data available, data",76,,0,"Modeling of unscheduled maintenance requires knowledge of the failure behavior of the respective components or systems. When sufficient historic data are available, (parametric or non-parametric) failure distribution functions can be calculated (Hölzel et al., 2012). The presented approach uses discrete component lifetimes randomly drawn from the estimated failure distribution functions to model unscheduled removals on component or sub-system level over the aircraft lifecycle (Figure 3)."
10.36001_phmconf.2015.v7i1.2724,"data available, data",121,,0,"One of the key issues of condition-based prognostic approaches is to develop a model providing an identification of degradation processes and their future evolution based on historical data from measurements and observations. Several examples of prognostic approaches based on historical data and symptoms resulting from diagnostic tools can be found in the literature; namely an approach based on Bayesian networks (Medina-Olivier et al. 2012), approaches based on Fault Trees (Junjie et al. 2011; Sun et al. 2012) and an approach based on Decision Trees structure (Lee et al. 2005). In this paper, the proposed approach uses another model to analyze symptoms and historical data."
10.36001_phmconf.2015.v7i1.2766,"data available, data",137,,0,"When a PHM system definition is limited to a specific electromechanical item, that item is often unable to utilize information from other elements in the larger system or information external to the system - even when that additional information could aid in health assessment.  Similarly, information from fundamentally different data sources may not be available to other sub-systems where it could systems’ PHM performance.  Occasionally, when the external information is available, it may not be sufficiently coded, defined or synchronized to be useful to other sub-systems.  External information, such as the weather and weather forecasts or future usage planning information is also often available but inaccessible to equipment centric PHM systems, even when use of that information could assist with health assessment and health prognosis."
10.36001_phmconf.2016.v8i1.2523,"data available, data",43,,0,"EHM designers may capitalize on the early stages of engine design, to ensure its efficiency for detection of engine conditions. They can also use test data for mitigation in the future, when real data from customer will be available."
10.36001_phmconf.2016.v8i1.2555,"data available, data",86,,0,"Beyond the assumptions stated earlier, the decision support problem is further complicated by the fact that different types of data are available for analysis. Although the terms “mixed data” and “heterogeneous data” have been used interchangeably in the literature, the mixed data term is preferred in this paper. At a very high level, what this means is that the data processed by the decision support system consist of continuous, discrete, transactional and categorical variables."
10.36001_phmconf.2016.v8i1.2555,"data available, data",101,,0,"This paper addressed how to use several well-established data analytics techniques to process system and controller data as a decision support tool. The decision support system is developed in the context of an industrial controlled process operating under the monitoring of a human operator. While the human operator only had access to high-level information about the system, the decision support was assumed to have more data sources available to it and therefore help reduce the uncertainty around the decision. Specifically, clustering and correlation techniques were established to provide better situational awareness for the human operator."
10.36001_phmconf.2016.v8i1.2555,"data available, data",104,,0,"The data used for clustering consists of discrete input/output of the digital controller, and discrete and continuous states of the system. There are total 76 variables. During operation the operator has four possible decision states (i.e. System inoperable, Normal operation, System in test, Unknown) to choose from. It is assumed that not all decision states are known by the decision support system, therefore Σ ⊆ Δ; and “Unknown” state is a catch-all condition for all other operator decisions. There are 277 events that have the associated data available."
10.36001_phmconf.2016.v8i1.2577,"data available, data",55,,0,"The adaptive algorithm is divided into three distinct levels: Physical (sensor/component- level), regulation (control)level, and EPGS performance-level, as shown in the Figure3. Once the reliable sensor data is available, the process of incipient fault detection and prediction is performed in the PHM block."
10.36001_phmconf.2017.v9i1.2306,"data available, data",39,,0,"This procedure can be systematically continued as soon as new experimental data become available. Furthermore, alternative functional forms of the two-phase ﬂow correlations can be compared with each other using proposed framework and available experimental databases."
10.36001_phmconf.2017.v9i1.2391,"data available, data",115,,0,"In a previous work (Flores et al., 2015), the authors proposed a methodology to model and predict future criminal activity based on spatial probabilistic risk functions and a characterization of their temporal evolution as new data becomes available. To accomplish this, Hot-Spots and Gaussian Mixture Models (GMMs) were used to characterize the spatial component of the criminal activity. And, for the temporal component, Importance Sampling and Bayesian Inference were used. The methodology can be summarized in three main points. The ﬁrst, the method uses geo-referenced information of public services (e.g., bars, banks, parks, shopping centers)"
10.36001_phmconf.2017.v9i1.2470,"data available, data",184,,0,"In the marketplace of remote monitoring & diagnostics (M&D), the question arises when is the anomaly expected to happen on the monitored assets and how much longer can the this anomaly. The user operate conventional analytics based on classical statistical techniques are hardly able to answer these questions. The availability of huge volume of machinery operation data provides the unprecedented opportunity for advanced analytics in the remote monitoring domain. This paper provides advanced data driven analytics for complicated event monitoring and prediction, to enable integration of a wide range of data, including fleet knowledge and unit operational data into the platform. It also offers unified view of all predictive or anomaly alerts/alarms/advisories in a single pane of glass. Single unified, integrated data storage for all available data allows all data to be utilized and analyzed seamlessly together as an integrated whole instead of having silo’d, non-integrated, non-interoperable data stores attached to different applications in the back end, as the traditional M&D services do."
10.36001_phmconf.2018.v10i1.492,"data available, data",229,,0,"The procedure of fatigue damage accumulation in composite structures is still unknown and depends on several parameters such as type and frequency of loading, stacking the sequence and material properties. Additionally, nonhomogeneous and anisotropic nature of composites result to a stochastic activation of the different failure mechanisms and make the estimation of remaining useful life (RUL) very complex but interesting task. Data driven probabilistic methodologies have found increasing use the last decade and provide a platform for reliable estimations of RUL utilizing condition monitoring (CM) data. However, the fatigue life of a specific composite structure has a quite significant scatter, with specimens that either underperform or outperform. These specimens are often referred as outliers and the estimation of their RUL is challenging. This study proposes a new RUL probabilistic model, the Extreme Semi Markov Model Non-Homogenous Hidden (ENHHSMM) which the NonHomogenous Hidden Semi Markov Model (NHHSMM). The ENHHSMM uses dynamic diagnostic measures, which are estimated based on the training and testing CM data and the adapts dynamically NHHSMM. The available CM data are acoustic emission data recorded throughout fatigue testing of open-hole carbon–epoxy specimens. RUL estimations from the compared. The ENHHSMM ENHHSMM is concluded as the preferable option since it provides more accurate outlier prognostics."
10.36001_phmconf.2018.v10i1.502,"data available, data",44,,0,"When both moment constraint and observation data were available, the Lagrange function includes the constraint in Eq. (2), (4) and (9). The solution for the posterior for parameter θ can be expressed as:"
10.36001_phmconf.2018.v10i1.503,"data available, data",158,,0,"Rapid growth of aviation in the US and worldwide since 2010 has further sharpened the focus on safety issues, since, even if the rate of incidents/accidents per passenger mile remained the same, the number of these events will continue to increase to socially unacceptable levels as the number of flight operations increase. Motivated by this factor, and the fact that a considerable amount of data on aviation operations is becoming available, NASA and the FAA have initiated research initiatives to consider methods for enhancing the system safety. System Prognostics (Roychoudhury, et al., 2015) has been identified as one of the technologies that has the potential for substantially improving the safety of the National Airspace System (NAS) by identifying safetycompromising situations in the system before they occur and adopt appropriate mitigation strategies. This program forms the foundation for the work reported in this paper."
10.36001_phmconf.2018.v10i1.544,"data available, data",68,,0,"The next generation (NextGen) national air transportation system will include a number of changes. In particular, a multitude of new and existing aviation data sources are expected to become available, such as from Automatic Dependent Surveillance - Contract (ADS-C) and Automatic Dependent Surveillance – Broadcast(ADS-B) (ADS-B) surveillance systems-based operations (McCallie, Butts et al."
10.36001_phmconf.2018.v10i1.544,"data available, data",114,,0,"Here, the BEN concept and the Dynamic Hierarchical Bayesian Networks (DHBN)  (Nannapaneni) are being combined for the integration of knowledge and aggregation of uncertainty sources across levels and time of the NAS networks. A BEN in general, can be constructed in three ways – (1) using physics-based models, (2) using data-driven models, and (3) a hybrid approach, combining physics-based and data-driven methods. Currently available techniques, such as greedy hill-climbing, minimal description length, Bayesian-Dirichlet equivalence, and Mutual Information Test (MIT) is under investigation to facilitate automatic DHBN learning of NAS networks."
10.36001_phmconf.2018.v10i1.544,"data available, data",182,,0,"integrate all available information (archived or real-time) to assist the final objective of PHM. Above discussion presents many different types of information available in NAS and a novel Bayesian Entropy Network (BEN) is used for this purpose. BEN is a generalized Bayesian Network (BN) where Entropy is used to enrich the inference if additional information constraint is available. Bayesian Network (BN) has been widely used for causal studies where the topology structure represents the causal relationships. Existing BN methods for causal inference only take the information from point other sources/types of information cannot be directly used by the BN. For example, these types of information include non- linguistic probabilistic information/opinions from human, physical constraints, and encoded probabilistic point data (e.g., only moment information is available due to data reduction). Rigorous and systematic inclusion of multiple sources information is critical for a complex system-of-systems such as the NAS. A new definition of Maximum Relative Entropy is used for inference and prognostics"
10.36001_phmconf.2018.v10i1.544,"data available, data",340,,0,"management includes: uncertainty quantification, uncertainty propagation through models, uncertainty reduction technique in prognostics, and decision making under uncertainties. In large complex networks such as NAS, heterogeneous sources of uncertainty are involved in the process of risk assessment. These uncertainty sources in general can be classified into two categories: aleatory uncertainty and epistemic uncertainty (Mahadevan and Haldar 2000). Aleatory uncertainty sources are irreducible sources of variation such as measurement noise and physical variability in system characteristics (such as ground operations and pilot-to-pilot variations). Epistemic uncertainty sources are theoretically reducible that arise out of lack of perfect information such as environmental conditions and vehicle state. Rigorous uncertainty modeling frameworks for the risk assessment of NAS networks must be capable of accurately capturing the important statistical properties of the uncertainty sources (e.g. dependence over time and space between different uncertainty sources) and assess their contributions to the system performance. To enable this level of rigor, the system includes both probabilistic and non-probabilistic approaches to model the uncertainty sources. Probabilistic approaches are being used to quantify appropriate sources of uncertainty as random variables, stochastic processes, and timefield, modeled using probability dependent distributions describe interdependencies. Non-probabilistic approaches such as interval analysis, evidence theory, and fuzzy numbers are being used to create initial models for epistemic sources of uncertainty or where too little data is available to confidently fit the parameters of a probability distribution, and then equivalent probabilistic distributions are determined using the maximum entropy principle. In addition, rare events, which are usually the root causes of failures of the NAS systems, are often caused by simultaneous occurrences which are not only dependent but tail dependent (Bedford and Cooke 2002). Tail dependence measures the co-movement in the lower and upper tails of the joint probability function of uncertainty variables, which is important for the risk assessment of NAS."
10.36001_phmconf.2019.v11i1.598,"data available, data",25,,0," Limit in content: Background removed Since data from tests of three ropes was available, these could be modelled in different ways:"
10.36001_phmconf.2019.v11i1.776,"data available, data",168,,0,"Uncertainties in these models need to be addressed and quantiﬁed to be able to make risk assessments and improve maintenance operations. Management of airlines will have to take careful decisions based on these risk-assessments which will become more straightforward with time and experience. Regarding the little amount of research into prognostic models, the development of these models is a difﬁcult and timeconsuming task. Moreover: data is needed. As we can observe from for example the competition set up by NASA (Saxena, Goebel, Simon, & Eklund, 2008), the supply of extensive reliable data from aircraft engines causes an impulse to research. Especially when this data is backed by some sort of competition as has been done by NASA. Airlines and aircraft companies such as Boeing and Airbus should make extensive data available to validate the models presented in literature. In this way, prognostic approaches can be validated and implemented in industry."
10.36001_phmconf.2019.v11i1.776,"data available, data",275,,0,"combinations of these three approaches are also possible and are called hybrids (Elattar et al., 2016; Sheppard, Kaufman, & Wilmer, 2009). The different approaches and their main advantages and disadvantages are shown in Table 1. The selection of the right approach for a certain problem is a difﬁcult task because approaches used in the estimation of the RUL do not only differ in accuracy and cost, but also need to consider noise and interference in the data from sensors. The statistics-based approach is the most used approach and depends on massive historical data about the same, mostly uncritical, mass produced components. Advantages of this approach include that it is simple and can easily be adapted to be applicable to other components for which extensive data is available. Many drawbacks of this approach include that the calculated Mean Time Between Failure (MTBF) is often not accurate and components can thus fail before being replaced, also it is hard to apply the models to new components because historic data is not available. The data-driven approach is very popular in the scientiﬁc community, due to its quick implementation and deployment, and the increasing interest in machine learning techniques. Moreover, it can be used even when it is impossible to obtain a mathematical degradation model. The approach mainly relies on machine learning techniques that can be used at low cost and without knowledge of the system physics. The drawback of this approach is that is often difﬁcult to let the model learn when no"
10.36001_phmconf.2019.v11i1.788,"data available, data",16,,0,1) Simple and efficient; 2) Good accuracy when abundant historical data is available
10.36001_phmconf.2019.v11i1.788,"data available, data",103,,0,"The similarity-based method for RUL prediction is also studied when abundant historical data is available. The TSBP was introduced in the PHM data competition 2008 (Wang, 2010) and won this data competition. In this analysis, the Logistical regression is used to estimate the HI of the engine, the similarity was evaluated based on the Euclidean distance between current and historical HIs. Another recent study in proposes to use Multi-Task Gaussian Process (MTGP) to achieve the reference-based prediction of the SoH of Li-on significant Their method batteries. improvements in the prediction accuracies."
10.36001_phmconf.2019.v11i1.788,"data available, data",187,,1,"When abundant historical Run-to-Failure (R2F) data is available, the similarity-based method is one of the favored options for Remaining Useful Life (RUL) prediction due to its simplicity and satisfactory accuracy. In this study, a novel is similarity-based methodology proposed. The proposed method has important processing steps: similarity matching and Weibull fitting. The similarity matching screens the historical records by a similarity testing called Kernel Two Sample Test (KTST), and only those records that pass KTST are adopted as references for RUL prediction. For the selected similar records, the RUL is predicted as the remaining time to failure. The Weibull fitting fuses the multiple RUL predictions given by similar historical records. The PDF of RUL is estimated as the fitted Weibull distribution. To demonstrate the effectiveness and superiority of the proposed method, the famous C-MAPSS (Modular Aero-Propulsion System Simulations) data about aero-engine degradation is adopted for model validation. The results demonstrate improved prediction accuracy comparing with other similarity-based approaches and the state-of-the-art deep learning predictors."
10.36001_phmconf.2019.v11i1.790,"data available, data",10,,0,Table 4. Potential HRA supporting data types and sources
10.36001_phmconf.2019.v11i1.792,"data available, data",18,,0,"Wherever data is available, similar models can be built and studied for cost of maintenance activity."
10.36001_phmconf.2019.v11i1.792,"data available, data",43,,0,opening times. MWOs for which there were missing dates and times were entirely ignored but this reduces the amount of correct data available to train the models. Such issues emphasize the importance of collecting accurate time related data during maintenance.
10.36001_phmconf.2019.v11i1.806,"data available, data",74,,0,"ity of the raw CNC machine control data variables, we have found that taking the ﬁrst principal component sufﬁces to achieve good performance while dramatically reducing processing time. On average, the time it takes to run AnomDB retaining all principal components is 16 times longer than when retaining only one. Generally, adding more control data principal components into the analysis degrades performance in the evaluations of Section 4.2."
10.36001_phmconf.2019.v11i1.838,"data available, data",177,,0,"Prognostics can enhance the reliability and availability of industrial systems while reducing unscheduled faults and maintenance cost. In real industrial systems, data collected from the normal operation conditions of system is available, but historical degradation data is often unavailable. Hence, this paper proposes a general data-driven prognostic approach dealing with the lack of degradation data in the ofﬂine phase. First, features are computed on the collected raw signal, then One Class Support Vector Machine (OCSVM) is used to detect the degradation, this anomaly detection method is trained using only normal operation data. Then, features are ranked according to the selection criteria. The feature having the highest score is chosen as Health Indicator (HI). Finally an adaptive degradation model is applied for the prediction of the degradation evolution over time and Remaining Useful Life (RUL) estimation. The proposed approach is validated using run-to-failure vibration data collected from a high speed shaft bearings of a commercial wind turbine."
10.36001_phmconf.2019.v11i1.842,"data available, data",57,,0,"5. Preserve: Data storage for Long-term access and reuse. The purpose of this phase is the guarantee long-term preservation, ease of search and retrieval, accessibility and usability of the data. This step employs multicopy/storage locations, long-term usefulness, accuracy and consistency, information security, metadata and file formats."
10.36001_phmconf.2019.v11i1.842,"data available, data",128,,0,"Data integrity involves creating, processing and maintaining the assurance, accuracy, consistency and completeness of data throughout its lifecycle. The content and meaning of data is maintained throughout its lifecycle. This also includes compliance with statutory requirements.  Quality represents the use of best practice protocols and methods of collecting and organizing data that ensures its accessibility, completeness, validity, accuracy, consistency, availability and timeliness.  Security involves the protection of data from unauthorized access to modify, use, delete and disclose. It includes protection against theft, breach of agreements, data protection laws and unintended or hateful modification. The computer system security, physical security and file security are all part of this step."
10.36001_phmconf.2019.v11i1.842,"data available, data",148,,0,"In a study of the problem of managing provenance of derived data in scientific research, Yang et al (2013) found that although initial CSMD model provisioned accessibility, usage and reuse of experimental raw data, it did not ""support access to the derived data produced during analysis, nor does it allow the provenance of data supporting the final publication to be traced through the stages of analysis to the raw data""(Parry 2016:613).  In other words, the original CSMD model recognizes the sources’ provenance of the derivative data but fails to describe the transformation provenance. They emphasized the significance of keeping track of previous work and the need for a resilient data management tool and computational workflows that would capture the flow of data, raw data to derived data through to final publication."
10.36001_phmconf.2019.v11i1.842,"data available, data",166,,0,"6. Publish/Share: Put together quality assured, metadata rich, platform or system-agnostic data, with relevant security safeguards and share with interested parties of stakeholders.  [Parallel (Metadata, Documentation): Establishes an obligation to create and upgrade metadata on any or all the stages of the lifecycle including the documentation of usage in specific systems, applications and settings. [Parallel to phases 1-6] Manage Quality: Mandatory to accurately undertake data collection, handling, processing, usage, and maintenance across all the phases of the scientific data lifecycle, is the use of protocols and methods. This implies effective and efficient quality assurance and quality control. [Parallel to phases 1-6] Back Up and Secure: Regularly create image backups of both files and data bases on either onsite or offsite devices. Access control and other security measures must be taken to prevent accidental data loss and data corruption."
10.36001_phmconf.2019.v11i1.842,"data available, data",167,,0,"We have identified key data lifecycle models (DLCM) and frameworks and found that though they had some of the elements for IVHM data, they lacked some essential ones. Thus, sustaining the cycle of data and knowledge management issues – creation, quality, storage, security and provenance. In the review, we found that the USGS DLCM encapsulated most data lifecycle models. The USGS reviewed more than 50 DLCMs to develop their one, and therefore was chosen as the ideal Data Life Cycle for integration. The strength of the proposed data lifecycle lies in the integration of key elements of the OSA-CBM framework, the CSMD, the engineering process model to create a scalable model that fits the depth and breadth of IVHM Research and Engineering operations.  This model supports the design and implementation of protocols for effective and efficient data management. It provides a foundation for Data- and knowledge management system requirements."
10.36001_phmconf.2019.v11i1.842,"data available, data",281,,0,"The creation, capturing, using and sharing of knowledge is based on data. The rate of data creation, collection, and elicitation through wide ranging experiments, simulations, observations and measurements is rapidly increasing within In Integrated Vehicle Health Management addition, Knowledge Management (KM), data abstraction, analyses, storage and accessibility challenges persist, resulting in loss of knowledge and increased costs.  This growth in the creation of research data, algorithms, technical papers, reports and logs, requires both a strategy and tool to address these challenges. A Data Life Cycle Model (DLCM) ensures the efficient and effective abstraction and management of both data- and knowledge outputs. IVHM is characterized by prognostics and diagnostics, which depend heavily on high quality data to perform data-driven, modelbased and hybrid computational analysis of asset health. IVHM does not yet have a systematic and coherent approach to its data management. The absence of a DLCM means that valuable knowledge might be lost or is difficult to find. Data visualization is fragmented and done on a project by project basis leading to increased costs. There is insufficient algorithm documentation and communication for easy transition between subsequent researchers and personnel. A systematic review of DLCMs, frameworks, standards and process models pertaining to data- and KM in the context of IVHM, found that there is no DLCM that is consistent with IVHM data- and knowledge management requirements. Specifically, there is a need to develop a DLCM based on Open System Architecture for ConditionBased Maintenance (OSA-CBM) framework."
10.36001_phmconf.2019.v11i1.857,"data available, data",304,,0,"The Diagnostic Reasoner (DR) is a cFS component, which monitors and diagnoses the vehicle on which AOS is running (Schumann et al., 2019). Based on sensor information from the vehicle, DR performs fault detection, and if an anomaly is found, it will perform fault isolation to identify the current failure mode(s). If it is not possible to isolate the fault to a single failure mode, the diagnosis result contains an ambiguity group of several potential failure modes. Analog sensor data received from AOS are preprocessed and checked against given value ranges and thresholds by the Limit Checker (LC), a built-in component of the cFS system. For diagnosis, DR uses a dependency matrix (D-matrix) approach (Luo, Tu, Pattipati, Qiao, & Chigusa, 2005) to determine the state of a system component as “GOOD”, “BAD”, “SUSPECT”, or “UNKNOWN”: based upon the results of tests, e.g., Ubatt > 16V , the algorithm consults the D-matrix to determine, which components are might be affected by these test results. Tests results are deﬁnes as discrete values “PASS”, “FAIL”, or “UNKNOWN”, if no data are available. Even for large D-matrices relating hundreds of tests to thousands of failure modes, the algorithm (described in (Schumann et al., 2019)) is efﬁcient enough to enable real-time diagnosis. The binary D-matrix, which relates the diagnostic tests for a component to the failure modes of that component is generated automatically from our system and failure models as described in Section 4."
10.36001_phmconf.2019.v11i1.876,"data available, data",275,,0,"PHM systems can evolve in time, growing their capabilities from anomaly detection, towards diagnostics and prognostics (Sikorska, Hodkiewicz, & Ma, 2011; Bussey, Nenadic, Ardis, & Thurston, 2014). As more failure data becomes available, representation learning features of deep learning can be exploited to learn better Condition Indicators (CIs). The experiments with real world data have demonstrated the potential of the deep learning models to effectively detect anomalies. However, because the ground truth of failure is typically not accessible without signiﬁcant feature engineering (see, e.g., the fundamental axioms of structural health monitoring, and Axiom IVa in particular (Worden, Farrar, Manson, & Park, 2007)), we explored the effectiveness of deep learning models using simulated failures. The selection of the physical system and its model that generated synthetic data was based on the following criteria: the system had to be well-researched in PHM community (to provide familiarity with the nature of the solution and facilitate intuitive interpretations), it had to be nonlinear (to avoid trivial solutions), with information contained in multiple time scales, and to have existing published models (to leverage known results and reduce the debugging time). A lithium ion battery system quickly emerged from this criteria as a good candidate since the system was wellresearched in the PHM community (see e.g. (Saha & Goebel, 2008; Olivares, Munoz, Orchard, & Silva, 2013))."
10.36001_phmconf.2019.v11i1.888,"data available, data",84,,0,"Based on the previous studies on the ECS (Figuero, 2017), operating temperature and pressure range throughout the system is estimated to be -33.15 ºC to +186.85 ºC and 100 KPa to 350 KPa. However, no previous data on the humidity was available. Thus, this experiment will be contributing directly in humidity within the HPWS. A total of 56 sensors have been installed, which will provide a clear understanding of the system performance characteristics."
10.36001_phmconf.2019.v11i1.898,"data available, data",9,,0,1.1. The Flight Data Recorder – Data Variability
10.36001_phmconf.2019.v11i1.898,"data available, data",69,,0,"Each AD technique has a slightly different way of approaching the AD problem depending on available data and anomalous behaviour they wish to detect. The available flight data could be FDR, voice, maintenance data, safety reports, cockpit display messages, or weather data. Analysts may wish to detect anomalies in flight operations, components, crew actions or even predict system failures."
10.36001_phmconf.2019.v11i1.898,"data available, data",74,,0,"were aimed at identifying hazards before they led to accidents thereby aiding the establishment of appropriate mitigation strategies to prevent such occurrences. Data gathering for FDM programmes are available from onboard aircraft recorders, voice systems, maintenance reports, safety reports, cockpit display messages, or weather systems. The onboard data recorder sources, however produce the “Big Data” mostly useful for the analysis phase of FDM."
10.36001_phmconf.2019.v11i1.898,"data available, data",77,,0,"Data-driven approaches are used to describe system behaviour using only the data. Data-driven methods for data analysis exploits large amounts of data available to better understand behaviours and operational faults. These approaches learn by observing and analysing system behaviour of complex systems. The goal here is to build systems that can observe, diagnose, recognize unusual events and inform operators for more effective decision making (Biswas et al., 2016)."
10.36001_phmconf.2020.v12i1.1162,"data available, data",97,,0,"When fault data is not available, system model methods are reliable alternatives. These methods use normal data to learn system model and apply the trained model to compute expected value of the systems. When the expected value does not match the current value, we can conclude that the system is not behaving as expected and therefore, may be in a fault mode. Unlike classiﬁer methods, system model methods do not require fault data and therefore, are more practical. A system model method is shown in Figure 4."
10.36001_phmconf.2020.v12i1.1162,"data available, data",123,,0,"The classiﬁer approaches use both normal and fault data to train a classiﬁer which classify each sample point (or each time window which includes several successive samples points) as normal or fault modes. When fault data is not available, system model methods are reliable alternatives. These methods use normal data to learn system model and apply the trained model to compute expected value of the systems. When the expected value does not match the current value, we can conclude that the system is not behaving as expected and therefore, may be in a fault mode. Unlike classiﬁer methods, system model methods do not require fault data and therefore, are more practical."
10.36001_phmconf.2020.v12i1.1162,"data available, data",153,,0,"The classiﬁer methods use both normal and fault data to train classiﬁers which classify each sample point (or each time window which can include several successive samples points) as normal or fault modes. Data-driven methods use normal and close to fault historical data to train their classiﬁers for fault detection. In many domains, improvements in the production technology has led to more reliable systems. As the systems become more reliable and less likely to fail, fewer historical fault data is available to train the classiﬁers. A common solution is to generate fault data in the lab environment by using different methods such as accelerated aging. These methods subject the system to high stresses to create different faults in a short period of time in order to gather fault data for training. Clearly this approach can be very expensive especially for complex systems."
10.36001_phmconf.2020.v12i1.1162,"data available, data",203,,0,"Data-driven FDI methods operate exclusively on measured data without detailed knowledge of the system. Therefore, we do not need to have access to the system model. Moreover, as more data become available it is possible to keep the model updated through retraining. There are two main datadriven FDI approaches: 1) classiﬁer methods and 2) system model methods (Salfner et al., 2010). Classiﬁer method is shown in Figure 3. The classiﬁer approaches address FDI in two steps; 1) feature selection and feature extraction and 2) fault classiﬁcation. The ﬁrst step is designed to select a subset of relevant measurements or extract a set of new features from the measurement data. Typically, this would represent a subset of measurements or extracted features that are sensitive to the faults and, at the same time, invariant or at least robust to noise and disturbances in the system. Among feature extraction methods, Principal Components Analysis (PCA) is the most widely used (Venkatasubramanian et al., 2003; Wang, Zheng, Farahat, Serita, & Gupta, 2019; Wang,"
10.36001_phmconf.2020.v12i1.1205,"data available, data",125,,0,"Table 1 summarizes the comparison between the different training schemes based on the above measures with a detection threshold of α = 0.0001. The ﬁrst line in the table displays the performance scores for a ”Limited data” scenario, that is, assuming that only three months of data of turbine A0 (the reference set) are available for training. As expected, we observe considerably lower recall, precision and F1 scores than if we use 9 months of training data as in the Baseline scheme. This is a direct consequence of the lack of representative training data when it is based only on one season of the entire year. The Baseline single-turbine scheme in turn yields"
10.36001_phmconf.2020.v12i1.1205,"data available, data",142,,0,"2. Cross-turbine Scheme: training and validation sets from turbine S (different from T) are used to train the CNN. The trained CNN is then used to predict the output variable of turbine T at all times. The cross turbine scheme includes an additional step of rescaling of the prediction errors, see Algorithm 1. We expect discrepancies between turbines in the typical normal values of certain input and output variables. We correct for these discrepancies by means of a linear regression of y(t) on the prediction ˆy(t) using data from a short period of three months (the ”reference set” denoted by R). We assume that such data are available also for a newly installed turbine after a time period of several months."
10.36001_phmconf.2020.v12i1.1205,"data available, data",249,,0,"Machine learning algorithms for early fault detection of wind turbines using 10-minute SCADA data are attracting attention in the wind energy community due to their cost-effectiveness. It has been recently shown that convolutional neural networks (CNNs) can signiﬁcantly improve the performance of such algorithms. One practical aspect in the deployment of these algorithms is that they require a large amount of historical SCADA data for training. These are not always available, for example in the case of newly installed turbines. Here we suggest a cross-turbine training scheme for CNNs: we train a CNN model on a turbine with abundant data and use the trained network to detect faults in a different wind turbine for which only little data are available. We show that this scheme is able to considerably improve the fault detection performance compared to the scarce data training. Moreover, it is shown to detect faults with an accuracy and robustness which are very similar to the single-turbine scheme, in which training and detection are both done on the same turbine with a large and representative training set. We demonstrate this for two different fault types: abrupt and slowly evolving faults and perform a sensitivity analysis in order to compare the performance of the two training schemes. We show that the cross-turbine scheme works successfully also when training on turbines from another farm and with different measured variables than the target turbine."
10.36001_phmconf.2020.v12i1.1300,"data available, data",34,,0,3. Unsupervised TL: the source and domain tasks are different but related (similar to inductive TL) and there is no labeled data available in the source or target domains.
10.36001_phmconf.2020.v12i1.1300,"data available, data",63,,0,"Inductive TL: the target task is different from the source task (TS (cid:54)= TT ) while the source and target domains can be either different or the same. In Inductive TL, label data is available in the target domain but not necessarily In this type of TL, TS and TT in the source domain."
10.36001_phmconf.2020.v12i1.1300,"data available, data",72,,0,"2. Transductive TL: the source and domain tasks are the same, while there is a domain shift or a distribution change between the source and the target (DS (cid:54)= DT , TS = TT ). In this type of TL, labeled data is only available in the source domain. Transductive TL is also called ”Domain Adaptation” in many studies."
10.36001_phmconf.2020.v12i1.1303,"data available, data",62,,0,"Although fewer data points are available for the lower temperature analysis during early aging as compared to the higher trial temperatures, less extrapolation to typical operating temperatures makes selection of the lowest aging temperature more representative of expected real-time aging. Therefore the 57C data was used as the reference temperature for determination of acceleration factors and sensor activation energy."
10.36001_phmconf.2020.v12i1.1303,"data available, data",63,,0,"As discussed previously, the sensor is co-aged with the target product to provide the aging data. However, where co-aging data is not available, Arrhenius methodology may be used to adjust the times and temperatures of either the product or sensor aging data so that the same data points on the correlation curve define points of equivalent thermal age."
10.36001_phmconf.2020.v12i1.1303,"data available, data",87,,0,"consists of a polymeric dielectric between conductive plates.  As the dielectric shrinks from thermal aging, the plate separation decreases, thus increasing capacitance measured between the plates. As in the resistive thermal age sensor, the electrical property (capacitance this case) always represents the integrated time-temperature of its environment at its characteristic activation energy. Arrhenius behavior of the capacitance in a variable thermal environment allows correlation modeling of RUL for materials and products for which thermal aging data is available."
10.36001_phmconf.2021.v13i1.2968,"data available, data",97,,0,"Analytics for prognostic applications are, generally, degradation models.  The model may estimate a remaining useful life (RUL) or, more generally, generate an alert for failure proximity. As more data have become available and as cloud computing capabilities grow, so does the potential for large scale model deployment. Regulatory bodies are considering prognostic monitoring regimes (Air Transport Assoc. of America, 2018) such as condition-based maintenance credits (Le, Ghoshal & Cuevas, 2011) as valid substitutes for certain routine maintenance inspections."
10.36001_phmconf.2021.v13i1.2983,"data available, data",137,,0,"This section shows how a data-driven method can be used to remedy some of the drawbacks of ﬁnding faults in batchwise production with physical systems. In the past, qualitative physics (Forbus, 1984b) and consistency-based diagnosis algorithms (De Kleer & Williams, 1987; de Kleer & Brown, n.d.) were used. However, the amount of expert knowledge and time required is prohibitive for many companies. Especially for the small and medium sized enterprises (SMEs), which often use smaller production systems such as injection molding machines. Therefore, we propose our data-driven method. By relying only on the available process data we expect our method to perform worse than many traditional methods. However, it may provide a ﬁrst approach to make"
10.36001_phmconf.2021.v13i1.2983,"data available, data",215,,0,"The production of rubber products requires a batch-wise compounding process in which different ingredients are mixed according to a deﬁned recipe. For this experiment we used time series of single batches encompassing 6506 rows of 87 signals. Of these 35 were identiﬁed to be quality control signals from laboratory data, thus forming set M and the rest being contained in set S. Table 2 contains the results for system 2 over different thresholds. The ﬁrst row shows, whether the generated rules were suitable for diagnosis. The second row shows whether the diagnosis algorithm was able to determine the injected faults. In contrast to systems 1 and 2 signals within the compounding process are not highly correlated. This is partly due to some signals being aggregated over time. For example the laboratory data is only available once a batch is ﬁnished and has been stored for some time. As a result only few distinct rules exist such that Φ is comparatively small (about 8 rules with τ = 0.3. Above a threshold τ = 0.5 only one rule exists, which is insufﬁcient for diagnosis. The injected faults were simulated through randomly setting some signals in set M to false. This simulates"
10.36001_phmconf.2021.v13i1.3009,"data available, data",100,,0,"RUL estimation and failure prediction models: when enough data is available, our proposed maintenance decision-making framework can learn an optimal policy from the data without requiring a separate RUL estimation or failure prediction unit. However, it is very common for industries to develop reliable RUL estimation models that can estimate the remaining useful life of equipment. When these models are already available, their output can be used as an input to our decisionmaking model to improve accuracy and simplify the training process as it is shown in Figure 1. The formulation is:"
10.36001_phmconf.2021.v13i1.3014,"data available, data",131,,0,"The objective of this study is to ﬁnd an indicator that correlates to the wear of the pump. Since the assessment of the true wear state is not a trivial problem, it is assumed to be roughly correlated to the operating time. Therefore, we are looking for features that are closely related to the operating time of the pump. In a data-driven approach, appropriate features to reach that goal are found. That means, real world measurement data with known operating time are analyzed in order to ﬁnd features that correlate to the operating time. During the development, many possible indicators were deﬁned, extracted and evaluated. Only the best indicator of the evaluation is presented subsequently in Section 4."
10.36001_phmconf.2021.v13i1.3052,"data available, data",25,,0,sampling. Implement alternative algorithms to the leader algorithm for prototype selection. Investigate methods to improve the labeling of the data into classes.
10.36001_phmconf.2021.v13i1.3052,"data available, data",41,,0,"Overall, there seems to be opportunity in each method to improve the consistency of the classifiers and resilience to new unseen failure data. In combination with trying to access more failure data which would help the classifiers prepare for"
10.36001_phmconf.2021.v13i1.3059,"data available, data",148,,0,"vate further investigations of the framework in a wider set of time series data to identify if vibration and process data have more suitable feature extraction techniques only to them. Furthermore, an investigation of additional active learning query strategies would be beneﬁcial to understand the limitations of the framework. Active learning frameworks are a useful approach to deal with the large amounts of unlabeled data that is constantly produced by IoT systems. Further work is required to integrate these approaches into an efﬁcient production pipeline that can manage the unlabeled data as it becomes available and users just need to respond to uncertain conditions that might require further investigation. Active learning can help in reducing the amount of work and expenses related to the labeling process of data while helping to maintain a level of consistency on the labeling procedure across users."
10.36001_phmconf.2021.v13i1.3059,"data available, data",190,,0,"The condition monitoring, maintenance, and health management ﬁelds make ample use of time series for diagnostics, monitoring, and prognostics tasks. These time series originate from electrical sensors that sample individual measurements (e.g. temperature or pressure) or waveforms, which are full-time series by themselves, such as vibration. The demand for raw time series waveforms is increasing given the interest to build data-based models around them. Unfortunately, most of this data, which is becoming widely available, comes without labels. Labels are an important component to build these data-based models and describe the conditions to be identiﬁed by such models. However, these labels are a scarce resource. This is especially true for machine fault detection issues where collecting large amounts of data is easier than obtaining the corresponding labels, especially when the faults evolve naturally over time (Zhang et al., 2020). Furthermore, producing high-quality labeled data is expensive, time-consuming, and a lot of times inaccurate given the uncertainty surrounding the labeling process and the annotators."
10.36001_phmconf.2015.v7i1.2656,"data available, database, data",61,,0,"By sorting different types of prognostic data into an isolated database, the files are immediately available for model development purposes. This database is standalone from the coupling algorithm and model development to allow for each to run simultaneously and independently from one another. The utilization of the extracted data as input to prognostic models is discussed below."
10.36001_phmconf.2015.v7i1.2710,"data available, database, data",162,,0,"When a custom analytic reaches a state that is mature enough for the production rollout phase. The algorithms as created by the PHM and system domain experts are translated into production software modules. The engineering and software teams work together to define a set of verification tests that evaluate all relevant logical paths within the software. These tests ensure that the production implementation matches exactly the approach that was validated during engineering sandbox development. The input and output data streams are established and any relevant contextual information is integrated as meta-data that is cataloged in the production database and made available to down-stream processes that consume the custom analytic output. Finally, the module output is integrated into the downstream ASHM processes that will serve this enhanced system condition information to the users. This includes configuration of custom data visualizations, data plotting and trending, and definition of parameter alerts including warning and alarms."
10.36001_phmconf.2015.v7i1.2646,"data available, dataset provided, data",131,,0,"The selection of the high-level prognostics algorithm group is driven by the available engineering resources. That is, when run-to-failure data or knowledge of system’s degradation equation is available, data-driven or model-based approaches are selected respectively. However, when both engineering resources are available, the selection of the high-level group incurs a trade-off decision between the availability of statistically signiﬁcant run-to-failure data and complexity of the degradation equation. It may be the case that the degradation equation is too complex to model the system behavior accurately. Accordingly, data-driven techniques can be selected, provided that statistically signiﬁcant run-to-failure data is available. Otherwise, hybrid prognostics techniques can be selected if the complexity is manageable and there is enough run-to-failure data."
10.36001_phmconf.2015.v7i1.2676,"data available, dataset provided, data",263,,0,"Security forces need to model risk patterns associated with criminal activity to study cause-effect relationships and predict new crimes. In this regard, criminal risk models are important to obtain relevant information for better resource allocation and prevention of future crime activity. This paper proposes a method to model and predict future criminal activity based on spatial probabilistic risk functions and a characterization of their temporal evolution as new data become available. This method uses geo-referenced information of public services (e.g., shopping centers, banks) and criminal incidents to approximate the prior risk function as a Gaussian Mixture Model (GMM). Temporal evolution of crime activity is characterized using an algorithm that is based on Sequential Monte Carlo Methods and Importance Sampling. This algorithm incorporates information from new measurements, in a recursive manner, to approximate the posterior spatial probabilistic risk function by updating particle positions in the map. Finally, we propose a novel prediction scheme for criminal activity that uses Gaussian ﬁelds centered on hypothetical future criminal events, which are sampled from a GMM that characterizes the spatial distribution associated with recent crime activity. The optimum number of centroids for each Gaussian kernel is evaluated using Silhouette algorithm. The time index related to each hypothetical future crime event is probabilistically characterized using an exponential distribution. Results using real data show that the majority of future events occur within risk modeled zones, information which can be used for resource allocation and improvement of intervention plans."
10.36001_phmconf.2019.v11i1.876,"data available, dataset provided, data",144,,0,"The layers of capability of Prognostics Health Monitoring (PHM) in ascending order are: anomaly detection, diagnostics, and prognostics. This manuscript is chieﬂy concerned with the ﬁrst capbility - anomaly detection. An early successful demonstration of an autoencoder neural network for vibration data provided the ﬁrst layer of Prognostics Health Monitoring (PHM) capability – anomaly detection (Japkowicz, Myers, Gluck, et al., 1995). Since then, there has been a revolution in training deep neural networks, which facilitated training of deeper, more expressive neural network models and demonstrated its performance on many important machine learning tasks (LeCun, Bengio, & Hinton, 2015). The signiﬁcant advantage of the deep learning approach in PHM is that it can be trained on abundantly available normal data"
10.36001_phmconf.2015.v7i1.2713,"data https, data",98,,0,"The two methods used to calculate ˆτ (t) require i) ofﬂine measurements and ii) online measurements. In our previous work (Celaya et al., 2011, 2012) EIS measurements (ofﬂine) was seen as the ground truth when estimating capacitor parameters. The opposing method (online) comes from calculation done to the transient voltage data collected. Fig. 6 is a comparison of the online to ofﬂine values of tau. Although these values have a comparable trend, they exhibit a distinct difference in magnitude."
10.36001_phmconf.2015.v7i1.2713,"data https, data",202,,0,"With regards to the experimental setup, the transient voltage data is collected by NI hardware with supporting LabVIEW software whereas the EIS data is collected by different hardware (SP-150 Biologic). A major effect difference in hardware may have is the calibration standards. By using ofﬂine EIS measurements we also change the circuit under which the capacitor is being observed, whereas, if the transient data is measurement online, the capacitor circuit will see the effects of the connected hardware. It would be very difﬁcult to determine the effect that the measurement hardware has on a circuit. When determining the capacitor characteristics from the EIS measurements we apply a circuit model to the data and extract a value for capacitance and ESR. We currently use a simpliﬁed lumped parameter model (C + ESR) when analyzing EIS data. One option to explain is that our model of the capacitor is no long accurate to the capacitors under test. As capacitors are left unused or while in use, their physical/chemical composition may change hence a simple C+ESR lumped parameter model may be obsolete in the later stages of aging."
10.36001_phmconf.2016.v8i1.2505,"data https, data",280,,0,"PHM applications are essential for the reliability of PV systems by facilitating condition based maintenance and minimization of cascading failures. This paper provided an overview of the different types of failure modes in the DC side of PV system. Next, it summarized the PV fault detection, diagnostics and prognostics approaches. The presented methods presented the different approaches documented in literature to address the faults in the DC side of PV systems. However, depending on the monitoring infrastructure, availability of physical models, and measured data, some solution approaches may perform better than the others due to the difference in the problem formulation. Through the integration of fault detection, diagnostics, and prognostics, future PV systems will possess the ability to sustain the power generation while increasing the reliability and resiliency of the system itself. A review of the PHM applications for PV systems paved the way to emphasize the key research gaps and challenges in the current practice as well as the available opportunities. Future studies are invited to fill the identified gaps by: (1) determining the parameters, location, resolution and precision of required sensors in the PV systems; (2) developing and testing new data-driven models; (3) generating new prognostics models for the different parts and materials of PV systems; (4) verifying and validating the developed models using experimental data; (5) designing online frameworks and algorithms to implement the PHM models; and (6) assisting the decisionmakers in their investigation of the ROI for PHM activities."
10.36001_phmconf.2018.v10i1.490,"data https, data",5,,0,PHM 2016 Data Challenge.
10.36001_phmconf.2018.v10i1.502,"data https, data",33,,0,"Giffin, A., & Caticha, A. (2007). Updating probabilities with data and moments. In AIP Conference Proceedings 74–84). (Vol. https://doi.org/10.1063/1.2821302"
10.36001_phmconf.2018.v10i1.502,"data https, data",90,,0,"Roychoudhury, I., Spirkovska, L., Daigle, M. J., Balaban, E., Sankararaman, S., Kulkarni, C. S., … Goebel, K. (2016). Predicting Real-Time Safety of the National Airspace System. AIAA Infotech @ Aerospace, (January), 1–13. https://doi.org/10.2514/6.2016-2131 Sankararaman, S., & Mahadevan, S. (2011). Likelihoodbased representation of epistemic uncertainty due to sparse point data and/or interval data. Reliability Engineering Safety. https://doi.org/10.1016/j.ress.2011.02.003"
10.36001_phmconf.2018.v10i1.503,"data https, data",16,,0,Eurocontrol. (2018). (April 30) Base of Aircraft Data. from
10.36001_phmconf.2019.v11i1.783,"data https, data",104,,0,"Situation-based PMSs are based on deﬁning speciﬁc vehicle states, to which the control parameters can be optimized (Gong et al., 2008). Vehicle states can be deﬁned heuristically or based on multiple characteristic variables from driving data history. For online application, vehicle states are recognized in real-time and the optimized solutions can be assigned accordingly. Situation-based PMSs are proved to have low computational requirements and being capable of yielding near-optimal solutions for the power management problem. An illustration of the working principle of situation-based PMS is given in Fig. 3. In this contribution,"
10.36001_phmconf.2019.v11i1.842,"data https, data",15,,0,DataONE. 2017. “Data Life Cycle.” Best Practices 1. 2017 December
10.36001_phmconf.2020.v12i1.1205,"data https, data",111,,0,"This is true also in the case of fault detection on wind turbines. A standard approach is therefore to train fault detection algorithms that do not rely on the exact nature of faults, but rather extract information from turbines under normal non-faulty conditions, also known as normal state modeling (Stetco et al., 2019; Schlechtingen & Santos, 2014). After training with normal data only, the algorithm is used for online detection, in which deviations from normality manifest in high prediction errors. In order to detect incipient faults such deviations from normal behavior should be detected as early as possible."
10.36001_phmconf.2020.v12i1.1288,"data https, data",331,,0,"of certain threshold is largely depended on experience. Other than empirical determination, such threshold can also be for both determined continuously monitored discretely monitored degradation. (Javed, Gouriveau, & Zerhouni, 2013) proposed a Fuzzy Clustering method determined by Subtractive-Maximum Entropy to dynamically determine the FT. (R.-y. Jiang, 2010) developed an online and offline alarm threshold determination methodology the inspection scheme in condition-based maintenance (CBM). The alarm threshold is optimized by trade off cost and machine degradation whose information is obtained and updated during last inspection. As for continuous monitoring, rather than a predetermined FT, uncertain FT or probabilistic FT are usually determined with adequate historical or peer information (Wang & Coit, 2007). (Emami-Naeini, Akhter, & Rock, 1988) use nonlinear inequalities to build a sensor threshold selector for fault detection and classification (FDC). The selector is based on errors from a statistical FDC model, signal noise properties and filters properties. (Chehade, Bonk, & Liu, 2017) develops a convex quadratic formulation which consider both historical and real time degradation data to online estimate the FT for each machine. An adaptive failure threshold method is proposed by (Hua, Zhang, Xu, Zhang, & Xu, 2013) to estimate reliability when there is degradation performance on a machine. The threshold is determined by estimating conditional probability density function using a sliding window based dynamic kernel estimation method. (Javed et al., 2013) also proposed an Extreme Learning Machine based method for dynamic threshold decision in continuous state prediction task. (L. Jiang, Feng, & Coit, 2011) developed a failure threshold determination model for dependent failure process. The method considers the dependency of coexisting soft and hard failure for both diagnosis and prognostics."
10.36001_phmconf.2017.v9i1.2449,"data https, data available, data",73,,1,"A major factor in enabling this work has been the ability to semi-automate cleaning the ~5000 raw maintenance work orders with our DEST tool. In order to promote the developments of other data cleaning tools, we are making our raw and cleaned data sets for Excavator Set A available through Library Prognostics https://prognosticsdl.ecm.uwa.edu.au/ . Each data set (raw and cleaned) has an associated metadata file describing the"
10.36001_phmconf.2019.v11i1.838,"data https, data available, data",344,,0,"In industry, data from different normal operating conditions can be found, otherwise there is a lack of past occurred degradation data or it is often unavailable (e.g., new machine). Also generating degradation data in the laboratory is very costly and the degradation dynamics do not behave as the reality due to the change in environmental and operating conditions (e.g., wind speed variability for wind turbine). Hence, applying a prognostic approach is challenging due to several reason. i) it is difﬁcult to select the most sensitive features in order to detect and predict the degradation using only normal conditions data, ii) it is arduous to construct the HI online; most of the works in the literature select the best features as HI using evaluation criteria in the ofﬂine phase (using run-to-failure degradation data). In order to address these challenges, a prognostic approach dealing with the lack of degradation data is proposed in this work. First of all, a general library is constructed in order to include as much as possible features sensitive to different degradation dynamics. Secondly, anomaly detection method is used since only normal conditions data is available. Anomaly detection or novelty detection is the identiﬁcation of new patterns which differ signiﬁcantly from the patterns generated during normal operation conditions. Hence, One Class Support Vector Machines is applied in order to detect the degradation and trigger the RUL estimation without using degradation data for training. Thirdly, features are ranked online according to meaningful evaluation criteria (monotonicity and trendability), where the feature with high score is selected as HI and used for the degradation predic This paper is organized as follows. Section 2 explains the proposed approach for degradation detection and RUL estimation. Section 3 describes the run-to-failure high speed shaft bearings data and presents the results of the proposed approaches. Conclusion and perspectives are given in Section 4."
10.36001_phmconf.2017.v9i1.2426,"data https, data repository, data",5,,0,(2008). data
10.36001_phmconf.2015.v7i1.2764,"data https, retrieve, data",31,,0,"Defect Avasarala, V., & Celaya, classiﬁcation of highly noisy nde data using classiﬁer ensembles (Vol. 6167). Retrieved from http://dx.doi.org/10.1117/12.659704 doi: 10.1117/12.659704"
10.36001_phmconf.2018.v10i1.503,"data https, retrieve, data",22,,0,FAA. (2018). (April 30) FAA Accident and Inciden Data. Retrieved from https://www.faa.gov/data_research/accident_incide nt/
10.36001_phmconf.2019.v11i1.806,"data https, retrieve, data",53,,0,(1996). A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in In Proceedings of large spatial databases with noise. the second international conference on knowledge discovery and data mining (pp. 226–231). AAAI Press. Retrieved from http://dl.acm.org/citation .cfm?id=3001460.3001507
10.36001_phmconf.2015.v7i1.2713,data repos,19,,0,"2 NASA Ames Research Center (SGT Inc.), Moffett Field, CA, USA chetan.s.kulkarni@nasa.gov"
10.36001_phmconf.2015.v7i1.2713,data repos,66,,0,"The authors would like to gratefully acknowledge the contributions of researchers at NASA Ames Research Center: Kai Goebel and Scott Poll. The funding for this research is pro vided by NASA ARMD System-wide Safety & Assurance Technology (SSAT) project. The work was done by Jos´e R. Celaya while working at the Prognostics Center of Excellence, NASA Ames Research Center."
10.36001_phmconf.2015.v7i1.2713,data repos,194,,0,"Chetan S. Kulkarni received the B.E. (Bachelor of Engineering) degree in Electronics and Electrical Engineering from University of Pune, India in 2002 and the M.S. and Ph.D. degrees in Electrical Engineering from Vanderbilt University, Nashville, TN, in 2009 and 2013, respectively. He was a Senior Project Engineer with Honeywell Automation India Limited (HAIL) from 2003 till April 2006. From May 2006 to August 2007 he was a Research Fellow at the Indian Institute of Technology (IIT) Bombay with the Department of Electrical Engineering. From Aug 2007 to Dec 2012, he was a Graduate Research Assistant with the Institute for Software Integrated Systems and Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN. Since Jan 2013 he has been a Staff Researcher with SGT Inc. at the Prognostics Center of Excellence, NASA Ames Research Center. His current research interests include physics-based modeling, model-based diagnosis and prognosis. Dr. Kulkarni is a member of the Prognostics and Health Management (PHM) Society, AIAA and Senior member IEEE."
10.36001_phmconf.2016.v8i1.2583,data repos,16,,0,"1 NASA Ames Research Center, Moffett Field, CA 94035, USA matthew.j.daigle@nasa.gov"
10.36001_phmconf.2016.v8i1.2583,data repos,23,,0,"2,3 SGT, Inc., NASA Ames Research Center, Moffett Field, CA 94035, USA shankar.sankararaman@nasa.gov indranil.roychoudhury@nasa.gov"
10.36001_phmconf.2016.v8i1.2583,data repos,64,,0,"Roychoudhury, I., Spirkovska, L., Daigle, M., Balaban, E., Sankararaman, S., Kulkarni, C., . . . Goebel, K. (2015, November). Real-time monitoring and prediction of airspace safety (Tech. Rep. No. NASA/TM2015-218928). Moffett Field, CA, USA: NASA Ames Research Center."
10.36001_phmconf.2016.v8i1.2583,data repos,64,,0,"he has been with SGT, Inc., at NASA Ames Research Center as a Computer Scientist. His research interests include hybrid systems modeling, model-based diagnostics and prognostics, distributed diagnostics and prognostics, and Bayesian diagnostics of complex physical systems. Dr. Roychoudhury is a Senior Member of the IEEE and a member of the Prognostics and Health Management Society."
10.36001_phmconf.2016.v8i1.2583,data repos,189,,0,"Shankar Sankararaman received his Bachelors degree in Civil Engineering from the Indian Institute of Technology, Madras in India in 2007 and later, obtained his Ph.D. in Civil Engineering from Vanderbilt University, Nashville, Tennessee, U.S.A. in 2012. His research focuses on the various aspects of uncertainty quantiﬁcation, integration, and management in different types of aerospace, mechanical, and civil engineering systems. His research interests include probabilistic methods, risk and reliability analysis, Bayesian networks, system health monitoring, diagnosis and prognosis, decision-making under uncertainty, treatment of epistemic uncertainty and multidisciplinary analysis. He is a member of the Non-Deterministic Approaches (NDA) technical committee at the American Institute of Aeronautics, the Probabilistic Methods Technical Committee (PMC) at the American Society of Civil Engineers (ASCE), and the Prognostics and Health Management (PHM) Society. Currently, Shankar is a researcher at NASA Ames Research Center, Moffett Field, CA, where he develops algorithms for system health monitoring, prognostics, decision-making, and uncertainty management."
10.36001_phmconf.2016.v8i1.2583,data repos,193,,0,"Matthew Daigle received the B.S. degree in Computer Science and Computer and Systems Engineering from Rensselaer Polytechnic Institute, Troy, NY, in 2004, and the M.S. and Ph.D. degrees in Computer Science from Vanderbilt University, Nashville, TN, in 2006 and 2008, respectively. From September 2004 to May 2008, he was a Graduate Research Assistant with the Institute for Software Integrated Systems and Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN. During the summers of 2006 and 2007, he was an intern with Mission Critical Technologies, Inc., at NASA Ames Research Center. From June 2008 to December 2011, he was an Associate Scientist with the University of California, Santa Cruz, at NASA Ames Research Center. Since January 2012, he has been with NASA Ames Research Center as a Research Computer Scientist. His current research interests include physicsbased modeling, model-based diagnosis and prognosis, simulation, and hybrid systems. Dr. Daigle is a member of the Prognostics and Health Management Society and the IEEE."
10.36001_phmconf.2017.v9i1.2306,data repos,17,,0,"3 NASA Ames Research Center, Moffett Field, CA, 94035, USA dogan.timucin@nasa.gov"
10.36001_phmconf.2017.v9i1.2306,data repos,154,,0,"Michael Khasin Dr Michael Khasin is a Senior Researcher in SGT Inc., working at NASA Ames Research Center. He holds in Physics (Honors Program), 2001, B.Sc. M.Sc. in physics, 2003, and PhD in chemical physics, 2008, from the Hebrew University of Jerusalem. As a postdoctoral researcher at Michigan State University, Massachusetts Institute of Technology, and University of Michi gan, Michael worked on the theory of non-equilibrium systems and their control and transport in disordered system. His research interests include applied physics, nonlinear and stochastic dynamics, large ﬂuctuations and their control in non-equilibrium systems and efﬁcient simulation of complex dynamics. He has been an invited speaker to many international and national conferences and workshops. Currently his research is focused on the theory of heat transfer, ﬂuid dynamics, and ﬂuid structure interaction."
10.36001_phmconf.2017.v9i1.2307,data repos,29,,0,"2,3,4,5,6 NASA Ames Research Center, Moffett Field, CA, 94035, USA stefan.r.schuet@nasa.gov j.brenton@nasa.gov dogan.timucin@nasa.gov david.smith@nasa.gov john.t.kaneshige@nasa.gov"
10.36001_phmconf.2017.v9i1.2307,data repos,136,,0,"Stefan Schuet received the B.S. and M.S. degrees in electrical engineering from Santa Clara University, Santa Clara, CA, USA, in 2001 and 2004, respectively. He has been a Research and Development Engineer with the National Aeronautics and Space Administration (NASA) Ames Research Center, Mountain View, CA, USA, since 2001. He is currently with the Intelligent Systems Division, NASA, with a focus on combining physics-based modeling methods with optimization and probabilistic inference to develop advanced autonomous systems. Mr. Schuet was a recipient of several distinguished awards, including the NASA Ames Honor for Excellence Award in the Engineer Category, the NASA Government Invention of the Year Award, and the Research and Development Magazine Top 100 Award."
10.36001_phmconf.2017.v9i1.2437,data repos,26,,0,"1,3,4 SGT Inc., NASA Ames Research Center, Moffett Field,CA, 94035, USA george.e.gorospe@nasa.gov shankar.sankararaman@nasa.gov chetan.s.kulkarni@nasa.gov"
10.36001_phmconf.2017.v9i1.2437,data repos,78,,0,"Eley Ng obtained her B.S. in Mechanical Engineering from The University of Texas at Austin. As an undergraduate student, she was involved with research in the mechanics and materials of stretchable electronics and soft robotics. She joined the Prognostics and Diagnostics Group at NASA Ames as a summer intern in 2017. Currently, she is working towards a graduate degree in Mechanical Engineering at Stanford University as a NSF Graduate Research Fellow."
10.36001_phmconf.2017.v9i1.2437,data repos,189,,0,"Shankar Sankararaman received his Bachelors degree in Civil Engineering from the Indian Institute of Technology, Madras in India in 2007 and later, obtained his Ph.D. in Civil Engineering from Vanderbilt University, Nashville, Tennessee, U.S.A. in 2012. His research focuses on the various aspects of uncertainty quantiﬁcation, integration, and management in different types of aerospace, mechanical, and civil engineering systems. His research interests include probabilistic methods, risk and reliability analysis, Bayesian networks, system health monitoring, diagnosis and prognosis, decision-making under uncertainty, treatment of epistemic uncertainty and multidisciplinary analysis. He is a member of the Non-Deterministic Approaches (NDA) technical committee at the American Institute of Aeronautics, the Probabilistic Methods Technical Committee (PMC) at the American Society of Civil Engineers (ASCE), and the Prognostics and Health Management (PHM) Society. Currently, Shankar is a researcher at NASA Ames Research Center, Moffett Field, CA, where he develops algorithms for system health monitoring, prognostics, decision-making, and uncertainty management."
10.36001_phmconf.2017.v9i1.2437,data repos,194,,0,"Chetan S. Kulkarni received the B.E. (Bachelor of Engineering) degree in Electronics and Electrical Engineering from University of Pune, India in 2002 and the M.S. and Ph.D. degrees in Electrical Engineering from Vanderbilt University, Nashville, TN, in 2009 and 2013, respectively. He was a Senior Project Engineer with Honeywell Automation India Limited (HAIL) from 2003 till April 2006. From May 2006 to August 2007 he was a Research Fellow at the Indian Institute of Technology (IIT) Bombay with the Department of Electrical Engineering. From Aug 2007 to Dec 2012, he was a Graduate Research Assistant with the Institute for Software Integrated Systems and Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN. Since Jan 2013 he has been a Staff Researcher with SGT Inc. at the Prog nostics Center of Excellence, NASA Ames Research Center. His current research interests include physics-based modeling, model-based diagnosis and prognosis. Dr. Kulkarni is a member of the Prognostics and Health Management (PHM) Society, AIAA and the IEEE."
10.36001_phmconf.2018.v10i1.542,data repos,13,,0,"Figure 1. NASA Ames Sustainability (Poolla et al., 2015)"
10.36001_phmconf.2018.v10i1.542,data repos,16,,0,"6NASA Ames Research Center, Moffett Field, CA, 94035, USA rodney.martin@nasa.gov"
10.36001_phmconf.2018.v10i1.544,data repos,19,,0,"2NASA Ames Research Center, Moffett Field, California, 94035, USA Yongming.liu@asu.edu kai.goebel@nasa.gov"
10.36001_phmconf.2019.v11i1.846,data repos,20,,0,"1 SGT, Inc., NASA Ames Research Center, Moffett Field, CA 94035, USA gina.sierra@nasa.gov"
10.36001_phmconf.2019.v11i1.846,data repos,80,,0,"the University of California, Santa Cruz, at NASA Ames Research Center. He received the B.S. degree in computer science and computer and systems engineering from Rensselaer Polytechnic Institute, Troy, NY, in 2004, and the M.S. and Ph.D. degrees in computer science from Vanderbilt University, Nashville, TN, in 2006 and 2008, respectively. He has published over 100 peer-reviewed papers in the area of systems health management."
10.36001_phmconf.2019.v11i1.846,data repos,118,,0,"Gina Sierra received the B.Sc. degree in Electronic Engineering (2008) and Master of Science in Information and Communications Technology (2013) from Universidad Distrital Francisco Jose de Caldas, Bogot´a, Colombia, and Ph.D. degree in Electrical Engineering (2018) from the University of Chile. She has worked in the areas of artiﬁcial intelligence and machine learning with application in diverse domains and topics such as electric autonomous vehicles, telecommunications networks, and control systems. Since October 2018, she has been with SGT, Inc. at NASA Ames Research Center as a Research Engineer. Her current research interests include physics-based modeling and model-based diagnosis and prognosis."
10.36001_phmconf.2019.v11i1.846,data repos,149,,0,"Kai Goebel is a Principal Scientist at Palo Alto Research Center where he investigates System Health for cyber physical systems. He is the founder and former director of the Prognostics Center of Excellence at NASA Ames Research Center (where much work reported in this article was conducted). After receiving the Ph.D. from the University of California at Berkeley in 1996, Dr. Goebel worked at General Electric’s Corporate Research Center in Niskayuna, NY from 1997 to 2006 as a senior research scientist before joining NASA. He has carried out applied research in the areas of artiﬁcial intelligence, soft computing, and information fusion and his interest lies in advancing these techniques for real time monitoring, diagnostics, and prognostics. He holds 18 patents and has co-authored more than 350 publications in the area of systems health management."
10.36001_phmconf.2019.v11i1.857,data repos,9,,0,"3 NASA Ames Research Center michael.r.lowry@nasa.gov,"
10.36001_phmconf.2019.v11i1.857,data repos,13,,0,"1 SGT, Inc., NASA Ames Research Center johann.m.schumann@nasa.gov,"
10.36001_phmconf.2019.v11i1.857,data repos,52,,0,"with a NASA DJI S1000+ octocopter at the NASA Ames research center (Figure 8). Although this case study does not exercise most of the advanced features and capabilities of AOS and DM, our successful test ﬂight shows the basic principles of on-board diagnosis and decision-making with DM."
10.36001_phmconf.2019.v11i1.857,data repos,107,,0,"Dr. Michael Lowry is the NASA chief scientist for Reliable Software Engineering. After receiving his BS/MS from MIT and PhD from Stanford, all in computer science, he joined the Kestrel Institute as PI working on program synthesis. In 1993 he joined NASA Ames as group lead then area lead, and was promoted to chief scientist in 2008. Dr. Lowry is the editor of MIT Press “Automating Software Design” and serves on the editorial board of the journal Automated Software Engineering. He has published numerous papers principally on the topics of program synthesis and software V&V."
10.36001_phmconf.2020.v12i1.1190,data repos,41,,0,"1, 2, 3 Vanderbilt University, Nashville TN, 37209 USA marcos.quinones@vanderbilt.edu timothy.s.darrah@vanderbilt.edu gautam.biswas@vanderbilt.edu 4 KBR. Inc., NASA Ames Research Center, Moffett Field, CA, 94035 chetan.s.kulkarni@nasa.gov"
10.36001_phmconf.2020.v12i1.1199,data repos,21,,0,"2 KBR, Inc., NASA Ames Research Center, Moffett Field, CA, 94035, USA chetan.s.kulkarni@nasa.gov"
10.36001_phmconf.2021.v13i1.2998,data repos,47,,0,"1,4 Department of Mechanical and Aerospace Engineering, University of Central Florida, Orlando, FL, 32816, USA renato.gn@knights.ucf.edu viana@ucf.edu 2,3 KBR LLC, NASA Ames Research Center, Moffett Field 94035 CA, USA matteo.corbetta@nasa.gov chetan.s.kulkarni@nasa.gov"
10.36001_phmconf.2015.v7i1.2596,"data repos, data",65,,1,"the methods are evaluated using ball bearing runAll training and truncated run-to-failure to-failure data for data for testing obtained from the 2012 PHM data challenge (Nectoux, Medjaher, Ramasso, Morello, & Zerhouni, 2012). The data is obtained through highly accelerated runto-failure experiments conducted at three different operating conditions shown in Table 1. Due to the highly accelerated"
10.36001_phmconf.2015.v7i1.2713,"data repos, data",99,,0,"Jason Renwick is a ﬁrst-year engineering student pursuing a BSc. Electrical and Computer Engineering at The University of the West Indies, St. Augustine Campus, Trinidad and Tobago. In 2014, he was awarded a Fall internship at Prognostics Center of Excellence, NASA Ames Research Center. Under the mentor-ship of Dr. Jos´e Celaya and Dr. Chetan Kulkarni, Jason engaged in performing aging experiments of electrolytic capacitors. He returned in Spring 2015 for a second internship period during which he performed detailed analysis on the experimental data collected in his previous placement."
10.36001_phmconf.2015.v7i1.2713,"data repos, data",117,,0,"Jos´e R. Celaya is a Senior Data Scientist at the Software Technology and Innovation Center, Schlumberger. Previously, he was the Lead Scientist and Co-lead at the Diagnostics and Prognostics Group and a founding member of the Prognostics Center of Excellence, both at the Intelligent Systems Division of NASA Ames Research Center. He received a Ph.D. degree in Decision Sciences and Engineering Systems in 2008, a M. E. degree in Operations Research and Statistics in 2008, a M. S. degree in Electrical Engineering in 2003, all from Rensselaer Polytechnic Institute, Troy New York; and a B. S. in Cybernetics Engineering in 2001 from CETYS University, M´exico."
10.36001_phmconf.2016.v8i1.2538,"data repos, data",14,,0,"set. data-repository. Repository, NASA Ames, Moffett Field, CA)"
10.36001_phmconf.2017.v9i1.2437,"data repos, data",193,,0,"Matthew Daigle Matthew Daigle received the B.S. degree in Computer Science and Computer and Systems Engineering from Rensselaer Polytechnic Institute, Troy, NY, in 2004, and the M.S. and Ph.D. degrees in Computer Science from Vanderbilt University, Nashville, TN, in 2006 and 2008, respectively. From September 2004 to May 2008, he was a Graduate Research Assistant with the Institute for Software Integrated Systems and Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN. From June 2008 to December 2011, he was an Associate Scientist with the University of California, Santa Cruz, at NASA Ames Research Center. From January 2012 to May 2017, he was with NASA Ames Research Center as a Research Computer Scientist. Since June 2017, he has been a Principal Data Scientist with NIO USA, Inc. His current research interests include physics based modeling, model-based diagnosis and prognosis, simulation, and hybrid systems. Dr. Daigle is a member of the Prognostics and Health Management Society and a Senior Member of the IEEE."
10.36001_phmconf.2018.v10i1.490,"data repos, data",23,,0,"Objective. The objective of the PHM data challenge task is to construct a machine learning model MPHM , so that the mean"
10.36001_phmconf.2018.v10i1.542,"data repos, data",173,,0,"There are reported works on using data-driven based approaches to analyze data from NASA Ames Sustainability Base, shown in Fig. 1, as a testbed for Deep Space Habitats. Poolla et al. (Poolla et al., 2015) used an artificial neural network the photovoltaic (PV) system. Basak et al. (Basak, Hosein, Mengshoel, and Martin, 2016) integrated dimensionality reduction and Bayesian network structure learning with a MATLAB adverse condition detection called ACCEPT to detect thermal discomforts of occupants. Iverson et al. (Iverson et al., 2012) used a distance-based anomaly detection method to monitor parameter values in the space operations include International Space Station flight control, satellite vehicle system health management, launch vehicle ground operations, and fleet supportability. Martin et al. (Martin, Schwabacher, & Oza, 2007) compared several different unsupervised anomaly detection algorithms on the Space Shuttle Main Engine (SSME) data."
10.36001_phmconf.2020.v12i1.1155,"data repos, data",29,,0,"It is a function deﬁned during the PHM data challenge competition (Saxena et al., 2008) in 2008 by the PHM community, that is given by"
10.36001_phmconf.2016.v8i1.2538,"data repos, data https, data repository, data",16,,1,(Visited: Oct.2007). Battery data http://ti.arc.nasa.gov/tech/dash/pcoe/prognostic(NASA Ames Prognostics Data
10.36001_phmconf.2016.v8i1.2553,"data repos, dataset",38,,1,"In this study, four battery datasets labeled #5 #6 #7 and #18 from the NASA Ames Prognostics Center of Excellence (B Saha & Goebel, 2007) were selected to demonstrate the"
10.36001_phmconf.2016.v8i1.2538,"data repos, dataset provided, data",169,,1,"We present the implementation of a particle-ﬁltering-based framework that estimates the State-of-Health (SOH) and predicts the End-of-Life (EOL) of Lithium-Ion batteries, efﬁciently incorporating variations of ambient temperature in the analysis. The proposed approach uses an empirical statespace model, in which inputs are explicitly deﬁned as the average temperature of operation and the output of an external module that detects self-recharge phenomena, on the other hand the output is a function that relates the current SOH and temperature with the Usable Capacity in that cycle. In addition, this approach allows to deal with data losses and outliers. In order to correct erroneous initial conditions in state estimates, an Outer Feedback Correction Loop is implemented. Finally, this framework is validated using degradation data from four sources: experimental degradation data from two Li-Ion 18650 cells, accelerated degradation data openly provided by NASA Ames Research Center, and artiﬁcially generated degradation data at different ambient temperatures."
10.36001_phmconf.2017.v9i1.2456,"data repos, dataset, data repository, data",91,,0,"Lee, J., Qiu, H., Yu, G., & Lin, J. (2009). Rexnord Technical Services (2007).'Bearing Data Set', IMS, University of Cincinnati. NASA Ames Prognostics Data Repository. Li, H., & Zheng, H. (2008). Bearing fault detection using envelope spectrum based on EMD and TKEO. Fuzzy Systems and Knowledge Discovery, 2008. FSKD'08. Fifth International Conference on, vol. 3, pp. 142-146"
10.36001_phmconf.2017.v9i1.2456,"data repos, dataset, data repository, data",146,,1,"The IMS bearing data set in the NASA Ames data repository can be downloaded from ‘Bearing Data Set’, IMS, University of Cincinnati NASA Ames Prognostics Data Repository (Lee, J et al. 2009.). They were used for the study in reference (Qiu et al. 2006). Four double row bearings (16 rollers) are installed on a shaft as shown in Figure 4, and the rotating speed are radial load are, respectively 200 rpm and 26.7kN. four data sets are made from the repeated experiment under this condition. Vibration data was collected every 20 minute with sampling frequency 20kHz and the data length was 20,480 points. The test was carried out until a significant amount of meta debris was found on the magnetic plug of the test bearing."
10.36001_phmconf.2020.v12i1.1199,"data repos, dataset, data repository, data",36,,0,"Kulkarni, C., Hogge, E., Quach, C., & Goebel, K. (2015). ”hirf battery data set”. In Nasa ames prognostics data repository. NASA."
10.36001_phmconf.2021.v13i1.2998,"data repos, dataset, data repository, data",30,,1,"Bole, B., Kulkarni, C., & Daigle, M. (2014b). Randomized battery usage data set. NASA AMES prognostics data repository, 70."
10.36001_phmconf.2021.v13i1.3009,"data repos, dataset, data repository, data",27,,1,"Saxena, A., & Goebel, K. (2008). Turbofan engine degradation simulation data set. NASA Ames Prognostics Data Repository, 878–887."
10.36001_phmconf.2017.v9i1.2426,"data repos, dataset, dataset provided",258,,1,"(ii) battery regains portion of its capacity after some rest time, known as recovery effect [Martin, 1999]. The main indicator of the battery’s State of Health (SOH) and RUL is its actual capacity value, which decreases over the working time of the battery. Due to this non-linearities in battery behavior, predicting the battery capacity is a challenging task. Here we consider a battery dataset provided by the Prognostics Center of Excellence at NASA Ames Research Center [Saha & Goebel, 2007]. The dataset consists of a set of 34 Li-ion batteries that were run through three different operational proﬁles (charge, discharge and impedance) at different ambient temperatures (4, 24, 44 deg. C). The charging of the batteries was carried out in a constant current mode at 1.5A until the battery voltage reached 4.2V. It then continued in a constant voltage mode until the charge current dropped to 20mA. The discharge of batteries was at different currents and modes (including ﬁxed discharge loads of 1A, 2A and 4A, as well as 0.05Hz square wave loading proﬁle of 4A amplitude and 50% duty cycle) until the battery voltage fell to a predeﬁned value. In some cases the experiments were continued until batteries reached a 20% or 30% fade in their rated capacity, while for other cases it is not the stopping criteria.10"
10.36001_phmconf.2021.v13i1.3009,"data repos, dataset, used dataset",62,,1,"In this section, we use the C-MAPSS dataset to show the application of our proposed maintenance decision-making algorithm. This dataset includes time-series sensor measurements of jet engine under different operational conditions and fault modes. The dataset is generated by the Prognostic Center of Excellence at NASA Ames using the C-MAPSS simulator (Saxena & Goebel, 2008)."
10.36001_phmconf.2017.v9i1.2426,"data repository, data",8,,1,1Available from www.mfpt.org/faultdata/faultdata.htm. 2Available from prognostic-data-repository.
10.36001_phmconf.2017.v9i1.2426,"data repository, data",9,,1,Ames Prognostics Data Repository. https://ti.arc.nasa.gov/tech/dash/ pcoe/prognostic-data-repository/
10.36001_phmconf.2019.v11i1.842,"data repository, data",91,,0,"To demonstrate the validity of their proposition, they extended the CSMD model to account for derived data and to record the data analysis process enough for each of their use case. They extended the CSMD model to a software agnostic one that contains resultant data product to include a description of transformation provenance which is not covered in the existing model. They carried a pilot implementation with scientists, with annotations employing the ICAT data catalogue scheme. They identified five fundamental factors for capturing provenance data."
10.36001_phmconf.2021.v13i1.2998,"data repository, data",31,,1,Figure 1. Example of constant (top panel) and randomized (bottom panel) loading discharge cycles from the NASA Prognostics Data Repository - Randomized Battery Usage Data-set.
10.36001_phmconf.2021.v13i1.2998,"data repository, data",214,,1,"In this work, we propose a hybrid model for Li-ion battery discharge and aging prediction that leverages fleet-wide data to predict future capacity drops. The model is built upon an hybrid approach merging physics-based and empirical equations, as well as neural network models in a recurrent neural network cell. The hybrid physics-informed neural network can predict voltage discharge cycles given the loading profile, and estimate the used capacity of the battery under randomloading conditions by tracking aging parameters connected to the residual capacity of the battery. By merging information on the battery aging parameters with existing fleet-wide aging data, the model can predict the future residual capacity of the battery that is being monitored, and therefore enable predictions of voltage discharge curves far ahead in the battery life cycle. We validated the approach using the NASA Prognostics Data Repository Battery data-set, which contains experimental data on Li-ion batteries discharged at random loading conditions in a controlled environment. The approach also allows the identification of discrepancies between the battery aging trend and the trend observed at the fleet level, so that batteries behaving differently from the rest of the fleet can be subject to closer monitoring and further testing to refine predictions."
10.36001_phmconf.2017.v9i1.2476,"data, database, provide implementation",275,,0,"Availability and performance of hydrogenerators are key features that have driven electrical utilities to implement monitoring and diagnostic methods in order to evolve to condition based maintenance (CBM). Ten years ago, HydroQuebec has implemented a home-built web-based application, called MIDA, to cover most of its power plants. MIDA centralizes diagnostic data from several tools, aggregates all diagnostic results and calculates a health index for each hydrogenerators. Data from MIDA used in conjunction with PHM techniques can feed a prognostic model that will provide useful equipment information and lead to the implementation of predictive maintenance. The prognostic framework used for hydrogenerators is based on a failure mechanism and symptom analysis (FMSA) approach. For stator, a major component of hydrogenerators, more than 100 failure mechanisms have been consigned in the form of causal trees or graphs. A large number of these failure mechanisms involve the presence of partial discharges (PD) before failure occurs. At Hydro-Quebec, PD measurements on hydrogenerators have been carried out over the past 30 years and a significant PD database is integrated in MIDA. The analysis of this huge amount of data is of paramount importance to understand the behavior and evolution of the discharge activity in order to build a robust prognostic approach using physics based as well as data driven models. To that end, this paper presents case studies that shed some light on key features related to the evolution of PD activity in hydrogenerators. The paper discusses how to use this data in the prognostic model to"
10.36001_phmconf.2015.v7i1.2646,"data, provide implementation",174,,0,"If the data reﬂects a simple linear monotonic degradation (0.8 ≤ m ≤ 1) Linear Regression is an appropriate solution for RUL estimation (e.g. see (Rudd, Catterson, McArthur, & Johnstone, 2011)). However, if the data is not clearly monotonic (m < 0.8), more sophisticated techniques are needed. If the goal is to perform a short-term prediction (e.g., 1 step ahead prediction), linear time-series models provide an easy to implement and accurate prognostics implementation (Ling, 2013): ARMA models are better suited for weakly stationary processes, while ARIMA is a generalization of the ARMA model able to deal with non-stationary processes. A weakly stationary process must satisfy two conditions: mean and variance must be constant; and the autocovariance between Xt and Xt+τ must only depend on the lag τ . (Ling, 2013) introduced a Bayesian updating method for ARIMA models for uncertainty management."
10.36001_phmconf.2016.v8i1.2578,"data, provide implementation",337,,0,"design of creep-fatigue life about various structures under operating conditions. For this work, the estimation of parameters is required using finite data set in structural analysis or the health management of structures. In the early stage of structural design, material properties are obtained from various experiments. It is affect to the validity of structural analysis. Meta-model model which can replace the finite element analysis (FEA) is employed to saving computational cost but it is that there are also inherent uncertainties due to experiment error and the lack of data. For quantifying the uncertainties of material parameters or model coefficients, stochastic and statistical manners are employed. the health management, degradation parameters of physical model in the deteriorating structures are estimated using monitoring data over times for the prognostics of creep-fatigue life. Bayesian framework for fatigue model determination, updating and averaging using trans-dimensional Markov Chain Monte Carlo (MCMC) simulation (C. Andrieu et al., 2003) is presented. Uncertainties are introduced by model choice, mechanism modeling, model parameter, and response measures are systematically included. Additional response measures are used to update model probabilities and parameter distributions associated with each of the models simultaneously via one trans-dimensional MCMC simulation in the general state space. The results of Bayes factors serve as a reference for model comparisons and determinations (X. Guan et al., 2010). This framework is also outlined for the parameter estimation that arises during the uncertainty quantification in the numerical simulation as well as in the prognosis of the structural performance. The form of posterior in parameters are estimated distribution conditional on the provided data. During the numerical implementation, MCMC method is employed, which is a modern computational technique for the efficient and straightforward estimation of parameters (Choi et al., 2010). X. Guan et al. compared with two probabilistic prognosis updating schemes."
10.36001_phmconf.2016.v8i1.2578,"data, provide implementation",341,,0,"For this work, the estimation of parameters is required using finite data set in structural analysis or the health management of structures. In the early stage of structural design, material properties are obtained from various experiments. It is affect to the validity of structural analysis. Meta-model model which can replace the finite element analysis (FEA) is employed to saving computational cost but it is that there are also inherent uncertainties due to experiment error and the lack of data. For quantifying the uncertainties of material parameters or model coefficients, stochastic and statistical manners are employed. the health management, degradation parameters of physical model in the deteriorating structures are estimated using monitoring data over times for the prognostics of creep-fatigue life. Bayesian framework for fatigue model determination, updating and averaging using trans-dimensional Markov Chain Monte Carlo (MCMC) simulation (C. Andrieu et al., 2003) is presented. Uncertainties are introduced by model choice, mechanism modeling, model parameter, and response measures are systematically included. Additional response measures are used to update model probabilities and parameter distributions associated with each of the models simultaneously via one trans-dimensional MCMC simulation in the general state space. The results of Bayes factors serve as a reference for model comparisons and determinations (X. Guan et al., 2010). This framework is also outlined for the parameter estimation that arises during the uncertainty quantification in the numerical simulation as well as in the prognosis of the structural performance. The form of posterior in parameters are estimated distribution conditional on the provided data. During the numerical implementation, MCMC method is employed, which is a modern computational technique for the efficient and straightforward estimation of parameters (Choi et al., 2010). X. Guan et al. compared with two probabilistic prognosis updating schemes. One is based on the classical Bayesian approach and the other is based on newly"
10.36001_phmconf.2015.v7i1.2564,database,36,,0,(2009). Mutation Testing for Java Database Applications. In Proc. of the 2009 Int. Conf. on Software Testing Veriﬁcation and Validation (pp. 396–405). doi: 10.1109/ICST.2009.43
10.36001_phmconf.2015.v7i1.2564,database,41,,0,"Tuya, J., Su´arez-Cabal, M. J., & la Riva, C. d. (2007, April). Mutating Database Queries. Inf. Softw. Technol., 49(4), 398–417. doi: 10.1016/j.infsof.2006.06.009"
10.36001_phmconf.2015.v7i1.2656,database,5,,0,7. PROGNOSTIC INFORMATION DATABASE
10.36001_phmconf.2015.v7i1.2656,database,8,,0,Figure 6. Prognostic information database example structure
10.36001_phmconf.2015.v7i1.2676,database,29,,0,"The proposed methodology is applied to an interesting case of study, which considers actual records of criminal activity over a populated urban area. The database includes:"
10.36001_phmconf.2015.v7i1.2710,database,169,,0,"The ASHM application checks for out of range “alert” conditions on selected incoming report parameters, looking for warning or alarm conditions that are higher or lower than expected under normal operating conditions. Each “alertable” parameter has its own set of thresholds defined in the database for low and high warning and alarms. There are also mechanisms in place to define two additional criteria which are when the thresholds are to be ignored, say when some (the same or another) parameter’s value meets a certain conditional relationship with a fixed value, e.g. <= some value, = some value, or >= some value. The parameter alert rules store parameter out of range conditions back to the database, where to display those anomalous conditions to the end user in the web application. In addition to simple thresholds, the ASHM application can invoke a custom analytic that performs calculations on the"
10.36001_phmconf.2015.v7i1.2724,database,3,,0,Physical state Database
10.36001_phmconf.2015.v7i1.2724,database,7,,0,Figure 4. Example of database sub-mechanisms
10.36001_phmconf.2015.v7i1.2724,database,50,,0,"Once the Sub-failure Mechanisms Analysis database has been finalized, the algorithm scans all sub-mechanisms and identifies inputs and outputs of each physical state in a matrix. For example, using the fictive sub-mechanisms in Figure 4, the resulting input/output matrix is presented in the Table 2."
10.36001_phmconf.2015.v7i1.2724,database,86,,0,"All sub-mechanisms interpreted have to be listed in a database, like the one shown in Figure 4. For example, it was found that the rotor degradation state e4 (ex: Rotor Interpol connection cracking) is caused by the degradation e3 (ex: Rotor Interpol connection mechanical fatigue stress) and can induce e5 (ex: Rotor Interpol connection failure). In this first rough analysis, all lines are independent and yet no links between sub-mechanisms exist."
10.36001_phmconf.2015.v7i1.2724,database,90,,0,"To generate a structured causal tree, the algorithm calls each root cause and scans the input/output database. Then a “While” loop is initiated generating failure mechanisms by adding each physical state output by output from a root cause up to failure modes. Still based on the same fictive example shown in Figure 4 and Table 2, a structured causal tree has been generated in the Figure 7. An important remark is that this logic can be easily reversed for root cause analysis."
10.36001_phmconf.2015.v7i1.2724,database,168,,0,"To relieve the expert from part of this work, a prognostic tool, that uses a Failure Mechanisms and Symptoms Analysis (FMSA), is under development. The approach is based on the understanding of the evolution of degradation processes for each failure mechanism. Failure mechanisms are structured as causal trees and defined as a sequence of physical states starting from a root cause and ending with a failure mode. A physical state corresponds to characteristic degradation condition of a component of the generator. Each physical state being defined by a unique combination of symptoms as measured with diagnostic tools. After consigning all possible mechanisms occurring in both the rotor and the stator, the symptoms logged into a database can be read to automatically identify all active physical state and active failure mechanisms. This approach has been under development in HQ for the stator for a number of years and is now extended to the rotors of hydro-generators."
10.36001_phmconf.2016.v8i1.2577,database,34,,0,The fault progression behaviour and RUL can be recorded in a database to be called by the power management algorithm.  A typical RUL plot for EPGS components is shown in the Figure-16.
10.36001_phmconf.2017.v9i1.2306,database,95,,0,"(2015b). Review of ﬂow boiling and critical heat ﬂux in microgravity. International Journal of Heat and Mass Transfer, 80, 469–493. Kutateladze, S. S. (1959). Critical heat ﬂux to ﬂowing, wetting, subcooled liquids. Energetika, 2, 229–239. Lemmon, E. W., Huber, M. L., & McLinden, M. O. (2013). NIST Standard Reference Database 23: Reference Fluid Thermodynamic and Transport propertiesREFPROP, Version 9.1, National Institute of Standards and Technology."
10.36001_phmconf.2017.v9i1.2391,database,79,,0,"The same database of our previous work (Flores et al., 2015) was used to test the proposed methodology (185 criminal events are used to test the updating stage, and 185 are used to validate the proposed risk prediction approach). In the same way, the results of the Off-line stage in (Flores et al., 2015) are used as an input to the On-line stage in the present work."
10.36001_phmconf.2017.v9i1.2457,database,111,,0,"Expert Systems are one of the earliest techniques to solve the failure diagnosis problem in Power Systems. The diagnosis process in an expert system can be rule based or model based. A comprehensive survey of such knowledge based approaches is available in (Sekine, Akimoto, Kunugi, Fukui, & Fukui, 1992). The expert systems in general suffer from a number of drawbacks related to the maintenance of the knowledge database and slow response time. These approaches are expected to work well if all the received alarms are correct. Missing and incorrect alarms force the diagnosis technique to produce wrong hypotheses."
10.36001_phmconf.2017.v9i1.2476,database,88,,0,"The maximum pulse amplitude (Qmax) and the PD intensity (NQN) are most of the time, the main criteria used in the industry to quantify PD (Stone & Warren, 2006). However, analysis of our database shows that these criteria alone are too coarse to feed our prognostic approach. As we will show with the next case studies, sometimes even a switch in discharge behavior should be used as trigger to indicate warning signs of imminent failure."
10.36001_phmconf.2017.v9i1.2476,database,115,,0,"Over the past 30 years, Hydro-Quebec has built an extensive Partial Discharge (PD) database including two types of measurement instruments. One of the instruments used is the Partial Discharge Analyzer (PDA) which gives a simple 2D representation of the PD activity as illustrated in Figure 3 and is measured yearly by plant personnel. Here the graph shows the discharge rate (PD/s) as a function of the amplitude in mV of positive discharges (in red) and of negative discharges (in yellow). Up to now, over 20 000 measurement files have been recorded using the PDA technique on about 170 hydrogenerators."
10.36001_phmconf.2017.v9i1.2476,database,148,,0,"Hydro-Quebec has an electric generating capacity of 36 GW from its 62 hydroelectric power plants. Its generating fleet comprises more than 350 hydrogenerators. These important assets are worth several million to tens of millions of dollars each and are subject to preventive maintenance comprising both systematic and conditional maintenance. An integrated diagnostic system for hydrogenerators was implemented in 2008 based on the aggregation of individual health indices of seven diagnostic tools (Hudon, Bélec, Nguyen, 2009). As of 2017, more than 320 hydrogenerators have their condition assessed with a health index ranging from 1 (excellent condition) to 5 (very bad condition). MIDA gives a ranking of all generators and thus helps the power plant management prioritize the generators for maintenance. The MIDA centralized database contains all diagnostic measurements performed on each generator."
10.36001_phmconf.2017.v9i1.2476,database,149,,0,"Instead of using the PD amplitude as criteria, the ratio of the NQN+ on the NQN- (NQN+/NQN-) was used to assess when the PD activity changed from one physical state to the next. The evolution of this ratio for phase B (after removing the gap PD contribution) is illustrated in Figure 7. From the PD database, we estimated that when the ratio is between 1 and 1.3, the main PD source is related to internal or delamination type, whereas above 1.3, it indicates the presence of corona PD or slot PD. Similarly, when the ratio goes below 0.90, the discharge process is mostly related to copper-insulation interfacial PD. We see that in some cases, the change in PD process is a better indicator of imminent failure than trending only overall PD intensity."
10.36001_phmconf.2018.v10i1.503,database,5,,0,System. https://asrs.arc.nasa.gov/search/database.html
10.36001_phmconf.2018.v10i1.503,database,54,,0,"The FAA Coded Instruments Flight Procedure (CIFP) database (FAA, Coded Instruments Flight Procedures, 2018) provides access to the complete list of published procedures at every major airport in the NAS. NATS software provides access to these procedures through several interface functions for use by the investigators."
10.36001_phmconf.2018.v10i1.544,database,2,,0,Aircraft database
10.36001_phmconf.2018.v10i1.544,database,2,,0,Fault/Failure database
10.36001_phmconf.2018.v10i1.544,database,2,,0,Weather database
10.36001_phmconf.2018.v10i1.564,database,130,,0,"Assuming the set of potential crack paths was ﬁxed, all remaining uncertainty about fatigue life, the prognostic quantity of interest, was attributed to the parameters describing the linear region of the crack growth rate curve, C and n, and the initial crack length at the zeroth4 cycle, a0. These were the random variables to be estimated via Bayesian inference. The remaining model parameters were ﬁxed at the calibrated values found in the NASGRO database. Evaluating the surrogate model thus involved re-integrating Equation 13 given a realization θ = [a0, C, n]T and a crack initiation location. In this particular study, the crack initiation location was known and ﬁxed at the observed initiation coordinates."
10.36001_phmconf.2019.v11i1.768,database,11,,0,Figure 5. A database representation for each patient evaluation.
10.36001_phmconf.2019.v11i1.806,database,44,,0,"(2008, 10). Automated inspection using database technology within the aerospace industry. Proceedings of The Institution of Mechanical Engineers Part B-journal of Engineering Manufacture - PROC INST MECH ENG B-J ENG MA, 222, 175-183. doi: 10.1243/ 09544054JEM938"
10.36001_phmconf.2019.v11i1.855,database,38,,0,"The input and output values, obtained from the database, are separated in two different matrices. Equation (3a) represents all the known inputs and Eq. (3b) is the corresponding output."
10.36001_phmconf.2020.v12i1.1261,database,41,,0,"Experiment on a crack propagation database shows that the presented method can give an accurate prediction of RUL for the aircraft structure. Thus, the presented method can facilitate the implementation of “Digital Twin” in real-world scenario."
10.36001_phmconf.2020.v12i1.1261,database,84,,0,"Specimen 68 is taken for testing in Case 2. The specimens 1-67 are regarded as the previously served structures that are recorded in the ﬂeet database. Thus specimens 167 are used to calculate the prior distribution of (m, logC). Fig. 10 shows the calculated bivariate Gaussian distribution Ncase2(µ1, µ2, σ2 2, ρ) where µ1 = 2.8752, µ2 = −16.2277, σ2 2 = 0.1620, ρ = −0.9895. We"
10.36001_phmconf.2020.v12i1.1261,database,112,,0,"To further facilitate the implementation of “digital twin” in real-world scenario, this paper presents a life prediction method for aircraft structure based on Bayesian inference. The main contributions of this paper lie in: (1) we present a method that can fuse heterogeneous information acquired from inspected physical entity, ﬁnite element software, historical database and predictive model, giving an accurate and real-time prediction of remaining useful life (RUL) for aircraft structure. (2) we illustrate how this method can be embedded into a digital twin framework, facilitating the implementation of “digital twin” in real-world scenario."
10.36001_phmconf.2020.v12i1.1261,database,132,,0,"This paper presented a life prediction method for aircraft structure, and illustrates how this method can be embedded into a “digital twin” framework. This method can fuse heterogeneous information including load condition, material prop erties, structure geometry, historical record document, degradation observation and pre-speciﬁed failure threshold. This method is based on the well-known Paris law. In its operation, the degradation behavior of the inspected structure is observed in an online manner. Fleet database is used for generating prior knowledge. The real-time load condition is used for calculating the stress intensity factor. Finally, the Bayesian theory is used to integrate these multi-sources of information and predict the remaining useful life (RUL) of the inspected structure."
10.36001_phmconf.2020.v12i1.1261,database,137,,0,"The parameter C and m are material constants for fatigue crack growth. Namely, for a speciﬁc structure, C and m are constants. For a same batch of aircraft structure, C and m are considered as statistically correlated random variables. Meanwhile, many studies (An, Choi, & Kim, 2012; Ortiz & Kiremidjian, 1988) point out that (m, logC) is assumed to obey a bivariate Gaussian distribution. In this paper, the bivariate Gaussian distribution of (m, logC) is acquired from the ﬂeet database which records the fatigue growth trajectories of a set of structures served previously. Giving a set of crack growth trajectories, we can calculate the corresponding bivariate Gaussian distribution by the following steps,"
10.36001_phmconf.2020.v12i1.1261,database,138,,0,"degradation trajectories are recorded in the ﬂeet database. Thus specimens 2-68 are used to calculate the prior distribution of (m, logC). Fig. 7 shows the calculated bivariate Gaussian distribution Ncase1(µ1, µ2, σ2 2, ρ) where µ1 = 2.8719, µ2 = −16.2242, σ2 2 = 0.1608, ρ = −0.9889. The calculated distribution is exhibited in Fig. 7 where each scatter denotes one pair of (m, logC) corresponding to a specimen. It is worth mentioning that even physically identical components made of the same type of material could demonstrate different fatigue behavior. From Fig. 7, we can see that in addition to a few outliers, the twodimensional Gaussian distribution can generally ﬁt these scatters."
10.36001_phmconf.2020.v12i1.1261,database,172,,0,"To further illustrate the implementation of “digital twin”, we exhibit an example in Fig. 2 where the aircraft structure, CF199 inboard leading edge ﬂap (ILEF) shown in Fig. 2(a), is modeled. First, the load condition can be acquired from the strain gauge located on the ILEF. Then, based on the material property and structure geometry, we can identify the critical location on ILEF which suffers from fatigue crack as shown in Fig. 2(b). The ﬂeet database records the fatigue crack growth and load history of ILEF of other aircraft in the ﬂeet. The degradation observation, i.e., the crack length at critical location, can be obtained by operator via NDT technology in maintenance activities. Then the presented method can output the prediction of the remaining useful life for the inspected ILEF. As a result, the real-world maintenance activities can be guided and optimized."
10.36001_phmconf.2020.v12i1.1261,database,205,,0,"Nowadays, the concept of “digital twin” has received great attention from both academia and industry. However, few methodological solutions have been reported in the existing studies. This paper presents a life prediction method for aircraft structure, and illustrates how this method can be embedded into a “digital twin” framework. This method can fuse heterogeneous information acquired from inspected physic entity, ﬁnite element software, historical database and predictive model, giving an accurate and real-time prediction of the remaining useful life (RUL) for aircraft structure. In the operation of this method, the degradation behavior of inspected structure is observed in an online manner. Historical record document is used for generating prior knowledge. The external load condition is fed into ﬁnite element software for calculating the stress intensity factor. The well-known paris law is adopted as the predictive model. Finally, the Bayesian inference is used to integrate the information and predict the future degradation of the inspected structure. Theoretical deviation and experiment on a public database demonstrate the effectiveness of this method, facilitating the implementation of “digital twin” in real-world scenario."
10.36001_phmconf.2020.v12i1.1300,database,94,,0,"Dai, W., Yang, Q., Xue, G.-R., & Yu, Y. (2007). Boosting for transfer learning. In Proceedings of the 24th international conference on machine learning (pp. 193–200). Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255)."
10.36001_phmconf.2015.v7i1.2631,"database, data",34,,0,Demands (D): Item’s quantity of demands at the retail level during a implementation.  Demand data is pulled from the ILAP (Integrated Logistics Analysis Program) database.
10.36001_phmconf.2015.v7i1.2631,"database, data",54,,0,Flight Hours (FH): The quantity of flight hours flown during the same timeframe before and after implementation. FH data is Department of the Army (DA) Form 1352 data pulled from the Logistics Information Warehouse (LIW) and utilizing the Readiness Integrated Database (RIDB) application.
10.36001_phmconf.2015.v7i1.2655,"database, data",33,,0,"Our database has several years of data sampled at once-perminute. For demonstration purpose, in this study, we use several months of data for one turbine. Specifically, we use"
10.36001_phmconf.2015.v7i1.2656,"database, data",54,,0,"It is necessary to provide background into the inputs for prognostic models; the link between the coupling algorithm and the model inputs is a detailed prognostic information database. The coupling algorithm is responsible for sorting extracted data into this database, which has a structure similar to that in Figure 6."
10.36001_phmconf.2015.v7i1.2656,"database, data",92,,0,"After extraction, the data is sorted depending on the corresponding prognostic model type, and stored for later use in lifecycle prognostic models. Historical data has a different utility than current cycle data. As the algorithm strips out a current cycle, the data can be used for monitoring. Once the information has been sent to a prognostic database, it can be used to update existing models as a separate task of continuous model improvement. Current cycle data is of key importance for critical decision-making."
10.36001_phmconf.2015.v7i1.2656,"database, data",94,,0,"Another aspect of the CMMS software that should be evaluated the need for asset-specific maintenance information. Current CMMS work orders are tailored to an application, but not always a specific system. To gather useful information on a specific part or component, a pump may need a different CMMS record than a motor. Availability of additional information may directly affect the resulting models. This type of customized CMMS database may be necessary as the data mining algorithm is being developed, evaluated, and validated during future research."
10.36001_phmconf.2015.v7i1.2656,"database, data",231,,0,"The purpose of extracting useful prognostic data from historical process data files is to improve maintenance efforts. Two of the primary concerns in the development of predictive maintenance models are quick response time, and high prediction/model confidence. There are several stages in the development of prognostic models when utilizing data gathered from the coupling algorithm. Lifecycle prognostic models can be updated/transitioned as additional information is gathered. The first information stored in the database will be simple failure times for the different components, which is Type I prognostics. As additional failure times are measured, monitoring efforts can be updated and prognostics based on current information can be assessed. When stressor information is incorporated with the preexisting failure times, the model can be updated to Type II prognostics. As the coupling algorithm reads in more data, it will be able to extract useful signal values related to the failure times of components. For example, if the inlet water temperature sensor value for a pump increases over each failure cycle, the coupling algorithm will identify that temperature signal as useful. Once the coupling algorithm has identified several useful degradation signals, it will store them as Type III prognostic data. All three forms of prognostic data can be used to update existing monitoring and prognostic efforts."
10.36001_phmconf.2015.v7i1.2656,"database, data",247,,0,"part’s or asset’s failure time. The coupling algorithm begins with the polling of the CMMS database. As seen in Figure 5, at step 1 the service information for a specific part is polled. This includes the current maintenance cycle and the previous cycle. With respect to the pump-motor system discussed earlier, if the pump impeller were serviced, the algorithm would poll the maintenance information for the current event (ex: impeller replaced on 1/4/15 at 02:44) and the past event of the same category (ex: impeller replacement on 10/14/14 at 16:00). In order to determine what data must be extracted from the plant computer, the CMMS polled maintenance records are passed to a sensor query (Figure 5 step 2). The sensor query looks at the system under maintenance (pumpmotor system) as well as the specific part (pump impeller) in order to return the associated sensors and their location within the plant computer records. The next step extracts the information from the plant computer (Figure 5 step 3). The data for each sensor related to the system is extracted for both maintenance services between the time of startup and time of failure. After this step is completed, the resulting sensor information is processed and passed to a prognostic information database (Figure 5 step 4)."
10.36001_phmconf.2015.v7i1.2710,"database, data",106,,0,"report, the report is parsed at a high level to determine the specific type of report, specifically the subsystem, report type, version that report type, and based on that information a determination is made and action taken to move that report to the proper staging directory. If the report is not recognized it is placed in a separate bucket. Then, the Data Transformation agent reads each report as one record of input, and parses the parameters from that report. It stores each parameter as part a unique record for that report in the application database."
10.36001_phmconf.2015.v7i1.2710,"database, data",174,,0,"The complex event processing portion of the ASHM software is shown in Figure 2. Data flows through RulePoint by report type, originating with a SQL Source that acquires parameter instance data from the ASHM database and pushes it into a RulePoint® Topic. A Rule references one or more topics and may use data from those topics to 1) determine anomalous conditions, e.g. value out of range, 2) compute new values based on those parameters, 3) send those computed values or detected conditions to a Responder that is responsible for storing new data back to the same ASHM database. For the ASHM project automatic alert rule generation based on thresholds defined in the database is employed. This custom tool uses a Java API Adapter to 1) connect to the development RulePoint instance, 2) remove all previously generated (as opposed to hand entered) rules, and 3) generate a new set of rules based on those thresholds."
10.36001_phmconf.2016.v8i1.2523,"database, data",109,,0,"airborne and the ground components are done through the use of the aircraft’s system. All the data are sent as soon as they are processed in the airborne system and generally before the end of the aircraft mission by wireless communication means (SATCOM, ACARS, 3G, Wi-Fi…). The ground system decodes and stores all these data in a dedicated database and then the EHM functions compute the trend indicators. These indicators are analyzed by the Customer Support Center team, translated into Customers Notification Reports (CNR) and made visible to the client through a web portal service."
10.36001_phmconf.2016.v8i1.2523,"database, data",114,,0,"Computed data are stored in a database and alerts generated by the THR ground function are analyzed by a PHM operator.  In future, root cause analysis will be available with alert/symptom to determine which Thrust reverser subsystem needs to be replaced in case of degradation, and provide troubleshooting capacity for Thrust Reverser subsystem. This troubleshooting capacity will be improve by use of existing fusion method like fuzzy logic, bayesian network or random forest. Mainly, bayesian networks are used for their interpretability and the help they can give for troubleshooting operations. Better results are given using random forest which ensure robustness of the decision but are less"
10.36001_phmconf.2016.v8i1.2577,"database, data",73,,0,The mathematical model developed in the previous sections have been inserted in to MATLAB based EPGS system and simulation studies has been carried out. These studies were done to collect data during various failure modes including Parametric and structural Faults. These data were recorded for the generation of Fault database for EPGS System.  The combined Diagnostic and Prognostic Algorithm for EPGS System has been developed and given in Figure-15.
10.36001_phmconf.2016.v8i1.2577,"database, data",334,,0,"This research work presents a novel approach that addresses the concept of an adaptive power management system design and development framed in the Prognostics and Health Monitoring(PHM) perspective of an Electrical power Generation and distribution system(EPGS).PHM algorithms were developed to detect the health status of EPGS components which can accurately predict the failures and also able to calculate the Remaining Useful Life(RUL), and in many cases reconfigure for the identified system and subsystem faults. By introducing these approach on Electrical power Management system controller, we are gaining a few minutes lead time to failures with an accurate prediction horizon on critical systems and subsystems components that may introduce catastrophic secondary damages including loss of aircraft. The warning time on critical components and related system reconfiguration must permits safe return to landing as the minimum criteria and would enhance safety. A distributed architecture has been developed for the dynamic power management for electrical distribution system by which all the electrically supplied loads can be effectively controlled.A hybrid mathematical model based on the DirectQuadrature (d-q) axis transformation of the generator have been formulated for studying various structural and parametric faults. The different failure modes were generated by injecting faults into the electrical power system using a fault injection mechanism. The data captured during these studies have been recorded to form a “Failure Database” for electrical system. A hardware in loop experimental study were carried out to validate the power management algorithm with FPGA-DSP controller. In order to meet the reliability requirements a Tri-redundant electrical power management system based on DSP and FPGA has been developed. Robin K Sebastian et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
10.36001_phmconf.2017.v9i1.2476,"database, data",67,,0,"To carry out a prognostic, it is necessary to determine the transition times between each pair of physical states (nodes) pertaining to a failure mechanism in the graph. This can be achieved by using expert elicitation and/or by using data from our physical state activity database. This database contains dates at which measurements were made and physical states were found active."
10.36001_phmconf.2017.v9i1.2476,"database, data",84,,0,"next step is to perform a complete analysis of the PD database with the help of appropriate data driven methods. The use of these techniques should make it possible to determine the frequency of occurrence of specific transition between each PD process over the entire database. However, since failure rate of generator is low and measurement data is relatively scarce we have to deal with left as well as right censored data to evaluate all transition times for our hydrogenerators."
10.36001_phmconf.2017.v9i1.2476,"database, data",112,,0,"In the hydrogenerator’s prognostic model based on FMSA, the last physical state before failure is in most of the cases related to PD activity. Data analysis has shown that we must first understand the degradation process and look for patterns, and then use data driven methods to extract specific parameters. The two case studies presented in this paper pointed out the importance of understanding the physics of degradation in order to assess warning signs coming from PD measurements. Then, accurate features can be used as an input for data driven methods to explore the PD database and refine more accurately the prognostic model."
10.36001_phmconf.2017.v9i1.2476,"database, data",171,,0,"Results presented in this paper indicate that it is essential to understand the fine details of each degradation process in order to properly correlate the PD diagnostic data to the physics of the degradation. In some cases, coarse data such as PD intensity, as often used in this industry, may be an acceptable indicator of incipient failures, but in both case studies presented in this paper, it is demonstrated that monitoring and trending of PD intensity is not an adequate criterion to assess the imminence of a failure. It is only by following the transition from one type of PD to another, using accurate features that it is possible to provide warning signs before failure. Regardless of the prognostic approach, it is always important to identify a set of relevant data before performing analysis of large data sets constituting the whole PD database. Once the determinant factors to track are identified, here specific transition of discharge mode, the"
10.36001_phmconf.2018.v10i1.502,"database, data",305,,0,"The air traffic control (ATC) system is critical in maintaining the safety and integrity of the National Airspace System (NAS). This requires the information fusion from various sources. This paper introduces a hybrid network model called the Bayesian-Entropy Network (BEN) that can handle various types of information. The BEN method is a combination of the Bayesian method and the Maximum Entropy method. The Maximum Entropy method introduces constraints and is given as an exponential term added to the classical Bayes’ theorem. The exponential term can be used to encode extra information in the form of constraints. The extra information can come from human experience, historical data etc. These knowledges, once written in a mathematical format, can be incorporated into the classical Bayesian framework. The BEN method provides an alternative way to consider common data types (e.g., point observation) and uncommon data types (e.g., linguistic description for human factors) in the NAS. The reported work is demonstrated in two example problems. The first example involves an air traffic control network model and the BEN uses information from various sources to update for the risk event probability. The second example is related to the prediction of the cause of runway incursion. A network model studying different sources of error is used to make predictions of the cause of runway incursion. The training and validation data is extracted from existing accident report in the Aviation Safety Reporting System (ASRS) database. The results are compared with that of the traditional Bayesian method. It is found that the BEN can make use of the available information to modify the distribution function of the parameter of concern."
10.36001_phmconf.2018.v10i1.503,"database, data",86,,0,"As in the case of arrival and departure procedures, NATS provides functions for accessing the complete list of jet routes and waypoints in the NAS, available in the CIFP database. These waypoints can be used to verify flight plans in the FAA traffic data feed, and to design valid flight plans for introducing test aircraft in the NAS simulations. Moreover, this data will be used by the controller and pilot models described in following sections to create weather-avoidance routes."
10.36001_phmconf.2018.v10i1.503,"database, data",87,,0,"Since it is nearly impossible to get accurate data for every aircraft that are operating in the NAS, a further simplification is made in the analysis. The simplification consists of employing the three differential equations describing the position kinematics of aircraft, together with rate of climb, rate of descent, and airspeed or Mach number bounds from well-known databases such as BADA (Eurocontrol, 2018). This database was assembled by Eurocontrol using actually observed trajectories in the European airspace."
10.36001_phmconf.2018.v10i1.544,"database, data",328,,0,"The goal is to identify fault/failure modes (which are rare compared with the normal working modes) by integrating the weak signals of such rare events from all the views. As a pre-processing step, the data are being converted from all the information sources into numerical forms. For example, to process the text information from ATIS, one first builds a vocabulary consisting of key words commonly used in aviation and airport environments. Each piece of recorded information is mapped into a numerical vector using, e.g., the bag-of-word model (Schütze 2008), or the Word2Vec model (Goldberg and Levy 2014). Then based on these numerical features from multiple views, novel multi-view rare category analysis algorithms are designed to output a list of events ranked in descending order of their probabilities of fault and failures. These algorithms are based on the alternating direction method of multipliers (ADMM) (Boyd, Parikh et al. 2011). End users of these algorithms can look through the top ranked events, and identify them as positive with a specific fault/failure mode or negative (normal). Such feedback is used as auxiliary label information (Jansen and McNeese 2005) to improve the performance of the multi-view rare category analysis algorithms. b) Multi-label learning for multiple fault/failure types. Given multiple types of fault/failure available in historical data, a natural idea is to construct separable models from each type independently. However, given the relatedness of different types of fault/failure, sometimes due to the co-existence of multiple causes, discarding the relationship among the different types might be sub-optimal (Gibaja and Ventura 2015). For example, a fault/failure might be caused by both pilot error and wake turbulence. To this end, a multi-label learning algorithm is used to simultaneously build models for"
10.36001_phmconf.2018.v10i1.544,"database, data",350,,0,"Another major component is the big data analytics module that is used to develop metrics-based safety measures that can be used for prognostics. Multiple data sets consisting of different types of historical aviation information are mined, including NTSB Aviation Accident Database & Synopses, FAA Aviation Safety Information Analysis and Sharing (ASIAS) Source Databases, Automatic Terminal Information Service (ATIS), to name a few. The goal is to build predictive models that take as input multi-sourced data, or multi-view data, and output the predictions regarding various types of fault/failure modes, such as wake turbulence related accidents, pilot errors, mechanical errors, and sabotage. A heterogeneous learning framework for scalable and real-time failure and risk identification from multi-modality input data consists of the following two key components:  a) Multi-view rare category analysis for detecting fault and failures. A single view only corresponds to the features from one information source (one view), e.g., text information from ATIS. The goal is to identify fault/failure modes (which are rare compared with the normal working modes) by integrating the weak signals of such rare events from all the views. As a pre-processing step, the data are being converted from all the information sources into numerical forms. For example, to process the text information from ATIS, one first builds a vocabulary consisting of key words commonly used in aviation and airport environments. Each piece of recorded information is mapped into a numerical vector using, e.g., the bag-of-word model (Schütze 2008), or the Word2Vec model (Goldberg and Levy 2014). Then based on these numerical features from multiple views, novel multi-view rare category analysis algorithms are designed to output a list of events ranked in descending order of their probabilities of fault and failures. These algorithms are based on the alternating direction method of multipliers (ADMM) (Boyd, Parikh et al. 2011)."
10.36001_phmconf.2019.v11i1.768,"database, data",153,,0,"The idea behinds MCATSS is to enable comparison of patient parameters during various operative stages - pre, in and post; therefore, we need to index the patient information in a manner that facilitates this co-relation as well as maintains patient anonymity. To anonymize patient data for storage, and enable future analysis without violating patients’ privacy, while being able to still look up information for a given patent, we index each patient record using a SHA-256 hash value of 3 parameters. The parameters are entered into the application - First Name, Last Name and Date of Birth. Given a hash’s irreversible nature we cannot infer identiﬁable patient information from the database, thus satisfying the de-identiﬁcation constrain. However, given the name and date of birth we can recomputed the hash and retrieve corresponding patient’s information."
10.36001_phmconf.2019.v11i1.768,"database, data",203,,0,"MCATSS uses Couchbase - a distributed NoSQL database that offers access control and encryption to satisfy the 3 re Figure 5 shows how MCATSS structures a single session object and stores it into the database. A session is deﬁned by the user logging into the application, selecting the current stage they are in - pre-op, op-day, post-op, discharge, at-home, or others. Each such session contains the timestamp it was started at, the patient ID to which it belongs, a meta-data object and at least one experiment. To increase reliability and decrease the likelihood of data loss, we commit the current state of the session object into our database after completion of each experiment. We call these intermediate commits as sub-sessions. To link these sub-sessions together we use a meta-data object illustrated in Figure 5. The meta-data ID contains a SHA-256 hash of the current timestamp when the session was started. Stage contains one of the many perioperative stages a patient can be in, with optional containing custom stage information, if the selected option was others. This allows MCATSS to quickly link all sessions belonging"
10.36001_phmconf.2019.v11i1.788,"database, data",125,,0,"An overview of the proposed method is shown in Fig. 3, which has three key steps. The similarity matching finds the similar historical degradation profiles in the database using KTST. Only the historical data that passes the KTST are regarded as similar and are employed for RUL prediction. The dissimilar data are excluded from the RUL prediction. We also note that the historical degradation profiles in the database must be Run-to-Failure data that cover the whole life cycle of the machine. In the following discussion, the historical database is denoted as 𝑫 = {𝑑𝑘}𝑘=1,…,𝑁, where 𝑑𝑘 is the 𝑘-th R2F profile and 𝑁 is the total number of historical"
10.36001_phmconf.2019.v11i1.788,"database, data",133,,0,"We would like to note that the parameter tuning for similarity-based RUL prediction is meaningless. For most machine learning applications, the parameters are tuned by cross-validation, such as K-fold, Leave-One-Out (LOO), etc., on the training set. However, it is not clear how to implement cross-validation in the present case, since the data records in the R2F database are multivariate temporal sequences rather than vector-based data samples. If the parameters are tuned based on the testing set, the results might be seriously biased. Therefore, we test our method under default setting without any parameter tuning. We also note that the cross-validation on the training set is applicable when the regression-based methods are adopted for direct RUL prediction."
10.36001_phmconf.2019.v11i1.788,"database, data",137,,0,"Two Sample Test (KTST). Comparing with the existing discussions about SBM, the proposed method holds several advantages. 1) The proposed method is not HI based, and the similarity between two multivariate temporal sequences is directly evaluated by Maximum Mean Discrepancy (MMD); 2) The KTST (Gretton, Borgwardt, Rasch, Schölkopf, & Smola, 2012) is adopted to obtain an upper bound for the MMD to decide which R2F profile in the historical database is similar to the current data. Only similar data samples are employed as references to make predictions. 3) Weibull analysis is used to fuse multiple RUL predictions given by the historical data records and the PDF of RUL is obtained as the fitted Weibull distribution."
10.36001_phmconf.2019.v11i1.788,"database, data",238,,0,"Similarity matching is the key step in the proposed method to identify the similar profiles from historical database. A detailed illustration of the similarity matching is in Fig. 4. In this flowchart, the test data is first truncated using a time window and only the most recent observations are adopted for similarity matching. The windowed test data is denoted as 𝑥𝑡 and the length of the time window is B. Next, a sliding window with length B is applied to historical data 𝑑𝑘. The data in this sliding window is compared with the test data 𝑥𝑡 to find the best match. The best match is obtained by minimizing MMD value along the time axis and the time at 𝑘 , as in Fig. 4. Since 𝑑𝑘 is the best match is denoted as 𝑡𝑚𝑎𝑡𝑐ℎ 𝑘  a R2F failure profile, the time at the End of Life (EoL) is 𝑡𝐸𝑜𝐿 and the predicted RUL based on 𝑑𝑘 at time 𝑡 is written as: 𝑘 𝑘 − 𝑡𝑚𝑎𝑡𝑐ℎ (2) After finding the best matching, the windowed data from 𝑑𝑘 and 𝑥𝑡 needs to pass the KTST to be adopted for final RUL prediction. If the KTST is not rejected, the test data 𝑥𝑡 and the historical data 𝑑𝑘 is regarded as dissimilar and the RUL prediction 𝑃𝑘 is discarded."
10.36001_phmconf.2019.v11i1.790,"database, data",89,,0,"Already using controlled vocabulary — If an organization is already tied to a particular DB architecture, it is possible to illicit technician feedback on pain points through current usage patterns to either better communicate the level of detail needed for future operational usage of the database (low-cost), or design a more functional and user-driven interface through HMI principals (medium-high cost), or— preferrably—both. Failure to do so will likely result in unwanted data-entry behavior indeﬁnitely, as a feeling of resent 10"
10.36001_phmconf.2019.v11i1.790,"database, data",101,,0,"HMI and data-entry design This point in the decision process is much closer to the original intent of this HRA. Risks are primarily found in the disconnect between the needs of a structured database (its abstraction level, ability to query past events, modeling of asset/system architecture, etc.) and the needs of a technician (pressure to be thorough vs timely, use of efﬁcient jargon, reliance on tacit knowledge through expertise). These can be mitigated, at relatively low cost, depending on the level of existing data entry adoption."
10.36001_phmconf.2019.v11i1.790,"database, data",134,,0,"We have proposed a HRA for the quantiﬁcation and potential mitigation of human error in MWO data entry through categorization, with the goal of increasing the reliability of these human-machine systems. This is done by assessing the components of a proposed two-part process humans use to structure data: determine distinguishing features of a given datapoint, and organize those features into the database schema for future queries. Using principals of cognition, behavior, and risk assessment, it is hoped that the sources of risk in each can be mitigated at low-cost through implementing education, design, and socially-driven strategies, along with a re-thinking of the current MWO paradigms to incorporate human-in-the-loop data-driven pipelines that take advantage of technicians’ expertise, rather than antagonize it."
10.36001_phmconf.2019.v11i1.790,"database, data",219,,0,"To deal with this, one might enforce data-entry in a graphical user-interface (GUI) into pre-speciﬁed functional categories (generally using some form of controlled vocabulary). This is not always successful, and consistent reports from industry indicate that data entry remains a process fraught with signiﬁcant errors (Unsworth et al., 2011; Molina et al., 2013). These human errors are unintentional actions or decisions1 (whether slips, lapses, or mistakes), which lead to undesirable outcomes —in this case, the recording of un-useful data. Here we deﬁne “un-useful data” as mistake-prone, inconsistent, or incomplete data, recorded in a way that is not sufﬁciently structured for use in analytics as needed by an organization. Quantifying and managing sources of human error is a primary concern of Human Reliability Analysis (HRA). Applications of HRA methods to manufacturing are ongoing (Schemeleva et al., 2012), but there remains a need for user-based, error resistant data collection within manufacturing maintenance. This paper offers a framework and methodology for applying HRA to quantify and understand human errors associated with entering un-useful maintenance workorder (MWO) data into a controlled database (DB)."
10.36001_phmconf.2019.v11i1.792,"database, data",58,,0,"Bokinsky, H., McKenzie, A., Bayoumi, A., McCaslin, R., Patterson, A., Matthews, M., . . . Eisner, L. (2013). Application of natural language processing techniques to marine v-22 maintenance data for populating a cbmIn Ahs airworthiness, cbm, and oriented database."
10.36001_phmconf.2019.v11i1.830,"database, data",224,,0,"This study focuses on an approach to the vibration-based fault diagnostic methodology that can distinguish gear faults in a planetary gearbox in an excavator using vehicle-based test. First, we analyze the types of gear faults and the parts where failures occur mainly through the database of field failures. From this result, several fault types to be used in the experiment are selected, and an arbitrary fault is applied to gears. Secondly, since the vibration data is acquired directly from the excavator, the signal processing method that can remove the noise as much as possible and distinguish the fault is selected. Finally, optimized features are selected to minimize the uncertainty impacts that cannot be eliminated or unknown. Through this study, we confirmed the effect of the signal processing method which can be used in the planetary gearbox of the excavator as follows: 1) Several kinds of fault can be distinguished. 2) Faults and methodologies that can be distinguished in the constant speed Keon Kim et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
10.36001_phmconf.2019.v11i1.842,"database, data",36,,0,"Data Manipulation (DM): The DM block processes raw data from the DA block by means of mathematical algorithms, generating computed and the virtual sensor readings which are stored in a database."
10.36001_phmconf.2019.v11i1.842,"database, data",173,,0,"The OSA-CBM framework provides the parameters for systems architecture required for the successful application of Integrated Vehicle Health Management ( IVHM) to a product(Redding 2011). Thus, the structure of OSA-CBM architecture provisions the foundation for evaluating important IVHM technologies and database standards (Goebel 2011). This is significant because IVHM is a datadriven and data acquired from transducers and sensors directs much of the thinking surrounding it. In addition data management, data integrity, data quality are imperative for features or faulty conditions extraction-fundamental in condition-based maintenance (CBM) and Prognostic health management (Goebel 2011; Dibsdale 2011). IVHM systems require the capability to organize and manage small as well as large data sets in linked tables to promote the easeful appreciation and deliver a comprehensive language for data definition, retrieval, and update. Therefore, making data management an imperative competence in the operations room in particular and IVHM in general (Dibsdale 2011)."
10.36001_phmconf.2020.v12i1.1261,"database, data",79,,0,"• Fleet database: This database records the degradation history of the same batch of aircraft structures. After every physical aircraft ﬂies, data about the ﬂight will be downloaded from individual aircraft tracking (IAT) system. These historical information is used to parameterize the individual aircraft models from a probabilistic view. Speciﬁcally, various parameters related to the aircraft structure will be described in mean, standard deviation, distribution type, etc."
10.36001_phmconf.2021.v13i1.3003,"database, data",313,,0,"The focus of this paper is the gear tooth root crack failure mode. Gear tooth root cracks manifest as changes in gear mesh stiffness which changes the gearbox’s vibration characteristics. Both analytical models (Chaari, Fakhfakh, & Haddar, 2009; Chen & Shao, 2011; Liang, Zuo, & Hoseini, 2015) and numerical models (Cooley, Hood, & Wang, 2021) have been developed to better understand the relationship between crack size and the resulting acceleration. (Nenadic, Wodenscheck, Thurston, & Lewicki, 2011) conducted a series of experiments to develop a database of seeded fault experiments that carefully tracked crack size and gearbox housing acceleration. This data serves the purpose of model validation and diagnostic algorithm development. While analytical models have suggested a monotonic change in classical CIs with crack growth, this was not consistently observed in our experiments across multiple test gears (Nenadic et al., 2013). This inconsistency has also been observed by others. For example, (Sharma & Parey, 2016a) calculated condition indicators for three spur gears tests with different crack sizes. Wire Electrical Discharge Machine (EDM) was used to introduced different sized ﬂaws into two of the gears and results were provided for different speed ﬂuctuations. They found that the classical CIs did not perform well with increasing damage for all speed ﬂuctuations. They introduced two new CIs, PS-I and PS-II that were able to track the test cases and showed promise, however only one gear at each damage level was tested. (Bechhoefer & Butterworth, 2019) also found many CIs performed poorly on their own when analyzing three undamaged gearboxes and one with a cracked spiral bevel gear"
10.36001_phmconf.2019.v11i1.768,"database, dataset provided, data",154,,0,"The goals in designing an end-to-end automated MCATSS are: (1) Automate the collection of data using internal / external sensors; (2) Provide automated interventions for tracking patient progress during various peri-operative phases; (3) Reduce errors during patient evaluation steps by medical personnel; and (4) Decrease the effort and time required for medical personnel to record data required by the individual test. Figure 1 presents the implemented MCATSS, consisting of a phone based uniﬁed application that is either used by a medical personnel or a patient at home, a NoSql database for storage and retrieval of de-identiﬁed (tagged using ids) patient information, and external sensors used to automatically collect physiological data. The MCATTS phone application is uniﬁed but presents different functionality based on a login type. Medical personnel are presented with complete testing func 2"
10.36001_phmconf.2015.v7i1.2646,dataset,12,,0,(2) The dataset is monotonic: m = 0.81.
10.36001_phmconf.2015.v7i1.2646,dataset,59,,0,"(1) According to the design requirements, there is no need to extract the PDF of the RUL estimation. However, conﬁdence bounds are necessary. (2) The dataset is not monotonic: m = 0.71. (3) The aim is to predict the RUL at least 1 hour in advance,"
10.36001_phmconf.2015.v7i1.2646,dataset,80,,0,"Finally, if none of the above conditions are satisfactory, it is possible to fuse alternative DD approaches with the same dataset (i.e., DD1(x) || DD2(x), where x is the available dataset) to improve the accuracy of the RUL estimation (e.g., see (Hu, Youn, Wang, & Yoon, 2012) for an ensemble of multiple algorithms combined with a weighted-sum formulation)."
10.36001_phmconf.2015.v7i1.2646,dataset,89,,0,"Series and parallel combinations of model-based approaches with the same input dataset are scarce due to the lack of complementary properties between PoF techniques when combining or fusing two different degradation equations of the same system. An example of series combination with the same dataset conﬁguration (i.e., MB1(x) + MB2(x)) is presented in (Yoon & He, 2015) using UKF to estimate the state of the degrading system, and Particle Filtering to estimate the RUL."
10.36001_phmconf.2015.v7i1.2646,dataset,95,,0,"As for the conﬁgurations comprised of model-based combinations, datasets with different features or scenarios of the same system could be combined to improve the ﬁnal estimation (i.e., MB1(x) || MB1(y), where x and y indicate different input datasets). For instance, (Baraldi, Mangili, & Zio, 2012) implement an ensemble of Kalman Filter models. The other possible conﬁguration for model-based intracombinations is to add expert knowledge to model-based prognostics predictions in order to manage uncertainties (e.g.,"
10.36001_phmconf.2015.v7i1.2655,dataset,68,,0,"detection as a classification problem and used traditional neural networks and logistic regression as the classifiers. However, they didn’t do any feature engineering to extract features. Rather they directly used the exhaust temperature profile as the inputs to classifier, which showed a reasonable detection performance on the small dataset the authors picked, but may not generalize well in real applications."
10.36001_phmconf.2015.v7i1.2686,dataset,19,,0,"Figure 5. Schematic representation of the available dataset and its division into train, validation and test sets"
10.36001_phmconf.2015.v7i1.2686,dataset,23,,0,", is randomly selected from the training dataset and its distance to the weight vectors of all the SOM neurons is computed;"
10.36001_phmconf.2015.v7i1.2758,dataset,41,,0,"The experiment was conducted by randomly sampling 6 bearings out of the dataset to estimate the Weibull λ, k parameters. The estimated RUL was compared to the actual life, in a process similar to the simulation study."
10.36001_phmconf.2016.v8i1.2521,dataset,27,,0,Figure 4. Depiction of the original 30 crack growth simulations comprising the base of the training dataset. Screenshot taken from the fracture analysis software FRANC3D.1
10.36001_phmconf.2016.v8i1.2521,dataset,80,,0,"Figure 9. RUL vs. time plot for the ﬁve updates of the diagnosis dataset. The solid black line represents the experimentally observed RUL over time. The shaded strip (constant width) and the cone-shaped regions represent the α = 0.1 accuracy bounds corresponding to the P H and α-λ metrics, respectively. The probabilistic prediction of RUL at each time, tDi, is represented by a box plot as described in Section 2.4."
10.36001_phmconf.2016.v8i1.2542,dataset,159,,0,"Dr. Gregory W. Vogl is a Mechanical Engineer at Institute of the National Standards and Technology (NIST) located in Gaithersburg, Maryland. He received his B.S. (2000), M.S. (2003), and Ph.D. (2006) degrees in Engineering Mechanics from Virginia Tech, Virginia, USA. Currently, the Prognostics and Health Greg Management for Smart Manufacturing Systems (PHM4SMS) project, which seeks to develop a methodology, protocols, and reference datasets to enable robust real-time diagnostics and prognostics systems. Previously, he designed, fabricated, and experimented on microelectromechanical systems as a National Research Council Postdoctoral Researcher at NIST. He then joined the Production Systems Group, in which he worked on machine tool metrology and standards development. His interests include machine tool spindle health, diagnostic and prognostic methods, nonlinear dynamics, engineering mechanics, and metrology."
10.36001_phmconf.2016.v8i1.2555,dataset,38,,0,"It is possible to use the clustering results for feature selection. Features being the variables themselves, the goal is to find a subset of variables that would give the same clustering as the full dataset."
10.36001_phmconf.2016.v8i1.2569,dataset,59,,0,"Each of the AE signal dataset was transformed into a 2-D spectrum matrix as explained in Section 2.1. 10 patterns from each of the 5 bearing conditions: inner race fault, outer race fault, cage fault, ball fault and healthy condition were generated.  Therefore, there were a total of 50 patterns generated."
10.36001_phmconf.2016.v8i1.2569,dataset,131,,0,"Using the AE signals collected during the bearing seeded fault tests, 50 patterns were used to train the LAMSTAR model, 10 for each bearing condition. The dynamically built neurons in SOM modules enable the large reduction on training time as the search time to find the winning neuron was reduced to a small number of neurons in many cases. The neural network learns as it goes even if untrained.  In addition to LAMSTAR, another deep learning algorithm convolutional neural network (CNN) was used to perform the bearing fault diagnosis using the same datasets for the purpose of comparison.  Table 2 and 3 show the bearing fault diagnosis result obtained by LAMSTAR and CNN for normal speeds at 45 and 30 Hz."
10.36001_phmconf.2016.v8i1.2569,dataset,223,,0,"initially. Thus, 40 SOMs (hidden layers) were generated for 40 sub-patterns sampled from each spectrum matrix. In each SOM module, the dynamic neurons were constructed for storing representative value of cell in matrix. Thus, the number of neurons in each SOM varied from 0 to 2500.  Considering there are 5 bearing conditions in this study: inner race fault, outer race fault, cage fault, ball fault and healthy condition, three LAMSTAR network output neurons were used to give a complete permutation of 6 firing sequences with each sequence representing a condition. Table 1 shows the bearing conditions and their LAMSTAR output neuron firing sequence representations.  The error tolerance was set to be 10-9; and learning rate alpha was set to be 0.8 as a constant, and error tolerance for winning neuron decision in dynamic SOM weights construction was set to be 10-7. Based on the principle of ANN application, the collected dataset was divided into training and validation groups for training and validating generated network. In this paper, the ratio of training to testing dataset was 60% to 40% against total. The graphic procedure of LAMSTAR network structure in this paper is displayed in Figure 2 as above."
10.36001_phmconf.2017.v9i1.2189,dataset,79,,0,"NIST’s work cell research requires the identification of numerous use cases that can be represented in test methods that are realized within a physical test bed. NIST is constructing such a test bed where the initial configuration should be complete and active by late 2017. The construction of this test bed will promote development of measurement science test methods, reference datasets, and software tools that will ultimately support open standards and guidelines."
10.36001_phmconf.2017.v9i1.2189,dataset,182,,0,"science products (e.g., performance metrics, test methods, reference datasets, software tools) to promote the design, deployment, verification, and validation of monitoring, diagnostic, and prognostic technologies at the factory floor level to yield smarter maintenance and control strategies (National Institute of Standards and Technology, 2017). This paper documents NIST’s efforts to further develop use cases and construct a test bed to enable the development of performance metrics, test methods, reference datasets, use case scenarios, and software tools for an industrial arm robot work cell. Section 2 presents NIST’s research focus with respect to industrial arm robot system PHMC. Section 3 highlights several use cases and key characteristics that have been identified through discussions site visits with various manufacturers. Section 4 details the initial test bed use case configuration. Section 5 provides information on major components being used in the initial test bed configuration. Lastly, Section 6 concludes the paper and discusses future efforts."
10.36001_phmconf.2017.v9i1.2189,dataset,207,,0,"To promote greater viability and adoption of PHMC technologies, NIST is conducting research at the component, work cell and system levels. NIST’s PHMC research of robot work cells focuses on current industry challenges in maintaining the health of industrial arm-based automated processes. NIST has identified representative manufacturing robot work cell use cases. These use cases are serving as the foundation for NIST to construct its own use case and physical test bed. The initial configuration has been specified and is being integrated. In parallel, additional capabilities (e.g., external sensors) are still being specified and are expected to be integrated in the future. This will provide a sufficiently flexible and expandable test bed where PHMC methods can be developed, verified and validated and reference datasets can be collected. NIST is actively constructing the test bed and is expected to have the initial configuration complete and operational in late 2017. As the test bed is being constructed, test methods are being designed that will leverage the test bed’s capabilities and support the further development of the use case. These test methods will be presented in future work."
10.36001_phmconf.2017.v9i1.2189,dataset,214,,0,"The National Institute of Standards and Technology (NIST) is performing research to advance the state of the art in monitoring, diagnostic, and prognostic technologies (collectively known as prognostics and health management (PHM)) to enhance decision-making at the factory floor to promote smarter maintenance and control strategies. One specific thrust in this hierarchical research is focused at the work cell level. A robot system is the focus of this research level where the manufacturing community would benefit from measurement science (e.g., performance metrics, test methods, reference datasets, software tools) to design, deploy, verify, and validate PHMC technologies aimed at a identification of robot representative manufacturing robot work cell use cases will provide the foundation for which it will construct its own physical test bed. The test bed is designed to emulate the chosen robot system use case and afford sufficient flexibility to add, subtract, or upgrade components and capabilities to be commensurate with common industrial practices. This paper presents various use case options that NIST has considered and highlights the one that will be the foundation of the physical test bed. Additionally, the initial test bed design is introduced."
10.36001_phmconf.2017.v9i1.2391,dataset,124,,0,"The results of implementing this new strategy show an increase in the AUC from 0.934 (previous work) to 0.957 (current work) in the computation of the posterior spatial risk function. Furthermore, an increase in the AUC from 0.932 (previous work) to 0.946 (current work) in the computation of the predicted spatial risk function is also achieved. Two elements associated to NG implementation might be responsible for this improvement. The ﬁrst is related to the characteristic of searching a representation for the underlying PDF of the analyzed dataset. The second is associated to the capacity of maintaining the particles inside the interest area after all the new criminal events were processed."
10.36001_phmconf.2017.v9i1.2426,dataset,9,,0,"problem, we create the dataset as follows:"
10.36001_phmconf.2017.v9i1.2426,dataset,20,,0,Figure 4. (Bearing Dataset) Classiﬁcation error on the test dataset using RPFB and ﬁxed-window feature sets.
10.36001_phmconf.2017.v9i1.2426,dataset,22,,0,Figure 6. (Battery dataset) Prediction error on the test dataset using RPFB feature sets and ﬁxed-window feature sets.
10.36001_phmconf.2017.v9i1.2426,dataset,23,,0,Figure 5. (Turbofan Engine dataset) Prediction error on the test dataset using RPFB feature sets and ﬁxed-window feature sets.
10.36001_phmconf.2017.v9i1.2426,dataset,23,,0,"The dataset consists of three time series including a baseline (good condition bearing/class 0), an outer race fault (class"
10.36001_phmconf.2017.v9i1.2427,dataset,33,,0,"Future research steps include further model veriﬁcation using datasets from more wind turbine ﬂeets, so that model robustness is increased. Testing other types of learning algorithms will also be considered."
10.36001_phmconf.2017.v9i1.2447,dataset,4,,0,4.1. Datasets Description
10.36001_phmconf.2017.v9i1.2447,dataset,10,,0,STEP 4: Acquire testing datasets from online system.
10.36001_phmconf.2017.v9i1.2447,dataset,12,,0,STEP 3: Get the best model from the offline datasets.
10.36001_phmconf.2017.v9i1.2447,dataset,19,,0,STEP 2: Perform the offline training and testing processes with the three-fold CV with the training datasets.
10.36001_phmconf.2017.v9i1.2447,dataset,38,,0,"STEP 1: Obtain the lithium-ion battery capacity degradation dataset regarding with charge/discharge cycle from 3 offline battery units, which are used as training datasets, and normalize the datasets to [0, 1]."
10.36001_phmconf.2017.v9i1.2447,dataset,47,,0,"training LSTM modelSelect best model by three-fold CVOffline processUsing best LSTM model to testGet the RUL prediction valueOnline processTraining datasets of three offline battery datasetsTesting datasets of online battery11111tanh**tanh*tttfXfhftttiXihitttgXghgttttttttoXohotttfWxWhbiWxWhbgWxWhbcgicfoWxWhbhco,1toutoutpredreludropoutWhb211()niiiMSEypredn  ANNUAL CONFERENCE OF THE PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2017"
10.36001_phmconf.2017.v9i1.2447,dataset,55,,1,"The rest of this paper is organized as follows: Section 2 briefly introduces related works of RNN and LSTM-RNN. Section 3 describes the LSTM-RNN-based RUL prediction method. In Section 4, CALCE battery datasets are used to demonstrate the effectiveness of the proposed method. Conclusions are drawn in Section 5."
10.36001_phmconf.2017.v9i1.2447,dataset,104,,1,"The lithium-ion battery experiment datasets are derived from Center for Advanced Life Cycle Engineering (CALCE) in University of Maryland (He W., Williard N., Osterman M., & Pecht M., 2011). In this experiment, the lithium-ion batteries were to discover battery capacity degradation. The test was implemented on the Arbin BT2000 battery testing system under the room temperature. The experiment datasets with 1.1 Ah rated capacity are used in this paper. Figure 4 shows the capacity degradation curves of #35, #36, #37 and #38 lithium-ion batteries."
10.36001_phmconf.2017.v9i1.2447,dataset,123,,0,"The Three-fold cross validation is used in the offline process to evaluate the accuracy of a given model. We choose three offline batteries datasets (or folds) Y1, Y2 and Y3 for training and validating different models. Among the three datasets, one is selected as the test set and the other two datasets are used as training sets, for example, if Y1 and Y2 are used as training datasets, so Y3 will be testing datasets. The CV process is performed three times, with each of the three datasets used exactly once as the test set. Thus, the best model can be obtained by comparing the MSE on three test sets."
10.36001_phmconf.2017.v9i1.2450,dataset,14,,0,"1. Assemble the dataset, “N” observations, each having a"
10.36001_phmconf.2017.v9i1.2450,dataset,66,,0,"In the next step, we completed a Latent Class clustering analysis on the same dataset, where the continuous variables we binned based on optimal normal mixture density clustering. The algorithm picked 3 clusters, which is more tractable from a marketing segmentation and maintenance planning perspective.  Details on the composition of each fleet segment is shown in the share chart below."
10.36001_phmconf.2017.v9i1.2450,dataset,72,,0,"loads and prices. Most of these measurements come from surveys, unstructured text, third party datasets, market indicators, etc., and do not fit easily into the tools PHM engineers typically use to design systems. The authors of this paper have been working on this problem, and believe that this is where techniques from consumer and marketing research can have an important role to play."
10.36001_phmconf.2017.v9i1.2450,dataset,111,,0,"In this case study, the dataset consists of N = 879 unique wind turbines where 44 bearing failures have been recorded. To protect proprietary information, all variables have been anonymized and age has been scaled. This does not impact any resulting analysis or conclusion. The turbine age at failure has been recorded, along with other variables like the bearing type, customer name, wind farm name, the type of service relationship (full maintenance, partial, no agreement or unknown), and the capacity of the turbine (two levels, dependent on upgrades). This is summarized below."
10.36001_phmconf.2017.v9i1.2450,dataset,148,,0,"The field of Digital Marketing is rapidly growing, driven by the availability of massive datasets regarding customer transaction histories, their social media preferences and internet browsing user experiences. This has led to the development of many extremely sophisticated statistical methods designed to model the activity of existing and future customers, and Chaffey, D., and Chadwick, F.E., (2015) provide a comprehensive overview of the field. At the heart of this is Predictive Customer Lifetime Value, (CLV) which is the total amount a customer is expected to pay over the course of his relationship with a supplier. In an engineering context, this would be the expected revenue generated from asset sales, part sales, maintenance services and other valueadded services (software subscriptions, upgrades) that go along with the asset."
10.36001_phmconf.2017.v9i1.2485,dataset,77,,0,"Figure 9 shows the progression of the total damage indicated across the rail at each test stage for both the IMU- and reference-based datasets. These charts show the integral of the measured damage in both the vertical and horizontal directions, which are the directions most affected by the induced ‘spalling’. In both instances, Stage 1 is taken as the baseline and therefore by definition has the trivial value of zero."
10.36001_phmconf.2018.v10i1.245,dataset,104,,0,"After a preliminary prioritization assessment based on a benchmarking exercise, a particular type of rotating asset in a particular industrial application was identified as a candidate for prioritization based on high repair cost and counts relative to peer values. The dataset identified for the case study consists of N = 180 assets observed over a period of 5 units of time (such as a year) at 3 different sites. There were about 4,000 repairs observed total over that period of time, but not all of the repairs were failures (the breakdown indicator was not used)."
10.36001_phmconf.2018.v10i1.464,dataset,77,,0,"This article evaluates the effectiveness of brake pad prognostics with a dataset of brake pad thickness measurements at the ﬂeet level, obtained with TrainScanner from November 1, 2016, to March 1, 2017. It comprises the evaluation of 11836 brake pad assets. Each set of carbon pad thickness measurements needs to be preprocessed to add robustness to the prediction. To this end, the following issues are taken into account:"
10.36001_phmconf.2018.v10i1.464,dataset,128,,0,"The range of the input values needs to be normalised around a unitary magnitude to guarantee an effective learning convergence. The pad thickness linear prediction feature is normalised to its maximum (i.e., 34mm), and the location features are treated as binary variables. The non-linear smoothing function for all the units is set to be the logistic sigmoid. Therefore, the maximum output value of the network is normalised and scaled to the 0.79 value to avoid saturation. The network is trained with stochastic gradient descent using backpropagation, with a fast learning rate of 0.2 that is checked to avoid cost overshooting, and a maximum number of dataset iterations of 30 to consider early stopping and improve generalisation."
10.36001_phmconf.2018.v10i1.464,dataset,245,,0,"The friction brake system reduces the speed of the train by transforming the kinematic energy into heat through the abrasion between the carbon pads and the disk. The British Rail Class 390 ﬂeet (Pendolino) features a very high availability, running 1000 miles a day on average, so their wear rate is monotonic and acceptably constant. The prognostics for brake pad degradation are typically conducted with a robust online linear regression technique, which seamlessly accommodates asset-based idiosyncrasies, like the different effort that is exerted on the pad given its location on a motor or a trailer car, on the left or the right hand side of the caliper, etc. This technique is also resilient to abrupt measurement changes due to asset replacements, sensor imprecision, and acquisition failures, while retaining the physical evolution of the wear, which erodes the surface of the pad. This article evaluates the effectiveness of this approach with a dataset of brake pad thickness measurements, at the ﬂeet level (around 12000 asset instances), using a sliding window technique, and reﬁnes its performance with a neural network ensemble, which blends physical and location features. The results of the analysis prove that this method meets the requirements of the maintenance staff and thus yields a new avenue for business improvement through the application of the predictive maintenance approach for brake pads."
10.36001_phmconf.2018.v10i1.465,dataset,43,,0,"• Fine-grained: the goal is to calculate a reﬁned EC value and maximise the specialisation of the network. This is attained with a slow learning rate of 0.1, and a greater number of training dataset iterations up to 10000."
10.36001_phmconf.2018.v10i1.465,dataset,60,,0,"• Coarse-grained: the goal is to calculate an approximation to the corrected EC value and maximise the generalisation of the network. This is attained with a fast learning rate of 0.5 that is checked to avoid cost overshooting, and a maximum number of training dataset iterations of 300 to consider early stopping and thus prevent overﬁtting."
10.36001_phmconf.2018.v10i1.474,dataset,104,,0,"Alternatively, these datasets may be simulated using flight simulators that have the ability to simulate realistic flight scenarios with specified faults. In this work, the commercial desktop flight simulator X-Plane-11 is used to simulate the four-engine Boeing 747-400, which is a wide-body commercial passenger jet airliner, at a constant altitude cruising flight phase with realistic weather conditions and faults. The simulation conditions are representative of a real flight phase. Once the aircraft is cruising at the desired flight level various autopilot controls maintain the aircraft speed, heading, and altitude that are programmed using the in-plane"
10.36001_phmconf.2018.v10i1.474,dataset,216,,0,"A total of 458 flights from the FDR datasets are investigated in this study. The parameters in the cruising phase are extracted; same parameters and altitude as mentioned in Section 3.1 are selected for a direct comparison with the simulated dataset; this results in a sample size of 732,000. Due to the high dimensionality of the dataset, the scale of the is very small. global performance probability value Therefore, the distribution of global performance probability is logarithmic scale for enhanced visualization, as shown in Figure 3. It should be noted that the F-1 score based cross-validation method is not possible to be used to find the threshold of this dataset, since the status of health of the dataset is not labeled. Therefore, to investigate the cases that possess the largest deviation compared with the normal operations, a threshold of -200 is selected, and the distribution of samples that fall out of this margin are illustrated in Figure 3 in a zoom-in plot. The sampling points whose probabilities are less than -200 in logarithmic scale are regarded as the moments when the aircraft are under abnormal operations; such sample points are found to be in three out of 458 flights."
10.36001_phmconf.2018.v10i1.490,dataset,131,,0,"Given an optimal solution c∗ to the optimization problem (2), we can then perform A(c∗, {(X tr i=1), which returns an ML model M ∗, our desired ML model that has a low out-ofsample error (In particular, a minimum prediction error on the i )}N va validation dataset {(X va Unfortunately, the optimization problem (2) is usually intractable. Indeed, the search space is typically large. In addition, the function OOSV is typically not convex nor monotonic in c. Moreover, the evaluation of OOSV could be computationally expensive. For example, in the case of MLP, the evaluation requires training deep neural networks on the"
10.36001_phmconf.2018.v10i1.509,dataset,89,,0,"SVMs are an advanced computational learning technique which is based on statistical learning theory and have been utilized effectively in many real world classiﬁcation problems like image classiﬁcation, Bioinformatics classiﬁcation, and hand written character recognition (Widodo & Yang, 2007). The primary beneﬁt of SVM classiﬁcation and the reason it is employed in this work is because it is designed to be very effective in dealing with large datasets where the dimension of feature vectors does not impact the performance of the SVM."
10.36001_phmconf.2019.v11i1.598,dataset,90,,0,"Håkon Jarle Hassel received his M.Sc. in the field of artificial the Norwegian University of Science and Technology (NTNU) in Trondheim, Norway 2017. He previously received his B.Sc. in computer from Sør-Trøndelag University College (HiST) in Trondheim, Norway 2015. He started working for Idletechs in 2017, where he provides analysis support to customers and assists in running experiments and software development. He particularly enjoys working together with domain experts to uncover physical variation patterns contained in datasets."
10.36001_phmconf.2019.v11i1.785,dataset,9,,0,Figure 4. The available dataset of battery voltage
10.36001_phmconf.2019.v11i1.785,dataset,97,,1,"proposed model tackles the capacity degradation as the traveling distance of a Brownian particle in a given time interval. Then, the PF is used to estimate the drift parameter of the Brownian Motion. The NASA Ames Prognostics Center of Excellence Battery Dataset was used. In (Yan, Zhang, Zhao, Weddington & Niu, 2017), an approach based on Lebesgue Sampling is utilized for Li-Ion batteries RUL estimations. The proposed approach accommodates properly the changing of fault dynamics and provides precise mean RUL estimations including uncertainty quantification."
10.36001_phmconf.2019.v11i1.785,dataset,140,,0,"It is evident, that the mean RUL predictions of a battery which is not outlier converge quite satisfactorily with the real RUL values (Figure 7). The confidence bounds manifest a desirable behavior, which is to converge as time progresses and is reached. The performance of the NHHSMM in the left outlier dataset shown last in Figure 8 is interesting. Even though the battery in that mission discharged quite rapidly as compared to all other missions, and despite the fact that in this case’s training dataset there is no similar behavior, the NHHSMM manages to capture the trend relatively well although it overestimates RUL at all times. It’s interesting to note that despite the very bad initial predictions, as time passes predictions tend to improve."
10.36001_phmconf.2019.v11i1.790,dataset,47,,0,"However, 6 of the PSFs we deﬁned contain context-speciﬁc information, with levels of important detail that the CREAM CPC defaults would approximate with too broad a brush. Three of these could be addressed through statistical analysis of existing MWO datasets—due to their importance in"
10.36001_phmconf.2019.v11i1.792,dataset,8,,0,4.3. Application of pipelines to manufacturing dataset
10.36001_phmconf.2019.v11i1.792,dataset,25,,0,This section illustrates the application of predictive models described in the previous section on a manufacturing dataset and presents the performances of the models.
10.36001_phmconf.2019.v11i1.792,dataset,27,,0,"With this generic methodology in place, we now describe a case study that illustrates the application of the methodology to analyze a real manufacturing dataset."
10.36001_phmconf.2019.v11i1.792,dataset,31,,0,Figure 1. Distribution of times for the dataset. Various timescales are indicated using markers. (The y-axis represents the kernel density estimate for the time values).
10.36001_phmconf.2019.v11i1.792,dataset,53,,0,"This work utilized a dataset from the domain of manufacturing maintenance. Similar analyses can be performed on MWOs from other domains, such as aerospace, shipping, heating ventilation and cooling (HVAC), to identify common and domain speciﬁc parts of language that relate to the maintenance duration."
10.36001_phmconf.2019.v11i1.792,dataset,55,,0,"A need exists to further analyze the rich yet under-explored knowledge of MWOs. In particular, there is a need to capitalize on the potential usefulness of capturing and understanding time-durations of maintenance activities. This paper, as described in the next section, discusses two potential methods to analyze MWO datasets."
10.36001_phmconf.2019.v11i1.792,dataset,91,,0,"The MWO dataset used for this study was sourced from a real automotive manufacturer and consisted of 47,798 MWOs. The dataset is in a spreadsheet format, and some of the ﬁelds are Workorder Number, Status, Actual Start Date and Time, Actual Finish Date and Time, Asset Number, Textual Description, Location and Reported By. More details about information contained in MWOs can be found in (Brundage, Morris, Sexton, Moccozet, & Hoffman, 2018). The most important"
10.36001_phmconf.2019.v11i1.792,dataset,93,,0,"The machine learning models can be improved and more generalized observations can be derived. This is possible with larger and varied datasets and more tagging on datasets (such as greater time spent on tagging and tagging of bigram phrases). It could lead to generalized observations of tags that are indicative of maintenance domain. For example, some terms might relate to expensive or time consuming solutions, such as needed a replacement part. This could potentially contribute to language standards for MWO recording practices in maintenance."
10.36001_phmconf.2019.v11i1.792,dataset,98,,0,"The rest of the paper is structured as follows. The state of the art for natural language analysis in manufacturing and current time metrics in maintenance is covered in Section 2. Research challenges and overall methodology for carrying out the study is described in Section 3. A use case showing the implementation of the methodology on a manufacturing MWO dataset is demonstrated in Section 4. Results and issues from the analysis, and scope for extending this research are discussed in depth in Section 5, and conclusions are presented in Section 6."
10.36001_phmconf.2019.v11i1.792,dataset,102,,0,"In preparation for the analysis portion of this work, the nonuniformity of the dataset presented some unique challenges that are likely to be common in real world applications. Every MWO dataset has its own characteristic ﬁelds, such as Asset Number, Workorder Number, Problem Description, Requested By, Solved By, Opening Time and Cost Incurred. For this research, the text descriptions of the problems and actions taken and time-related ﬁelds are of interest. The text descriptions were annotated with tags using the methods described in (Sexton et al., 2017)."
10.36001_phmconf.2019.v11i1.792,dataset,151,,0,"A combination of human annotation and computer assistance in the form of Nestor is used to identify the various problem, solution and item features contained in the MWOs. The produced tags, or ideological concepts, are clean representations of noisy unstructured MWO text. For example, Replace, which is an alias for all related indications (e.g., Repalce <=> Replace <=> Replacing), is tagged as a SOLUTION. Use of tagged words lowers the variations in morphological forms and spellings as compared to raw MWO text, since a human has clariﬁed their correct spelling with the correct alias. For example, in the dataset described in this paper (see Section 4.1), action words (verbs) extracted by Natural Language Processing resulted in 577 solution words, whereas tagging resulted in only 65."
10.36001_phmconf.2019.v11i1.806,dataset,68,,0,"Density Based Scanning of Applications with Noise (DBSCAN) is a well-known clustering algorithm that produces arbitrarily-shaped clusters formed by maximal sets of density-connected points (Ester, Kriegel, Sander, & Xu, 1996) (Louhichi, Gzara, & Abdallah, 2014). DBSCAN can reveal possible latent classes within a dataset given userspeciﬁed parameters for the minimum number of objects in"
10.36001_phmconf.2019.v11i1.815,dataset,108,,0,"Hyper-parameter tuning is performed for the techniques to obtain optimal values of hyper-parameters and the best possible anomaly detection models. For PCA, the number of principal components explaining 95% of variance is considered. For OCSVM, RBF kernel was used. Parameters, gamma & nu are tuned in the ranges 1-9 to 13 and 0.01 to 0.2 (incremented by a factor of 10) respectively. In IF, the number of base estimators is tuned in the range 80 to 200 in increments by 10; the proportion of outliers (contamination) in the dataset was optimized in the range"
10.36001_phmconf.2019.v11i1.815,dataset,139,,0,"for anomaly detection and diagnosis on an interacting quadruple tank system and a continuous stirred tank reactor system. We studied the anomaly detection performance of PCA, MD, OCSVM, IF, EE, Dense-AE and LSTM-AE in semi-supervised mode. Empirical studies on two industrial datasets demonstrated that MD and LSTM-AE have the highest anomaly detection capability, followed by PCA and OCSVM. For detected anomalies, variables responsible for each of the anomalies are diagnosed using PCA, MD and LSTM-AE techniques. All three techniques provide very similar diagnosis results on both datasets. This study helped conclude that statistical anomaly detection and diagnosis techniques deliver results comparable to more complex ML/DL techniques, and may be therefore considered alongside ML/DL techniques for anomaly detection and diagnosis in manufacturing and process industries."
10.36001_phmconf.2019.v11i1.830,dataset,27,,0,"the most in the equipment. This should be taken into the amount of dataset and account, communication the to performance of models and algorithms."
10.36001_phmconf.2019.v11i1.838,dataset,203,,0,"size of WTs which increase the Operation and Maintenance (O&M) cost. Prognostic is a part of the PHM strategy and focus to deﬁne the remaining time to a failure occurrence, named the Remaining Useful Life (RUL). It is estimated by using the HI, where the HI represents the evolution over time of the system performances or conditions, it is used to predict the degradation evolution and estimate the RUL. RUL is estimated based on two main approaches (Abid, SayedMouchaweh, & Cornez, 2018): Experience based (reliability, similarity), and degradation modeling based (model, datadriven) approaches. Experience based approaches require a large datasets about the degradation dynamics, which is not often available in industrial systems. It is difﬁcult to apply model based approaches in real complex systems, due to the interaction between different components and the environmental variations. Despite the lack of interpretation, datadriven approaches are the most suitable methods for the prognostics of industrial complex systems. These methods offer a trade-off in terms of applicability, precision, implementation, and cost."
10.36001_phmconf.2019.v11i1.842,dataset,2,,0,raw datasets
10.36001_phmconf.2019.v11i1.842,dataset,9,,0,3. Linking software executions with datasets: Software
10.36001_phmconf.2019.v11i1.842,dataset,14,,0,Authorization: The CSMD model can specify access controls on investigation and datasets.
10.36001_phmconf.2019.v11i1.842,dataset,33,,0,Dataset: Research projects can include more than one dataset on which diverse and multiple samples are analyzed. Research activity can include raw datasets on which analyzed datasets are subsequently inserted.
10.36001_phmconf.2019.v11i1.842,dataset,42,,0,"executions are linked to input datasets and output datasets from an execution of a program and associating multiple software executions to input datasets extended to to many relationships between investigator and dataset, to capture their context in the provenance process."
10.36001_phmconf.2019.v11i1.842,dataset,47,,0,4. Associating parameters with a software execution:  Parameters must be linked with no less than one of many possible programs [can take zero or more parameters] executions corresponding to unique datasets but with assorted output datasets and runtime parameters. 5. Re-introducing
10.36001_phmconf.2019.v11i1.842,dataset,50,,0,"Parameter: Defines explicit aggregates like pressure, temperature, volume or scattering angle connected to the research. These aggregates can be used to describe sample parameters, study environment or the variables being measured. Parameters can be linked at various levels with datasets and metadata elements."
10.36001_phmconf.2019.v11i1.861,dataset,6,,0,2.2. Problem Deﬁnition and Dataset
10.36001_phmconf.2019.v11i1.861,dataset,37,,0,"For the Nashville dataset, we have 3,724 unique TMCs. For each TMC we have collected speed values for a total of 6000 timesteps. Each timestep speciﬁes a small time interval of 10 minutes."
10.36001_phmconf.2019.v11i1.861,dataset,133,,0,"To compare which model is producing a better result, we have run the model with their optimal number of neurons and timesteps. Based on our experiments, the optimal number of neurons for RNN and LSTM is 80 and 180 respectively. RNN works the best with 10 timesteps and LSTM with 15 timesteps. So, we ran both the models for the ﬁrst 100 TMC to see which delivers the best result. Figure 4 shows the losses for the ﬁrst 100 TMCs. It is visible in the ﬁgure that LSTM produces the least loss in most cases. The average loss from RNN is 7.04 × 10−4, and average loss from LSTM is 6.55 × 10−4. So, LSTM works best for this dataset."
10.36001_phmconf.2019.v11i1.869,dataset,138,,0,"Motivated by these challenges, a real-time monitoring framework for aircraft engine systems using the deep neural network (DNN) is presented. A historical flight dataset recorded from onboard sensors in commercial aircraft is utilized the proposed method and detect performance anomalies in the aircraft engine system. Signal processing using decimation and Savitzky-Golay filter method is conducted to preprocess the flight dataset. Subsequently, the DNN model is trained by the processed flight features to estimate normal engine system behavior of commercial aircraft associated with flight dynamics. The potential anomalies are detected when the estimation error exceeds pre-defined rational bound due to significant deviation in monitoring features. Demonstration of anomaly detection is performed with simulated historical flight dataset in real-time to validate the real-time detection capability of the proposed monitoring method."
10.36001_phmconf.2019.v11i1.869,dataset,196,,0,"Subsequently, is validated through a demonstration of anomaly detection in the FDR dataset. In particular, the selected features during cruise phase are simulated for real-time analysis. The detection time with a duration of anomalies in engine performance and flight dynamic features are analyzed. As shown in Figure 4, sudden changes in engine performance features are detected during cruise phase. It should be note that four features in each engine performance feature are averaged for visualization. For instance, four features in PLA (i.e., PLA 1-4) are averaged and then denoted as PLA in this work. In general, the value of engine performance features such as PLA, FS, CS, and OIT are relatively stable during cruise phase compared to the other flight phases. However, a sudden drop in PLA, FS, and CS is detected at 1749 seconds with a duration of 22 seconds (Figure 4 (a)-(c)) while a sudden increase in OIT is captured at 1769 seconds with same anomaly duration (Figure 4 (d))."
10.36001_phmconf.2019.v11i1.869,dataset,214,,0,"This paper presents a real-time aircraft engine health monitoring framework using DNN, which detects flight performance anomalies during cruise flight operation. A historical flight dataset recorded from multiple onboard sensors in aircraft was utilized to perform the proposed method. Signal processing technique called decimation, and Savitzky-Golay filter was used to synchronize different sampling frequencies and reduce inherent noise in the FDR dataset. The DNN model was adopted to estimate normal engine system behavior related to flight dynamics and trained by the preprocessed features, including engine performance and flight dynamic features. The accuracy of the trained DNN model was evaluated by error measurements (0.0148 RMSE and 0.8432 𝑅2) and the estimation showed good agreements with the features in the dataset. In order to perform anomaly detection, the trained model was combined with a normal estimation error bound for a safety baseline. The real-time detection capability of the proposed monitoring framework was validated through the FDR dataset simulated for realtime and the results showed promising ability in detecting anomalies in engine performance and flight dynamic behavior. Future work will include a development of a monitoring framework that can monitor the whole aircraft system in all flight phases to improve aircraft safety management."
10.36001_phmconf.2019.v11i1.876,dataset,220,,0,"In addition to voltage protection, complexity was added by including multiple linear capacity aging rates to represent healthy and faulty capacity fade over the lifetime of the battery. Capacity aging was deﬁned as a time-dependent adjustment to the battery capacity as shown in Eq. (1) with the aging factor K deﬁned in Eq. (2). For the simulations, normal capacity aging was set to reach our failure criteria, 70% of the initial capacity C0, in approximately 3.1 years, that is Cf =70%. This typical time-based aging is shown as the healthy slope in Figure 3, interrupted by the accelerated slope due to a simulated fault. Low-rate degradation always accompanies normal battery operation, therefore we labeled this region as healthy. This region was used to train the autoencoder (see Section 4). The slope was adjusted by the damage factor md in the faulty region which started at the onset of failure point tonset, set at 62.5 hours in our simulations. As listed in Section 6, Table 2, multiple slopes above and below the baseline of md = 400 were considered in our simulation to get a variety of datasets for testing the autoencoder model."
10.36001_phmconf.2019.v11i1.898,dataset,53,,0,"Aircraft Condition-Based Maintenance (CBM). AD for Maintenance Operations was motivated by the possibility that large flight datasets could be useful in understanding undesirable events like aircraft failures and need for scheduled maintenance. Maintenance activities ensure that the system does not fail, is functional and available for operation."
10.36001_phmconf.2020.v12i1.1130,dataset,24,,0,The test results for the front faulty rotors are shown in Figure 6. There are totally 725 test datasets from the healthy rotors
10.36001_phmconf.2020.v12i1.1130,dataset,33,,0,The test results for the faulty rear rotors are shown in Figure 7. There were in tola 725 test datasets from healthy rotors and 90 test datasets from faulty rear rotors.
10.36001_phmconf.2020.v12i1.1134,dataset,6,,0,3. CREATION OF THE DATASET
10.36001_phmconf.2020.v12i1.1134,dataset,13,,0,𝒟𝒟 = 𝑿𝑿� Figure 5. Overview of procedure for creating dataset.
10.36001_phmconf.2020.v12i1.1134,dataset,38,,0,Figure 5 is an overview of the procedure for creating the dataset. The first step divides the static-firing test results into results from sensors that can be modeled by the SLS  and from those that cannot
10.36001_phmconf.2020.v12i1.1134,dataset,126,,0,"The sequence in Fig. 3 is strongly affected by the valve behavior. The flow direction around the pump in Fig. 4 is different between the combustion and chill-down phases as well. Res2 in Fig. 4 measures the temperature of the outside surface of the bearing, not the fluid temperature. Res3 measures the fluid temperature after it flows through the impellers and complicated channels, and it is affected by heat transfer from thermal mass especially during the chill-down phase. This study mainly aims to investigate whether the regression model can complement the dataset using a simple linear regression model, Ridge regression, but plans on using a nonlinear regression model to improve the prediction accuracy further."
10.36001_phmconf.2020.v12i1.1162,dataset,155,,0,"• For each target variable vt, we start with all the variables, and see if they can estimate vt accurately (there exists a machine learning model that can estimate vt using the variables with the required accuracy). If they cannot, it means there is no residual in the dataset for vt as even all the variables cannot estimate vt. In this case, we move to the next variable. If they can, all the variables plus vt create an ARR. However, we cannot be sure that this ARR is minimal. For each variable in V , we remove the variable and check if the subset still can estimate vt, if no subset of V can estimate vt, it means V plus vt is a minimal ARR. We save this ARR and move to the next variable."
10.36001_phmconf.2020.v12i1.1288,dataset,7,,0,Dataset Degradation Mode 𝑁𝑡𝑟𝑎𝑖𝑛 𝑁𝑡𝑒𝑠𝑡 100 FD001
10.36001_phmconf.2020.v12i1.1288,dataset,13,,0,Figure 9. Degradation mode detection of C-MAPSS dataset training samples using PCA+Kmeans
10.36001_phmconf.2020.v12i1.1288,dataset,47,,0,"In this paper, the proposed methodology is illustrated in the aero-engine RUL prediction problem using C-MAPSS dataset. The details of the dataset is summarized in Table 1, where Fan degradation and Higher Pressure Compressor (HPC) degradation are two failure modes in different"
10.36001_phmconf.2020.v12i1.1288,dataset,49,,1,The rest of the paper is organized as follows. Section 2 gives an overview of the previous studies about the failure thresholding. Section proposed methodology. Section 4 illustrate the benchmarking results based on the public datasets. The conclusion remarks are given in Section 5.
10.36001_phmconf.2020.v12i1.1300,dataset,115,,0,"In the PHM context, (Qian, Li, Yi, & Zhang, 2019) have proposed using Kullback-Leibler (KL) divergence as a criteria to measure the discrepancy between the source and target domain datasets. They have deﬁned a loss function that is the sum of ﬁrst to nth order moment discrepancies between the two domains. This term is fused into the objective function of the simultaneous training of the source and target fault diagnosis networks to be minimized. This way, the networks would learn to ﬁnd domain independent features that helps learning the classiﬁcation task in target domain using the labels in the source domain."
10.36001_phmconf.2020.v12i1.1300,dataset,135,,0,"One of the most studied applications of TL is computer vision and visual classiﬁcation problems, especially using deep Convolutional Neural Networks (CNN). All of the identiﬁed inductive TL studies have used images and CNN in their approaches. For example, (Gao & Mosalam, 2018) have used inductive TL for training deep CNN for image-based structural damage recognition. In this study, the low-level feature extractors from the VGG-16 (Simonyan & Zisserman, 2014) model that is trained using the ImageNet dataset (Deng et al., 2009) for classifying different structures (walls, bridges, buildings,etc.) are transferred to the relatively smaller target domain (structural ImageNet) to discern structures with spalling from the healthy ones."
10.36001_phmconf.2020.v12i1.1300,dataset,172,,0,"Considering the explained architecture and TL techniques one can imagine applying TL to prognostics and Remaining Useful Life (RUL) prediction as well. As demonstrated by (A. Zhang et al., 2018), a predictive deep learning model such as a Long Short Term Memory (LSTM) network can be trained for a turbofan engine RUL prediction (using CMAPSS datasets) in one operating condition and then be transferred to another operation condition using the same architecture that is shown in Figure 1 (some parameters are kept and some are retrained). Also, using the same dataset, (da Costa, Akc¸ay, Zhang, & Kaymak, 2020) has proposed training a LSTM model for RUL prediction of turbofans with a subtle difference that no labels are used for training in the tar get domain. To classify the unlabeled target domain, LSTMDANN is architecture is proposed which is built upon the explain DANN architecture in Figure 2."
10.36001_phmconf.2021.v13i1.2983,dataset,47,,0,"using OpenModelica 1.13 on Windows in 64-bit. The dataset of the simulation contains 1939 variables of which only a few have been selected for this running example by ﬁltering the signal names to only those which were associated with temperature, ﬂow, and water level"
10.36001_phmconf.2021.v13i1.2983,dataset,181,,0,"The simulation of the four-tank system created with OpenModelica 1.13 contained 1939 signals including all Modelica-internal variables. Of these, 18 were identiﬁed as measurement variables, by ﬁltering the signal names to only those which were associated with temperature, ﬂow, and water level. Given the size of the dataset we argue that our approach shows good results as can be validated on the running example and by choosing a reasonable value for τ . Table 2 contains the results for system 2 over different thresholds. The ﬁrst row shows, whether the generated rules were suitable for diagnosis. The second row shows whether the diagnosis algorithm was able to determine the injected faults. Again, faults were injected through randomly setting some signals in set M to false, which simulates wrongly closed valves or a leaky tank. This evaluation shows that apart from injection molding machines, which perform very well for this approach, the standard four-tank systems established in the literature perform quite well given this uninformed approach."
10.36001_phmconf.2021.v13i1.3000,dataset,91,,0,"tonomous condition monitoring application, Hoyer index can be said to be the most useful, since it is scaled in between zero to unity. For the vibration signal dataset investigated in this study, the last measurement where the Hoyer index reached a value around 0.8 is already the one where the machine failed. Considering that, further studies may be performed to set a threshold to assess the severity of the damage and to warn the end-user of the potential degradation of a component of interest."
10.36001_phmconf.2021.v13i1.3003,dataset,124,,0,"After demonstrating the potential of autoencoder-based anomaly detectors the next step in the PHM development will be to examine their potential for the next level of capability, damage assessment. In addition to understanding the conditions that give rise to spontaneous damage assessment, the pre-trained autoencoders will be ﬁne-tuned, using transfer learning to learn damage level and crack-propagation In adsensors as the ground truth of damage progression. dition, alternative models using RNNs (LSTMs or GRUs) or transformers will be explored for predicting damage, by using a subset of damage progression for training and the rest for validation. The team is also working to prepare the dataset to be shared with the gear research community."
10.36001_phmconf.2021.v13i1.3003,dataset,197,,0,"Figure 11 depicts the autoencoder MSE errors associated with the eight baselines and single propagation datasets. This speciﬁc asymmetric autoencoder employed only ﬁve fully connected layers (associated neurons are 4096-256-64-16-10244096) and ReLU activation function for the hidden layers, was trained on baselines 1, 3, and 6, and evaluated on all baselines and the propagation. Note that the topology of the autoencoder is asymmetric: the encoding sub-network, deﬁned by 4096-256-64-16, has three layers of weights, while the decoding sub-network, deﬁned by 16-1024-4096 has two layer of weights. This topology was found through experimentation and was selected by its ability to create error that tracks damage level, as further described at the end of this section. The best performance was attained using Adam as the optimizer, zero dropout (although values up to 20% were experimented with), learning rate of η = 10−4, and 100 epochs. The plots show that the error associated with baselines not involved in training is large than those that were used in training, but the propagation error is still larger."
10.36001_phmconf.2021.v13i1.3007,dataset,51,,1,The rest of the paper is organized as follows. Section 2 gives a literature review of the Copula Model and its application. Section 3 elaborates the proposed methodology. Section 4 illustrates the benchmarking results based on the public datasets. The conclusions are given in Section 5.
10.36001_phmconf.2021.v13i1.3007,dataset,90,,1,This paper proposes a Gaussian Copula model based multivariate risk quantification model to determine the failure threshold for RUL prediction tasks. A systematic methodology for RUL prediction is proposed based on the failure threshold determination method and the similarity enhanced RBPF extrapolator. The work discussed the pros and cons of the proposed risk quantification method with a tri-variate and a bi-variate example. The obtained bi-variate failure threshold is further validated on the aero-engine RUL prediction task using the C-MAPSS dataset. Several essential conclusions are achieved:
10.36001_phmconf.2021.v13i1.3007,dataset,143,,1,"The proposed methodology for risk quantification and failure threshold determination is validated on the RUL prediction task on the C-MAPSS dataset. The prediction performance of proposed methods shows superiority over benchmarked methods from some recent literature in terms of Root Mean Squared Error (RMSE) and Score calculated using Eq.(9). deep The Convolutional Neural Network (DCNN) and Long ShortTerm Memory (LSTM) from (Li, Ding, & Sun, 2018), Deep Belief Networks (DBN), Multi-Objective Deep Belief Networks Ensemble (MODBNE), Random Forest (RF), Gradient Boosting (GB), Support Vector Machine (SVM) and Least absolute shrinkage and selection operator (LASSO) reported from (Zhang, Lim, Qin, & Tan, 2016), and"
10.36001_phmconf.2021.v13i1.3009,dataset,8,,0,3.2. Generating a dataset for training decision-making
10.36001_phmconf.2021.v13i1.3009,dataset,19,,0,Figure 4. RUL vs RUL estimation for second test dataset using LSTM neural networks. R2 = 0.42
10.36001_phmconf.2021.v13i1.3009,dataset,24,,0,Figure 3. RUL vs RUL estimation for our ﬁrst test dataset (unit number 251 to 260). R2 = 0.53.
10.36001_phmconf.2021.v13i1.3009,dataset,31,,0,"average in the second test dataset (see the purple line). This is because in the second dataset, the episode often ends before reaching its failure point."
10.36001_phmconf.2021.v13i1.3009,dataset,55,,0,"Figure 6. Average cumulative rewards as a function of expected reward for the second test dataset.The maximum average cumulative rewards for the model with RUL achieves at the total expected reward = 75, and the maximum average cumulative rewards for the model without RUL achieves at the total expected reward = 96."
10.36001_phmconf.2021.v13i1.3009,dataset,57,,0,"Figure 5. Average cumulative rewards as a function of expected reward for the ﬁrst test dataset. The maximum average cumulative rewards for the model with RUL achieves at the total expected reward = 79, and the maximum average cumulative rewards for the model without RUL achieves at the total expected reward = 100.5."
10.36001_phmconf.2021.v13i1.3009,dataset,57,,0,We can see in Figure 6 that the difference between the perfect action and the action based on RUL estimation is wider in the second dataset compared to the ﬁrst test dataset. This is because the RUL estimation in the second dataset is less accurate and thus makes decisions based on RUL estimation less reliable.
10.36001_phmconf.2021.v13i1.3009,dataset,90,,0,"the high cost of failure. The blue line in Figure 5 shows this scenario. We achieved 88.3 average cumulative reward using this approach for the ﬁrst test dataset. Of course, this is not a practical solution because it requires the knowledge of exact RUL. An alternative option would be to use the RUL estimation instead of the actual RUL. The orange line in Figure 5 shows this scenario for the ﬁrst test dataset. We achieved 82.7 average cumulative reward using this approach."
10.36001_phmconf.2021.v13i1.3009,dataset,109,,0,"Note that the ofﬂine RL outperforms decisions based on RUL estimation in the second test dataset (see Figure 6). Figure 6 shows that the performance of the model with the RUL estimation peaked at total requested rewards equal to 75 and the performance of the model without RUL estimation peaked at total requested rewards equal to 96. However, both models perform better than the decision based on RUL estimation for a fairly wide range of total requested rewards. This presents a huge potential for ofﬂine RL in maintenance decision-making, especially for real-life problems when estimating RUL is not a trivial task."
10.36001_phmconf.2021.v13i1.3059,dataset,129,,0,"We deﬁne the initial formulations of these algorithms as the original formulation. However, these original formulations can be invariant to some signal transformations, particularly in large datasets. Thus, we also adopt the imaging encoding formulations proposed by Rodriguez-Garcia et al. (2021), which we refer to as the modiﬁed formulation. Under this modiﬁed formulation, GASF/GADF uses a dataset-wide rescaling step instead of instance-wide rescaling. Meanwhile, in MTF, the width of the bins is proportional to the quantiles of distribution centered in the mean values instead of uniform width. Likewise, the mean value is considered in the estimation of the transitions for the RP encoding, which is not considered under the original formulation."
10.36001_phmconf.2021.v13i1.3069,dataset,1,,0,dataset
10.36001_phmconf.2021.v13i1.3069,dataset,4,,0,4.1. Dataset Description
10.36001_phmconf.2021.v13i1.3069,dataset,40,,0,"where represents the true label. It can be seen that focal loss function introduces . These hyperparameters can make loss focus on the learning of fault samples, so that the model has excellent performance on unbalanced datasets."
10.36001_phmconf.2021.v13i1.3069,dataset,41,,0,"In this section, the dataset and experimental settings are firstly elaborated. Then, the effectiveness of the proposed method is verified. Finally, the comparative experiments verify that the WT-GAN has excellent performance on unknown wind turbines."
10.36001_phmconf.2021.v13i1.3069,dataset,116,,0,"It can be seen from Section 3.1 that, the loss function of WTGAN consists of two parts, namely FLy obtained by the label classifier and Ld obtained by the domain discriminator, as shown in Figure 1. In the network training process, two tasks are required to be finished. The first one is to achieve accurate classification of the source domain dataset, that is, to minimize FLy. The second task is to try to confuse the source domain dataset and the target domain dataset. That is to achieve the maximization of Ld of the domain discriminator. The loss function of WT-GAN can be defined as:"
10.36001_phmconf.2021.v13i1.3069,dataset,148,,0,"constructed. Compared with the only difference is that the WT-CNN has no domain transfer capability. The WT-CNN is mainly composed of the feature extractor and the label classifier presented in Figure 1. The experimental settings and the training parameters are exactly the same. 75% of the samples in the SCADA dataset of wind turbines #15 and #21 is used for training, and the remaining 25% for testing. Therefore, for wind turbines #15 and #21, their training samples are 20641, and their test samples are 6880. In this experiment, the batch size is 128. In order to avoid the randomness of the experimental results, each experiment is repeated for five times, and its average and standard deviation are calculated. The experimental results are shown in Table 3."
10.36001_phmconf.2015.v7i1.2564,"dataset provided, data",210,,0,"Also the process of identifying an effective selection of mutation operators/functions to be used for some project will be a target of our future research. That is, mutation testing relies on the Competent Programer Hypothesis and the Coupling Effect, which assume that (a) a faulty program is close to the correct one and suggest that (b) a test suite that catches simple mutations is also effective at catching more complex faults (see e.g. the discussion in (Offutt, 1992)). Newer ﬁndings like the discussion in (Gopinath, Jensen, & Groce, 2014) investigate the impact of the selection of mutation operators on the performance for a speciﬁc project, and suggest that for a speciﬁc project, further empirical data should provide a solid empirical footing for the underlying hypotheses’ validity. In this context, we limit our algorithms currently to injecting single faults, where the accommodation of multiple faults for more complex scenarios is subject to future work. In the latter respect, combinatorial testing like it was used in (Wotawa & Pill, 2014) for conﬁguration testing in an automotive context will be of interest."
10.36001_phmconf.2015.v7i1.2655,"dataset provided, data",79,,0,"As described in the previous section, our data is highly imbalanced between normal and abnormal classes (with majority-to-minority ratio of approximately 150), which deserves a special attention in classifier modeling. In literature there are many different strategies handling imbalanced data. He and Garcia (2009) provided a comprehensive review of different imbalance learning strategies. In this paper we take advantage of ELM’s capability of weighting samples during learning."
10.36001_phmconf.2015.v7i1.2676,"dataset provided, data",89,,0,"Analyzing the data provided in Table 2 it must be noted that, independently of the inﬂuence range associated with each particle, as the resolution becomes smaller the AUC of the prediction model improves. This result is intuitive since it implies that crime could be better predicted if every block is monitored independently. However, police resources are limited, and there is an optimum AUC subject to that constraint. However, there is an optimum inﬂuence range in terms of the model performance."
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",83,,0,Custom analytics provide the means of generating more sophisticated health indicators from the raw data. These health indicators provide actionable information in the following ways: diagnostic and or prognostics indicators augment the raw report within ASHM’s report viewer; alerts based on computed parameters are displayed alongside the ones based on raw reported parameters; and finally computed parameters can be included in the ASHM graphs pages to be monitored and observed visually for trends or anomalous behavior.
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",84,,0,"operational state values or calculated values that have been derived from the raw parametric data. Often, the aircraft system data is reported along with aircraft level data such as altitude, air temperature, and Mach number that provide important context about the operating conditions at the time of acquisition. The data may be acquired as a single snapshot in time, a set of statistical metrics such as mean or peak value, or as a time series history."
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",94,,0,"The transition from engine driven hydraulic subsystems to distributed electric motor driven subsystems has led to ACMF reports that capture characteristics from a wide array of systems despite being targeted to one component. A smart motor controller may now be responsible for 3 or more different tasks throughout the duration of a single flight. These tasks range from air management, to motor start procedures. The case study presented here will examine the complexities of the Boeing 787® motor controller system and its effect on the interpretation of ACMF data."
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",104,,0,"ASHM and the associated Data Analytics Tool also provide actionable information for a different audience with a longer term teams responsible for supporting fielded systems as well as new product design. ASHM aggregates operational field data that is invaluable in terms of closing the loop between Engineering and Field Support. It provides unprecedented visibility into how the systems that Engineering designs are operating in the field. This allows a closed loop refinement of all the assumptions made at design time, to both improve the current product offering and enable better design assumptions for the next generation of new products."
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",127,,0,"A case study for the Boeing 787® Common Motor Start Controller subsystem illustrates how trending and alerting on raw data alone is not enough for effective aircraft system monitoring. A system connectivity and operation states is required. The analytics developed for the CMSC subsystem follow three basic steps. First, they filter and down select the data. Each analytic is designed to target a specific system component. This filtering is achieved through the use of the operational mode flags, system connectivity information, and ACMF report information. Second, statistical power features, oil monitoring, temperature monitoring, and speed monitoring parameters are calculated from the data. These calculated features provide a summary of the target component during"
10.36001_phmconf.2015.v7i1.2710,"dataset provided, data",282,,0,"The Common Motor Start Controller (CMSC) ACMF reports contain parametric data on component power draw, fluid temperatures, power frequency, and active mode information. Among these reports, data is provided in two different formats; time series data and snapshot data. The time series data provides prognostic systems with valuable information regarding how a system reacts to applied power. With this data it is possible to measure spool up time for an engine, or when a starter generator is drawing maximum power. ACMF reports which contain time series data typically target vital aircraft procedures, like Main Engine start or APU start. The snapshot data, in contrast, allows maintainers to track and trend parameters and features over extended periods of time. ACMF reports, which contain snapshot data, capture data at key points throughout each portion of a flight, from Taxi to Landing. The snapshot data is generally divided into two capture methods, peak data value and average data value. Both of these data points are useful in application to component fault detection. While the ACMF data provides vital information about the functions of these subsystems, it is equally as important to understand the operational modes and connectivity of the CMSC subsystem. Without this understanding, the data loses its diagnostic and prognostic value. Figure 4 is a simplified representation of the CMSC component assignments. In this figure, each CMSC is connected to two unique components, Cabin Air Compressors (CAC) and Generators (G), which then provide power to critical systems."
10.36001_phmconf.2015.v7i1.2766,"dataset provided, data",44,,0,"(cloud based) PHM functionality to allow for growth and adaptation and enable data mining and analytics. In operation, provide timely and relevant health information to all participants in the value chain.  Gather feedback - both good and bad."
10.36001_phmconf.2016.v8i1.2523,"dataset provided, data",63,,0,"If data needs to be transmitted manually, for example, high volume of data in case of troubleshooting, an interface for customer should be provided to facilitate the transfer via a web portal. If customers need to integrate their own input and models, a data link should be provided. Open environment is a key to successful collaboration."
10.36001_phmconf.2016.v8i1.2542,"dataset provided, data",173,,0,"Future tests will reveal the effectiveness of the new IMUbased method for on-machine application through use of an ‘industrial IMU’. The IMU and the laser-based system (for V&V) will be utilized on various machine tools within the National Institute of Standards and Technology (NIST) Fabrication Technology machine shops. Metrics will be defined based on the collected data to quantify machine tool linear axis degradation, to inform the user of the magnitude and location of wear and any violations of performance tolerances. If the data collection and analysis are integrated within a machine controller, the process may be streamlined for the the optimization of maintenance, supporting development of self-diagnosing smart machine tools. When coupled with existing data exchange and formatting standards, verified and validated data from an ‘industrial IMU’ could provide manufacturers and machine tool operators with near-real-time equipment health, diagnostic, and prognostic intelligence to significantly enhance asset availability and minimize unscheduled maintenance."
10.36001_phmconf.2016.v8i1.2578,"dataset provided, data",239,,0,"Here, L(y|) is the likelihood of observed data y conditional on the given parameters , p(is the prior distribution of , and p(yis the posterior distribution of  conditional on y. The equation states that our degree of belief on the parameter  is expressed as posterior pdf in right of the given data y. As more data are provided, the posterior distribution is again used as a prior at the next step, and the values are update to more confident information. This is called Bayesian updating. The Procedure to obtain posterior distribution p(yconsists of proper definition of likelihood and prior probability distribution respectively (Leem et al., 2011). For estimating posterior distribution of the lethargy coefficient, Markov model is widely used in various fields in which sequence of the data is very meaningful. Markov chain consists of Markov model defines probability of posterior event given the prior events. The idea of MCMC basically the same as the Markov model in that it defines posterior position of the sampling point based on the prior information of the sampled points. Most important technique can be employed in MCMC, the Metropolis-Hastings (M-H) algorithm which is the most simplified MCMC method can be performed using Equation (18) by the following steps."
10.36001_phmconf.2017.v9i1.2456,"dataset provided, data",308,,0,"On the other hands, prognostics focus on predicting when failure will occur in the future and provides advantages to maintenance readiness in advance. Compared to diagnosis, prognostics still have many challenges to overcome. For an effective prognosis, extracting features which reflect degradation pattern and fault severity properly is the most important stage. And several works have been made in this sense. Siegel et al (2011) used the bearing vibration data from Bearing Prognostics Simulator provided by SpectraQuest and study feature trending using RMS, kurtosis and peak value at bearing fault frequencies acquired from envelope analysis.  Randall & Antoni (2011) published bearing diagnostics tutorial especially for envelope analysis and other signal processing techniques to enhance its performance. Li & Zheng (2008) utilized envelope spectrum analysis using Empirical Mode Decomposition (EMD) and Teager-Kaiser Energy Operator (TKEO) for bearing fault detection. Zhang et al. (2011) applied weight window function to envelope spectrum to detect the incipient bearing fault. However, most of them are limited to single failure mode and rarely consider comprehensive health index (HI). There are other approaches using data analytics and machine learning algorithms. Qiu et al (2003) employed Self-Organized-Map (SOM) and used Minimum Quantile Error (MQR) as a feature to obtain monotonic trend. Tobon-Mejia (2011) suggested bearing prognostics using wavelet packet decomposition (WPD) and these Hidden Markov Model (HMM). Even algorithms show good performance, it is lack of physical meaning and data analytics based algorithms needs many data sets. To overcome these limitations and extract effective feature, this paper suggests the feature extraction methods using correlation coefficient of envelope trends."
10.36001_phmconf.2017.v9i1.2470,"dataset provided, data",66,,0,"Real-time remote operation monitoring and event prediction of turbomachinery takes advantage of the four critical components: high-speed internet, big data, advanced analytics and domain expertise to provide a state-of-the-art digital solution for asset performance management (APM). It has become a key part of the Internet of Things (IoT) and industry internet ego. The remote monitoring and prediction"
10.36001_phmconf.2018.v10i1.245,"dataset provided, data",108,,0,"The ideal situation is to use different sources of data that provide independent perspectives on the performance of particular pieces of equipment. Out-of-the box templates are helpful sources of information with respect to providing exhaustive lists of possible failure modes and actions, which can help supplement the perspective of a domain expert.  Peer and historical maintenance data may be helpful with providing quantitative starting points for observed frequency and consequence of historical failure patterns.  Developing work processes in which analytics can automate and leverage different data sources in a workflow integrated with a human user can overcome many limitations associated with RCM/FMEA approaches."
10.36001_phmconf.2018.v10i1.464,"dataset provided, data",144,,0,"her, K. and Dersin, P. and Lamoureux, B. and Zerhouni, N., 2017). In this regard, ALSTOM has developed the TrainScanner, see Figure 1, which is a train monitoring system that is aimed at optimising the maintenance of brake pads, pantograph carbon strips, and wheelsets, through the deployment of the PHM methodology and its associated techniques. TrainScanner integrates a series of acquisition subsystems with lasers and 3D cameras that capture the related measures as a train traverses its portal. Then, it automatically conducts the processing and analysis of the collected data, and ﬁnally it triggers alarms and issues reports to the maintenance staff. This work is particularly focused on the brake pad prognostics that are attainable with the carbon thickness data provided by TrainScanner over time."
10.36001_phmconf.2018.v10i1.544,"dataset provided, data",130,,0,"One of these strategic thrusts is Real-Time System Wide Safety Assurance (NASA, 2017). Continuous efforts to reduce risk in commercial aviation over the last few decades have made it the safest mode of transportation. Yet, as aviation adopts new technologies to enhance the capacity, efficiency, and uses of the NAS, maintaining a safe system will require detection and timely mitigation of safety issues as they emerge and before they become hazards. Needed are capabilities to ensure safe operations in a more complex airspace through proactive detection, prognosis, and resolution of emergent threats to system-wide safety. Envisioned is a safety net that utilizes system-wide data to provide alerting and mitigation strategies in real-time to address emerging risks."
10.36001_phmconf.2018.v10i1.585,"dataset provided, data",115,,0,"The development of an intelligent, low-power health monitoring system carries with it a wide array of technical challenges that must be overcome to provide a suitable solution for equipment owners and maintainers.  First and foremost, development of an ultra-low power system capable of continuous operation requires a unique design, optimized using the appropriate components and operational methodology. Additionally, selection and implementation of suitable sensor technologies is critical for obtaining highly accurate and repeatable measurements while maintaining ultra-low power consumption operations.  Finally, efficient evaluation of operational status using embedded data and classification algorithms could provide insights into the system state and current usage of the monitored asset."
10.36001_phmconf.2019.v11i1.790,"dataset provided, data",36,,0,3E−3 Persuade - promote positive interaction of operator and tecnhician 1E−2 Control/Design - designate time as set aside for data-entry throughout the day 2E−1 Educate - provide high and low-level system architecture as reference material at
10.36001_phmconf.2019.v11i1.838,"dataset provided, data",165,,0,"The data provided by the article (Bechhoefer et al., 2013) are labeled as normal for the ﬁrst 10 days of the original vibration, where the remaining days from 11-50 are labeled as degradation data. 10 days corresponds to 10 samples of features which is not sufﬁcient for training the OCSVM, features are computed on a moving window of 6s with a moving step of 1.2s in order to augment the normal data samples for the training. Hence 50 samples of normal data corresponding to the 10 ﬁrst days are computed for each feature and used for the OCSVM training. For both the training and prediction, features are normalized (z-score normalization) using the mean and standard deviation of normal data. The predicted OCSVM score is then smoothed using the moving median with a window of 3 samples. The anomaly is detected when the score is below 0 (outside the boundary)."
10.36001_phmconf.2019.v11i1.876,"dataset provided, data",41,,0,"Because the focus was to assess anomaly detection capability of an autoencoder-based model, the model was trained on normal data to learn the internal representation. The details of data preprocessing and model development are provided in Section 4."
10.36001_phmconf.2019.v11i1.898,"dataset provided, data",142,,0,"This paper thus explores existing data-driven flight AD techniques, their applications, strengths, limitations and proposes a more robust hybrid AD technique to detect a wider range of significant anomalies aimed at enhancing flight safety. The paper is organised as follows. An introduction of FDM, the FDR data variability and FDA are provided in this section. Section 2 reviews the flight AD evolution, its architecture, aviation applications and its methods as well as the strengths and limitations of these methods. Section 3 summarises limitations in existing flight AD techniques and proposes a hybrid AD opportunity to mitigate some of them. Also, the application of AD to the area of Standard Operating Procedure (SOP) deviation detection was introduced as another opportunity. Lastly, section 4 concludes the paper."
10.36001_phmconf.2021.v13i1.2983,"dataset provided, data",80,,0,The article is structured as follows: The next section will analyze prior art and identify relevant research gaps. Section 3 will show how a typical industrial process is modelled using qualitative physics. Section 4 will use the created model to introduce a novel diagnosis methodology. The following section will present results using real data from an injection molding machine as well as provide a theoretical evaluation of the approach. Section 7 summarizes the ﬁndings.
10.36001_phmconf.2021.v13i1.2999,"dataset provided, data",153,,0,"To empirically validate the theoretical framework discussed above, the physically redundant MEMS accelerometer array was used to collect data in both healthy and faulty conﬁgurations. Two types of data were gathered for both healthy and faulty arrays. Impulsive data was collected by placing the MEMS array mount on open cell foam in the orientation shown in Figure 1. Open cell foam was used to isolate the sensor array from external vibrations and simulate free-free boundary conditions, the array was struck normal to the sensitivity axis. Separately, low bandwidth roller element bearing data, < 10 kHz valid bandwidth, was collected using a magnetic sensor mount placed on the case of a three phase induction motor with the sensitivity axis aligned radially with the roller element bearing. The impulse and motor vibration data provide real world signals similar to the synthesized signals studied previously."
10.36001_phmconf.2021.v13i1.3052,"dataset provided, data",38,,0,"Details of the training and testing sets for the random sampling data selection strategy were provided in section 5.1, including the two deteriorated time periods that were considered and individual vehicle and combined vehicle data sets."
10.36001_phmconf.2021.v13i1.3052,"dataset provided, data",49,,0,Optimization of hyperparameters seemed to provide some benefit based on the results obtained for random sampling. These results are very promising as they indicate that providing samples of all available data in training can lead to models that can provide some early warning of known failure types.
10.36001_phmconf.2021.v13i1.3052,"dataset provided, data",58,,0,"This paper describes the comprehensive results that were obtained using the various approaches and combinations, highlighting the respective benefits and limitations. Promising results were obtained using the random sampling approach, indicating that providing samples of all available data in training can lead to models that can provide some early warning of known failure types."
10.36001_phmconf.2021.v13i1.3052,"dataset provided, data",130,,0,"Rapid developments in sensor technology, data processing tools and data storage capability have helped fuel an increased appetite for equipment health monitoring in mechanical systems. As a result, the number of sensors and amount of data collected for health monitoring has grown tremendously. It is hoped that by collecting large quantities of operational data, predictive tools can be developed that will provide operational, maintenance and safety benefits. Fault detection, fault diagnosis, prognostics and health management for mechanical systems is an active research field. Data mining and machine learning techniques are at the core of this analysis and have been key to the ongoing advancements in this area to address the challenge of extracting useful results from the data collected."
10.36001_phmconf.2020.v12i1.1298,"dataset provided, dataset, case study data, benchmark, data",303,,1,"The proposed method is then tested on the PRONOSTIA bearing dataset provided by FEMTO-ST Institute for RUL estimation (Nectoux et al., 2012). The PRONOSTIA bearing dataset is a popular benchmark dataset for RUL estimation since its usage in PHM 2012 data challenge. The winner in the PHM 2012 data challenge presents three methods with different features extracted including spectral kurtosis, various time-frequency domain features using wavelet transform, and human deﬁned features after thorough inspection of the data (Sutrisno et al., 2012). Another health indicator named weighted minimum quantization error crafted by fusing multiple features is used in a model-based RUL prediction method with Particle Filter based algorithm (Lei et al., 2016). In (Guo et al., 2017), related-similarity features are extracted to construct health indicators using RNN but only the similarities to the normal state are considered. In (Chen et al., 2019), the frequency spectrum of the vibration measurements is used directly as input to a RNN based encoder–decoder framework to estimate health indicator and then predict RUL. In the proposed method, similarity-based features from frequency domain are extracted by considering both the normal and failure state. Meanwhile, various typical time domain features are also included. Since the learning data in PRONOSTIA dataset is limited with only 2 run-to-failure bearings for each of the three operating conditions, we construct an ensemble model which contains multiple LSTM networks to estimate the health indicator from the extracted features. The RUL prediction result for the PRONOSTIA bearing dataset produced by our method has a higher accuracy compared to the state-of-the-art studies. The case study on the PRONOSTIA bearing dataset is described in Section 3."
10.36001_phmconf.2020.v12i1.1294,"dataset, benchmark",40,,1,IMS dataset has been used as a benchmark in various researches in the ﬁeld of bearing prognostics. Four published prognostic methods based on this dataset are reviewed and used as the targets to compare against the proposed approach.
10.36001_phmconf.2021.v13i1.3007,"dataset, benchmark",174,,0,"The effects of varying tolerable risk on the prediction score are illustrated in Figure 8. It can be seen from (a)-(d) when 𝑅𝑒𝑎𝑟𝑙𝑦 is not applied, an optimal score is found for FD001 to FD004 when the risk levels are 76%, 86%, 47%, 68%. This suggests that practitioners can flexibly obtain such a useful failure threshold based on practice operation risk. This risk value can statistically reflect the probability of survival under healthy status. Comparatively, the optimal scores are found on relatively higher risk levels when 𝑅𝑒𝑎𝑟𝑙𝑦 is applied. This is because 𝑅𝑒𝑎𝑟𝑙𝑦 generally removes the penalty of scores when a longer RUL is predicted, which could accept a higher risk level. The risk level for each dataset to achieve the best score is summarized in Table 2 and Table 3 which compare the RMSE and score performance of the proposed method against benchmark methods with 𝑅𝑒𝑎𝑟𝑙𝑦 = 0 and 125."
10.36001_phmconf.2020.v12i1.1298,"dataset, benchmark, case study data, data",132,,1,"The PRONOSTIA data, a benchmark vibration dataset for bearing failure prognostics, was used in IEEE PHM 2012 prognostic challenge. It contains a set of real experimental data measured during the whole life span of bearings. The experimental Platform of PRONOSTIA has been extensively discussed in (Nectoux et al., 2012). There are multiple causes of bearing failure, including inner race, outer race, ball, improper lubrication, etc.. In PRONOSTIA experiments, cause of bearing failure could be one or more types of failures, which represents a real life situation. In the platform, there are two accelerometers measuring the vibration along horizontal and vertical direction respectively. In the data challenge, 3 operating conditions were considered:"
10.36001_phmconf.2020.v12i1.1300,"dataset, benchmark, case study data, data",133,,0,"TL, (W. Li, Gu, Zhang, & Chen, 2020) have transferred fault diagnosis knowledge from simulation data of a continuously stirred tank reactor and the pulp mill plant benchmark problem to real-world data. (X. Wang, Ren, & Liu, 2018) have used a portion of the Tennessee Eastman (TE) process simulation data as the labeled source dataset and the remainder as the unlabeled target dataset. Pseudo-labeling technique and adversarial training between the classiﬁer and domain discriminator is used in this study as well. Considering the experimental data as the source domain and following a similar technical approach, (Yang et al., 2019) have transferred locomotive bearing fault diagnostics knowledge to unlabeled real-world operation data."
10.36001_phmconf.2019.v11i1.785,"dataset, case study data",45,,0,"As already mentioned, the case study consists of ten voltage discharge histories. The training dataset employs nine of them in order to estimate the NHHSMM’s parameters θ* and keeps the tenth voltage discharge history as the testing diagnostic/prognostic dataset."
10.36001_phmconf.2019.v11i1.806,"dataset, case study data, data",119,,0,"AnomDB has also been implemented in a production environment since June of 2018 (with settings as in the next subsection). The algorithm is run by machine on streaming data using all part cycles of common part type collected over the previous 4-24 hours. Typically, this involves approximately 10–500 part cycles, each containing several hundred instantaneous observations of O(10’s) of control variables.3 We present a case study of one example anomalous part cycle drawn from this data set in Section 4.1. A small subset of the production data is also used for algorithm performance evaluations and comparisons in Section 4.2, via the injection of synthetic anomalies."
10.36001_phmconf.2021.v13i1.3009,"dataset, case study data, data",204,,1,"Several machine learning and deep learning frameworks have been proposed to solve remaining useful life estimation and failure prediction problems in recent years. Having access to the remaining useful life estimation or likelihood of failure in near future helps operators to assess the operating conditions and, therefore, provides better opportunities for sound repair and maintenance decisions. However, many operators believe remaining useful life estimation and failure prediction solutions are incomplete answers to the maintenance challenge. They argue that knowing the likelihood of failure in the future is not enough to make maintenance decisions that minimize costs and keep the operators safe. In this paper, we present a maintenance framework based on ofﬂine supervised deep reinforcement learning that instead of providing information such as likelihood of failure, suggests actions such as “continuation of the operation” or “the visitation of the repair shop” to the operators in order to maximize the overall proﬁt. Using ofﬂine reinforcement learning makes it possible to learn the optimum maintenance policy from historical data without relying on expensive simulators. We demonstrate the application of our solution in a case study using the NASA C-MAPSS dataset."
10.36001_phmconf.2020.v12i1.1205,"dataset, code, data",143,,0,"months can be used for training. As expected, when training on a small data set, the prediction errors and thus the extracted health indices are much noisier, whereas training with 9 months allows for a much clearer distinction between healthy and faulty periods. From the color code of these two plots it is clear, that the detection with little training data is less precise (not all faulty points are above the detection threshold) and suffers from more false positives (some healthy points are detected as faulty). In order to achieve a high detection quality, while using only three months of data, we apply the cross-turbine scheme, thereby training on large data sets from other turbines and rescaling the results using the three months reference set of turbine A0."
10.36001_phmconf.2015.v7i1.2597,"dataset, data",3,,0,Data set 1
10.36001_phmconf.2015.v7i1.2597,"dataset, data",3,,0,Data set 2
10.36001_phmconf.2015.v7i1.2597,"dataset, data",3,,0,Data set 3
10.36001_phmconf.2015.v7i1.2597,"dataset, data",24,,0,Figure 7. The ﬁrst three principal components of the data set using ﬁve different radial forces as a load of the tested bearing
10.36001_phmconf.2015.v7i1.2597,"dataset, data",38,,0,Figure 10. Run to failure data set represented by the PSD amplitudes of the stator current at a frequency of 29 Hz. The failure was created by ﬂuting with an voltage of approximately 9 V.
10.36001_phmconf.2015.v7i1.2597,"dataset, data",115,,0,"In this section the focus is on the distinction between the load conditions applying the data processing and reduction method, described in section 3. Therefore 100 records of the current signal for each ﬁve load conditions are used to generate a 46 × 500 feature matrix F. By means of the reduction method the feature matrix is transformed into a 3 × 500 matrix G. The remaining components cover about 94 percent of the variance of the original data set. This means, instead of 46 features only the ﬁrst three principal components are used for the load determination. The data of the resulting matrix is illustrated in Figure 7."
10.36001_phmconf.2015.v7i1.2597,"dataset, data",155,,0,"The coefﬁcients eji are eigenvector components, which are obtained from the covariance matrix of the feature matrix F. Performing the transformation for j = 1...J by using equation 1 gives the principal components in order of signiﬁcance (highest to lowest). In other words, the ﬁrst component (j = 1) describes the most variance of the training data set, whereas the second component explains the second most variance, and so forth. Hence, the former components give the most insight into a change of the motor condition. On the other hand, discarding the last components in order to reduce the dimension is possible, since the loss of information is insigniﬁcant. However, the rate of reduction depends on the data set F and the level of variance, which is covered from the remaining components, see section 4.2."
10.36001_phmconf.2015.v7i1.2597,"dataset, data",167,,0,"The PCA is a widely accepted technique for data compression and feature extraction. A detailed description of the method including the mathematical equations can be found in (Jolliffe, 2002) or (Alpaydin, 2014). In general, the PCA is used to transform the feature matrix F in a new N × J matrix G. One aim of the method is that J < I is valid without much loss of information. For this purpose correlated variables of the data set F are combined by the PCA into a set of linearly uncorrelated variables. The PCA identiﬁes the so-called principal components of the feature matrix, which emphasize variation and patterns in the data set. Each feature vector fn is mapped to a new vector gn = (gn1, gn2, gnj, ..., gnJ ), where gnj is the jth of a total of J principal components. The transformation is done by the"
10.36001_phmconf.2015.v7i1.2597,"dataset, data",192,,0,"Since the PCA is suitable to highlight variance of the determined features over the time (see section 4.2), the described method is applied to extract a health indicator from the current signal of the ﬁrst three measured run to failure data sets. For this purpose, the ﬁrst data set is selected as training data, which is used to determine the eigenvalue coefﬁcients of the transformation Equation 1. According to the resulting coefﬁcients, the ﬁrst three principal components are determined of the three run to failure data sets. The ﬁrst principal components of the three experiments are shown in Figure 12. The last 22 hours are displayed before the failure of the bearing occurred. It can be ascertained that all three courses differ strongly and no clear tendency of the bearing’s degradation is visible. Furthermore, the ﬁnal failure limit is located on different values. Thus, the ﬁrst principal component is not suitable as a health indicator for the motor. Since the other components reveal a similar behavior a presentation is neglected at this point."
10.36001_phmconf.2015.v7i1.2597,"dataset, data",287,,0,"The application of radial force as a load of the tested bearing provides several experimental possibilities, which will be discussed in this section. Most of the trials focus on the generation of run to failure data by means of a variable applied force. The data set which is the basis for the investigations in this section has e.g. ﬁve different load states: ﬁrst state is nearly with no force at the beginning of the trial (corresponds to 16 revolutions of the crank lever; at this point, the spring is still unstressed). The second state is an increased load of about 1680 N or 28 revolutions. In case of the third, forth and ﬁfth state the crank lever is turned once at each time (every revolution introduces an increase of 140 N) so that the ﬁfth state corresponds to a load of 2100 N and 31 revolutions, respectively. The results in the next sections are based on the analysis of the stator current signal and its variations due to the radial load only. The mechanism behind these variations can be explained as follows: an increase in the load of the shaft results in a higher displacement of the shaft’s rotation axis. Thus, a varying load also leads to a changing air gap in the asynchronous motor which can be detected in the stator current. The inﬂuences of this varying radial force concerning the PSD and the resulting damage cases are presented in a ﬁrst part. Afterwards, the data reduction to distinguish the single load states by means of the PCA is shown."
10.36001_phmconf.2015.v7i1.2602,"dataset, data",22,,0,"confidence level, length of current and normal data set and other relevant parameters determine the performance of fault detection scheme."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",32,,0,"If the run-to-failure histories in the dataset were enough to consider case-based reasoning techniques, the data-driven ﬂowchart suggests Match matrix as an appropriate solution for prognosis of long-term multivariate systems."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",41,,0,where n is the number of data windows in the dataset and t is the time scale. Monotonicity is a relevant degradation parameter under the assumption that an asset will not go through repair until reaching the system failure.
10.36001_phmconf.2015.v7i1.2646,"dataset, data",72,,0,"If there is no need to extract the PDF of the RUL according to design requirements, there are alternative solutions depending on the complexity of the data, prediction span (short-term or long-term prediction), system speciﬁcations, available dataset, and knowledge of reliability distributions. Monotonicity (m) is used as a measure of the data complexity calculated as follows (Coble, 2010):"
10.36001_phmconf.2015.v7i1.2646,"dataset, data",89,,0,"putational efﬁciency. Otherwise, if the dataset has a single feature, distance based approaches are more efﬁcient for online applications. If there is expert knowledge to deﬁne the similarity or difference among alternative runs, Fuzzy-Based Similarity (Zio & Maio, 2010) evaluates the distance between alternative run-to-failure data based on Fuzzy membership functions instead of crisp distance evaluations. For online univariate implementations without expert knowledge, Trajectory Based Similarity (TBS) (Tianyi, 2010) approaches could be implemented."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",98,,0,"Otherwise, if the degradation process can be represented with the Markovian memoryless property, there are different options depending on the monotonicity of the dataset: if the dataset represents a monotonic degradation pattern (0.8 ≤ m ≤ 1) Gamma process based prognostic implementations are feasible (Son, Fouladirad, & Barros, 2012); otherwise, Wiener process is more appropriate for nonmonotonic degradation patterns (S. Tang, Yu, Wang, Guo, & Si, 2014). Both approaches require ﬁtting the data to the process-speciﬁc parameters."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",113,,0,"If the designer implements a data-driven approach, gets unsatisfactory results, and if they do not have PoF knowledge, it is possible to create data-driven combinations to improve the accuracy of the results. If the designer has datasets with different features or datasets of different scenarios of the same system, then parallel fusion combinations (i.e., DD1(x) || DD1(y), where x and y indicate different input datasets) may improve the system’s prediction accuracy (e.g., see (J. Liu, Vitelli, Seraoui, & Zio, 2014) for an ensemble of SVR models)."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",126,,0,"Data-driven prognostics algorithms rely on the available data to ﬁt a model of the system behavior. The data must include run-to-failure conditions of the component under study in order to predict the RUL. Generally data-driven approaches are based on statistical pattern recognition and machine learning techniques. The main assumptions of data-driven approaches are that (i) the statistical features remain unchanged until a failure occurs or they change in a predictive way as the fault progresses; and (ii) availability of run-to-failure data. Thereby, the quality of the dataset determines the performance of the data-driven prognostic application. In some ﬁelds it is difﬁcult to obtain the run-to-failure data (e.g., safety-critical or new systems)."
10.36001_phmconf.2015.v7i1.2646,"dataset, data",157,,0,"if it is not possible to specify the system beHowever, havior through state-based approaches, the system’s behavioral pattern may be inferred from past historical experience. If multiple run-to-failure datasets of the same component are available, case-based reasoning approaches may be implemented. These techniques analyze data, deﬁne the health index (or baseline) based on data features, and accordingly evaluate if the new test data is healthy or not and predict the RUL. These approaches assume that components used for testing and training go through the same degradation process and require multiple run-to-failure histories to reuse knowledge and create predictions. If the available dataset has multiple different features Match Matrix is an appropriate solution (J. Liu, Djurdjanovic, Ni, Casoetto, & Lee, 2007). Match matrix improves ARMA models for long-term multivariate predictions, but it suffers from com 6"
10.36001_phmconf.2015.v7i1.2686,"dataset, data",101,,0,"an optimization set, which will be used to optimize the quadruplet of the model parameters according to the procedure in Section 3.4, and, finally, a test set which will be used to verify the performance of the method. The data relative to the other four IGBTs (namely C, D, E and F) aged by 1800 and 2700 thermal cycles will be divided into an optimization set for the model parameters optimization and a test set. For ease of comprehension, Figure 5 shows how the available dataset has been divided."
10.36001_phmconf.2015.v7i1.2686,"dataset, data",111,,0,"The proposed approach has been verified considering experimental data collected at the Centro de Estudios e Investigaciones Técnicas (CEIT, San Sebastian, Spain) from an inverter providing the required three phases AC current to an electric power train. The remaining part of the paper is structured as follows: Section 2 identifies the problem statement and the aim of the methodology; Section 3 illustrates the data pre-processing and the method; Section 4 presents the experimental dataset and describes the data collection process; Section 5 discusses the application of the developed monitoring system; finally, Section 6 recalls the concluding remarks and results."
10.36001_phmconf.2015.v7i1.2711,"dataset, data",81,,0,"A cross validation scheme was applied to the data to divide it into training and validation datasets. In prediction problems it is important to separate training and validation test generalization for data independent datasets. The partitioning scheme was selected as a stratified hold out cross-validation which retained the proportions of the class labels for the training and validation partitions. Additionally, the scheme was reshuffled 10 times to provide additional validation data sets. The length of the"
10.36001_phmconf.2015.v7i1.2758,"dataset, data",38,,0,"Simulation was used to develop the analysis routine and then evaluate the performance of a notional component. Once the analysis engine was tested via simulation, it was applied to a real world fault data set."
10.36001_phmconf.2016.v8i1.2500,"dataset, data",92,,0,"preparation for time-frequency analysis, each data set collected is decomposed into individual machine cycles using crank shaft position measurement to establish the beginning and end of a crank cycle. An STFT is generated for each cycle and then averaged over ten cycles to generate a single TFD, or observation. A ﬁve cycle overlap is used to increase the number of observations per data set and promote TFD consistency. The result is multiple TFD’s per data set representing experimental observations from the particular fault case."
10.36001_phmconf.2016.v8i1.2500,"dataset, data",123,,0,"Applying the described methodology, the valve seat wear fault condition is seeded in the ESH-1 compressor under ﬁve possible cases, representing various degrees of severity, and vibration data is collected. Raw vibration data is pro cessed through a transformation from the time to angular position domain. Time-frequency diagrams are determined and image-based features extracted. The resulting features are exhaustively search by training linear and quadratic discriminant classiﬁers. Each classiﬁer is evaluated on its ability to correctly classify a validation data set with feature space dimensions not exceeding three. Detailed results for four scenarios are given as follows with the features resulting in the highest classiﬁcation of the validation data using a quadratic classiﬁer depicted."
10.36001_phmconf.2016.v8i1.2521,"dataset, data",122,,0,"eled crack front and its corresponding driving force proﬁle are treated as a single data point to be used to train the surrogate models via supervised machine learning. If each simulation provides η unique couples of crack front and corresponding driving force proﬁles, the collection of all crack fronts from all training simulations make up a training dataset of size (cid:80)Φ s=1 ηs. For example, 30 crack growth simulations where ηs = 100 for s = 1, . . . , 30 yields 3, 000 data points that can be used for training. Speciﬁcs regarding the fatigue crack growth model can be found in (Leser et al., 2016)."
10.36001_phmconf.2016.v8i1.2521,"dataset, data",143,,0,"Assuming that an in-situ SHM system would likely be less accurate than an NDE scan, a mounted piezoelectric sensor array was chosen as inspiration for the noise model. A simple linear array would likely have to be mounted somewhere toward the bottom of the specimen and oriented in the xdirection. Therefore, it was also assumed that SNRx >SNRy since the incident waves would be approximately perpendicular to the crack faces growing in the x-direction. Based on these assumptions, SNRx = 5 and SNRy = 2.5 for the experiment.The resulting dataset was thinned by 20% and divided into ﬁve intervals; i.e., ﬁve predictions of RUL would be made, each after a new interval of data was appended to the total set. The vector of times at which these data were gath 6"
10.36001_phmconf.2016.v8i1.2521,"dataset, data",309,,0,"The training data were developed in three steps. Originally, the number of complete simulations Φ = 30 as shown in Figure 4. All of the simulations were ﬁxed at a single a0 and varied y0 only. Next, 330 additional, single-step simulations with 11 different values for a0 per each of the original y0 values were added to the dataset under the assumption that these small cracks would result in paths nearly identical to the original 30 simulations. Finally, the dataset was augmented by manually varying the crack front shape at each growth step, in all simulations. Five different crack front shapes were used ranging from perfectly straight (i.e., a midpoint extension of zero for a parabolic curve) to an exaggerated curve (i.e., a midpoint extension of 0.127 mm). These augmented shapes allowed the surrogates to reach equilibrium during growth in spite of small numerical errors in the crack propagation. In total, the training simulations resulted in 16, 229 training points, each consisting of one unique crack front and its corresponding SIF proﬁles (i.e., KI , KII , and KIII at each front point, γω). The crack growth algorithm and surrogate model were veriﬁed and validated using FRANC3D simulations that were not part of the original training set along with experimental crack growth data. The details of the training process, the veriﬁcation, and the validation were reported in (Leser et al., 2016) and will be omitted here for brevity. Replacing the FE-based crack growth simulations with the surrogateassisted approach yielded a reduction in computation times of three orders of magnitude on average (i.e., from nearly 3 hours per simulation to < 7 seconds)."
10.36001_phmconf.2016.v8i1.2555,"dataset, data",15,,0,Figure 3. Four-class clustering results using Hamming distance on the reduced data set.
10.36001_phmconf.2016.v8i1.2555,"dataset, data",79,,0,"This paper is organized in five sections including the present one. Section the approach problem formulation for the proposed decision support environment and states the background definitions and assumptions. Section three describes the type of analysis that can be performed on mixed data modalities that enable automated decision making under uncertainty. In section four, the results for techniques on an anonymized dataset are presented. The paper is summarized and concluded in section five."
10.36001_phmconf.2016.v8i1.2569,"dataset, data",298,,0,"Different from widely used shallow neural network architecture with only one hidden layer, the branch of machine learning methods with multiple hidden layers are regarded as deep learning method.  Deep learning attempts to model complexity and internal correlation in dataset by using multiple processing layers, or with complex structures, to mine the information hidden in dataset for classification or other goals (Hinton & Salakhutdinov, 2006). As a deep learning method, LAMSTAR has been applied in multiple fields such as image recognition (Girado et al., 2003), biomedical diagnosis (Nigam et al., 2004, Sivaramakrishna & Graupe, 2004, Waxman et al., 2010, Isola et al., 2012), with solid result showing the ability of LAMSTAR for rapidly processing large amount of data and less error percentage than regular machine learning algorithms. As one of the representative classical neural networks, the BP neural networks have been widely used in machinery fault diagnosis (Paya et al., 1997, Huang et al., 2007). However, the reported studies based on BP neural networks have shown the strict requirement of signal pre-processing using complicated signal processing methods including HHT and wavelet packet transformation. With the introduction of forgetting, rewarding/punishing features into a traditional neural network, LAMSTAR works on a closer level of mimicking the working process of a natural brain. Without the limitation on network size, LAMSTAR can grow/shrink in dimension without changing the original structure and maintain fast training speed. As an input, the LAMSTAR network accepts data defined by the user, such as system state and system parameter. Then, the system builds a model"
10.36001_phmconf.2017.v9i1.2391,"dataset, data",89,,0,"The Neural Gas algorithm (Martinetz, Berkovich, & Schulten, 1993) is an iterative algorithm to train a network of nodes or prototypes for vector quantization (the process of approximating a large data set of multidimensional data by a reduced number of “prototype” vectors whose probability density closely resembles the probability density function (PDF) of the input data) (Ancona, Rovetta, & Zunino, 1997; DeAlarcon, Pascual-Montano, Gupta, & Carazo, 2002)."
10.36001_phmconf.2017.v9i1.2391,"dataset, data",193,,0,"The improvement obtained in the AUC metric, may be explained by the capability of the NG algorithm to resemble the probability density function of a determined multidimensional dataset. Hence, the method used to update the particles is different from the one proposed in the previous work. The previous method incorporates a dynamic model for the movement of the particles, which considers the distance among the particles to the new events, and a process noise for uncertainty characterization. This kind of model may have problems with large amount of criminal data events, because the process noise may position the particles outside the interest area. A example of this is shown in Figure 11, where some particles in the posterior spatial risk function calculated in (Flores et al., 2015) are outside of the interest area. On the other hand, Figure 6 shows the posterior spatial risk function calculated in the present work, where all the particles are contained inside the interest area. Both posterior risk functions were calculated using the same amount of particles and criminal events."
10.36001_phmconf.2017.v9i1.2427,"dataset, data",43,,0,Data is collected for the wind turbines at various time steps prior to failure. The oldest dataset dates back to 2.5 years prior to failure and according to the maintenance reports the gearbox at this time is in a healthy state.
10.36001_phmconf.2017.v9i1.2447,"dataset, data",90,,0,"This paper proposes a LSTM-based RUL prediction method for lithium-ion battery, which is a data-driven battery RUL predictor. The design and implementation of the proposed method are discussed in detail. Experiments on CALCE lithium-ion battery datasets are presented to demonstrate the effectiveness of the proposed method. The results show that the proposed method has higher prediction accuracy, more stable and reliable prognostic performance than ESN. Our future work will focus on the improvement of training speed and the uncertainty expression of predicted results."
10.36001_phmconf.2017.v9i1.2447,"dataset, data",196,,1,"Lithium-ion batteries play critical roles in many electronic devices. It is necessary to develop a reliable and accurate remaining useful life (RUL) prediction approach to provide timely maintenance or replacement of battery systems. A novel RUL prediction approach based on Long Short-term Memory (LSTM) Recurrent Neural Network (RNN) is proposed in this paper. LSTM is able to capture long-term dependencies and model sequential data among the capacity degradation of lithium-ion batteries. The advantages of our proposed method include: 1) obtaining high prediction accuracy without accurate physics-based model or expertise and 2) decreasing the cumulation errors by multi-step ahead prediction each time, while traditional RUL method predicts one-step ahead once and then uses the current estimated value to predict next one, which causes cumulation errors increased. The Center for Advanced Life Cycle Engineering (CALCE) battery datasets are used to demonstrate the effectiveness of the proposed method. The results show that, compared with echo state networks (ESN), the proposed method has higher accuracy, more stable and reliable performance for lithium-ion batteries RUL prediction."
10.36001_phmconf.2017.v9i1.2470,"dataset, data",97,,0,"PCA technique is usually used to reduce a higherdimensional data set to lower dimensions for subsequent analysis. the eigenvalue decomposition or singular value decomposition of a data set, usually after mean centering the data for each attribute. The aim of MWPCA approach is to remove the noise in the original signal through selecting the principal components from the wavelet signal decomposition. A thresholding rule is employed in this method to remove potential noises from the decomposed coefficients, where the components associated with eigenvalues greater 0.05 times the sum of all eigenvalues."
10.36001_phmconf.2017.v9i1.2470,"dataset, data",112,,0,"same dimension q × N, where q = 47 and N = 25900 (in comparison to N = 26000 in the raw data set). In addition, the multivariate time series have different magnitude or units. In order to use multiple time series variables simultaneously for damage diagnosis and prevent the undue domination of variables with large numerical values over the variables with small numerical values, each time series is divided by its corresponding maximal value, thus normalizing the variables into dimensionless vector with the same range of -1 to 1. The normalized multivariate data series will be used in the subsequent analyses."
10.36001_phmconf.2017.v9i1.2470,"dataset, data",228,,0,"2) Data preprocessing (Cleaning, Outlier analysis & Filtering)All variablesEach variable3) Build data set with a rolling window for subsequent signal processing (one day) 4) Performance wavelet decomposition analysis on each processed data set of a variable SixN(i=1,…,q) (N = number of processed data points)Each variable5) Perform MWPCA on a given signal6) Reconstruct a cleansed signal for each variable from denoised wavelet coefficientsEach variableEach variable8) Determine the reduced dimension d, using PCA approach and 95% threshold for multivariate time series All variables10) Calculate entropy value of each groupAll variables12) Calculate the sigma of the entropy ratio feature at validation domain (baseline)13) Evaluate the entropy ratio feature at each rolling window of the testing/application domainEt>3sv?Each pointContinue monitoringNo14) Count persistenceYes15) Evaluate Persistence Pt>Pthre?NoAlarmingYes16) Notify the customer with recommendation1) Read raw time series data, RqxM(q=number of variables, M=number of data points)9) Perform probabilistic PCA data fusion to generate reduced data matrix dxN11) Calculate entropy ratio at the rolling window7) Estimate effectiveness of the MWPCA denoising method quantitatively and graphically All variables   ANNUAL CONFERENCE OF THE PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2016"
10.36001_phmconf.2017.v9i1.2471,"dataset, data",155,,0,"Washing to remove the fouling of compressor system is normally carried out at an intervals of 5-6 weeks for the system under study. The washing schedule followed during the operation-maintenance cycle for the GT was made available and used to extract the required data. For each of the fouling monitoring data set for just 5 days (i.e. 120 hrs.) at the beginning of a cycle was selected and combined to create a separate group for analysis. It is assumed that these data set are from fouling free condition of the compressor (start of fouling). In addition, data for 5 days was also collected from the end of the cycle (prior to next washing cycle) and this is assumed to be from fully fouled stage (end of fouling). This approach facilitates meaningful comparison of fully fouled and no-fouled conditions."
10.36001_phmconf.2017.v9i1.2485,"dataset, data",73,,0,"Future work will focus on the development of comprehensive methods for identifying and isolating both truck-based defects and rail-based defects. Additional data will be generated in an ongoing effort to verify and validate the IMUbased method and associated metrics as well as to produce standard datasets on which novel diagnostic algorithms can be developed. Ultimately, such algorithms could enable the use of IMU-based methods for diagnostics, prognostics, and"
10.36001_phmconf.2017.v9i1.2485,"dataset, data",140,,0,"Reference data were collected bidirectionally at finite positions of travel, specifically every 1 mm between travel positions 1 mm and 321 mm. The IMU data (accelerometer and rate gyroscope data) for each run was processed according to the IMU-based method to yield the 6-degree-offreedom error motions (Vogl et al., 2016): one positioning error motion, two straightness error motions, and three angular error motions. Thus, six error motions are associated with each of fifty runs for all fifteen stages. Also, since every error motion is a “downsampled” function of the carriage position, the “big” time-based sensor dataset was transformed to a “small” position-based error dataset. These error motions were used for physics-based diagnostics described in the following sections."
10.36001_phmconf.2018.v10i1.245,"dataset, data",64,,0,"The survivor function can be estimated parametrically or non-parametrically.  Non-parametric estimates look at the proportion survived over a period of time running over the observed lifetime data. When right-censored data is included in the lifetime dataset (equipment that did not fail over the observation time window), the Kaplan-Meier estimate can be used to adjust the non-parametric survival function."
10.36001_phmconf.2018.v10i1.245,"dataset, data",70,,0,The rest of the article is organized as follows.  Section 2 summarizes common industrial data sources as well as reviews traditional RCM approaches and age-reliability analyses and challenges.  Section 3 details the numerical approaches proposed for applying an RCM approach to large datasets and numerical challenges. Section 4 presents a case study illustrating the methodology.  The paper ends with concluding discussions and suggests research directions.
10.36001_phmconf.2018.v10i1.245,"dataset, data",75,,0,The workflow based on Nowlan and Heap (1978) for asset strategy identification and prioritization is shown in Figure 3. The input is assumed to already have passed through the data quality and a prioritization workflow such as Criticality Analysis.  Analytics to treat data quality challenges and estimate the hazard functions are integrated in the workflow to automate the process across broad datasets and address the limitations discussed in Section 2.2.
10.36001_phmconf.2018.v10i1.245,"dataset, data",80,,0,"As mentioned in the data quality background (Section 2.1), one key data quality challenge for conducting reliability analytics across broad datasets are lack of failure event classification, and even more fundamentally, recording whether an event was a failure event or not. Any large-scale data mining effort across historical maintenance records for reliability analytics is fundamentally challenged by missing or inconsistently coded breakdown indicator fields, which records whether an asset failed or not."
10.36001_phmconf.2018.v10i1.245,"dataset, data",94,,0,"A new challenge that arises from applying analytics to broad datasets is processes and methods for determining optimal ways for stratifying a population. We have observed through experience both first-hand and with applying different statistical methods to CMMS/EAM data that different sites and companies tend to be the most meaningful explanatory variable for differences between subpopulations.  For this reason, we group assets of the same type by site in the case study, but remark that there is opportunity for future statistical work here towards automating and improving this workflow."
10.36001_phmconf.2018.v10i1.245,"dataset, data",106,,0,"Characterizing the shape of the estimated smoothed hazard inspection.  is straightforward function Challenges arise when attempting to systematically characterize shapes across hundreds of different subpopulations across broad datasets (hundreds of equipment types across many units or sites), and also for then drilling down into the data for determining a prioritization strategy.  To tackle the first problem, the performance measures and criticality assessments performed previously should be used for prioritization of where to start.  In order to identify candidates for a PHM initiative, we can identify which populations resulted in age-related failure patterns as candidates to first investigate."
10.36001_phmconf.2018.v10i1.245,"dataset, data",106,,0,"mode for each observed failure event well characterized. If the analysis is conducted on a small data set describing wellknown asset failures, this can be effective, however, this method does not scale up well for large datasets spanning many populations of assets across many sites and possibly across several different industrial companies and verticals.  Once a Weibull is fit from each failure mode, the hazard function can be estimated.  In a Weibull analysis, the 3 possible regimes (infant mortality, random, and wear out) are the only possible shapes the hazard function can take."
10.36001_phmconf.2018.v10i1.245,"dataset, data",111,,0,This paper walks through the steps with a data-driven use case towards prioritizing and getting started with a PHM initiative through an RCM basis for choosing where to apply PHM technology.   We focus on a particular example of to looking at different age-reliability characteristics determine candidates for condition and health monitoring based on identifying observed failures with failure patterns physically possible for applying prognostics models.  We discuss the steps we took and the challenges we discovered in generalizing a classical approach to a large dataset. This approach can be used to identify how and where to get started in a PHM initiative with respect to business opportunity.
10.36001_phmconf.2018.v10i1.245,"dataset, data",183,,0,"The approach we take is to use a machine learning classifier trained on labeled data.  Major strengths of deploying classification algorithms are consistency and scalability.  Two similar inputs will always have the same classification by a computer model, which is not always the case with human labeling.  Further, computer models can run very quickly over large amounts of records (can label thousands of records in minutes). We use commercial software in the GE Digital APM offering, which classifies a repair event as a failure or not a failure based on a machine learning algorithm trained on a large dataset describing repair events across an aggregate of industrial peer companies and reviewed and labeled by a team of subject matter experts (GE Digital APM, 2017). Naïve-Bayes was the algorithm we used for a couple of reasons.  A lazy learner was desired because of datasensitivity issues when training a classifier on aggregated confidential data and applying it elsewhere.  Of the lazy learners explored, Naïve-Bayes had the highest accuracy as"
10.36001_phmconf.2018.v10i1.245,"dataset, data",204,,0,"New sets of challenges arise when applying classification models to broad datasets. While consistency is a strength, it can also be a weakness, as the definition of “functional failure” may differ between identical assets in different operating contexts. Additionally, fundamental challenges in work history descriptions can lead to problems. Models are only as good as the data they are trained on, and edge cases abound in work history descriptions.   For example, the information about what was done to repair a failure or what was observed (eg: “replace bearing”, “pump is not pumping”) may not give the desired level of information, and while a human may figure out intelligently with to do with this information, a computer algorithm will not. It has been in our experience that computer models adequately get most of the labels correct, which significantly reduces the time and effort required for a human, and further, that meaningful patterns, trends, and insights can be obtained even from the data not being 100% labeled and catching every functional failure specifically in every operating condition."
10.36001_phmconf.2018.v10i1.465,"dataset, data",80,,0,"The dataset consists of a compilation of wheel proﬁles captured with MiniProf for two trains, with 72 wheelsets each, on October 2016 and January 2017. Nevertheless, there are three wheelsets that do not show consistent data, either by missing one of the wheels, or by showing a bad proﬁle acquisition. We attribute this issues to improper handling of the tool (we trust that MiniProf is adequate for acquiring the proﬁles)."
10.36001_phmconf.2018.v10i1.465,"dataset, data",116,,0,"Deep Learning are a type of complex computational models composed of multiple non-linear processing units stacked into layers which learn representations of data with multiple levels of abstraction (LeCun, Y. and Bengio, Y. and Hinton, G., 2015). Their greatest success has been driven by their good performance with discovering intricate structure in large datasets of raw signals. They display a neural network architecture (Hertz, Krogh, & Palmer, 1991), and share many traits with them. Like neural networks, they exploit the connectionist learning approach, where the links among the units are weighted in order to accomplish a speciﬁc task."
10.36001_phmconf.2018.v10i1.490,"dataset, data",60,,0,"As exempliﬁed in the preceding examples, the choice of an HP-conﬁg has a profound impact on the resulting machine learning model. More precisely, an HP-conﬁg c (together with the training data) determines the resulting machine learning model M , which we wish to evaluate its prediction accuracy on a validation dataset. We evaluate the"
10.36001_phmconf.2018.v10i1.490,"dataset, data",72,,0,"training data, which could be time consuming when the number of layers is large or when the training dataset is large. Consequently, it is infeasible to conduct a brute force search, and to evaluate OOSV on every HP-conﬁg c for minimizing OOSV. This motivates the design of Bayesian optimization algorithms, which converge to a near-optimal HP-conﬁg c with a small number of evaluations of OOSV."
10.36001_phmconf.2018.v10i1.490,"dataset, data",107,,0,"i )}N tr where M = A({(X tr i=1, c). Essentially, OOSV(c) evaluates the performance of the resulting ML model M , i )}N tr which is constructed with the training dataset {(X tr i=1, on another set of unseen validation data {(X va i , yva i=1. To identify an effective HP-conﬁg c, we would like to identify a c such that OOSV(c) is small by using Bayesian optimization algorithms, which are introduced and motivated in the next section."
10.36001_phmconf.2018.v10i1.490,"dataset, data",115,,0,"Altogether, we have deﬁned the construction procedure APHM i )}N tr in Algorithm 3. It inputs the training dataset {(X tr i=1 and an HP-conﬁg cPHM, and returns a random forest regression model MRF, which can be used to predict the average removal rate by Algorithm 5. The out-of-sample validation error OOSVPHM(cPHM), which is on the validation dataset {(X va i=1, is deﬁned as follows. First, we compute the random forest regression model MRF according to equation (4). Then, for each time series data X va i , we compute the prediction ˆyva"
10.36001_phmconf.2018.v10i1.490,"dataset, data",120,,1,"In this Section, we demonstrate the effectiveness of Bayesian optimization for automating the HP-conﬁg optimization of the machine health prognostic task for the PHM 2016 dataset (PHM, 2016). We ﬁrst describe the ML task and the dataset involved in Section 4.1. Then we describe in Section 4.2 our construction procedure APHM for constructing an ML model MPHM for the PHM 2016 Data Challenge task. We also highlight the HP-conﬁg cPHM involved in the construction. Then, in Section 4.3, we deﬁne the hyper-parameter optimization problem for the task, and also provide the search space CPHM for the optimization. Finally, we present the numerical results in Section 4.4."
10.36001_phmconf.2018.v10i1.490,"dataset, data",172,,0,"Let’s consider a supervised machine learning (ML) task involving a collection of training dataset {(X tr i=1, where i ∈ X ⊆ RD is the feature vector of the ith training samX tr ple, ytr i ∈ Y ⊆ R as the label of the ith training sample, and N tr is the number of training samples. In the context of machine health monitoring, feature vector X could be the time series sensor data measured on a machine part in a time interval, and label y could be the corresponding remaining useful life. The goal of an ML task is to construct an ML model M : X → Y, so that the output M (X) accurately predicts the corresponding label y. The ML model M belongs to a parameterized class of ML models, and the construction is to identify an effective choice of the parameters that makes the ML model M accurate."
10.36001_phmconf.2018.v10i1.509,"dataset, data",26,,0,sumed a stationary process. The bottom plot in the ﬁgure is the extracted data set for the entire cutting process minus the transient events.
10.36001_phmconf.2018.v10i1.509,"dataset, data",43,,0,"Figure 5. Raw acoustic measurement for tool condition = light, spindle speed = 1800, feed rate = 2, microphone = 1 (top). Extracted data set (t = 5-20) minus startup and shutdown events."
10.36001_phmconf.2018.v10i1.509,"dataset, data",95,,0,"Taking the input data to be Xk where k = [1, 2, 3, ..., N ], N represents the total number of samples. The samples of this data set Xk to have two classes namely negative and positive classes. Each class is assigned class labels, yk = [1, −1], and represents the positive and negative class, respectively. A hyperplane which obtains the class boundaries G(x) = 0, from which the linear classiﬁer can be written as"
10.36001_phmconf.2018.v10i1.509,"dataset, data",181,,0,"Next, the extracted data set is segmented into one second samples and converted to the frequency domain by fast Fourier transform (FFT). A zoomed example of this spectrum is shown in Fig. 7, where the top ﬁgure shows the spectrum between 0-1000Hz, which is a signiﬁcant reduction of the complete spectrum which continues to 22,050Hz. The bottom ﬁgure further zooms the spectrum to a range of 0-200Hz. The intent of both of these ﬁgures is to illustrate that the three microphones at various angles and distances identify nearly the exact sample frequency peaks as one would expect. The magnitudes would have variation due to distance from the work piece and ampliﬁer gain settings but the identiﬁed frequency content of the signals appears to be very repeatable. Note that for the proposed method the full frequency spectrum up to the Nyquist sampling limit is used in this classiﬁcation method described later. The zoomed plots shown in Fig. 7 are simple shown to show repeatability of the sensing method."
10.36001_phmconf.2018.v10i1.544,"dataset, data",240,,0,"PHM of NAS requires accurate and real-time state awareness and this is achieved by multimodality safety monitoring (Figure 1). For example, vehicle level flight information can be used to obtain engine status data (e.g., temperature and speed) which can extract their features to indicate health and fault states (see Figure 3). Once the data are obtained, advanced data analysis is used for data reduction and classification. The current study focuses on a robust real-time aircraft health monitoring framework using a machine the multivariate learning based approach, specifically Gaussian mixture model (mGMM), for the detection of in-air operational anomalies of an aircraft system. Sensor fusion and noise filtering algorithms have also been adopted to reduce dimensionality of the feature space while avoiding the elimination of useful information from the original flight data. Random noise in each feature, induced by the aircraft sensors and data acquisition system, is filtered out using a weighted averaging window while maintaining inherent variances. The filtered dataset is then fused according to the underlying physics of each sensed feature to reduce redundant features and subsequently trained using the mGMM. The methodology allows monitoring the behavior of each feature as well as correlations between features, significantly improving detection sensitivity. The high computational efficiency of this approach permits real-time monitoring of an aircraft system."
10.36001_phmconf.2018.v10i1.585,"dataset, data",13,,0,Figure 6. Complete data set of initial status classification test data.
10.36001_phmconf.2018.v10i1.585,"dataset, data",269,,0,"Exposure storage, transportation, and handling can have significant effect on various system assets, making it important to understand the full life-cycle exposure of sensitive equipment. To quantify a system’s status timeline, a wireless, intelligent, low-power life-cycle monitoring device is being developed. No current to measure monitoring systems have environmental conditions experienced by an asset and use that data to determine time spent in various states such as storage, transportation, handling and deployment. The sensor system being developed will employ a multi-modal sensing approach that will provide on-device analysis of the various parameters such as acceleration, temperature, and other environmental conditions for the duration of asset storage, handling, and distribution.  Extremely low power and potentially easy to integrate into existing platforms, the monitoring system will include on-system intelligence for distinguishing between different statuses and is designed to use reliable, wireless, low-profile, and inexpensive sensing technologies. Embedded system intelligence is designed to use collected datasets to accurately quantify the asset’s current status and the amount of time spent within each state.  The ability to monitor critical parameters and use them to classify this status throughout the asset’s life-cycle could provide some of the diagnostic information that would facilitate condition based maintenance.  Engineers and operators can review exposure conditions and analyze how status and exposure effects contribute to the current condition of their equipment.  The monitoring system will provide maintainers additional data points in assessing the historical use of the system."
10.36001_phmconf.2019.v11i1.768,"dataset, data",140,,0,"quirements: (1) Scalability in a distributed manner to keep up with the growing dataset and offer redundancy; (2) Capability of storing heterogeneous data, new data type can be introduced later in the form of additional experiments; and (3) Provide access control and encryption capability. In addition, Couchbase maintains an automatic buffer of unsynchronized data locally (data not yet pushed to the server) and synchronizes it with the remote storage whenever Internet access is available, further simplifying deployment and data collection. Couchbase allows us to create clusters, where we can disrtibute I/O accross different nodes. This increases scalibility by allows MCATSS to distribute reads accross multiple nodes. Data replication on the nodes gives MCATSS avaliablity in case of certain node failures."
10.36001_phmconf.2019.v11i1.785,"dataset, data",6,,0,3.1. Li-Po data set description
10.36001_phmconf.2019.v11i1.785,"dataset, data",121,,0,"Figure 5 present the left outlier’s SOH estimations, i.e. Battery 9, and Figure 6 the SOH estimation of Battery 1 which is a non-outlier case. The choice of presenting the results of Battery 1 was random, similar results were obtained for the other batteries. Battery 9 as already mentioned is the left outlier in the sense that is starting with a lower voltage and reaching the EOD critical limit quite earlier than the rest test cases. The discharge history of Battery 9 has not been included in the training process. Consequently, the minimum discharge time of this training data set is 9,857 min while Battery 9 discharge time is 5,736"
10.36001_phmconf.2019.v11i1.785,"dataset, data",245,,1,"On the other hand, data-driven approaches do not require prior knowledge of the physics of the system since datadriven approaches rely on measured data in order to derive the discharge process of Li-Po batteries. In (Cadini, Sbarufatti, Cancelliere & Giglio, 2019), a method for predicting the end of discharge (EoD) of Li-Ion batteries was recently proposed. This study combined particle filters with an Radial Basis Function Neural Network (RBF-NN). The proposed algorithm was validated successfully on dataset containing measurements of the voltage at the terminals of a Lithium-ion (Li-ion) battery, obtained during charge-discharge laboratory tests at constant current. Si in (Si, 2015) models the degradation process as Wiener process with the dynamic part being described by Brownian motion added to a nonlinear drift function. Kalman filtering was applied to update a key parameter in the drifting function through treating this parameter as an unobserved recursively. The state variable in expectation maximization conjunction with Kalman smoother to achieve this aim. The probability density function of the estimated RUL was derived with an explicit form and data from the NASA AMES Prognostics Center of Excellence Battery Dataset are used for validation. A similar approach was followed in (Dong, Che, Wei & Ling, 2018), a Brownian motion based degradation model was combined with particle ﬁltering. The"
10.36001_phmconf.2019.v11i1.788,"dataset, data",57,,0,"A summary of the CMAPSS is presented in Table II. There are four different units running under different working conditions. The data set FD001 and FD002 has only one working condition but less historical records than another two units. In comparison, the data set FD002 and FD004 have 6 different working conditions."
10.36001_phmconf.2019.v11i1.788,"dataset, data",67,,0,The effectiveness of the proposed method is demonstrated based on the CMAPSS dataset for aero-engine RUL prediction. The given data in the training set are multiple multivariate temporal sequences that describe the life time degradation of the areo-engines. Each multivariate series contains 21 sensor readings. The data records in the testing set are partial degraded data and RUL for each unit is unknown.
10.36001_phmconf.2019.v11i1.790,"dataset, data",81,,0,"Another key limitation is in the ability for useful PSF values to be estimated from existing MWO datasets, which are a very common existing data-type for maintenance operations. The size of these datasets, and their structuring quality, is severely lacking for highly-dimensional statistical models necessary—thus the existence of this HRA in the ﬁrst place. It is not yet known if results across organizations and domains (which will be needed to obtain sufﬁcient data) will"
10.36001_phmconf.2019.v11i1.790,"dataset, data",139,,0,"There have already been some successes in mitigating human data-entry error by automatically parsing unstructured natural language records e.g. the medical ﬁeld. Patient medical records have a number of similarities with MWO’s, since both outline symptoms and diagnoses, along with actions taken, all via unstructured text. Much of recent medical research has been directed toward mining text from patient records (Heinze et al., 2001; Tremblay et al., 2009; Zhou et al., 2006). However, it is important to acknowledge signiﬁcant advantages when dealing in medical records, compared to the engineering domain of manufacturing maintenance: (1) data-sets are often larger, covering longer time-spans, and (2) medicine has existing controlled vocabularies with wide adoption by experts."
10.36001_phmconf.2019.v11i1.792,"dataset, data",88,,0,"Clean the data by using Nestor to tag the data. This preprocessing results in a representation of data that is possible to be analyzed. Calculate number of time classes for maintenance time durations. An appropriate number is chosen by looking at the distribution of times such that the number of entries in each class is comparable. Choose a machine learning classiﬁer depending on computing resources, dataset size and expected performance levels. A couple of examples have been discussed here, but there"
10.36001_phmconf.2019.v11i1.792,"dataset, data",132,,0,"Assignment of Duration Categories: During the course of the investigation, various demarcations of time duration categories were explored to best describe the data. For the MLP classiﬁer, the binary classiﬁer performance decreased by reducing number of classes from 5 to 4. For the overall classiﬁer, as well as decision tree classiﬁer, reducing the number of classes led to improved performance. Also, within the multistep classiﬁcation of the MLP classiﬁer, the performance is noticeably better for the binary classiﬁer than for the second classiﬁcation step for four/ﬁve classes. These speciﬁc results are somewhat dataset dependant, but the general trend of searching for classes that are adjacent to each other are largely expected to to improve performance regardless of the dataset."
10.36001_phmconf.2019.v11i1.792,"dataset, data",172,,0,"MWOs contain dates and times in different formats and hence must be preprocessed to get maintenance time durations. To improve consistency, all time data is converted to days with a range across the data set spanning from zero to hundreds of days. The dataset used for this paper had only the starting and ending times for the workorders, not the actual task work hours. Hence, it was not possible to ascertain the actual duration of maintenance solutions. The assumed duration is deemed to be the end time minus the start time listed on the MWO. These time calculations do not always accurately reﬂect the actual time taken for maintenance, but are instead rough estimates due to inconsistencies and variations in recording times. Often, missing time entries exist for some maintenance activities. For this case study, there are 4,914 entries with incorrect formats and missing time entries. There are a further 843 entries with negative total duration. These"
10.36001_phmconf.2019.v11i1.806,"dataset, data",38,,0,These classes of anomalies were chosen based on our visual inspection of human-tagged production anomalies in the full data set. An example of the synthetic anomaly insertion for these three classes is shown in Figure 5.
10.36001_phmconf.2019.v11i1.806,"dataset, data",53,,0,We structure the problem of anomaly detection for CNC machines in the context of unsupervised machine learning. Using such an approach allows for a high degree of ﬂexibility by automating the identiﬁcation of regular structures in a data set without explicit labels or strong assumptions about the data’s content.
10.36001_phmconf.2019.v11i1.806,"dataset, data",73,,0,"A review of the tools used in the algorithm, as well as some alternative tools, are given in Section 2. Speciﬁcs of the demonstration data set and parameter settings for the algorithm are then detailed in Section 3. AnomDB’s performance on a real data anomaly and in a variety of simulated anomaly scenarios is evaluated in Section 4. Finally, Section 5 contains concluding discussions."
10.36001_phmconf.2019.v11i1.806,"dataset, data",80,,0,"5We emphasize that anomaly detection is not a uniquely-deﬁned problem, and that true-positive/false-positive rates are not universally well-deﬁned concepts in this regard. Nor is the identiﬁcation of anomalies generally to be thought of as a labeled learning problem. Our purpose here is to evaluate the algorithms’ ability to ﬂag certain classes of known anomalous patterns, not to “learn” these speciﬁc patterns from one data set and “predict” them in another."
10.36001_phmconf.2019.v11i1.806,"dataset, data",80,,0,"Principal Component Analysis (PCA) is used on multidimensional data sets to capture an orthogonal set of statistically uncorrelated coordinates in the feature space. It allows for dimensionality reduction while retaining the dominant variations within a data set by projecting away all but the highestvariance coordinate directions (Hotelling, 1933) (Hyndman et al., 2015). The number of dimensions retained is a free parameter that can be speciﬁed depending on the problem."
10.36001_phmconf.2019.v11i1.806,"dataset, data",120,,0,"Note that the use of DBSCAN represents a departure from the corresponding anomaly-detection step in (Hyndman et al., 2015). There, methods were employed that function roughly as multidimensional generalizations of percentiles, allowing for identiﬁcation of the “most outlying N points” or “most outlying X% of points”. These require pre-specifying that a certain amount of the data will be categorized as “anomalous.” With the density-based DBSCAN method, by contrast, anomalies may or may not be found in any given data set. This is more appropriate to a dynamic manufacturing environment, where anomalies are genuinely rare events of unknown frequency across different jobs."
10.36001_phmconf.2019.v11i1.816,"dataset, data",59,,1,"Data sets derived from the Statlog (Landsat Satellite) data set (Dheeru & Karra Taniskidou, 2017). The original data set contains seven classes (with no instances with class 6). In this study, only classes 1 and 2 are considered – class 1 vs. all, class 2 vs. all."
10.36001_phmconf.2019.v11i1.816,"dataset, data",61,,1,"Occupancy Detection data set (Candanedo & Feldheim, 2016; Dheeru & Karra Taniskidou, 2017). To create an unbalanced data structure, one out of 10 class 1 are included in the data sets (index 1, 10, 20, etc.) and the remaining nine eliminated, all 0 class are included."
10.36001_phmconf.2019.v11i1.816,"dataset, data",73,,1,"HTRU2 data set (Lyon, Stappers, Cooper, Brooke, & Knowles, 2016; Dheeru & Karra Taniskidou, 2017). Pulsar candidates collected during the HTRU survey. Pulsars are a type of star, of considerable scientiﬁc interest. Candidates must be classiﬁed in to pulsar and non-pulsar classes to aid discovery. First 12,000 are used for train ing and the last 5,898 for validation."
10.36001_phmconf.2019.v11i1.838,"dataset, data",67,,0,"As future work, it is planned to improve the RUL estimation by computing RUL conﬁdence, where the conﬁdence increases while collecting more data. Then, the proposed approach will be validated on other degradation data-sets related to other wind turbine components, where the system is impacted by the environmental variation (e.g., wind speed) and the change in operating conditions."
10.36001_phmconf.2019.v11i1.842,"dataset, data",35,,0,"This step includes the preparation and dissemination of datasets, derived data as well as findings or outcomes to relevant stakeholder communities. It improves accessibility as well as being a recommended best practice."
10.36001_phmconf.2019.v11i1.842,"dataset, data",75,,0,"provenance – the transformation it has gone through in the lifecycle. The ICAT prototype does not yet allow for the propagation of the complete provenance of output data without unpacking the datasets to facilitate the querying of used transformations for researchers and neither does the extended proposed CSMD model. In addition, there is still scope for improving the software and hardware environment which have equally not been covered in this proposition."
10.36001_phmconf.2019.v11i1.842,"dataset, data",80,,0,"The data has to be accurate, complete, timely, context relevant, reliable and explicit (Dibsdale 2011). The absence of any or all of these qualities in any dataset creates uncertainties and increases the probability of misdiagnosis, modelling errors (Arahchige and Perinpanayagam 2017) and inaccurate predictions. This essentially the malfunction of the IVHM system because it is “the assembly of data related to the current and future activities of a"
10.36001_phmconf.2019.v11i1.842,"dataset, data",96,,0,"The data life cycle models were then grouped by their respective numbers of data life cycles, i.e. five-phase, sixphase, seven-phase, eight-phase and nine-phase models.  The review revealed nine standards and frameworks relating to data life cycles. In the reviewed studies, the data life cycle has been described as the set of activities that affect the short and the long-term preservation of datasets through a system from planning, creation, maintenance, re-use and purging (Simonet, Fedak, and Ripeanu 2015;Beaujardière 2016)."
10.36001_phmconf.2019.v11i1.842,"dataset, data",128,,0,"The preprocessing phase aims to flag out of range data values, missing values to mitigate the risk of making decisions based on misleading results. It is fundamental to IVHM machine learning activities. It is used to clean the original signal by eliminating noise and to improve object component condition. In other words, this phase represents the low-level computation of sensor data and constitute a key element of the OSA-CBM architecture. Sensor data is transformed into an understandable format. This is the stage where data cleansing –detecting and correcting mistakes, incomplete, inaccurate, irrelevant and incorrect records from datasets. This step employs best practices to ensure that data is free of inconsistencies, correct, usable and reliable."
10.36001_phmconf.2019.v11i1.855,"dataset, data",179,,0,"In a previous work (Perez, Quintero, et al., 2018), the authors proposed a Li-ion battery degradation model, which incorporates the impact of arbitrary discharge currents. This model was represented by a state-space structure, and ﬁtted using data from a Sony battery (Sony US18650 1.4 Ah Li-ion battery (Ning & Popov, 2004)). Even though the proposed model was initially ﬁtted for a speciﬁc data set, it was subsequently extended to other brands of Li-ion batteries. The original results showed that the state-space model was able to characterize the degradation process properly when the discharge current was equal to its nominal value (1C). However, when the discharge current was doubled (2C), the proposed model was not able to follow the experimental degradation results, although it did have a biased trend. One of the ﬁndings demonstrated that if one of the coefﬁcients of the previously proposed model was ﬁtted freely, the bias was eliminated."
10.36001_phmconf.2019.v11i1.861,"dataset, data",205,,0,"Ma, Tao, Wang, Yu, and Wang (2015) presented an LSTM neural network to predict travel speed using microwave detector data. They collected 1-month trafﬁc speed data from two sites in Beijing expressway. They compared three different typologies of recurrent neural network (i.e. Elman NN, TDNN and NARX NN) as well as other non-parametric and parametric methods (i.e. SVM, Time Series and Kalman Filter) with the LSTM NN based on the same dataset. The numerical experiments proved that the LSTM NN performes better than other algorithm in terms of accuracy and stability. Tian and Pan (2015) introduced a model called Long ShortTerm Memory Recurrent Neural Network (LSTM RNN) which represents long-term dependencies and determines the optimal time lags for time series problems. The study used data from the Caltrans Performance Measurement System (PeMS) and included a comparison of the LSTM RNN model with other four established prediction models, i.e., RW (Random Walk), SVM (Support Vector Machine), FFNN (Feed Forward NN) and SAE (Sum of Absolute Errors). This study"
10.36001_phmconf.2019.v11i1.869,"dataset, data",291,,0,"where 𝜀 and 𝑛 are the MSE and number of measured features. In addition, a strategy called dropout is implemented in this work to enhance the generalization capability of the DNN model, which enables the DNN model to detect off nominal behavior in the FDR dataset. The dropout randomly excludes a certain percentage of neurons in input and hidden layer during training the DNN model, which leads to model averaging to mitigate overfitting problem (Xu, Du, Dai, & Lee, 2014). In this case, flight performance anomalies that significantly deviates from standard behavior may not much contribute to train the DNN model. Because the significant off nominal response in the selected feature is a minor portion of the FDR dataset and therefore these trends are most likely ignored during the training process. Therefore, the DNN model parameters are mainly determined by the majority of the FDR data, which is considered as normal response in engine related and flight dynamic features. To fulfill the estimation accuracy and generalization capability, the percentage of dropout is set to be 5%. With this assumption, a large estimation error between features from the FDR and trained model is expected to be observed under the presence of operational anomalies. The estimation error is measured by root mean square error (RMSE) and the coefficient of determination (𝑅2). In particular, the RMSE are assumed to 𝑗 is utilized be normally distributed. A normal error bound 𝛿𝜀𝑖 for a safety baseline and defined by the mean and variance of the observed RMSE in the 𝑖𝑡ℎ features for 𝑘𝑡ℎ time step as"
10.36001_phmconf.2019.v11i1.869,"dataset, data",297,,0,"In this section, the training process of the DNN model is described and the performance of the trained model is evaluated. Initially, the monitoring features (47 features) are introduced as normalized form (between 0 and 1) to train the DNN model. The normalized features are then scaled back to the original scale in the testing phase for anomaly detection. The adaptive moment estimation (Adam) method is used for the training algorithm to utilize its performance on sparse gradient problem (Kingma & Ba, 2014). Among the 2044 flights investigated in this work, 1635 flights (80% of data) are utilized for training, and 409 flights (20% of data) are used for testing. To obtain the generalized model, the DNN model is trained with a five-fold cross-validation method. In particular, the preprocessed dataset is randomly divided by five segments; four segments are used to train the model, and one segment is utilized to validate the model. The overall estimation errors are measured by 0.0148 RMSE and 0.8432 𝑅2 in training phase, and 0.0195 RMSE and 0.7313 𝑅2 in testing phase, respectively, which shows a high estimation accuracy of the trained model. An example is presented in Figure 3, where the estimated GS (Figure 3 (a)) and FS (Figure 3 (b)) in testing phase are compared with the recorded GS and FS. In general, the estimated GS and FS show good agreements with the recorded GS and FS in the FDR dataset. The behavior of GS and FS are estimated within 3% error."
10.36001_phmconf.2019.v11i1.869,"dataset, data",317,,1,"Sanitized commercial flight data recorder (FDR) datasets, obtained the National Aeronautics and Space Administration (NASA) Discovery in Aeronautics Systems Health (DASH) link network (Monroe, Freeman, & Jones, 2012), is utilized to perform the proposed monitoring method. The FDR dataset is comprised of flight features recorded from the onboard sensors in commercial aircraft during inflight and on-ground operation and is generally used for flight analytics after flight accidents. The FDR dataset includes 186 flight features that can be categorized into continuous and discrete features. The continuous features generally indicate aircraft states and performance such as fan speed, engine gas temperature, and velocity and angle components of aircraft coordinate system while the discrete features denote binary information such as system alarms and landing gear status. Besides, the sampling frequency of flight features varies between 0.25 Hz to 16 Hz, which restricts the direct application of the dataset to the proposed framework. In this study, a total of 2044 flights are investigated, where the flight times in the dataset vary from 56 minutes to 3 hours and 22 minutes and flight phases are labeled in to 7 digits, parking (1), taxi (2), takeoff (3), ascent (4), cruise (5), descent (6), and landing (7) as shown in Figure 1. Among the flight phases, the proposed monitoring work focuses on detecting engine performance anomalies in cruise phase. The magnitude of engine-related features such as fan speed and exhaust gas temperature are ideally maintained during cruise phase, while fluctuations are observed during multiple ascent and descent phases. Therefore, off-nominal behavior of flight performance is relatively distinguishable compared to other flight phases."
10.36001_phmconf.2019.v11i1.869,"dataset, data",322,,0,"The FDR dataset includes 186 flight features that can be categorized into continuous and discrete features. The continuous features generally indicate aircraft states and performance such as fan speed, engine gas temperature, and velocity and angle components of aircraft coordinate system while the discrete features denote binary information such as system alarms and landing gear status. Besides, the sampling frequency of flight features varies between 0.25 Hz to 16 Hz, which restricts the direct application of the dataset to the proposed framework. In this study, a total of 2044 flights are investigated, where the flight times in the dataset vary from 56 minutes to 3 hours and 22 minutes and flight phases are labeled in to 7 digits, parking (1), taxi (2), takeoff (3), ascent (4), cruise (5), descent (6), and landing (7) as shown in Figure 1. Among the flight phases, the proposed monitoring work focuses on detecting engine performance anomalies in cruise phase. The magnitude of engine-related features such as fan speed and exhaust gas temperature are ideally maintained during cruise phase, while fluctuations are observed during multiple ascent and descent phases. Therefore, off-nominal behavior of flight performance is relatively distinguishable compared to other flight phases. However, health metric and standard operation ranges are not presented in the FDR dataset, so it prevents cross-validation of captured operational anomalies from flight accident reports and literature because of unknown information about aircraft type, flight plan, and weather condition. With this limitation, a decision boundary was formed under the assumption that the anomalous flight behavior is uncommon patterns in dataset and presents significant deviation from common patterns due to nonstandard flight operations or mechanical issues in the aircraft systems."
10.36001_phmconf.2019.v11i1.876,"dataset, data",66,,0,"(Yan & Yu, 2015), whereas classical machine learning models require a more statistically large data set (Bishop, 2013), with a signiﬁcant number of instances of failure, which are expensive to collect. By the time of this study, autoencoders have become the ﬁrst choice method for anomaly detection in PHM (Eklund, 2018)."
10.36001_phmconf.2019.v11i1.898,"dataset, data",99,,0,Figure 3 gives an overview of the generic AD architecture. Flight AD is generally based on the assumption that most of the flights in the dataset represent normal operations. The DA process thus involves preparing the data for analysis and the AD process begins with the output from the DA into a detection phase and then followed by the characterisation of the anomalies found as the output of the AD process. The detection phase generally entails performing similarity measures between data points thereby determining the nominal group and using this to ascertain the outlier groups;
10.36001_phmconf.2019.v11i1.898,"dataset, data",110,,0,"Anomaly Detection refers to the task of identifying new or unknown patterns which, in many cases are abnormal or inconsistent from the norm. AD finds patterns in data that do not conform to expected behaviour (Chandola, Banerjee, & Kumar, 2009). These patterns are called anomalies or outliers and they are often sufficiently different from the majority of data points in the dataset. Since the aircraft operations are standardized, it is assumed that the majority of flights represent normal operations at that given time. Flight data analysis has given rise to a number of AD techniques for discovering anomalies."
10.36001_phmconf.2019.v11i1.898,"dataset, data",134,,0,"Sequence Miner technique (Budalakoti, Srivastava, & Otey, 2009) was developed as an unsupervised method to analyse discrete parameters and reveal atypical flight sequences. Only discrete parameters were considered, and it solves the problem of detecting and describing anomalies in large datasets of high-dimensional symbol sequences. It does this by using the nLCS (normalized Longest Common Subsequence) based distance measure. It is able to detect anomalies across a fleet of aircraft. It is, however, limited to analysing similar flights and aircraft since discrete sequences can vary greatly depending on the route, airport and equipment. It is unable to integrate its findings of discrete actions with other continuous flight parameters since its performance degrades with interaction with continuous data."
10.36001_phmconf.2019.v11i1.898,"dataset, data",138,,0,"Vivian R. Igenewari is a PhD researcher in the Integrated Vehicle at Health Management Cranfield University, since November 2017. Her Bachelors degree was in Computer Science from Babcock University, Nigeria in 2004 and her Masters degree was in Advanced Computer Science (Cloud Computing) from the University of Leeds in 2014. Vivian has worked as a Systems Analyst/Administrator and Application Support Engineer/Analyst at several companies including Vanfrank Limited, TEAM Energy and Cognisco Limited. As an Application Support Analyst, Vivian was part of business improvement projects working with querying databases for the purpose of finding useful patterns and developing intelligent reporting tools which provided additional sources of income for the organisations. Her current research interests involve big data analytics, machine learning and anomaly detection techniques for aerospace-related datasets."
10.36001_phmconf.2019.v11i1.898,"dataset, data",176,,0,"Developed by (Bay & Schwabacher, 2003), Orca is a knearest neighbour based unsupervised AD algorithm. It has a nested loop structure to calculate pairwise distances between data points but uses a simple pruning rule to keep the time complexity significantly less than the number of data points achieving near-linear performance with high dimensional data thereby making it appropriate for analysing large datasets. It can process both continuous and binary data format. A limitation of this method is that since each data point is scored independently, anomalies in the time domain cannot be detected (Das, Sarkar, Ray, Srivastava, & Simon, 2013). iOrca (Bhaduri, Matthews, & Giannella, 2011) is a scalable version of Orca since the developers of iOrca introduced an indexing strategy and an early termination criterion to make it scalable to extremely large datasets during processing (Das et al., 2013) but still suffers from the time domain limitation of Orca."
10.36001_phmconf.2019.v11i1.898,"dataset, data",308,,0,"One of the more recent techniques is the Multiple Kernel Anomaly Detection commonly known as MKAD (Das, Matthews, Srivastava, & Oza, 2010). It has been extended and investigated for small and medium-sized medical and finance datasets (Gautam, Balaji, Sudharsan, Tiwari, & Ahuja, 2019). MKAD is a semi-supervised algorithm proposed for AD. Heterogenous parameters (continuous and discrete) were handled together. Discrete binary switch sequences are used in the discrete kernel and discretized continuous parameter features are used to form the continuous kernel. MKAD combines multiple kernels into a single optimization function using the One-Class Support Vector Machine (OCSVM) framework. The OCSVM uses a training set of nominal examples (in this case, flights) and evaluates test examples to determine whether they are anomalous or not. It combines the strength of both vector space and sequential AD techniques to allow a wider range of anomaly detection. Its ability to combine information from both data types makes it able to identify more complex behaviours of the crew in the anomaly detection process, unlike previous methods. It is, however, more able to find anomalies in discrete data than it is able to detect those from continuous data. For example, while is it was able to detect anomalies in an auto-landing state due to an autopilot mode anomaly, it was unable to detect a high-speed approach based on relevant continuous parameters (Das, Li, Srivastava, & Hansman, 2012). MKAD also needs nominal or near nominal data to learn proper hyperplanes hence, is not appropriate for including multiple nominal data patterns since the trained model could identify other nominal patterns as anomalous."
10.36001_phmconf.2020.v12i1.1130,"dataset, data",38,,0,where i is the number of the data set selected for the FFT analysis. (cid:1839)(cid:3036)(cid:2879)(cid:2869) is the average order magnitude calculated from the previous datasets.
10.36001_phmconf.2020.v12i1.1130,"dataset, data",116,,0,"and 44 test datasets from the faulty front rotors. After running our algorithms, there is no decision for 305 cases due to the data issue. For the remainder of 464 tests, the algorithm accuracy is 100%, the false positive rate is 0%, and the false negative rate is 0%. For 31 tests with one faulty rotor, our algorithm accurately determine the location to be front left for 30 cases. For the one case that the algorithm is not able to localize the fault, the algorithm output is “not severe fault”. The accuracy of the localization algorithm is 96.8%."
10.36001_phmconf.2020.v12i1.1130,"dataset, data",163,,0,"Although the performance of aforementioned approaches is good for certain test datasets, their robustness under different noise factors or other failure modes is still a concern. Furthermore, some sensors required in these approaches are not available in most vehicles. To address these issues, a novel diagnostic approach using existing vehicle signals for brake DTV fault is proposed in this work. As described in our previous work (Kazemi, Du, Dixon, & Sadjadi, 2019), three vehicle signals are selected to extract fault signatures based on vibration characteristics. A unique fault isolation logic with sophisticated design of enabling conditions and some data processing procedures is used to detect the fault and isolate the fault to each vehicle corner. For the remainder of this report, the details of the fault isolation algorithm are described in section 2, followed by the validation using the test data from a real vehicle."
10.36001_phmconf.2020.v12i1.1134,"dataset, data",2,,0,Data set
10.36001_phmconf.2020.v12i1.1134,"dataset, data",102,,0,". Finally, by combining 1 , ⋯ , 𝒀𝒀� �𝒀𝒀� 1 the dataset �𝑿𝑿� 𝑓𝑓(𝑿𝑿) � �, ⋯ , �𝑿𝑿� This study uses 24 cases out of over 50 cases of static-firing test results to train the regression model, and the gross number of  is approximately 650,000. The test data for the regression model is the time series shown in Fig. 3, and its is approximately 30,000. sensors listed in Table 1 and signals of valves, and three members: Res1, Res2, and Res3 shown in Fig. 4."
10.36001_phmconf.2020.v12i1.1147,"dataset, data",41,,0,Cross-validation approach is employed in this example to train the CNN model. The training image data set is used to tune the model parameters while the validation image data set to monitor the model accuracy and prediction error. The
10.36001_phmconf.2020.v12i1.1162,"dataset, data",55,,0,"Deﬁnition 6 (Isolable faults) Fault fi is isolable from fault fj if there exists a data-driven ARR, r = (vt, V ), in the data set where there is signiﬁcant statistical difference between ˆvt = model(V ) when fault fi occurs and when fault fj occurs."
10.36001_phmconf.2020.v12i1.1162,"dataset, data",58,,0,"Deﬁnition 5 (Detectable fault) A fault f is detectable if there exists a data-driven ARR, r = (vt, V ), in the data set where there is a statistically signiﬁcant difference between ˆvt = model(V ) when fault f occurs and when it does not (normal operation)."
10.36001_phmconf.2020.v12i1.1162,"dataset, data",87,,0,"Deﬁnition 2 (FDI) Fault detection determines the occur Deﬁnition 3 (Data-driven ARR) A set of variables in the dataset, V , plus a target variable vt where vt / V represent 2 a data-driven ARR if there exists a machine learning model that can estimate vt using V with a given accuracy, ✏, for the validation data, ˆvt = model(V ), where ˆvt ✏ . The || data-driven ARR for variable vt is rt = ˆvt"
10.36001_phmconf.2020.v12i1.1162,"dataset, data",121,,0,"Not all the analytical redundancies (data-driven ARRs) are useful for fault detection and isolation. For each fault mode in the system, we go through all the ARRs and select the best set of ARRs to detect the fault. Different metrics can be used to measure the performance of ARRs in fault detection and isolation. For the sake of demonstration, we use the following simple approach: For each residual we use Z-test to quantify the difference between the residual in faulty and normal dataset. We select the residuals that their hypothesis tests show statistically signiﬁcant difference between normal and faulty data. The selected residuals can also be used to isolate faults."
10.36001_phmconf.2020.v12i1.1162,"dataset, data",140,,0,"Forward feature selection: The exhaustive search ﬁnds all the minimal data-driven ARRs in the dataset and therefore, it is guaranteed to achieve maximum detectability and isolability. However, this algorithm is computationally expensive and may not be practical for systems with large number of measurements. To address this problem, we propose a forward feature selection algorithm as an alternative solution. We use forward feature selection to select the minimum number of variables which can estimate the value of each variable in the dataset. • For each target variable, vt, in the set of variables we go through the dataset and measure the estimation score for each variable in the dataset. We add the variable with the highest estimation score as the ﬁrst variable in the residual list, R."
10.36001_phmconf.2020.v12i1.1162,"dataset, data",150,,0,"In the previous section, we deﬁned a data-driven ARR as a set of variables in the dataset, V , plus a target variable vt where we can use V to estimate vt. To ﬁnd these set of variables for each target variable, we propose two method 1) exhaustive search, 2) forward feature selection. The exhaustive search algorithm can ﬁnd several ARRs for each target variable. This method can be helpful when there are several faults in the system and more ARRs can help isolating faults from each other. However, the exhaustive search is computationally expensive. The feature selection algorithm is an efﬁcient alternative solution. The feature selection method ﬁnds at most one ARR per target variable. Even though this may lead to missing some ARRs, we believe feature selection algorithm is sufﬁcient for most applications."
10.36001_phmconf.2020.v12i1.1205,"dataset, data",58,,0,Figure 2(a) displays the measured and predicted values of the generator bearing temperature of wind turbine A0 over a period of about 2 years. The ﬁrst 9 months of data from A0 were used for training and validation and the rest for testing (standard single-turbine training with a large data set).
10.36001_phmconf.2020.v12i1.1205,"dataset, data",63,,0,"scheme in these examples. However, when comparing all cross-turbine results with the single turbine case with limited data, the advantage of using the cross-turbine scheme is clear: the performance and robustness of all cross-turbine training sets is closer to the ideal single turbine Baseline (solid black) than to the limited data set (solid grey)."
10.36001_phmconf.2020.v12i1.1205,"dataset, data",139,,0,"Figure 5 shows the resulting health index h(tw) in degrees Celsius for each time window. Figure 5(a) shows the results of training and predicting on the same turbine B0 with only limited training data of 3 months. The results can be contrasted with the one of panel (b) of this ﬁgure, which displays the Baseline results of training and predicting on B0 using a large data set of 9 months. Clearly, the prediction errors and thus the extracted health indices in the limited data case suffer from a lower signal to noise ratio, leading to a later detection time. The cross-turbine scheme is examined in Figures 5(c) and (d). In these cases training was performed on large data"
10.36001_phmconf.2020.v12i1.1205,"dataset, data",161,,0,"ing scheme. This means that one can train the CNN on a turbine for which historical data are abundant and use the trained network to detect faults early and with high precision on another turbine in the wind farm for which there are very little data. This is particularly useful for newly installed wind turbines in already existing wind farms. • Training our CNN model in the cross-turbine scheme shows a considerable improvement in the detection accuracy compared to the single-turbine training with only limited data. At the same time, it shows almost no inferiority to training it with the single-turbine scheme with a large and representative data set. This is observed for diverse fault types: abrupt and slowly evolving in different components (generator or main bearing) and different wind farms. The faults were detected as early and with a similar conﬁdence level and robustness as in the singleturbine-large-data case."
10.36001_phmconf.2020.v12i1.1205,"dataset, data",170,,0,"We used a new CNN architecture that we developed previously (Ulmer et al., 2020) for early fault detection based on 10-minute SCADA data of wind turbines. The CNN was originally developed for the standard single-turbine fault detection scheme, that is, training it with historical SCADA data from a certain turbine in order to detect faults of the same turbine. Here we extend the usage of this CNN model to a cross-turbine scheme. We train the model on a certain turbine S, and use the trained network for on-line fault detection of a different turbine T from the same wind farm. The crossturbine algorithm includes a post-processing step of rescaling of residuals in order to compensate for turbine differences. This step requires only a small reference data set from the target turbine T. We tested the performance of the cross-turbine scheme on two fault types in different wind farms: abrupt faults and slow degradation. We showed that:"
10.36001_phmconf.2020.v12i1.1205,"dataset, data",183,,0,"The conclusion from analyzing the results for detection of slowly degrading faults is that the CNN model with crossturbine training in case of limited training data performs very well and rather similarly to the standard single-turbine training with a large data set. Compared to a single-turbine training with only little data, the cross-turbine scheme offers a considerable improvement concerning the earliest fault detection time. The cross-turbine scheme can thus be used as an alternative to the standard training scheme in case of limited data for the target turbine. However, the choice of the source turbine, i.e the turbine used for training, can inﬂuence to some extent the accuracy and reliability of the detec Figure 6. Sensitivity analysis for detection of slowly evolving faults. Dependence of the ﬁrst detection date on the desired conﬁdence level C = − log10 α for the 4 schemes: Limited data (trained on Turbine B0), Baseline (trained on Turbine B0), cross-turbine trained on Turbine B1, and cross-turbine trained on Turbine B2."
10.36001_phmconf.2020.v12i1.1274,"dataset, data",188,,0,"The estimate of when it is appropriate to do maintenance is a threshold setting problem. We have approached this as hypothesis testing (Bechhoefer, 2007, 2011). That is, does the measured set of condition indicators have enough evidence to suggest that the component is no longer good (e.g. reject the Null Hypothesis that the component if normal). While many classification problems have been solved using artificial neural networks (ANN) or deep learning techniques, this problem is better suited to this hypothesis testing process. Data driven the classification of know-knows (e.g. there is a training set of known nominals and known faults to train against). Hypothesis test functions in the domain of know-unknows. Know-unknowns is used to describe systems or components which are typically nominal, and where fault cases are rare. This is the condition normally found in condition monitoring, as the vast amount of data collected is asymmetric – there are few in any examples of damage and most of the data set are nominal."
10.36001_phmconf.2020.v12i1.1288,"dataset, data",245,,0,"datasets. The data processing, HI prediction and risk quantification are deployed sequentially following the procedures from Figure 1. The prediction performance is validated using RMSE and Score calculated using Eq.(5). As noticed from literature, a constant value 𝑅𝑒𝑎𝑟𝑙𝑦 is assigned as target RUL before degradation period (X. Li, Ding, & Sun, 2018). In this case, 𝑅𝑒𝑎𝑟𝑙𝑦 is set to 125 for result evaluation. The selection of risk level for estimating RUL depends on whether 𝑅𝑒𝑎𝑟𝑙𝑦 is applied or not. When 𝑅𝑒𝑎𝑟𝑙𝑦 is not applied, a 75% risk level is selected for relatively conservative prediction. When 𝑅𝑒𝑎𝑟𝑙𝑦 is applied, the largest prediction RUL is limited to 125 therefore the risk level can be selected as large as possible. In this case 95% risk level is selected when 𝑅𝑒𝑎𝑟𝑙𝑦  is applied. To compare and evaluate the performance of the proposed method, some benchmarked methodologies are summarized from literature: DCNN and LSTM from (X. Li et al., 2018), DBN (Deep Belief (Multi-Objective Deep Belief Networks), MODBNE Networks Ensemble), RF (Random Forest), GB(Gradient Boosting), SVM (Support Vector Machine) and LASSO reported from (Zhang, Lim, Qin, & Tan, 2016), and RULCLIPPER from (Ramasso, 2014)."
10.36001_phmconf.2020.v12i1.1300,"dataset, data",83,,0,"In the PHM domain, TL could save great endeavors to manually labeling data and retuning models for new problems. Specially, given the fact that high-quality labeled operation data that also includes failures is rare. Therefore, it is highly desirable to be able to use a model that is trained with a good dataset on a speciﬁc equipment and working condition on other related problems. One could imagine several valuable applications for this possibility, such as:"
10.36001_phmconf.2020.v12i1.1300,"dataset, data",97,,0,"(Zhong, Fu, & Lin, 2019) proposed training a CNN-based anomaly detector (only two classes: normal and abnormal) on a gas turbine dataset. Then transferring the convolution layers to the target domain classiﬁer (parameter-based TL) and feeding those features to a Support Vector Machine (SVM) for classifying the gas turbine condition into four different classes (1 normal and 3 fault classes). Their approach has signiﬁcantly improved the fault diagnosis performance with an small amount of labeled data in the target domain."
10.36001_phmconf.2020.v12i1.1300,"dataset, data",112,,0,"As shown by (Xu et al., 2019), using a digital twin, it is possible to generate data (source domain) with an acceptable volume and variety for training a proper initial deep learningbased fault diagnostics model. They have developed such model by training a Stacked Sparse Auto Encoder (SSAE) on data from the digital twin of a car body-side production line and transferred the obtained model parameters (parameterbased TL) to be ﬁne tuned using physical monitoring dataset. Other types of simulation have also been used in the literature as the source domain. To mention a few, using feature-based"
10.36001_phmconf.2020.v12i1.1300,"dataset, data",134,,0,"transferred the low-level feature extractors of the VGG-16 to a new deep learning model that its task is machine fault diagnosis and condition monitoring. The original sensor data in the target domain are in time series format. The authors have used time-frequency imaging to convert the sensor data to images and increase the similarity between the source and target domains. Following the same approach, (Wen et al., 2019) has used the ResNet-50 (pre-trained on ImageNet dataset) deep CNN network (He, Zhang, Ren, & Sun, 2016) as the feature extractor for a machine fault classiﬁcation network. They too, have converted time series sensor data to RGB images in order to make it compatible with the ResNet-50 inputs."
10.36001_phmconf.2020.v12i1.1300,"dataset, data",241,,0,"In another study by (Tong, Li, Zhang, & Zhang, 2018) on domain generalization for bearing fault diagnosis, in order to reduce the marginal distribution difference between the domains and extract maximally domain-invariant features, Maximum Mean Discrepancy (MMD) measure is used. They use this measure to regularize the two dataset in a way that minimizes the MMD measure. Afterwards, ﬁrst the classiﬁer is trained on the transformed source domain (labeled). Then, pseudo-labels are generated for the target domain data (unlabeled) which is simply picking up the class that has the maximum predicted probability, as if they were true labels (Lee, 2013). Having labels for both domains, the difference between the class-conditional probability distributions can be calculated (using modiﬁed MMD measure) and be incorporated into the model training loss function to be minimized and provide a domain-invariant feature generator. To address the same problem as deﬁned by (Tong et al., 2018), (Sun, Wang, Liu, Huang, & Fan, 2019) has used the MMD to measure the difference between the source and target domains hierarchically-obtained features in a Sparse stacked denoising autoencoder architecture. This difference terms are then summed and fused into the training loss function to be minimized and help obtaining domain-invariant features."
10.36001_phmconf.2021.v13i1.2983,"dataset, data",111,,0,"The injection molding machine produces plastic casings for Raspberry PI systems. For our method we split the time series data from the machine into the subsets S and M . In set S we captured signals such as temperatures, cycle times, pressure values, and speed of various components. In set M we represented measurements from an optical quality control system. The size of the datasets was 11 signals for S and 10 signals for M . Faults were injected through randomly setting some signals in set M to false, which corresponds to wrong size measurements of the produced parts in the real world."
10.36001_phmconf.2021.v13i1.2997,"dataset, data",10,,0,MABox data set (min) 136 72 72 70
10.36001_phmconf.2021.v13i1.3000,"dataset, data",144,,0,"This paper investigated the performance of sparsity-based blind ﬁltering approaches on experimental data. It is proven that, with a known the average or the instantaneous shaft speed, a blind ﬁltering approach is capable of detecting the incipient rolling element bearing fault of a complex industrial rotating machine. Furthermore, results of the broad parameter study stress that ﬁlter length is a signiﬁcant parameter that inﬂuences the convergence characteristic of the iterative solution that maximizes the generalized Rayleigh quotient. For the given dataset, the conclusion can be drawn that ﬁlter initialization induces no effect on either the numerical stability or the computation time of the iteration process. In summary, the blind ﬁltering method can be used as an autonomous health surveillance tool for complex industrial applications in which exact characteristic frequencies of mechanical components are lacking."
10.36001_phmconf.2021.v13i1.3000,"dataset, data",203,,0,"The last measurement in the data set corresponds the one where the machine has failed. The damaged inner ring of the rolling element bearing can be seen in Fig. 1. The machine is stressed at full capacity for the duration of the experiment which induces high loads on the bearings and signiﬁcantly accelerates the bearing degradation. The evolution of the bearing fault in the zoomed squared envelope spectra is discernable in Fig. 2 with an increase in the amplitude at the ballpass frequency inner race (BPFI) order of 8.29. Approximately after the measurement 30, which is indicated with the vertical dashed line, the SES amplitude around order 8.29 becomes distinct, which can be easily monitored by tracking the BPFI order. Two basic statistical indicators estimated using time-domain waveform of the signal are demonstrated in Fig. 3. An increasing trend for both the root-mean-square and the kurtosis of the signal is observed from measurement 43 onwards, which is an indication of a potential bearing fault. While these statistical indicators can be considered ’blind’, they do not include any ﬁlter optimization nor do they try to"
10.36001_phmconf.2021.v13i1.3000,"dataset, data",316,,0,"The vibration signals investigated in this study were sampled at 40 kHz for 2 seconds at every 10 minutes and a dataset containing 73 measurements was obtained. The average rotational speed of the high-speed shaft was 279 Hz. In general angular resampling is necessary for a proper vibration based condition monitoring of a rotating machine. The angular resampling requires the knowledge of the instantaneous speed of the shaft so that the data can be transformed to the angular domain in order to compensate for the speed variations (Peeters et al., 2019). Nevertheless, the present dataset was sampled under negligible speed variations, hence the angular resampling is not needed. Accordingly, the signal spectrum is dominated by pronounced shaft harmonics. Another potential pre-processing method to improve the effectiveness of blind ﬁltering is the deterministic content removal. The highenergy deterministic content protrudes in the squared envelope spectrum and might mask the bearing fault signature. Since the localized bearing faults manifest themselves as a stochastic process, the energy level of which are lower in the squared envelope spectrum compared to deterministic content originated from the other machine components (Antoni & Randall, 2003). Thus, in order to ensure that the sparsity measure of the blind ﬁltered signal is not skewed by the highenergy harmonics, the deterministic content of the signal can be removed prior to blind ﬁltering using several techniques discussed in literature, i.e. cepstral editing or discrete-random separation (Peeters et al., 2020). However, the signals are not pre-whitened in this study, but instead bandwidths to which blind ﬁltering is applied are adjusted in such a way as to exclude the prominent amplitudes related to the shaft harmonics in squared envelope spectrum. This step is explained in the next section."
10.36001_phmconf.2021.v13i1.3003,"dataset, data",100,,0,"cles and re-assemblies. These datasets were labelled B1 to B8. Baselines B7 and B8 involve partial gearbox disassembly for which the top gear was removed and reinstalled. The number of baselines was somewhat arbitrary (and the fact that it coincides with the number of crack propagation was just a coincidence): the objective was to collect data on a number of baselines, but at the same time to avoid unintended crack propagation (due to (cid:39)30% fatigue bending overload) before gear tooth is equipped with a crack-propagation sensor."
10.36001_phmconf.2021.v13i1.3003,"dataset, data",101,,0,"A successful implementation and deployment of an autoencoder predates the emergence of deep learning (Japkowicz, Myers, & Gluck, 1995). More recently, autoencoder-based anomaly detectors have been shown to have considerable promise because, unlike classical classiﬁers that demand balanced datasets, their training can be based on data associated with normal operation, which comes in abundance, as opposed to data associated with failures, which is difﬁcult to come by (Eklund, 2018; Yan & Yu, 2015). The performance of autoencoders was compared to classical CIs."
10.36001_phmconf.2021.v13i1.3003,"dataset, data",140,,0,"The main dataset consists of eight tests, each denoted by the label of the gear with a cracked tooth, viz. Gear 207 for gear pair 207/208 and Gear 209 for pair 209/210, etc. All the gears were the same new NASA-designed spur gears (NASA, 1994). The number of gears employed in the experiment was limited by the time required to successfully conduct the laborintensive experiments that require multiple gearbox assemblies following a detailed checklist of measurements, crack initiation, and crack veriﬁcation (see Figure 1). Acceleration data, along with the tachometer and CP sensor data, was captured at 100 kHz sampling rate and saved in separate ﬁles representing one-second of data. The accelorometer place Figure 1. Block diagram of the test process."
10.36001_phmconf.2021.v13i1.3007,"dataset, data",91,,1,"To illustrate the training procedure and discuss the key features of the proposed risk quantification model, a trivariate example is shown based on life, sensor #2 and sensor #4 (denoted as ‘Sensor 1’ and ‘Sensor 2’ for simplicity) from C-MAPSS dataset. These two representative sensors are selected based on the distribution disparity at healthy and faulty operation status as shown in Figure 2 (a) and (b). The faulty data generally follow a normal distribution."
10.36001_phmconf.2021.v13i1.3007,"dataset, data",133,,1,"To address these challenges, this study proposes 1) a Gaussian Copula model-based risk quantification method to determine multivariate failure thresholds, and 2) a Similarity enhanced Blackwellized Particle Filter (RBPF) to predict future system conditions. The proposed Gaussian Copula Model could establish the joint effects of life and multiple variables. Life is essentially used in reliability analysis, and other variables can be health indicators or raw sensory readings. Based on the proposed risk model, a systematic methodology for RUL prediction is proposed. Two cases of establishing bi-variate and tri-variate failure thresholds are demonstrated and discussed. The proposed RUL prediction methodology is validated on the aero-engine RUL prediction task based on the C-MAPSS dataset from the PHM society data competition 2008."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",23,,0,"training and testing data set, the total for each being 119616 since there are seven failure incidents in the data set."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",37,,0,"Table 5 describes the approximate number of leaders used for the optimization training and testing data set for Vehicle 1, the total for each being 102,528 since there were 6 failures in the data set."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",42,,0,"As with the randomly sampled data, the leaders clustering algorithm was used with the blind failure data sets to reduce the size of the data set. Table 8 describes the approximate number of leaders used for the combined vehicle optimization"
10.36001_phmconf.2021.v13i1.3052,"dataset, data",45,,0,"Table 7 in section 5.2.1 describes the number of points associated with each class in all the configurations training and testing data set. Figure 15 displays the scores from the training, while Figure 16 shows the results of testing of each classifier."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",47,,0,"Table 2, Table 3, and Table 4 describe the number of points associated with each class in all the configurations training and testing data sets for Vehicle 1, Vehicle 2, and the combined data set with both vehicles’ data, respectively."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",69,,0,"Table 6 describes the approximate number of leaders used for the Vehicle 2 optimization training and testing data set, the total for each being 17,088 since there was one failure event in the data set. Since the leader algorithm could not be used to achieve a specific number of points but only a similarity threshold, the final data sets may vary by ± 100 points."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",81,,0,"SMOTE (Chawla, Bowyer, Hall, & Kegelmeyer, 2002) restores balance in the data set by synthesizing new examples of the minority class. By generating more failure examples to train the classifier, it is less likely that a biased classifier with low precision is created. SMOTE uses a kNN algorithm to find the nearest neighbors in the feature space and place new samples on the lines connecting existing samples in the feature space."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",90,,0,"In the CV-inspired approach, described further in section 5.3, three techniques were used to over- and under-sample the existing data in order to achieve the desired data distribution in each fold: Synthetic Minority Over-sampling Technique (SMOTE), random under-sampling, and a leader clustering algorithm. The latter algorithm was also used prior to hyperparameter optimization in the random sampling (section 5.1.2) and blind failure (section 5.2.2) data selection strategies in order to reduce the size of the data set."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",94,,0,"The leader clustering algorithm (Hartigan, 1975) is used to construct samples from the total data set to facilitate the analysis of data. Given Euclidian distance as a similarity measure, the algorithm reduces the data set to representative objects (leaders) that cluster similar (close in distance) samples into fewer elements. In this way, the constructed samples are inclusive of the entire space covered by the original data set and no outliers are left unaccounted for, as may occur with conventional random sampling."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",99,,0,"It should be noted that comprehensive studies like what was done in this work can be useful in other failure modeling or fault detection applications. Understanding the impact of the statistical properties of the data set, as well as understanding the variations in the classifiers are both important. They provide further knowledge of the problem, and can be used to study the consistency and reliability of the models built. Exploring different classifier types, data sampling techniques, and hyperparameter optimization are all ways to complete a thorough study of any modeling problem."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",114,,0,"From the Vehicle 1 results, it is noted that AdaBoost, XGBoost, kNN experienced slight improvements from optimization. However, RF and SVM performance significantly reduced from optimization, which could be because the hyperparameter values that were explored were not well suited to the data set. From the Vehicle 2 results, many classifiers performed very well, showing improvements on the approach with no optimization. Most models achieved scores very close to the (1,1) ideal result except for AdaBoost and GausssianNB, which is consistent with the results obtained previously. The Vehicle 1 results were not as strong as those for Vehicle 2."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",121,,0,"Figure 4 describes the process performed to create the folds for cross-validation. The goal was to create equal folds with the same class distributions, as it is important in crossvalidation for the folds to have similar statistical properties. To try to reduce the class imbalance existing in the original data set, it was decided that each fold should contain approximately 50% healthy points, 40% failed points, and 10% deteriorated points. Three techniques were used to over- and under-sample the data that already existed to obtain the desired distribution in each fold, described in section 4.2: Synthetic Minority Over-sampling Technique (SMOTE), random under-sampling, and leaders."
10.36001_phmconf.2021.v13i1.3052,"dataset, data",172,,0,"On the left side of Figure 4 the process to select the healthy data is shown. The identified healthy leaders were randomly shuffled and split into the five equal groups. The right side of Figure 4 displays the process to select the failure and deteriorated data points for each fold. Each failure event was separated into groups containing the failed points and accompanying deteriorated points that occurred prior to the failure. To try to not heavily oversample or under sample a specific failure event, all of the failures were either extracted or reduced to the average number of failed points, 6835. If a failure event had more failed data points than the average, the data set was reduced further using random under-sampling. If a failure event had less points than the average, SMOTE was used to generate more points. SMOTE was also used to oversample each group of deteriorated points to create the 10% representation desired in each fold."
10.36001_phmconf.2021.v13i1.3059,"dataset, data",9,,0,dataset and a large set of unlabeled data.
10.36001_phmconf.2021.v13i1.3059,"dataset, data",22,,0,Step 4 The oracle gives a label to the previously selected data point and the labeled and unlabeled datasets are updated.
10.36001_phmconf.2021.v13i1.3059,"dataset, data",72,,0,The time series data in the production dataset is formed by instances of different lengths resulting in the need for data padding to ensure equal data lengths for ease of calculations. Padding was carried out by extending or clipping the beginning of the time series to the initial stationary value of the series. An earlier labeling effort of the 2000 available instances was the ground truth for this dataset.
10.36001_phmconf.2021.v13i1.3059,"dataset, data",77,,1,"We are interested in the performance of the time series classiﬁcation model with respect to the number of queries carried out by the active learning framework. We consider two case studies to evaluate this performance. We analyze vibration signals from the bearing data center at Case Western Reserve University (Loparo, 2003) and time series data from a control problem belonging to one of our customers, henceforth as the production dataset."
10.36001_phmconf.2021.v13i1.3059,"dataset, data",79,,0,"The performance of the active learning framework in this dataset is also evaluated using learning curves. Figure 6 shows the learning curves for the imaging scenario where the production time series data was converted into images. Figure 6a presents the original imaging implementation, while Figure 6b presents the modiﬁed implementation with the update proposed by Rodriguez-Garcia et al. (2021). It should be noted that it was not possible to evaluate the production"
10.36001_phmconf.2021.v13i1.3059,"dataset, data",106,,0,"The number of clusters to be identiﬁed by the clustering algorithm is deﬁned by the number of categories upon which the data is expected to be classiﬁed. Alternatively, the elbow method (Marutho et al., 2018) can be used to determine the optimal number of clusters when this information is not known. In addition, the number of points to be selected out of each cluster is deﬁned by a pre-deﬁned threshold, which can be in the order of 1-2%, that describes the size of the initial dataset with respect to the size of the unlabeled dataset."
10.36001_phmconf.2021.v13i1.3059,"dataset, data",146,,0,"Sample query is Step 3 in the active learning framework. In this step, the data sample from the unlabeled dataset to be presented to the oracle for labeling is selected. The selection of the data sample requires evaluating the usefulness of the unlabeled dataset to select the sample that might contribute the most to the improvement of the model. The are multiple query strategies and Kumar & Gupta (2020) presents a review of all of them. The query strategy selected in this work is uncertainty sampling given its simplicity and straightforward understanding. Under this strategy, the oracle queries the instances upon which it is least certain on how to label. In particular, the framework utilizes the method of classiﬁcation uncertainty. Under this method, the uncertainty for each data point x is deﬁned as"
10.36001_phmconf.2021.v13i1.3059,"dataset, data",160,,0,"Active learning algorithms have multiple label gathering scenarios that deﬁne the query process. The most common scenario, which considers the real-world problem of large amounts of unlabeled data, is known as pool-based active learning Settles (2009). Under this scenario, a small set of labeled data is required before starting to select new samples to label from the pool of unlabeled data. The cold-start problem originates from this scenario where there are no labels in the beginning to train the initial classiﬁcation model. Therefore, we solve this problem by introducing a pre-clustering step. During pre-clustering, we apply an unsupervised clustering algorithm to the unlabeled data and select the points closest to the centroids of each cluster as the initial instances of the labeled dataset. This procedure is based on the work by Souza et al. (2017). The steps in this procedure consist of:"
10.36001_phmconf.2021.v13i1.3059,"dataset, data",301,,0,"Recent technical developments have facilitated the collection and storage of large amounts of time series data for many condition monitoring and maintenance processes. However, most of this data is unlabeled, and producing high-quality labeled data is expensive, time-consuming, and a lot of times inaccurate given the uncertainty surrounding the labeling process and annotators. Active Learning (AL) has emerged as an approach that enables cost and time reductions of the labeling process. Here, we present an active learning framework to be used in the classiﬁcation of time series from industrial process data, which can be vibration waveforms or control process data. Previous work has focused on active learning for image classiﬁcation problems. Alternatively, when active learning has focused on time series classiﬁcation problems, it has not dealt with the cold start problem, which consists of a complete absence of labels at the beginning of the training process. The active learning framework proposed incorporates a pre-clustering step to create an initial labeled dataset. Furthermore, we incorporate two strategies for the generation of features to be used in the AL framework, which are time series imaging and automatic feature generation. We study the learning curves of the different feature extraction techniques and evaluate them in two case studies. The ﬁrst case is based on vibration data from a ball bearing experiment with faults seeded in the bearings. The second case is based on a produc Sergio Martin-del-Campo et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",42,,0,"In actual industrial applications, labeled data are very expensive and scarce. The SCADA system of each wind turbine will generate a large amount of monitoring data. It is time-consuming and laborious to analyze these data prepare a dataset."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",48,,0,"As mentioned before, due to the difference in equipment parameters, electrical parameters and working environment, it is different for the feature distribution of SCADA data collected by different wind turbines. Therefore, the model trained on the dataset of one specific wind turbine performs"
10.36001_phmconf.2021.v13i1.3069,"dataset, data",94,,0,"as well. In order to make the network learn features as much as possible, all the monitoring information are fed into the network model. It can be found from Table 1 that the ratio of health data to icing data in wind turbine #15 is 14.7, and the ratio of wind turbine #21 is 15.8, so this dataset presents a serious data imbalance problem. In addition, due to the difference and working environment, the data distribution between the two wind turbines is different."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",132,,0,"Therefore, an unsupervised transfer learning algorithm based on Generative Adversarial Networks for wind turbine health status prediction (WT-GAN) is proposed. WT-GAN can not only remove the domain shift between wind turbines, but also it is an unsupervised learning method. This means that only the unlabeled data for the target domain is required, which solves the problem of labeling data. In order to evaluate the effectiveness of WT-GAN on the condition monitoring of a fleet of wind turbines, this method is applied to one dataset turbines. The icing detection of wind about blade experimental results prove that the proposed method can predict the health status of the wind turbine well. In addition, it can significantly reduce the domain shift among different"
10.36001_phmconf.2021.v13i1.3069,"dataset, data",142,,0,"poorly on unknown wind turbines, easily resulting in wrong decisions. The domain adversarial network introduced above can effectively reduce the distribution difference of SCADA data from different wind turbines. Therefore, a GAN-based deep unsupervised transfer learning algorithm is proposed to proceed the health status prediction of wind turbines. In addition, the proportion of abnormal data in the collected SCADA data is tiny, so a serious data imbalance problem exists in the dataset. Therefore, the focal loss function is introduced to solve the problem of data imbalance, thereby increasing the network's attention on abnormal samples, and ultimately improving the knowledge transfer ability of the network model on different wind turbines. In this section, the proposed WT-GAN network and the optimization process of the model is described in detail."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",158,,0,"where TP, TN, FP, FN refer respectively to the number of true positive samples, true negative samples, false positive samples and false positive samples. If the proportions of each category in the dataset are similar, the Accuracy can evaluate whether the model performs well or not. However, for the wind turbine dataset, the health data and the fault data are severely unbalanced. Therefore, the Accuracy metric cannot accurately evaluate the performance of the model. Therefore, the Precision, the Recall, the F1, and the Score are introduced to comprehensively evaluate the model performance as much as possible. In the Score evaluation index, the weights of the health category and the fault categories are the same and have nothing to do with the number of samples, this metric is more reasonable for performance evaluation of sample imbalance tasks."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",174,,0,"Considering the problem of lack of labels and data imbalance in the SCADA data of wind turbines, this paper proposes a deep unsupervised transfer learning network with the focal loss function. The proposed model uses the idea of adversarial training to reduce the domain shift among different wind turbines. This enables the network model trained on one wind turbine to be well transferred to the fault diagnosis of other unknown wind turbines. In addition, the focal loss function makes the network model to pay more attention on the feature learning of fault samples, which significantly the model's performance with unbalanced dataset. Therefore, the proposed method makes it easy to be applied in practical industrial applications. In order to verify the effectiveness of the proposed model, the proposed method is applied to the case of blade icing detection of a wind farm. The experimental results confirms the model transfer ability and the ability to solve the data imbalance of the proposed method."
10.36001_phmconf.2021.v13i1.3069,"dataset, data",185,,0,"Therefore, this paper proposes an unsupervised transfer learning algorithm based on Generative Adversarial Networks for wind turbine health status prediction (WTGAN). WT-GAN can significantly reduce the domain shift of among different wind turbines, and realize the condition monitoring of unknown wind turbines. This deep unsupervised transfer learning paradigm not only solves the problem of the lack of labeled data, but also solves the domain shift problem among different wind turbines. However, due to the scarcity of failure situations, the obtained SCADA data only contains a small number of fault samples, most of which are healthy samples. Therefore, the data set has a serious sample imbalance problem. Therefore, this paper introduces the focal loss function (Lin et al., 2017) to balance healthy samples and faulty samples instead of the cross entropy the performance of WT-GAN on unbalanced datasets. The effectiveness of the method is verified on  a case of blade icing detection of wind turbines. The main contributions of this paper are summarized as follows:"
10.36001_phmconf.2021.v13i1.3069,"dataset, data",191,,0,"In order to verify the effectiveness of the proposed method, this paper uses a real wind turbine SCADA dataset. Table 1 shows the data collection time of the two wind turbines and the percentage of icing data. This dataset is mainly used for blade icing detection of a wind farm. Specifically, the dataset includes SCADA data for two wind turbines. In this study, the dataset of wind turbine #15 is used as the source domain, and the dataset of wind turbine #21 is used as the target domain. The physical characteristics and parameters of the operating state of the wind turbine, such as wind speed, motor temperature, power generation, etc. Each sample has a corresponding time record, and the label of the sample is given, icing or no icing. Table 2 shows the physical characteristics and the operating parameters collected by the SCADA system. If the blades of wind turbines freeze, it will affect the rotor speed, the power generation efficiency, and the blade balance of wind turbines"
10.36001_phmconf.2021.v13i1.3069,"dataset, data",206,,0,"The condition monitoring and health status prediction of a fleet of wind turbines are essential for the safety of wind turbines. At present, the Supervisory Control And Data Acquisition (SCADA) system has been widely used in wind turbines, which can monitor and collect various physical information and sensor information of wind turbines in realtime. Due to the fact that the amount of data obtained by SCADA systems is extremely large, developing an intelligent decision-making system based on deep learning is a very valuable research. Therefore, this paper is committed to exploring a health status prediction algorithm of wind turbines based on deep learning and SCADA systems. However, yet in actual industrial applications, it is very timeconsuming and expensive to obtain a large amount of labeled data. In addition, as failures rarely occur, there is a serious sample imbalance problem in the datasets. More importantly, due to the difference in working environment and physical parameter setting, there are significant differences in the feature distribution of different wind turbines data, which leads to a significant drop in the performance of the deep learning model on unknown wind turbines."
10.36001_phmconf.2021.v13i1.3081,"dataset, data",109,,0,"Finally, Tab. 1 outlines the distribution of all compressor cycle observations used in this work. Performance of the classifiers are evaluated by comparing the predicted classes of both the training set and a test set to their known classes. The overall classification accuracy of the test data sets is used to assess how well the proposed methodology, and associated time-frequency technique, produced unique fault signatures of the valve seat wear cases tested. 80% is used for training data, and presented here, but note that other training data set sizes were studied (70%) with similar results."
10.36001_phmconf.2021.v13i1.3081,"dataset, data",142,,0,"via the crank shaft position as provided by a mounted rotary encoder. The operating RPM of the ESH-1 is essentially constant with negligible variation. Under these conditions the crank shaft position is a linear function of time and therefore sample rate. Given that the DAQ sample rate is fixed at 25.6kHz and for the compressor operating actually around 383 RPM the samples per cycle nominally is about 4012 samples/cycle. Thus, the degrees per sample is θsample = 360◦ Ncycle with an average value of 0.0897 degs/sample. Based on calculated θsample for each data set, vibration and pressure data are converted to the angular domain and separated into sets of 360◦ cycles. It is defined in this work that a shaft position of 0◦ corresponds to the piston’s position at bottom-dead center,"
10.36001_phmconf.2020.v12i1.1300,"dataset, data available",93,,0,"Transductive TL or domain adaptation is the most widely used type of TL for PHM applications, specially fault diagnostics in the source and target domains. Domain generalization is the main application of TL in PHM that have been identiﬁed in the literature. In this application, one general model that is applicable to the source and target domains is produced and labeled source dataset is available while the target dataset is unlabeled. The source domain information is used to predict accurate labels for the target dataset. In most"
10.36001_phmconf.2016.v8i1.2500,"dataset, data available, data",155,,0,"The numbers of data samples per cycle, Ncycle, is determined via the crank shaft position as provided by the rotary encoder sensor. The operating rpm of the ESH-1 is essentially constant with negligible variation. Under these conditions the crank shaft position is a linear function of time and therefore 360◦ sample rate. Thus the degrees per sample is Θsample . Ncycle The number of samples per cycle varied slightly between data sets but was found on average to be 4012 samples/cycle with an average Θsample of 0.0897 degs/sample. Based the Θsample calculated for each data set, vibration and pressure data is converted to the angular domain and separated into sets of 360◦ cycles. The total number of cycles extracted for each fault case varied based on the amount of data available. Figure 8 shows the raw vibration and cylinder pressures for a single machine cycle."
10.36001_phmconf.2018.v10i1.245,"dataset, data available, data",235,,0,"There are many benefits from implementing a prognostics and health management (PHM) initiative in an industrial facility, such as realizing potentials from reducing unplanned downtime and increased asset efficiency.  Many industrial take advantage of PHM companies would technologies and algorithms their business objectives, but identifying how to get started can be a daunting challenge. The classical approach is to begin with a Reliability Centered Maintenance (RCM) program supported by failure modes and effects analysis (FMEA) where all possible failure modes, their risks, and mitigating actions are evaluated in the context of asset function. In this framework, application of PHM technologies is viewed as a maintenance strategy effective at mitigating certain failure modes in specific cases that are both feasible and costthere are many challenges and effective.  However, limitations to traditional RCM where data-driven analytics embedded in these work processes can help overcome and/or automate.  On the other hand, the use of data-driven approaches introduces new challenges surrounding available data, data quality, and identifying numerical methods that are scalable across large datasets. In this paper, we present a case study applied to historical maintenance data for identifying and prioritizing where to start a PHM initiative, and discuss the work processes and various challenges encountered when embedding data analytics in classical reliability approaches."
10.36001_phmconf.2019.v11i1.792,"dataset, data available, data",92,,0,"Maintenance Data Collection: The time data available in the dataset were only the actual start and ﬁnish times. There is no more speciﬁc time information e.g., when the maintenance technician arrived, or when the workorder was opened. Such ﬁner time data would lead to improved inferences about the actual duration of maintenance. Further details of what other time data are useful can be found in (Brundage et al., 2018). Efﬁcient data collection strategies are needed for better maintenance time data capture."
10.36001_phmconf.2019.v11i1.792,"dataset, data available, data",165,,0,"The intended output for the predictive models is a categorical window of the maintenance duration. Though maintenance times in MWOs are real-valued, predicting the output times as precise real-valued numbers is far less useful than practical task assignment windows because jobs are typically scheduled into some window of an expected duration of the task. In other words, short tasks may be scheduled in 5 minute blocks, but longer tasks are more commonly blocked off in terms of hours. For this data set, some concessions of the designation of the duration categories is also fed from the small number of data points and low accuracy of predictions found in initial studies with the time data. With larger volumes of more accurate data this could be overcome. Despite these concessions, the conclusions and methods developed in this work could easily be extended to the level of granularity most useful and feasible for any target use case."
10.36001_phmconf.2020.v12i1.1205,"dataset, data available, data",67,,0,"Figure 3(a) shows the results of training and predicting on the same turbine, A0 in the ”Limited Data” scenario, in which only three months of data are available for training the CNN model. This should ﬁrst be compared with the ideal ”Baseline” scenario of panel 3(b) in which a large data set of 9"
10.36001_phmconf.2020.v12i1.1205,"dataset, data available, data",154,,0,"Cross-turbine Training with data from turbines B1 or B2 clearly enables early detection of the degraded state of turbine B0 in case only little data from B0 is available. The detection is naturally not quite as early as if trained with a large data set from the same turbine, if such data is available, but the proﬁt in using the cross-turbine training is clearly demonstrated. The accuracy is lost especially if we lower the detection threshold and aim at a low or intermediate conﬁdence level, thus allowing for some level of false positives in order to detect faults as early as possible (these false positives can be eliminated in a later stage of aggregated thresholding or some process control logic). For very high conﬁdence levels, training with turbine B2 yields as accurate, early and stable fault detection as with the original turbine B0."
10.36001_phmconf.2020.v12i1.1294,"dataset, data available, data",104,,0,"With the assist of simulation data, the physical information of bearing degradation is embedded in the source domain. The neural network training will further result in the emergence of domain-invariant and monotonic features, which can be used to track the bearing degradation. Since the training of the target domain is unsupervised, the proposed DGT-based DANN model is able to circumvent the labelling issue of the runto-failure dataset. In practical applications, the available unlabelled measurements increase with the machine operating and could be used to enhance the target domain, thus leading to a more precise prediction."
10.36001_phmconf.2021.v13i1.3007,"dataset, data available, data",150,,0,"Figure 3. Risk quantification model of a) FAN degradation mode and b) HPC degradation mode given by the Gaussian Copula model. In practice, the standard of choosing tolerable risk varies from case to case and largely depends on expert experience. When historical R2F data is available, a suitable risk level could be determined by the cross-validation strategy, where the data is split in multiple folds for training and validating the risk quantification model. Since the EoL information is the could available, corresponding risk level for further use. To apply this methodogy for data competition tasks like the C-MAPSS RUL prediction in the case study, the risk level could be tuned as a hyperparameter for better prediction accuracy. In this case, 75% is selected to achieve the most balanced prediction performance on all four datasets."
10.36001_phmconf.2021.v13i1.3052,"dataset, data available, data",142,,0,"In the random sampling training strategy, optimization of the classifier hyperparameters was investigated as described further in section 6.1. Smaller data sets were used in the hyperparameter optimization, as the computational costs were very high for running the optimization on all the available data. These smaller groups of data were found using the leaders clustering algorithm, described in section 4.2. The size of the smaller data sets was chosen to correspond with the size of the folds in the CV-inspired process, described in section 5.3. For each failure incident in the data set, 17,088 points were added to the total points in the training and testing optimization data sets. The total points were then separated into training and testing based on the percentage used in the overall training and testing set."
10.36001_phmconf.2021.v13i1.2999,"dataset, data https, retrieve, data",17,,1,(n.d.). High speed gear dataset. Retrieved from http://data -acoustics .com/ measurements/gear-faults/gear-1/
10.36001_phmconf.2021.v13i1.2999,"dataset, data repos, data https, data repository, data",45,,1,"Lee, J., Qiu, H., Yu, G., Lin, J., & Services, R. T. (2007). Bearing data set ims, university of cincinNASA Ames Prognostics Data Repository nati. (http://ti.arc.nasa.gov/project/prognostic-datarepository)."
10.36001_phmconf.2020.v12i1.1155,"dataset, dataset provided",156,,1,"Contributions: This paper proposes a noisy multi-path parallel hybrid model for RUL prediction, referred to as the (NMPM). The (NMPM) method integrates three parallel paths followed by another one to combine the outputs and form the target result. The key contributions of this paper are: (i) A new multi-path parallel hybrid DNN framework that integrates three neural networks architectures (BLSTM and LSTM, MLP, and CNN) is proposed for estimating remaining useful life of complex systems. (ii) To the best of our knowledge, the NMPM is the ﬁrst multi-path parallel hybrid DNN model for RUL estimation. (iii) The efﬁciency of the proposed NMPM solution is then veriﬁed and evaluated using the C-MAPSS dataset provided by NASA (Saxena & Goebel, 2008) and the achieved outcomes were the best among the existing methods."
10.36001_phmconf.2015.v7i1.2710,"dataset, dataset provided, data",146,,0,"routine during a flight leg. Understanding which routine the ACMF report is designed to monitor provides important information regarding which CMSCs will be providing parametric data within this that information with the CMSC operational mode flag will result in the isolation of data which was captured when a specific CMSC was driving a specific component. More importantly, from producing false alarms on irrelevant data. This method of data fusion was prevalent throughout the development of analytics for the CMSC subsystem on the Boeing 787. After filtering the data, simple statistical analyses were performed on the data to provide parametric data and binary flags. An example of these statistical analyses on a filtered data set, is shown in Figure 6. The parametric data derived from the ACMF reports resulted in the application of the following statistical features:"
10.36001_phmconf.2015.v7i1.2758,"dataset, dataset provided, data",231,,0,"In many instances, condition monitoring equipment has not been installed on machinery. Yet, operators still need guidance as to when to perform maintenance that is better than what is offered by the equipment manufacturers. For these systems, running hours, counts, or some other measure of usage may be available. This data, along with failure rate data, can provide an expected time to failure, and the estimated remaining useful life. The failure rate (even small sample size) is used to estimate the shape and scale parameters the conditional expectation of the truncated survival function of the Weibull is used to estimate the time to failure. This is an actuarial technique to solve the conditional survival function problem of: given that the equipment has survived to time x, what is that probability of the equipment surviving to time x + y. The inverse cumulative distribution of the truncated survival function can then be used to estimate the remaining useful life, that is: a time when the conditional likelihood of failure is small, such as 10%. The 90% confidence of the shape and scale parameters is then used to give a bound on the remaining useful life. This method is then tested on a real world bearing dataset."
10.36001_phmconf.2016.v8i1.2538,"dataset, dataset provided, data",62,,1,"The proposed scheme is validated using data from four different sources. The ﬁrst data used (DS1) was provided by NASA Ames public datasets. It consists of a battery exposed to an accelerated degradation at 296[K] (Saha & Goebel, Visited: Oct.2007). Furthermore, cycles 19 to 23 were deleted and the"
10.36001_phmconf.2018.v10i1.245,"dataset, dataset provided, data",176,,0,"Due to these benefits, many industrial companies would like to take advantage of PHM technologies and algorithms to meet their business objectives, but are faced with the challenge of how  Theoretically, implementation of RCM to establish optimal maintenance strategies, applying PHM technologies, and leveraging asset data and information in order to satisfy business objectives works perfectly, however, there are many challenges when implemented in practice.  Challenges include how to get started and prioritize an initiative, and how to measure what approaches are effective.  Fortunately, there are many opportunities to address many of these challenges through embedding data-driven analytics in workflows towards identification prioritization. Automating analytics enhanced workflows can use different data sources and provide information to the user which may quantify certain challenging areas to quantify. However, when tackling data challenges across broad datasets, many new challenges arise around data quality problems, noise, and numerical methods that are robust enough to automate an analysis across a broad dataset."
10.36001_phmconf.2019.v11i1.833,"dataset, dataset provided, data",138,,1,"Experiments to evaluate the performance of proposed model used data from the battery aging experiments provided by the CALCE Battery Research group at the University of Maryland (He, Williard, Osterman, & Pecht, 2011). The dataset was created by repeatedly charging and discharging the battery over several months. The battery named CS2 used in the experiment is a lithium-ion battery with 1.1Ah of rated capacity. We use 6 experimental datasets conducted with CS2, i.e. CS2_33, CS2_34, CS2_35, CS2_36, CS2_37 and CS2_38. These cases are divided into two types based on discharge current. CS2_33 and CS2_34 cycled at constant discharge current of 0.5C while others cycled at constant discharge current of 1C. Figure 3 shows the degeneration curves of test cases."
10.36001_phmconf.2020.v12i1.1155,"dataset, dataset provided, data",242,,1,"Recently, there has been an increasing surge of interest on development of parallel-hybrid models of different Deep Neural Networks (DNNs) architectures for Remaining Useful Life (RUL) estimation. In this regard, the paper introduces, for the ﬁrst time in the literature, a new parallel-hybrid DNNbased framework for RUL estimation, referred to as the Noisy Multipath Parallel Hybrid Model for Remaining Useful Life Estimation (NMPM). The proposed NMPM framework comprises of three parallel paths, the ﬁrst one utilizes a noisy Bidirectional Long-Short Term Memory (BLSTM) that used for extracting temporal features and learning the dependencies of sequence data in two directions, forward and backward. The second parallel path, employs noisy MultiLayer Perceptron (MLP) that consists of three layers to extract different class of features. The third parallel path utilizes noisy Convolutional Neural Networks (CNN) to extract a complementary class of features. The concatenated output of the three parallel paths is then fed into a Noisy Fusion Center (NFC) to predict the RLU. The proposed NMPM has been trained based on a noisy training mechanism to enhance its generalization behavior, as well as strengthen the model’s overall accuracy and robustness. The NMPM framework is tested and evaluated using CMAPSS dataset provided by NASA illustrating superior performance in comparison to is state-of-the-art counterparts."
10.36001_phmconf.2020.v12i1.1294,"dataset, dataset provided, data",62,,0,"Compared to the DANN method, the non-adapted model fails to adapt its outputs to the real data, which gives rise to high prediction errors. Although the simulation dataset could provide enough training samples to the neural network, the shift between the simulation and real is still considerable since the DGT cannot perfectly replicate the degradation process,"
10.36001_phmconf.2020.v12i1.1298,"dataset, dataset provided, data",52,,1,"The proposed method is tested on the PRONOSTIA bearing dataset provided by FEMTO-ST Institute for remaining useful life prediction (Nectoux et al., 2012). The process is comprised of two stages: Training and Testing. In training, the input data is the sequences of vibration snapshots measured"
10.36001_phmconf.2020.v12i1.1298,"dataset, dataset provided, data",129,,0,"As shown in Table 2, six run-to-failure bearing datasets are provided to the participants as learning data to build the prognostic models. For the 11 bearings in test set, vibration data is truncated and provided till some point before failure. Thus, the task will be using the learned prognostic model to predict failure time and remaining useful life for the 11 bearings in test set. The learning set is quite small with only 2 run-tofailure bearings under each operating condition. Meanwhile, the spread of the lifespan of all bearings is wide, which varies roughly from 1.5 to 7.8 hours as shown in Table 3. Therefore, learning a good model to accurately predict RUL becomes difﬁcult and challenging."
10.36001_phmconf.2021.v13i1.3059,"dataset, dataset provided, data",191,,0,"The vibration signals of the ball bearing dataset are generated by a rotating machine, which consists of an electric motor, a torque transducer, a dynamometer, and a ball bearing supporting the motor shaft. Vibration data is recorded via an accelerometer located at the drive end of the motor with a sampling rate of 12 kHz. The machine operates with a varying load between 0 HP and 3 HP, resulting in varying motor speed between 1800 and 1730 rpm. Faults are manually introduced at the inner raceway, outer raceway, and balls. Thus, there are four possible labels that the oracle can assign to each instance corresponding to the three fault location cases and a case without no faults. Meanwhile, the production dataset originates from data provided by one of the customers of Viking Analytics. The time series data belongs to a control system, where samples were taken with a sampling rate of 25 Hz. This is a binary classiﬁcation problem where the annotator has to classify the presence of a bump in the data."
10.36001_phmconf.2019.v11i1.879,"dataset, github",8,,1,0The dataset is available at https://github.com/IndustrialNetwork/GraphDataset
10.36001_phmconf.2019.v11i1.816,"dataset, github, data",59,,1,"Credit Card Fraud data set (//github.com/ellisvalentiner /credit-card-fraud), it contains credit card transactions over a two day collection period in September 2013 by European cardholders. There are a total of 284,807 transactions, of which 492 (0.172%) are fraudulent. First 200,000 are used for training and the last 84,807 for validation."
10.36001_phmconf.2018.v10i1.477,"dataset, publicly available",68,,1,"The identiﬁcation of features/feature vectors applicable to EEG signal analysis with respect to epilepsy seizure detection and BCI-related tasks is performed on publicly available datasets. The CWT is applied to aforementioned datasets to extract a number of time-frequency-based features, whereas the classiﬁcation results obtained concerning NN- and SVMbased classiﬁers are compared in terms of classiﬁcation accuracy and the capability of particular features for classiﬁcation purposes."
10.36001_phmconf.2018.v10i1.477,"dataset, publicly available",181,,1,"The regulation of functions such as respiratory or heart rate in human body as well as the control of motor movements are under the control of nervous system. As these actions and correlated tasks are directly inﬂuenced by the brain, the brain monitoring gives the possibility to differentiate the tasks, enabling at the same time the prediction of further actions. In this contribution, publicly available electroencephalography (EEG) datasets are analyzed with respect to the detection of epileptic seizure occurrence and BCI-related actions (here: cued motor imagery). For these purposes, timefrequency-based feature extraction alongside different classiﬁcation methods is used. To perform the classiﬁcation, Artiﬁcial Neural Network (ANN) and Support Vector Machine (SVM) are utilized and compared with previously obtained results. The feasibility of particular features for the detection of epileptic seizures and BCI-related tasks is discussed. Four different feature vectors per analyzed problem are identiﬁed. Acceptable accuracy of classiﬁcation using ANN- and SVMbased classiﬁers is achieved using identiﬁed feature vectors."
10.36001_phmconf.2020.v12i1.1199,"dataset, publicly available, data",77,,1,"With example upper and lower bounds on the maximum capacity decay for a variety of ﬂights from different batteries in the publicly available HIRF repository (Kulkarni et al., 2015), we are interested in building upon previous studies to further study the capacitance degradation. The publicly available data set contains HIRF tests and experimental data conducted on an electric ﬁxed wind UAV Edge 540. The e-UAV is a 33% sub 1"
10.36001_phmconf.2019.v11i1.842,"dataset, retrieve, data",93,,0,"The data acquisition phase represents data creation from scratch or the extraction of existing raw or derived data for reuse. In this stage, observational, experimental, simulation data creation takes place as well as the retrieval of existing datasets, derived or reference data for reuse. Data governance standards and best practices ensuring integrity, security, quality and metadata are considered. The quality of the process is also vital as it has a direct impact on the quality of the data created of retrieved for reuse."
10.36001_phmconf.2019.v11i1.792,"dataset, used dataset",4,,0,4.1. Dataset used
10.36001_phmconf.2018.v10i1.509,"dataset, used dataset, data",144,,0,"For this work 90 different conditions were tested for the six levels of tool wear as outlined in Tab. 2. During the test the three microphones recorded approximately 30 seconds of acoustic data per test. After removing the startup and shutdown part of the audio signal the sound ﬁle is segmented into one second sound bytes. These sound bytes are then transformed to the frequency domain as described previously. Randomly chosen, 70% of the one second ﬁles are used to train the SVM classiﬁer with 30% retained to be used as a validation data set. The magnitude from the entire frequency spectrum of a one second acoustic signal is used as the feature vector. The feature vectors for all wear conditions, independent of spindle speed and feed rate, is compiled into the"
10.36001_phmconf.2019.v11i1.806,"dataset, used dataset, data",55,,0,"For the development and evaluation of AnomDB, we have used a data set collected from approximately 600 CNC machines operating between June 2018 and June 2019. Control data was sampled at 1–4 Hz, depending on the machine. Control data was extracted with proprietary adapters as developed by MachineMetrics, Inc."
10.36001_phmconf.2021.v13i1.2999,"dataset, used dataset, data",153,,0,"The ﬁrst dataset analyzed was from a 3 megawatt wind turbine, the vibration readings for the data labeled ‘Case 1’ were considered high, and the turbine was pulled off-line for inspection. A faulty pinion gear was diagnosed (’Bechhoefer, n.d.). The impulsive waveform typically seen in gear faults was viewed as a good test case for correlation based sensor function tests based on the results of the perfectly impulsive test signal used in Figure 7. This dataset was recorded with a sampling rate of 97656 Hz and the units of acceleration were not noted so the arbitrary unit of Waveform Units [W U ] is used in Figure 8 below. The resulting auto-correlation show a distinct singular peak in correlation with high signal-to-noise ratio (SNR) that can clearly be differentiated from an uncorrelated signal such as Figure 10."
10.36001_phmconf.2021.v13i1.3059,"dataset, used dataset, data",241,,0,"We present an active learning framework that focuses on time series data, such as vibration or industrial process signals, belonging to condition monitoring and maintenance applications. In particular, we investigate the use of time series imaging and automatic feature extraction to generate features to be presented to an annotator for labeling and subsequently to be used by a classiﬁer algorithm. The aim is to facilitate the annotation of time series data by having created an interface that enables to assign labels to the time series shown to the annotator. Furthermore, in our framework, we had considered the situation where there are no labels present at the beginning of the active learning algorithm and we solve that challenge by incorporating a pre-clustering step that intends to identify some samples belonging to a predeﬁned number of expected labels. We ﬁnd that the framework proposed by this work is capable of achieving high accuracy by just having to label 10% of the overall unlabeled dataset. These results are observed across two case studies, which included vibration data and industrial data. However, each dataset has a different performance depending on the feature extraction technique used. The ball bearing dataset achieved high accuracy at a faster rate using automatic feature extraction techniques, while the production dataset achieved high accuracy with the time series imaging techniques. These results moti 7"
10.36001_phmconf.2018.v10i1.477,"dataset, used dataset, database",153,,0,"ergy contained in the last four scales of CWT per particular channel. Concerning polynomial relationship between CWT scales and frequency bandwidths, the low frequency bandwidth is contained in the last four scales. Contrary, individual feature K4 of feature vector F12 is a scalar value which corresponds to the minimum of maximum obtained CWT values per channel. By analyzing extracted features, it becomes noticeable that the feature vectors contain wavelet- and statistics-based quantities. Furthermore, feature vectors F21, F22, F23, and F24 have identical structure to feature vectors F11, F12, F13, and F14 but also slightly changed scaling of frequency range due to different frequency bands of interest. It is important to emphasize that only three channels are used from Dataset IVa (BCI Competition III database), whilst sixteen channels are used from datasets originating from TUH."
10.36001_phmconf.2018.v10i1.477,"dataset, used dataset, dataset provided, data",136,,1,"The datasets related to the recognition of BCI-related task used here is Dataset IVa from BCI Competition III provided by Fraunhofer FIRST, Intelligent Data Analysis Group. “The data set was recorded from ﬁve healthy subjects. Subjects sat in a comfortable chair with arms resting on armrests. Visual cues indicated for 3.5 s which of the following 3 motor imageries the subject should perform: (L) left hand, (R) right hand, (F) right foot.” (Dornhege, Blankertz, Curio, & Muller, 2004). The EEG signal is captured from 118 channels of extended 10-20 conﬁguration. In this contribution, only subset of 170 labeled data from 3 channels originating from one subject (al) is used."
10.36001_phmconf.2018.v10i1.503,"download, data",64,,0,"NATS provides functions to download weather data from the NOAA web site (NOAA, 2018), and make it available for use in trajectory simulations. The weather data can also be used to simulate controller functions such as aircraft rerouting in severe weather. In addition to the strategic rerouting actions, the controllers may also issue tactical flow control advisories"
10.36001_phmconf.2019.v11i1.898,"download, data",132,,0,"For analysis purposes, both the QAR (Quick Access Recorder) and DAR data are used based on ease of information retrieval and/or variability of parameters they are able to provide. The QAR is a copy of the FDR on a non-crash resistant recorder which has the specificity to be easy to download and interchangeable (ATR, 2016). DARs are like QARs (ICAO, 2016) however, while the QAR data originates from the FDIU with minimum regulatory parameters as the FDR, the DAR data, on the other hand, is the product of the DMU able to include many more convenient parameters. Both systems were developed to ease data recovery for the airlines’ needs and contain high-frequency sensor data."
10.36001_phmconf.2021.v13i1.2978,github,3,,0,https://github.com/HoChiak/Gearbox
10.36001_phmconf.2021.v13i1.2978,github,14,,0,"Henss, M., (2020). Gearbox Toolbox. GitHub Repository."
10.36001_phmconf.2021.v13i1.2998,github,9,,0,(2015). Keras. https://github.com/
10.36001_phmconf.2021.v13i1.2998,github,15,,0,"fchollet/keras. GitHub. Daigle, M. J., & Kulkarni, C. S."
10.36001_phmconf.2021.v13i1.3030,github,5,,0,1for https://github.com/darrahts/uavTestbed/blob/main/sql/table schema.sql
10.36001_phmconf.2021.v13i1.3054,github,10,,0,{https://github.com/eloquentarduino/ EloquentTinyML}. 2021])
10.36001_phmconf.2021.v13i1.3054,github,15,,0,Tensorﬂow Development Team. (2021). TFLite Micro Kernel {https://github.com/tensorflow/
10.36001_phmconf.2021.v13i1.3054,github,27,,0,(2019). Recurrent Neural Networks for Predictive Maintenance. {https://github.com/ umbertogriffo/Predictive-Maintenance -using-LSTM}. ([Online; accessed 19-May-2020])
10.36001_phmconf.2020.v12i1.1142,"github, data",67,,2,"In this study, all the tests are done in an environment with a GPU of Nvidia Quadro K2100M. The software is based on Faster R-CNN with a ZF model pretrained on VOC 2007 trainval and released on https://github.com/rbgirshick/pyfaster-rcnn. Test data is collected from an instrumented test vehicle. 50 image frames are sampled from the video recorded by the front camera."
10.36001_phmconf.2019.v11i1.857,open-source,30,,0,"The NASA core Flight System1 (cFS) is an Open-Source platform and framework that has been developed at the NASA Goddard Space Flight Center. Its component-based design,"
10.36001_phmconf.2019.v11i1.857,open-source,48,,0,"Meier, L., Honegger, D., & Pollefeys, M. (2015). PX4: A node-based multithreaded open source robotics frameIn 2015 IEEE work for deeply embedded platforms. International Conference on Robotics and Automation (ICRA) (pp. 6235–6240)."
10.36001_phmconf.2019.v11i1.857,open-source,57,,0,"AOS is communicating with a low level ﬂight control software to obtain sensor and aircraft status information and to issue low level commands. In our case, we use a slightly modiﬁed version of the Open-source ArduCopter software,2 running on a PixHawk hardware,3 which directly interfaces with sensors and controls the motors of the aircraft."
10.36001_phmconf.2019.v11i1.857,open-source,116,,0,"The DSML and the associated model instances were created in Web-GME (Maroti et al., 2014), an Open-source metaprogrammable platform. Web-GME provides a graphical platform to deﬁne the rules of the DSML in a meta-model. Thereafter, it allows users to create model instances that corIt has extensive API respond to the deﬁned meta-model. support for writing custom plugins to generate artifacts from the models. It provides a web-based collaborative platform where multiple users can view and edit the models simultaneously. It has a built in version tracking system that allows the users to create tags, branch, fork and merge during the model development process."
10.36001_phmconf.2019.v11i1.876,open-source,80,,0,"Fritzson, P., Aronsson, P., Pop, A., Lundvall, H., Nystrom, K., Saldamli, L., . . . Sandholm, A. (2006). Openmodelicaa free open-source environment for system modeling, simulation, and teaching. In 2006 ieee conference on computer aided control system design, 2006 ieee international conference on control applications, 2006 ieee international symposium on intelligent control (pp. 1588–1595)."
10.36001_phmconf.2021.v13i1.3054,open-source,26,,0,"Broll, B., & Whitaker, J. (2017). Deepforge: An open source, collaborative environment for reproducible deep learning."
10.36001_phmconf.2021.v13i1.2999,open-source data,117,,0,"Figures 23 and 24 show the result of a loosely mounted Sensor C exposed to an impulsive signal in the time domain and the cross-correlation pairs produced. As seen in the time domain of Figure 23, Sensor C, the ‘faulty’ sensor, has a drastically different impulse response than Sensors A and B. The large negative acceleration peak recorded by Sensor C is evident in the correlation results as the element wise multiplication and integration of correlation produces negative values of correlation and multiple peaks of signiﬁcant amplitude instead of a singular peak. The noise ﬂoor is low due to the physical isolation provided by the open cell foam."
10.36001_phmconf.2019.v11i1.815,"open-source data, open-source, data",81,,0,"For this study, the model equations of the IQT system (Eqs. 1 to 4) are simulated using Xcos (Scilab), an open source software environment used for modelling & simulation, to generate operational data. Frictional losses in the IQT system are considered to be negligible. Normal operation (consisting of start-up, set point up and down, shut down) as well as faulty operation of the system are simulated."
10.36001_phmconf.2015.v7i1.2767,"open-source, dataset provided, data",68,,0,"Numerical simulations were carried out using k-Wave, an open source MATLAB toolbox used to model elastic waves (MATLAB, 2010) (Treeby et al., 2014). From the simulation, a FMC data structure can be captured. The FMC provided sets of waveform data to be processed in the prior discussed conventional beamforming frameworks with and without the global matched coefﬁcients."
10.36001_phmconf.2017.v9i1.2306,package,33,,0,(1993). Development of a heat transfer package applicable to a large variety of ﬂuids (Scientiﬁc and Technical Research Reports No. EUR 14985 EN). European Commission.
10.36001_phmconf.2017.v9i1.2307,package,24,,0,"We will now consider solution of this problem using traditional technique and a freely available package GPOPS (Rao et al., 2010)"
10.36001_phmconf.2017.v9i1.2307,package,48,,0,The GPOPS package is easy to install and to use. It was shown to perform well for multiple aerospace applications including optimization of the descent aircraft trajectory at National Aerospace Laboratory (Netherlands). We have chosen this package for the evaluation of the traditional technique
10.36001_phmconf.2017.v9i1.2307,package,49,,0,"We therefore conclude that although GPOSP package is potentially very useful for trajectory optimization at present it can only be used for off-line applications. To enable fast online optimization of the multiphase descending trajectory we propose a novel algorithm, which is considered in the next section."
10.36001_phmconf.2017.v9i1.2307,package,59,,0,"The convergence time for maximization problem can vary between 50 and 300 sec and the convergence is not robust. We note that the best reported performance of the optimized implementation of the multiphase pseudospectral algorithm (de Jong, 2014) in C++ using PSOPT package was 30 sec for the whole descent and ﬁnal approach trajectory."
10.36001_phmconf.2017.v9i1.2307,package,69,,0,"To evaluate the standard approach to the solution of this problem we used Matlab package GPOPS based on the pseudospectral optimization method. We considered simpliﬁed scheduling problem with 4 phases due to different ﬂaps and It was shown that the packlanding gear conﬁgurations. age can solve complex multiphase problems in general form. However, its online applications are at present limited by slow convergence time."
10.36001_phmconf.2018.v10i1.245,package,31,,0,"López-de-Ullibarri, I. & Jácome Pumar, M.A. (2013). survPresmooth: Presmoothed Estimation in Survival Analysis. American Statistical Association. R package version 1.1-10."
10.36001_phmconf.2018.v10i1.245,package,136,,0,"We calculated the non-parametric smoothed hazard function for each asset population which were grouped by site and show the curves in Figure 5. We used the ‘muhaz’ R package for the kernel smoothing method, which first estimates the cumulative hazard function with the Nelson-Aalan estimator, then smoothed using an Epanechinikov kernel. The hazard function is returned as the first order difference of the smoothed function. In the method, to avoid boundary effects that occur from bias problems near the left endpoint, we used the (default) maximum time for the hazard function estimator to when 10 assets remained at risk. From the survival curves in Figure 4, we can see how most of the assets at Site 3 failed before 3 time units."
10.36001_phmconf.2018.v10i1.539,package,20,,0,"von Alt, C. (2003). Remus 100 transportable mine countermeasure package. Proceedings of Ocean 2003."
10.36001_phmconf.2018.v10i1.585,package,56,,0,"Li, Z., Wei, Z., Yue, Y., Wang, H. (2011).  An Adaptive Hidden Markov Model for Activity Recognition Based on a Wearable Multi-Sensor Device. J Med Syst (2015) Midè, What Really Happens to Your Package During https://blog.mide.com/transportation Shipment?. vibration-monitoring-what-really-happens-duringshipment"
10.36001_phmconf.2019.v11i1.806,package,24,,0,"Table 1. Higher-level time series features extracted using anomalous. Unless otherwise indicated, we use default package parameters, where relevant."
10.36001_phmconf.2019.v11i1.806,package,61,,0,"The speciﬁc tools that we employ include dimensionality reduction via Principal Component Analysis (PCA), timeseries feature extraction using the anomalous package (Hyndman, Wang, & Laptev, 2019), and Density Based Scanning of Applications with Noise (DBSCAN) (Hahsler & Piekenbrock, 2018). In outline, the steps are:"
10.36001_phmconf.2019.v11i1.806,package,67,,0,"Two of the chosen features (Level Shift and Variance Change) require partitioning the time series into blocks and measuring changes between the blocks. For these, we take ten blocks as a default. We have veriﬁed that performance is only weakly sensitive to this choice. Otherwise, we use anomalous package defaults for any other parameters needed in the feature calculations."
10.36001_phmconf.2019.v11i1.857,package,18,,0,"a shortened ﬂight plan that sacriﬁces not so important goals, for example, a package delivery,"
10.36001_phmconf.2020.v12i1.1190,package,119,,0,"A primary task for ensuring safe UAV ﬂight operations requires careful ﬂight planning and trajectory generation based on a series of four-dimensional waypoints (latitude, longitude, altitude, and arrival time), while satisfying a set of constraints that assure safety during ﬂight (e.g., distance from nearest obstacle ≥ 2m, battery charge ≥ 10%, risk threshold ≤ 15%), and optimizing a set of performance parameters (e.g., ﬂight time, power consumption). We assume our package-carrying UAV is a low ﬂying craft that typically ﬂies over roads between large buildings. Our path planning algorithm assumes the existence of a map of possible routes that"
10.36001_phmconf.2020.v12i1.1190,package,195,,0,"Our overall goals is to support safe operations of package delivery UAVs operating in urban environments. We have developed a decision-making framework to maintain system safety during a UAV mission (ﬂight from a starting point to a destination going through a sequence of pre-determined way points) by minimizing the overall risk of mission failure, considering a number of risk factors along with uncertainties in the environment and the operating state of the vehicle (UAV). In this paper, we develop a Risk analysis approach that computes and updates risks associated with projected UAV ﬂight paths by considering two potential hazards:(1) collision (with static obstacles), and (2) depleting battery charge below a pre-speciﬁed safe threshold. In addition, we also take into account the effects of degradation in system components on overall UAV ﬂight. In general, multiple components of a system may degrade at the same time, therefore, we develop methodologies for computing system performance using a system-level prognostics approach that we have developed in past work (Khorasgani et al., 2016)."
10.36001_phmconf.2020.v12i1.1190,package,205,,0,"The use of Unmanned Aerial Vehicles (UAVs) is increasing at unprecedented rates across across a wide range of applications that include surveillance, package delivery, photography, cartography, remote sensing, agriculture, military missions, and more1. The FAA passed regulations in 2016 that authorized the commercial use of UAVs. As adoption and use of drones increase, so does the risk of collisions and mishaps that can result in a loss of money, time, productivity, or most important, human lives. There is an abundance of research on the technical aspects of UAV systems: their design, implementation, operation, diagnostics, and stability (Moir & Seabridge, 2012). However, a more holistic approach to ensuring safe operations in a heterogeneous airspace is re Marcos Qui˜nones-Grueiro et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. 1UAV sales are expected to top $12 billion in 2021 (https://www. businessinsider.com/drone-technology-uses-applications"
10.36001_phmconf.2020.v12i1.1215,package,27,,0,The description of the low power wireless module is not included in this paper. The goal is to validate the functionality of the sensor/ASIC package.
10.36001_phmconf.2020.v12i1.1215,package,36,,0,"sensor’s performance, functionality of the combined sensor wakeup ASIC package, and finally a description and test results of the integrated Sensor-ASIC package subject to a replica of the actual vibration signature."
10.36001_phmconf.2020.v12i1.1215,package,40,,0,The ARISE sensor and wake-up module package has been validated with a replicated vibration signature acquired from a select rotating machinery subject to progressive damage at the Structural Dynamics Laboratory at Raytheon Technologies Research Center RTRC. The failure precursor
10.36001_phmconf.2020.v12i1.1215,package,64,,0,"The ARISE technology, which leverages N-Zero investment, has shown to be a viable means of low power wireless monitoring of rotating machines. An accelerated failure test which spans the various stages of progressive failure was the concept. The selected, for functionality of the sensor and sensor/ASIC package were checked separately, each of them meeting the design requirements."
10.36001_phmconf.2020.v12i1.1215,package,74,,0,"After the functionality check of the assembled sensor/ASIC package, it was tested with the more realistic vibratory input from the rotating machine shown earlier in Figure 7. The test setup is shown in Figure 12. The measured acceleration is fed into B&K LASER controller, which has capability to control the shaker to replicate the waveform. Accelerations of the shaker table and the packaged sensor are monitored."
10.36001_phmconf.2021.v13i1.3030,package,44,,0,"1. INTRODUCTION The use of Unmanned Aerial Vehicles (UAVs) has rapidly grown over the last few years across a wide variety of applications that include aerial photography, surveillance, package delivery, cartography, agriculture, military missions, and"
10.36001_phmconf.2021.v13i1.3052,package,36,,0,"The Synthetic Minority Over-sampling Technique (SMOTE) was implemented through the imblearn package (Lemaˆıtre, Nogueira, & Aridas, 2017), as was the random undersampling technique described in section 4.2."
10.36001_phmconf.2017.v9i1.2189,"package, code package",95,,0,"The On Robot RG2 Gripper was selected as the end effector for the material handling operation in the initial test bed configuration (Figure 6 shows the RG2 mounted to the UR5 within the PHMC test bed). The RG2 is relatively low cost and comes with a software package for URs making it quick to integrate and operate., limiting integration time and complexity. The RG2 is an electrically actuated gripper that is grip force and/or position controllable. This allows the gripper to be used in both time-based control and event-based"
10.36001_phmconf.2017.v9i1.2307,"package, code package",99,,0,ALT - altitude; ATC - air trafﬁc control; CAS - calibrated airspeed; DST - distance; FL - ﬂight level; FMS - ﬂight management system; GPOPS - general pseudospectral optimization software; GS - gliding slope; LOC -localizer; NOMAD - Nonlinear Optimization by Mesh Adaptive Direct Search (software); OPTI - OPTimization Interface Toolbox; PSOPT - pseudospectral optimal control software package; RTA - required time of arrival; RWY - runway; SFO - San Francisco Airport; SPD - speed; TBO - trajectory based optimization.
10.36001_phmconf.2017.v9i1.2449,"package, code package",81,,0,"Two approaches to estimating system reliability are used. The first is a traditional RBD as described in (O'Connor, 2012). The second is to use the Monte-Carlo simulation in the AWB commercial software package.  As mentioned earlier, it is a series system with a reliability block for each subsystem as shown in Tables 1 and 2. A summary of the results is shown in Table 3 based on 100,000 simulations over a 24 hour"
10.36001_phmconf.2018.v10i1.245,"package, code package",31,,0,The first step was to classify which of the repairs were failure events.  We used the classifier in the GE Digital APM commercial software package which classifies a repair event
10.36001_phmconf.2018.v10i1.503,"package, code package",89,,0,"This paper described a software package for analyzing the safety of the National Airspace System. It allows the user to model every phase of aircraft operations such as taxi, takeoff, climb-to-cruise, descent-to-land and taxi to gate. It also allows introduction of human, equipment and environmental errors, faults and perturbations for the assessment of conditional probability density functions corresponding to various adverse events that can occur in the National Airspace System. Two examples on the use of the software were described."
10.36001_phmconf.2018.v10i1.503,"package, code package",168,,0,"This paper advances a computational framework in which factors impacting the safety of national airspace operations can be modeled and analyzed to assess emerging safety issues. This framework will be termed as the National Airspace Traffic-Prediction System (NATS) throughout this paper. NATS is implemented as a server-client software package that incorporates realistic models of three major subsystems: Equipment, Entities and Environment. The Equipment category includes aircraft, flight-deck automation equipment, ground vehicles, and surveillance and communication systems. The Entities category includes error models of all human operators involved in NAS operations such as pilots, air traffic controllers and ground vehicle operators. Thirdly, the Environment subsystem consists of airports with ramp, taxiways and runways, en-route and terminal area flight operations procedures, terrain, and weather. Any other subsystem models to be considered in the analysis can be modeled by the analyst and integrated with NATS under one of these three categories."
10.36001_phmconf.2018.v10i1.544,"package, code package",194,,0,"A domain knowledge-based air-ground traffic simulation framework is required for prognostics and risk analysis (Figure 1.). The current study advances a computational framework in which factors impacting the safety of national airspace operations can be modeled and analyzed to assess emerging safety issues.  This framework is termed as the National Airspace TrafficPrediction System (NATS) throughout this paper. NATS is implemented as a server-client software package that incorporates realistic models of three major subsystems: Equipment, Entities and Environment. (1) The Equipment category includes aircraft, flight-deck automation equipment, ground vehicles, and surveillance and communication systems. (2) The Entities category includes error models of all human operators involved in NAS operations such as pilots, air traffic controllers and ground vehicle operators. (3) The Environment subsystem consists of airports with ramp, taxiways and runways, en-route and terminal area flight operations procedures, terrain, and weather. Any other subsystem models to be considered in the analysis can be modeled by the analyst and integrated with NATS under one of these three categories."
10.36001_phmconf.2019.v11i1.806,"package, code package",22,,0,(2019). anomalous: Unusual time series detection [Computer software manual]. (R package version 0.1.0)
10.36001_phmconf.2017.v9i1.2306,"package, code package, code",75,,0,"The results of the model predictions obtained in this work were compared with the results of simulations of the SPLS obtained using commercial software package SINDA/FLUIND (Kashani et al., 2014, 2015) and homogeneous front propagation model (Haﬁychuk et al., 2015; Ponizovskaya-Devine et al., 2015b). The present model outperforms both of codes in terms of accuracy, it is also substantially faster than commercial code."
10.36001_phmconf.2015.v7i1.2710,"package, code package, data",112,,0,"Regardless of the approach, the module must be designed so that it has sufficient robustness to accommodate variations in the report data that are not associated with system performance. The ASHM software package is independent of the on-board aircraft software and therefore must accommodate changes in report format and content as they occur. The nature of the report data may be affected by one or more component operating states, and these must be observed and tracked to ensure proper operation. The software must also be prepared to recognize on-board data collection irregularities and screen out the affected values so that spurious fault correlations are not reported."
10.36001_phmconf.2017.v9i1.2449,"package, code package, data",88,,0,"The resulting data sets are fitted to Weibull distributions using Isograph’s Availability Work Bench (AWB) software, a widely used industry reliability software package. Parameters from these distributions inform the development of the reliability block diagram (RBD). The RBD is essentially a series system of all of the main components. Simulation is also conducted using the AWB package. A 720 hour period is selected for analysis. At 100,000 simulations the results for the system reliability converge."
10.36001_phmconf.2018.v10i1.245,"package, code package, data",54,,0,"The next step is to drill down into the failure mode structures and identify strategy opportunities.  Due to data quality issues, we mine the failure mode information by identifying the maintainable item and failure mechanism in the work orders using the classification algorithms from the GE Digital APM commercial software package."
10.36001_phmconf.2019.v11i1.888,"package, code package, data",81,,0,"The DAQ includes voltage input modules (for pressure transducers, rotary sensors and humidity sensors) and temperature input modules (for thermocouples). As the experimental data will be collected at steady-state and at 1Hz, the voltage input modules and temperature input modules were selected with a minimum rating of 10 kHz and 50 Hz/ch respectively. The NI LabVIEW software package will be used for data acquisition and visual representation of the data being collected."
10.36001_phmconf.2021.v13i1.3030,"package, code package, data",166,,0,"Simulation tools such as Simulink, LabVIEW or Modelica are industry standard for modeling components, designing systems, and performing simulations. Gazebo is a tool that focuses on environmental simulation and visualization, has a powerful physics engine and is great for simulating interactions with the environment. Robot Operating System (ROS) is a framework for developing robotics applications, and, can integrate with other tools. However, these are open-ended tools, not prognostics frameworks. The Generic Software Architecture for Prognostics (GSAP) (Teubert et al., 2017) is a framework for developing prognostic applications. This is an excellent software package developed by researchers at NASA with well deﬁned interfaces and is easily extended. GSAP focuses on implementing common functionality across PHM applications, not so much on data management. The data management framework presented here can interoperate with any of the aforementioned tools and example usage is provided with Simulink."
10.36001_phmconf.2018.v10i1.245,"package, data",97,,0,"Hess, K. & Gentleman, R. (2010). muhaz: Hazard Function Estimation in Survival Analysis. R package version 1.5 Hodkiewicz, M., Kelly, P., Sikorska, J., & Gouws, L. (2006). A framework to assess data quality for reliability variables. Engineering Asset Management, 137-147. Hodkiewicz, M., and Ho, M.T.W. (2016). Cleaning historical maintenance work order data for reliability analysis. Journal of Quality in Maintenance Engineering, 22(2), 146-163."
10.36001_phmconf.2019.v11i1.806,"package, data",129,,0,"For AnomDB, we utilize the anomalous time series analysis package for R (Hyndman et al., 2019). This package provides computations of a number of higher-level features suggested in (Hyndman et al., 2015), ranging from simple mean and variance to more complicated features like spectral entropy. Not all of these are necessarily useful for detection of the types of anomalies encountered in CNC machine control data, and there may be other variables not included in anomalous that might also prove useful. For the present introductory study, we simply select a subset of anomalous variables based on empirical results (see Table 1), though inclusion of other features in AnomDB could be useful to explore."
10.36001_phmconf.2019.v11i1.806,"package, data",136,,0,"In addition to this PCA dimensionality reduction on the raw control data time series, AnomDB also includes a second PCA dimensionality reduction step in the space of higher-level time series features extracted using the anomalous package (discussed in the next subsection). The inclusion of this additional step was advocated for in (Hyndman et al., 2015). It not only further contains the complexity of the learning problem and helps avoid the “curse of dimensionality” (where all data points may appear in sparsely-populated regions when the number of features is large), but it also allows for great ﬂexibility in what speciﬁc features are to be included. Determining the most relevant combinations of higher-level features then itself becomes part of the automated learning problem."
10.36001_phmconf.2019.v11i1.806,"package, data",170,,0,"We propose the application of unsupervised machine learning to automatically detect anomalous behavior on Computer Numerically Controlled (CNC) machines. We achieve this through an approach that utilizes Principal Component Analysis (PCA), time series feature extraction with the anomalous package in R, and Density Based Scanning of Applications with Noise (DBSCAN). We call this method AnomDB. Time series data collected from CNC machines may beneﬁt from this technique due to its ability to consolidate noisy, multivariate data from CNC machine controls and detect anomalies without reliance on periodicity of signal. We perform experiments on CNC machine control data to demonstrate the effectiveness of this method in discovering anomalies over other commonly used methods of anomaly detection such as Interquartile Range (IQR) and kmeans clustering. We show the effectiveness of this method through a case study of an actual machine anomaly, and then on a series of real machining data with synthetic anomalies injected."
10.36001_phmconf.2019.v11i1.806,"package, data",183,,0,"Perhaps one of the simplest and most commonly-used anomaly detection methods for time series data is to set upper and lower thresholds on the raw signal based on the interquartile range (IQR). As noted in the previous subsection, for multivariate data this could be implemented by setting thresholds in each individual feature, though again some care needs to be taken when working in a high number of dimensions. (This is another place where PCA can be helpful.) For time series that cover a large range of values under “normal” conditions, windowing in time can also be employed. For ideally periodic processes like repetitive part production, we could instead compare different part cycles at the same moment of progress toward part completion. Throughout this paper, we employ IQR as deﬁned by the anomalize package in R (Dancho & Vaughan, 2018) using default settings, which conducts IQR on the residuals of the input signal after Seasonal and Trend decomposition after LOESS (STL)."
10.36001_phmconf.2019.v11i1.888,"package, data",186,,0,"Although the component-level condition monitoring has been widely researched, the open literature lacks exhaustive research work on a system-level approach. FLECS (C. Müller, D. Scholz, 2007; Scholz, Giese, & Erdmann, 2007) is a system-level simulation platform for an ECS and contains healthy models of different ECS components. Test data (temperature readings) at the PACK outlet and the cabin were used to verify the model. Flowmaster (Y. Tu & Lin, 2011; Yi Tu & Lin, 2010) is another simulation package, verified against temperature readings that are collected through experimentation. Boeing’s EASY5 (Burroughs & Hammond, 1983) was used to model the ECS for both Civil and Military aircraft. However, systematic validation of against experimentally acquired pressure, temperature and mass flow data has not been reported.   With inspiration from EASY5, the Modelica programming language has been used (Steinkellner, 2011; in Dymola to model an ECS Steinkellner, Andersson, & Gavel, 2009)."
10.36001_phmconf.2019.v11i1.806,"package, retrieve, code package",23,,0,anomalize: Tidy anomaly detection [Computer software manual]. Retrieved from https://CRAN.R-project.org/ package=anomalize (R package version 0.1.1)
10.36001_phmconf.2019.v11i1.806,"package, retrieve, code package",37,,0,(2018). dbscan: Density based clustering of applications with noise (dbscan) and related algorithms [Computer software manual]. Retrieved from https://CRAN.R-project .org/package=dbscan (R package version 1.13)
10.36001_phmconf.2019.v11i1.806,"package, retrieve, code package, data",74,,0,"(2015). kmodr: K-means with simultaneous outlier detection [Computer software manual]. Retrieved from https://CRAN.R-project.org/ package=kmodR (R package version 0.1.0) Hyndman, R. J., Wang, E., & Laptev, N. (2015, Nov). Largescale unusual time series detection. In 2015 ieee international conference on data mining workshop (icdmw) (p. 1616-1619). doi: 10.1109/ICDMW.2015.104"
10.36001_phmconf.2015.v7i1.2716,provide implementation,187,,0,"The initial investment cost C0 is assumed as 50 Mio. US$ (aircraft list price in 2008 less an assumed price discount of 35 %). This study does not provide cost estimates for the development and implementation of PHM systems. The goal is to derive maximum acceptable investment costs for PHM systems from the analysis results. Therefore, no additional fix costs for an airplane equipped with PHM are considered. The delay costs of 0.63 US$ per passenger per minute include costs of passenger compensation and rebooking for missed connections, but also considers the costs of potential loss of revenue due to future loss of market share as a result of lack of punctuality (Eurocontrol, 2007). The internal rate of return r, which is used for the discounted cash-flow calculation, is assumed at 8 %. The reference aircraft (see 3.2) has been calibrated with a calibration factor of 0.929 affecting the ticket revenues to an airline internal rate of return of 12 % after 10 years of operation."
10.36001_phmconf.2020.v12i1.1215,provide implementation,82,,0,"than the mechanical system being monitored. The N-Zero based proposed sensor systems developed by Sandia National Laboratories, as described in a series of papers from Griffin, McClanahan, Clews, Reger, Yen et al (2017), even though not an exact replacement for a wired accelerometer, represents a paradigm shift in rotating machinery monitoring and will provide the critical component for a low-cost practical and implementation of CBM systems for essentially all rotating machines."
10.36001_phmconf.2021.v13i1.3054,publicly available,7,,0,1Project publicly available on EdgeImpulse: .edgeimpulse.com/public/32815/latest
10.36001_phmconf.2015.v7i1.2716,"publicly available, data",47,,0,"Studies following the proposed assessment approach require extensive data, which is usually – at least partially – considered confidential by airlines and maintenance, repair & overhaul (MRO) companies. For this reason, the authors have preferably used publicly available information only or"
10.36001_phmconf.2018.v10i1.474,"publicly available, data",272,,1,"In this research, the analysis is performed using Flight data recorder (FDR) information containing on-board aircraft sensor data from commercial flights of the four-engined Boeing 747-400 which is publicly available at the National Aeronautics and Space Administration (NASA) DASHlink network. The chosen data corresponds to 186 flight parameters that are categorized into two classes, based on their features, discrete and continuous. The discrete flight parameters are integers denoting either an on-off state such as landing gear up/down, or a flight status including takingoff, cruising, etc.; the continuous flight parameters include body longitudinal acceleration, position of rudder in degrees, aileron degree and elevator degree, and engine exhaust gas temperature, etc. that are obtained by the on-board sensors and can represent the performance of the aircraft. Therefore, these flight parameters are used to monitor the current condition of aircraft system in this research. Random noise induced by aircraft sensors and the data acquisition system in each feature is filtered through a weighted averaging window while maintaining inherent variances. A robust version of local regression weight linear least square (LOESS) method, available in MATLAB, is used and the window size is set to be 0.2% of the total number of data points. The redundant features are fused to further improve the accuracy and efficiency. For example, the exhausted gas temperature values are averaged into a signal value, since they come from four engines separately, which are identical under most of conditions."
10.36001_phmconf.2018.v10i1.477,"publicly available, data",83,,1,"The contribution is organized as follows: i) after introductory part stating the problem of interest, the state-of-art in EEG signal analysis including commonly used feature extraction/selection and classiﬁcation approaches is discussed in the second section, ii) afterwards, publicly available experimental data sets and used approaches are introduced in the third section, iii) whilst the obtained results are discussed in the fourth section. At last, the contribution closes with the conclusion and outlook."
10.36001_phmconf.2019.v11i1.788,"publicly available, data",130,,0,"SSM is another family of prediction method for RUL prediction, which is essentially HI based. The measurement equation in SSM maps the machine data or noisy HI observations to a latent variable. The state function updates the state estimates first and then predicts the future propagation of failure. To give a few examples, D. An et al. (An, Choi, & Kim, 2013) presented a tutorial for particle filter-based prognostics algorithms and applies it to the Li-on battery degradation prediction. J. Sun et al. (Sun, Zuo, Wang, & Pecht, 2012, 2014) uses Kalman filters to estimate the RUL distribution of aero-engines based on the public data given by CMAPSS."
10.36001_phmconf.2019.v11i1.816,"publicly available, data",62,,0,"The rest of the paper is organized as follows. Acronyms in Table 1, a brief theoretical background is in section 2. Section 3 describes the MS criterion. To evaluate the performance of the criterion, a comparative analysis using many public data sets is presented in section 4. Section 5 shows the conclusions and future research."
10.36001_phmconf.2016.v8i1.2505,"publicly available, data available, data",145,,0,"A review of the literature shows the prognostics models of PV systems to be poorly studied by scholars and researchers. The reviewed models have solely studied the degradation of the PV modules. While additional studies and research are needed, cabling termination and connector degradation models are equally important and crucial. Moreover, different types of materials in PV systems require different studies. Another challenge for the development of PHM models is their verification and validation using experimental data. Knowing that the design life of PV systems is usually more than 30 years, the availability of experience feedback and real performance data over long periods highlights the opportunity to test such models in the future. Such data is also generally not publicly available and solutions to address gaps for shared and clean data streams are also needed."
10.36001_phmconf.2019.v11i1.816,"publicly available, data available, data",41,,0,"UMW and LSW data sets are privately stored in the Manufacturing Research Lab of General Motors. Because of the active research aimed at understanding and improving these processes, General Motors can not make these data sets publicly available."
10.36001_phmconf.2021.v13i1.3055,"publicly available, data available, data",128,,1,"event data which we obtained by handling real life event data. Unfortunately, such event data is not publicly available, and hence, we use the widely used C-MAPSS data for RUL estimation and convert the continuous time series data to sequential events by thoroughly analyzing the C-MAPSS data. Afterwards, we utilize several deep learning and machine learning techniques for solving the RUL estimation task using the generated event data. Concisely, this paper proposes a novel and innovative framework for effectively handling the event-based RUL estimation task. In future, we plan to apply the proposed framework using more complex event data. Furthermore, we plan to generalize the framework to be applicable in various domains involving event data."
10.36001_phmconf.2020.v12i1.1199,"publicly available, data repository, data available, data",101,,0,"With such a convention of the free parameter assignment, a range of exponentially decaying factors, allows for predictions to be realized. Through the experiment with the arrays of Fourier coefﬁcients that have been generated, the same approach can be applied to ﬂight patterns for which the time intervals of each stage are collected, from which the temporally adjusted weighted exponential decrease of the circuit can be computed which is dependent on the magnitude of the current exerted during stages of the ﬂight. Such data sets are publicly available from the NASA prognostics repository 15."
10.36001_phmconf.2015.v7i1.2676,"publicly available, dataset provided, data",66,,0,"For the generation of probabilistic risk models associated with criminal incidents, it is necessary to analyze three types of information sources: (i) deﬁnition of a geographic area of interest, (ii) geo-referenced data of public services (e.g., banks, supermarkets), and (iii) geo-referenced data of criminal events. These items are described below."
10.36001_phmconf.2016.v8i1.2521,python,24,,0,"(2010). PyMC: Bayesian stochastic modelling in Python. Journal of Statistical Software, 35(4), 1–81."
10.36001_phmconf.2016.v8i1.2521,python,94,,0,"Van Rossum, G., & Drake, F. J. (2011). An introduction to Python. Godalming, Surrey, UK: Network Theory Ltd. Vrugt, J. A., Ter Braak, C., Diks, C., Robinson, B. A., Hyman, J. M., & Higdon, D. (2009). Accelerating Markov chain Monte Carlo simulation by differential evolution with self-adaptive randomized subspace sampling. International Journal of Nonlinear Sciences and Numerical Simulation, 10(3), 273–290."
10.36001_phmconf.2017.v9i1.2457,python,59,,0,"The test system is modeled in Simulink6, where Simscape Power Systems7 toolbox provides models of physical components and Stateﬂow8 charts are used to create time triggered automatons of protection system. Different scenarios are simulated in Simulink and their outputs are serialized into XML ﬁles. These XML ﬁles are parsed by python based TCD diagnosis prototype."
10.36001_phmconf.2018.v10i1.490,python,51,,0,"Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830."
10.36001_phmconf.2018.v10i1.503,python,14,,0,Python Software Foundation. (2018). (May 03) Python.
10.36001_phmconf.2018.v10i1.503,python,18,,0,approach-departure procedures. This example demonstrates a Python function that use of the taxi route design interface.
10.36001_phmconf.2018.v10i1.503,python,71,,0,"Compute intensive portions of the NATS software are coded in C++ for speed and flexibility, and the client-server interface is implemented in Java, Python and MATLB. In addition, the NATS server software is being coded in a way the use of high-performance computing amenable platforms, such as Graphical Processing Units, in order to  handle the need for Monte Carlo simulations of the entire NAS."
10.36001_phmconf.2018.v10i1.503,python,71,,0,"The NATS software is based on a client-server framework in which the all the computational functionality is implemented on a server hosted in the cloud, with the client accessing this through a set of Java™(Oracle, 2018), functionality Python™(Python or MATLAB®(The MathWorks Inc., 2018) interfaces. A notional block diagram of the NATS software is given in Figure 9."
10.36001_phmconf.2018.v10i1.564,python,17,,0,Stochastic Reduced Order Models with Python (SROMPy) (Tech. Rep.). NASA/TM2018-219824.
10.36001_phmconf.2018.v10i1.564,python,22,,0,"Python: Performance improvements and MPI-2 extensions. Journal of Parallel and Distributed Computing, 68(5), 655–662."
10.36001_phmconf.2018.v10i1.564,python,34,,0,"Dalc´ın, L., Paz, R., & Storti, M. (2005). MPI for Python. Journal of Parallel and Distributed Computing, 65(9), 1108–1115."
10.36001_phmconf.2018.v10i1.564,python,42,,0,"Dalc´ın, L. D., Paz, R. R., Kler, P. A., & Cosimo, A. (2011). Parallel distributed computing using Python. Advances in Water Resources, 34(9), 1124–1139."
10.36001_phmconf.2018.v10i1.564,python,124,,0,"A parallel sequential Monte Carlo (SMC) algorithm was developed and implemented using the mpi4py module in Python 2.7. The algorithm was applied to prognosis of crack growth in a geometrically complex, metallic specimen subjected to variable amplitude fatigue loading. A surrogate model was developed for crack growth simulation, and the uncertainty in model parameters was quantiﬁed using the parallel SMC sampler. The results were compared to parameters estimated via Markov chain Monte Carlo (MCMC) methods for veriﬁcation and performance evaluation. The SMC sampler provided estimates of mean and variance within a relative percent difference of 5% when compared to the MCMC results and decreased total computation time by three orders of magnitude."
10.36001_phmconf.2019.v11i1.876,python,10,,0,(2017). Deep learning with python. Manning
10.36001_phmconf.2020.v12i1.1162,python,31,,0,Figure 7. u1 and y1 and their integrals. There is 5% gaussian noise in each measurement. We use SciP y library in python to compute the integrals.
10.36001_phmconf.2021.v13i1.3052,python,49,,0,"Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . others (2011). Scikitlearn: Machine learning in python. Journal of Machine Learning Research, 12, 2825–2830."
10.36001_phmconf.2021.v13i1.3052,python,64,,0,"De Rainville, F.-M., Fortin, F.-A., Gardner, M.-A., Parizeau, M., & Gagn´e, C. (2012). Deap: A python framework for evolutionary algorithms. In Proceedings of the 14th Annual Conference Companion on Genetic and Evolutionary Computation (p. 85–92). New York, NY, USA: Association for Computing Machinery."
10.36001_phmconf.2021.v13i1.3052,python,82,,0,"Optimization of hyperparameters of the machine learning models was carried out using Sherpa, which is a Python library for hyperparameter tuning of machine learning models (Hertel, Collado, Sadowski, Ott, & Baldi, 2020). It provides a variety of optimization algorithms that can be used to explore any number of hyperparameters. The GPyOpt Bayesian Optimization was selected as it was efficient when using multiple parameters and when the number of trials is large."
10.36001_phmconf.2016.v8i1.2521,"python, code",86,,0,"around a set of externally trained surrogate models which replace the ﬁnite element analysis (FEA) component of highﬁdelity fracture modeling. The code tracks a given crack front in space and evolves the crack based on driving forces obtained from the surrogate models. The code was developed using the Python computing language (Van Rossum & Drake, 2011) as a general tool to reduce computation times associated with high-ﬁdelity fracture simulation and is not limited to the work presented herein."
10.36001_phmconf.2018.v10i1.503,"python, code",34,,0,NATS software provides several functions that can be used to develop user-defined codes for developing taxi routes. The results from a Python code based on these functions are discussed in this section.
10.36001_phmconf.2018.v10i1.503,"python, code",73,,0,Figure 10 and Figure 12 show some of the interactive features of a user-designed Python code used in the taxi route design. The user begins the route design process by clicking on the gate node from which the taxi plan begins in the interactive figure and continues to click through the nodes that make up the taxi route. A double click at the runway entrance node completes the taxi plan.
10.36001_phmconf.2020.v12i1.1131,"python, code, data",137,,0,"Signatures are computed by a custom function taking the parameters of a single ﬂight as input, and outputs the resulting signature. The code is written in Python with standard numerical and data analysis libraries. This function has ﬁve parameters: the names of both input signals (x- and y-axis), the period and range of the x-axis (e.g. the regime range), and the type of operation used to aggregate points in each bin of the x-axis (e.g. average, max, quantiles, etc.). Parameters are set in the conﬁguration ﬁle, allowing to extract various signatures using the same generic code. The feature extraction module applies this function on ﬂights in parallel across the cluster, as illustrated in Figure 7."
10.36001_phmconf.2018.v10i1.503,"python, code, database, data",119,,0,"7. Aircraft Dynamics6. Pilot & Flight Deck AutomationWaypoints, AltitudesFlight Mode, Airspeed, Heading, AltitudeLatitude, Longitude, Altitude4. EnRouteController + AutomationConflict Resolution, Processing Flight Plan Deviation Requests2. Surface Controller + AutomationWeather / Airspace Avoidance RequestsGate, Taxiway, Runway Assignments, Ground Vehicle ControlSequencing, Merging, Spacing, Departure & Arrival ProceduresNOAA Winds & Weather9. NAS Safety Metrics8. Accident & Incident Database1. NAS, Airport, Terrain Database (FAA, USGS)CNS & CNS Error TemplatesHuman Error Templates5. Flight Plan3. Terminal Area Controller + AutomationNATS ServerInternetNATS ClientUser Code:1.Python2.JAVA3.MATLABUser Graphical User InterfaceUser Data Files      ANNUAL CONFERENCE OF THE PROGNOSTICS AND HEALTH MANAGEMENT SOCIETY 2018"
10.36001_phmconf.2021.v13i1.2983,"python, data",96,,0,"We executed all experiments with Python 3.7 on a 64-bit Windows computer with 16GB of RAM and Intel i7-9750H processor. To set up the diagnosis rules Φ we used only data from normal operating conditions without any anomalies to create the weak fault models. Faults were simulated by assigning a random value ({(cid:62), ⊥}) to all variables m ∈ M . This results in a varying amount of faulty components for each experiment run. Thus, calculating and tracking symptoms of the systems was omitted."
10.36001_phmconf.2021.v13i1.3069,"python, data",114,,0,"As shown in Table 2, the SCADA system will collect information of 27 parameters each time. In order to enable the network model to learn the changes of these parameters over time, a training sample is composed of 10 adjacent time monitoring data, and thus the sample dimension is 10×27. The models are all written with Python 3.6 and the deep learning framework Pytorch, and run on Ubuntu 16.04 system with GTX 2080 GPU. In this experiment, five metrics (Accuracy, Precision, Recall, F1, and Score) are used to comprehensively evaluate the network performance. Those can be expressed as:"
10.36001_phmconf.2020.v12i1.1261,"python, data available, data",157,,0,"Besides, some research efforts focus on studying potentwin data. tially available methodologies towards digital For example, Seshadri et al. (Seshadri & Krishnamurthy, 2017) studied damage size and location estimation of aircraft structure using the digital twin concept, where the software, Abaqus/CAE, Matlab GA optimization routine, Python scripting language were adopted for numerical simulation. Li et al. (Li, Mahadevan, Ling, Choze, & Wang, 2017), adopted dynamic Bayesian network for aircraft wing health monitoring. This method integrates the test data, mathematical models and expert opinions for aircraft wing crack growth prediction, and is validated using a numerical simulation example. Millwater et al. (Millwater, Ocampo, & Crosby, 2019) reviewed probabilistic methods with potentials for digital twin implementation and discussed the future challenges for “digital twin” modeling."
10.36001_phmconf.2021.v13i1.2998,"python, data available, publicly available, data repository, data",127,,1,"We validate our approach using data publicly available through the NASA Prognostics Center of Excellence Data Repository (Bole, Kulkarni, & Daigle, 2014b). Results showed that our hybrid battery prognosis model can be successfully calibrated, even with a limited number of observations, and the model can help optimizing battery operations by offering long-term forecast of battery capacity. The construction of the model and all computations showed in this paper were performed using Python programming language and the deep learning frameworks Keras (Chollet, 2015) and Tensorflow (Abadi et al., 2015). Other libraries we utilized include Matplotlib (Hunter, 2007), Numpy and Scipy (Virtanen et al., 2020)."
10.36001_phmconf.2021.v13i1.3052,"python, dataset",34,,0,"(2017). Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. The Journal of Machine Learning Research, 18(1), 559–563."
10.36001_phmconf.2018.v10i1.564,"python, open-source",182,,0,"In an effort to increase accessibility for general scientiﬁc and engineering applications, Nguyen et al. produced a general algorithm for implementation of the SMC theory presented in this section (Nguyen et al., 2016). Their algorithm was adapted for parallel computing on distributed memory systems herein, as outlined in Algorithm 1. This parallel version was implemented using the mpi4py module (L. Dalc´ın, Paz, & Storti, 2005; L. Dalc´ın, Paz, Storti, & DEl´ıa, 2008; L. D. Dalc´ın, Paz, Kler, & Cosimo, 2011) in Python 2.7. Since the particles are independent of one another for a given iteration, the steps of the algorithm that involve actual model evaluations – namely those associated with lines 4 and 15 in Algorithm 1 – can be carried out in an embarrassingly parallel fashion across all available processors. While not currently available, an effort will be made post-publication of this conference paper to release an open source version of the software."
10.36001_phmconf.2020.v12i1.1135,"python, open-source, data",131,,0,"In the implementation of an SVM, 800 normal cases out of 8,800 and 480 abnormal cases out of 3,360 were used as test data, all the rest as training data. The area under the curve (AUC) score of the receiver operating characteristic (ROC) curve (ROC-AUC score) was used as the detection perforInstead of directly using the sensor value x, mance score. the input data to the SVM was standardized so that the mean and standard deviation of each sensor value were 0 and 1, respectively. Hyperparameters of the SVM were determined by cross-validation using the ROC-AUC score. The SVM and cross-validation programs are implemented by scikit-learn, a widely used open-source Python library for machine learning."
10.36001_phmconf.2016.v8i1.2521,"python, package",138,,0,"cle included all of the measurements from the beginning of the test up to that point in time. Non-informative, or uniform, prior distributions were used for all of the parameters at each interval as shown in Table 1. This implies that each MCMC run was considered independent from the others, and no prior information was passed between the subsequent runs as might be the case for a true Bayesian updating scheme. The adaptive Metropolis algorithm included in the PyMC python package (Patil, Huard, & Fonnesbeck, 2010) was used to generate 2 × 105 samples, with a conservative burn-in of 1 × 105 samples to ensure Markov chain stabilization. Geweke’s timeseries approach was utilized to diagnose chain convergence (Geweke, 1992)."
10.36001_phmconf.2021.v13i1.3052,"python, package",37,,0,"The model development and analysis was carried out on the Python platform. In particular, the Scikit-learn package (Pedregosa et al., 2011) was used to implement the classifiers described in section 4.1."
10.36001_phmconf.2021.v13i1.3059,"python, package",45,,0,"Christ, M., Braun, N., Neuffer, J., & Kempa-Liehr, A. W. (2018). Time series feature extraction on basis of scalable hypothesis tests (tsfresh – a python package). Neurocomputing, 307, 72-77."
10.36001_phmconf.2018.v10i1.503,"python, package, database, data",205,,0,"Safety analysis of complex systems can be carried out using computational models of their interactions. A modeling framework for analyzing the safety of the national airspace operations is described. The framework enables investigators to collaboratively formulate models of the subsystems of the National Airspace System and insert off-nominal operating conditions to assess their impact on the overall system safety. The software is designed to allow popular computational frameworks such as Python and Java to interact with it in a seamless manner. Interfaces to one of the popular commercial computational package are also provided. The software incorporates detailed aspects of the national airspace system infrastructure such as airport taxi ways and runways, approach-departure procedures, jet routes, air traffic control Sector and Center boundaries. It also includes simplified human performance models of controllers, pilots and ground vehicle operators. Performance data for over 400 different aircraft types, derived from a wellknown database have also been incorporated. Airspace flight operations data is obtained from the Federal Aviation Administration data feed, and the weather data is derived from the National Oceanic and Atmospheric Administration web site. Two example applications of the software are given."
10.36001_phmconf.2018.v10i1.490,"python, package, retrieve",125,,0,"where the function OOSVPHM is deﬁned in equation (5). The HP-conﬁg search space CPHM is displayed in Table 1. The search space CPHM is the Cartesian product of the search ranges shown in Table 1. An HP-conﬁg can be retrieved by taking one element in the search range in each row. Note that the ﬁrst two HPs are for the pre-processing procedure BLOCKDEC. The last four HPs are for the random forest regression model MRF. For more details about these HPs, please consult the documentation from Scikit-Learn (Pedregosa et al., 2011), a Python package for ML which is used for our experimentation. It is interesting to note that there are altogether"
10.36001_phmconf.2018.v10i1.503,"python, retrieve",7,,0,Retrieved from Python: https://www.python.org/
10.36001_phmconf.2021.v13i1.2998,"python, retrieve",37,,0,"(2020). A tutorial on solving ordinary differential equations using Python and hybrid physics-informed neural networks. Engineering Applications of Artificial Intelligence, 96, 103996. Retrieved from https:// www.sciencedirect.com/science/ article/pii/S095219762030292X 10.1016/j.engappai.2020.103996"
10.36001_phmconf.2015.v7i1.2709,retrieve,1,,0,Retrieved
10.36001_phmconf.2015.v7i1.2709,retrieve,68,,0,"Vasili, M., Hong, T., & Ismail, N. (2011). Maintenance optimization models: analysis. a International Conference on Industrial Engineering and Operations Management, 1131–1138. Retrieved from http://www.iieom.org/ieom2011/pdfs/IEOM173.pdf Wang, H. (2002). A survey of maintenance policies of deteriorating systems. European Journal of Operational Research, doi:10.1016/S0377469–489. 2217(01)00197-7"
10.36001_phmconf.2015.v7i1.2724,retrieve,37,,0,"Based on those assumptions, an algorithm has been developed in order to assemble sub-mechanisms retrieved from the literature into a structured causal tree. The causal tree generation methodology is presented in the Figure 6."
10.36001_phmconf.2015.v7i1.2764,retrieve,36,,0,"Schwabacher, M., & Goebel, K. (2007). A survey of artiﬁcial intelligence for prognostics. In Aaai fall symposium: Artiﬁcial intelligence for prognostics. Retrieved from http://www.aaai.org/Library/Symposia /Fall/2007/fs07-02-016.php"
10.36001_phmconf.2017.v9i1.2189,retrieve,16,,0,"Hedberg, T., & Helu, M. (2016). Smart Manufacturing  Retrieved from"
10.36001_phmconf.2017.v9i1.2189,retrieve,27,,0,"National Institute of Standards and Technology. (2017). Prognostics, Health Management, and Control from Retrieved (PHMC). http://www.nist.gov/el/isd/ks/phmc.cfm"
10.36001_phmconf.2017.v9i1.2189,retrieve,49,,0,"Vogl, G. W., Weiss, B. A., & Donmez, M. A. (2014). Standards Related to Prognostics and Health Management (PHM) for Manufacturing (NISTIR 8012). Retrieved from Gaithersburg, Maryland, USA: http://dx.doi.org/10.6028/NIST.IR.8012"
10.36001_phmconf.2017.v9i1.2426,retrieve,2,,0,Retrieved from
10.36001_phmconf.2017.v9i1.2439,retrieve,1,,0,Retrieved
10.36001_phmconf.2017.v9i1.2439,retrieve,5,,0,Retrieved from http://www.easa.europa.eu
10.36001_phmconf.2017.v9i1.2457,retrieve,9,,0,ElecOpendss manual. Institute. Retrieved http://sourceforge.net/apps
10.36001_phmconf.2017.v9i1.2457,retrieve,20,,0,(2012). 2012 state of reliability (Tech. Rep.). Retrieved from http://www.nerc.com/files/2012 sor.pdf
10.36001_phmconf.2017.v9i1.2457,retrieve,95,,0,"Chen, W. H., Tsai, S. H., & Lin, H. I. (2011, April). Fault section estimation for power networks using logic causeeffect models. IEEE Transactions on Power Delivery, 26(2), 963-971. doi: 10.1109/TPWRD.2010.2093585 Chhokra, A., Dubey, A., Mahadevan, N., & Karsai, G. (2017). faults in cyber-physical energy systems using temporal causal International Journal of Prognostics and diagrams. Health Management, Submitted to. Retrieved from http://www.isis.vanderbilt.edu/sites /default/files/IJPHM_chhokrad_rev_0.pdf/"
10.36001_phmconf.2017.v9i1.2457,retrieve,136,,0,"Jung, J., Liu, C.-C., Hong, M., Gallanti, M., & Tornielli, G. (2001, Apr). Multiple hypotheses and their credibility in on-line fault diagnosis. IEEE Transactions on Power Delivery, 16(2), 225-230. doi: 10.1109/61.915487 Krˇc´al, P., Mokrushin, L., Thiagarajan, P., & Yi, W. In Con(2004). Timed vs. time-triggered automata. cur 2004-concurrency theory (pp. 340–354). Springer. Mahadevan, N., Dubey, A., Karsai, G., Srivastava, A., Temporal causal di& Liu, C.-C. (2014). agrams for diagnosing failures in cyber-physical the Prognostics systems. and Health Management Society. Retrieved from http://www.phmsociety.org/node/1439"
10.36001_phmconf.2018.v10i1.245,retrieve,26,,0,"SMRP Best Practices (2017). Atlanta, GA: Society for Maintenance & Reliability Professionals (SMRP). Retrieved from https://smrp.org/"
10.36001_phmconf.2018.v10i1.502,retrieve,1,,0,Retrieved
10.36001_phmconf.2018.v10i1.502,retrieve,13,,0,FAA. (2007). Flight Risk Assessment Tools. Retrieved from
10.36001_phmconf.2018.v10i1.502,retrieve,18,,0,FAA. (2015). Safety Management System (SMS). Retrieved from https://www.faa.gov/about/initiatives/sms/
10.36001_phmconf.2018.v10i1.503,retrieve,1,,0,Retrieved
10.36001_phmconf.2018.v10i1.503,retrieve,5,,0,Retrieved from https://www.aviationweather.gov/
10.36001_phmconf.2018.v10i1.503,retrieve,5,,0,Retrieved https://www.ntsb.gov/investigations/AccidentRepo rts/Pages/aviation.aspx
10.36001_phmconf.2018.v10i1.503,retrieve,16,,0,Oracle. (2018). (May 04) JAVA. Retrieved from JAVA:
10.36001_phmconf.2018.v10i1.503,retrieve,17,,0,Google. (2018). (May 03) Google MyMaps. Retrieved from Help:
10.36001_phmconf.2018.v10i1.503,retrieve,18,,0,BTS. (2018). (July 31) Fatalities. Retrieved from Bureau of Statistics:
10.36001_phmconf.2018.v10i1.503,retrieve,20,,0,The MathWorks Inc. (2018). (May 03) MATLAB. Retrieved from Programming: https://www.mathworks.com/products/matlab.html
10.36001_phmconf.2018.v10i1.503,retrieve,34,,0,"Abiy, T., Pang, H., & Khim, J. (2018). (May 03) Dijkstra's Shortest Path Algorithm. Retrieved from BRILLIANT: https://brilliant.org/wiki/dijkstrasshort-path-finder/"
10.36001_phmconf.2018.v10i1.542,retrieve,62,,0,"Saxena, A., Orchard, M. E., Zhang, B., Vachtsevanos, G., Tang, L., Lee, Y., & Wardi, Y. (2007). Automated Contingency Management for Propulsion Systems. Control Conference (ECC), 2007 European, 3515– 3522. from Retrieved http://ieeexplore.ieee.org.etechconricyt.idm.oclc.org/sta mp/stamp.jsp?tp=&arnumber=7068389"
10.36001_phmconf.2018.v10i1.544,retrieve,170,,0,"The work consists of two parts (see Figure 5): 1) develop a Bayesian Network (BN) to represent the quantitative relationship between anomalous behaviors of ATCOs, human errors, and accidents through a review of accident reports retrieved from ASRS. The developed BN provides risk knowledge about how anomalous behaviors of ATCOs cause human errors and lead to accidents according to the histories of ATC-related accidents; 2) develop a sensor-based human behavior monitoring algorithm to identify anomalous behaviors of air traffic controllers automatically. The developed algorithm first uses facial landmark detection to extract the Eye Aspect Ratio (EAR) as an observable feature. Then the algorithm uses the extracted EAR as input to the Hidden Markov Model (HMM) to detect anomalous human behaviors. The proposed approach uses the detected anomalous behaviors of ATCOs as inputs to the developed BN based on accident reports and provides probabilities of human errors and ATC-related accidents in real-time."
10.36001_phmconf.2019.v11i1.598,retrieve,1,,0,Retrieved
10.36001_phmconf.2019.v11i1.598,retrieve,3,,0,crane. Retrieved
10.36001_phmconf.2019.v11i1.783,retrieve,18,,0,(2011). NGPM – A NSGA-II Program in Matlab v1.4. Retrieved from de.mathworks.com/ matlabcentral /fileexchange
10.36001_phmconf.2019.v11i1.806,retrieve,16,,0,"(2001). Applications of machine learning Ecological Modelling, Retrieved from http://"
10.36001_phmconf.2019.v11i1.806,retrieve,20,,0,"(2003). features the control of cnc milling operations unInternational and Manufacture, Retrieved from http://"
10.36001_phmconf.2019.v11i1.806,retrieve,35,,0,"(1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24(6), 417–441. Retrieved from https://doi.org/10.10372Fh0071325 doi: 10.1037/h0071325"
10.36001_phmconf.2019.v11i1.806,retrieve,35,,0,"(2005). Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences, 102(33), 11629–11634. Retrieved from https://www.pnas .org/content/102/33/11629 doi: 10.1073/ pnas.0409746102"
10.36001_phmconf.2019.v11i1.806,retrieve,54,,0,"(2015). Anomaly detection for monitoring: A statistical approach to time series anomaly detection. O’Reilly Media. Retrieved from https://books .google.com/books?id=DnnuuQEACAAJ Kang, Y., Hyndman, R. J., & Smith-Miles, K. (2017). Visualising forecasting algorithm performance using time"
10.36001_phmconf.2019.v11i1.806,retrieve,66,,0,"Duan, C., Makis, V., & Deng, C. (2019). Optimal bayesian early fault detection for cnc equipment using hidden semi-markov process. Mechanical Systems and Signal Processing, 122, 290 - 306. Retrieved from http:// www.sciencedirect.com/science/ article/pii/S088832701830760X https://doi.org/10.1016/j.ymssp.2018.11.040 Ertekin, Y. M., Kwon, Y., & Tseng, T.-L. B. of"
10.36001_phmconf.2019.v11i1.806,retrieve,83,,0,"series instance spaces. International Journal of Forecasting, 33(2), 345 - 358. Retrieved from http:// www.sciencedirect.com/science/ article/pii/S0169207016301030 https://doi.org/10.1016/j.ijforecast.2016.09.004 Kumar, S., Nassehi, A., Newman, S. T., Allen, R. D., & Tiwari, M. K. (2007). Process control in cnc manufacturing for discrete components: A step-nc compliant framework. Robotics and Computer-Integrated Manufacturing, 23(6), 667 - 676."
10.36001_phmconf.2019.v11i1.816,retrieve,15,,0,(2017). UCI machine learning repository. Retrieved from http:// archive.ics.uci.edu/ml
10.36001_phmconf.2019.v11i1.842,retrieve,9,,0,Based Maintenance Retrieved (http://www.mimosa.org/mimosa-osa-cbm).
10.36001_phmconf.2019.v11i1.861,retrieve,1,,0,Retrieved
10.36001_phmconf.2019.v11i1.861,retrieve,22,,0,"Sengupta, S., Basak, S., Saikia, P., Paul, S., Tsalavoutis, A empharecent Retrieved from"
10.36001_phmconf.2019.v11i1.861,retrieve,34,,0,"(2001, November). learn simple context-free and context-sensitive languages. Trans. Neur. Netw., 12(6), 1333–1340. Retrieved from https://doi.org/10.1109/72.963769 doi: 10.1109/72.963769"
10.36001_phmconf.2019.v11i1.861,retrieve,43,,0,"(2019). Chaotic quantum double delta swarm algorithm using chebyshev maps: Theoretical foundations, performance Journal of Senanalyses and convergence issues. sor and Actuator Networks, 8(1). Retrieved from https://www.mdpi.com/2224-2708/8/1/9 doi: 10.3390/jsan8010009"
10.36001_phmconf.2019.v11i1.861,retrieve,54,,0,"(2018). Particle swarm optimization: A survey of historical and recent developments with hybridization Machine Learning and Knowlperspectives. edge Extraction, 1(1), 157–191. Retrieved from https://www.mdpi.com/2504-4990/1/1/10 doi: 10.3390/make1010010 Sengupta, S., Basak, S., & Peters, R. A."
10.36001_phmconf.2019.v11i1.876,retrieve,1,,0,Retrieved
10.36001_phmconf.2019.v11i1.898,retrieve,1,,0,Retrieved
10.36001_phmconf.2019.v11i1.898,retrieve,4,,0,release]. Retrieved
10.36001_phmconf.2019.v11i1.898,retrieve,5,,0,Commercial Retrieved https://www.boeing.com/company/about-bca/
10.36001_phmconf.2019.v11i1.898,retrieve,5,,0,Retrieved https://www.boeing.com/resources/boeingdotcom/comp any/about_bca/pdf/statsum.pdf
10.36001_phmconf.2019.v11i1.898,retrieve,6,,0,air. Retrieved https://blogs.sap.com/2015/01/20/why-airlines-need-tokeep-planes-in-the-air/
10.36001_phmconf.2019.v11i1.898,retrieve,16,,0,ICAO. (2013). State of Global Aviation Safety. Retrieved from Canada:
10.36001_phmconf.2019.v11i1.898,retrieve,20,,0,FAA. (2015). Safety Management System Manual. FAA Retrieved from https://www.faa.gov/about/initiatives/sms/reference_libr ary/policy_and_requirements/media/AST_SMS_Manual _V2.0.pdf.
10.36001_phmconf.2019.v11i1.898,retrieve,39,,0,"CIAIAC. (2008). Accident involving a McDonnell Douglas (MD-82) aircraft, registration EC-HFP, DC-9-82 operated by Spanair, at Madrid-Baraja Airport (Report A-032/2008). Retrieved Spain: https://www.fomento.gob.es/recursos_mfom/2008_032_ a_eng_3.pdf"
10.36001_phmconf.2021.v13i1.2978,retrieve,29,,0,"McFadden, D. (1986). The Choice Theory Approach to Market Research. Marketing Science, 5(4), 275-297. from Retrieved http://www.jstor.org/stable/184004"
10.36001_phmconf.2021.v13i1.3003,retrieve,54,,0,"Tooth spur gears with tooth root mesh modeling of ﬁnite element/contact crack SAE Technical Paper mechanics 2021-01-0699, Retrieved from 185-200. https://doi.org/10.4271/2021-01-0699 doi: https://doi.org/10.1016/j.engfailanal.2014.11.015 Eklund, N. (2018). Anomaly detection tutorial. In Proceedings of the annual conference of the prognostics and health management society."
10.36001_phmconf.2015.v7i1.2713,"retrieve, data",91,,0,"In this section we discuss degradation data observed in devices under test for the experiments performed. Plots in Fig. 4 show the impedance measurement for one of the devices under test. The ﬁgure plots the imaginary and real values for a spectrum of frequencies. Using a system identiﬁcation model C and ESR values are retrieved from the measured raw data. As can be seen from the plots, as the device ages the slope of the measurement values changes. This is due to the change"
10.36001_phmconf.2017.v9i1.2439,"retrieve, data",46,,0,"Lacaille, J. (2010). Standardization of Data used for Monitoring an Aircraft Engine. USA; FR. Retrieved from http://worldwide.espacenet.com/publicationDetails/bi blio?DB=EPODOC&adjacent=true&locale=en_EP&F T=D&date=20100708&CC=WO&NR=2010076468A 1&KC=A1"
10.36001_phmconf.2019.v11i1.790,"retrieve, data",89,,0,"One-line machine and process diagnostics. CIRP Annals - Manufacturing Technology, 33(2), 469 - 473. Retrieved from http:// www.sciencedirect.com/science/ article/pii/S0007850616301688 http://dx.doi.org/10.1016/S0007-8506(16)30168-8 Kirwan, B., Gibson, H., Kennedy, R., Edmunds, J., Cooksley, G., & Umbers, I. (2004). Nuclear action reliability assessment (nara): a data-based hra tool. In Probabilistic safety assessment and management (pp. 1206–1211)."
10.36001_phmconf.2019.v11i1.806,"retrieve, data",84,,0,"Chawla, S., & Gionis, A. (2013). k-means-: A uniﬁed approach to clustering and outlier detection. In Sdm. Chen, S.-L., & Jen, Y. (2000). Data fusion neural network for tool condition monitoring in cnc milling machining. International Journal of Machine Tools and Manufacture, 40(3), 381 - 400. Retrieved from http:// www.sciencedirect.com/science/ article/pii/S0890695599000668 https://doi.org/10.1016/S0890-6955(99)00066-8"
10.36001_phmconf.2019.v11i1.842,"retrieve, data",56,,0,"Retrieved (https://www.dataone.org/data-life-cycle). DCC. 2008. “The DCC Curation Life-Cycle Model.” Dibsdale, Charlie. 2011. “Integrated Vehicle Health Management Operations Rooms.” Pp. 113–23 in Integrated Vehicle Health Management: Perspectives on an Emerging Field, edited by I. K. Jennions. SAE International."
10.36001_phmconf.2019.v11i1.898,"retrieve, data",71,,0,"Iverson, D. L. (2004). Inductive system health monitoring.  Jingru Yan, J. H. (2013). Flight Data Monitoring and Human Factors Risks Identification: A Review of Best Practices. Retrieved website: https://uwaterloo.ca/humans-complex-systemslab/sites/ca.humans-complex-systemslab/files/uploads/files/523_yan_etal_aero_2013.pdf Key Dismukes, L. L. (2004). The Limits of Expertise: The Misunderstood Role of Pilot Error in Airline Accidents. Retrieved https://humanfactors.arc.nasa.gov/flightcognition/article2.htm"
10.36001_phmconf.2017.v9i1.2426,"retrieve, data repos, data https, data repository, data",16,,1,Battery data set. NASA Ames Prognostics Data Repository. Retrieved from https://ti.arc.nasa.gov/tech/ dash/pcoe/prognostic-data-repository/
10.36001_phmconf.2021.v13i1.3030,"retrieve, database",37,,0,and degradation proﬁles are loaded from the database. All the necessary degradation information can be retrieved with the following sql statement (listing 1) that joins three of the tables shown in Figure 2.
10.36001_phmconf.2015.v7i1.2724,"retrieve, database, data",74,,0,"Each physical state corresponds to a unique combination of symptoms with threshold values associated to the recorded diagnostic data and visual inspection observations. An example of threshold definition is given in Figure 2. The degradation symptoms of a generator are automatically retrieved from the diagnostic database, and compared with the individual thresholds defined by the experts. This makes states and it possible consequently all active mechanisms in the system."
10.36001_phmconf.2018.v10i1.503,"retrieve, download",42,,0,Retrieved http://www.eurocontrol.int/services/bada FAA. (2005). (September 16) FAA Advisory Circular. Retrieved from Aircraft Surveillance Systems and Operations: https://www.faa.gov/documentLibrary/media/Advi sory_Circular/AC%20120-86.pdf (2018). Procedures. https://www.faa.gov/air_traffic/flight_info/aeronav /digital_products/cifp/download/
10.36001_phmconf.2019.v11i1.888,"retrieve, download",26,,0,"Butler, S. (2012). Prognostic Algorithms for Condition Monitoring and Remaining Useful Life Estimation. IRELAND, NATIONAL MAYNOOTH.Retrieved from http://core.kmi.open.ac.uk/download/pdf/11527019.pdf"
10.36001_phmconf.2015.v7i1.2710,"supplementary data, data",287,,0,"After the diagnostic or prognostic method has been created, it is subjected to a series of validation tests to ensure that it provides the desired system heath information with an acceptable level of performance. Generally the performance of diagnostic methods is evaluated by determining the correctness of fault detection results, and the accuracy of fault severity assessment metrics. Of particular interest are the rates at which two diagnostic results occur: false alarms, or when the diagnostic system detects a fault but the condition of the system is not significantly degraded, and missed detection when the diagnostic system does not indicate a fault when one is known to be present. To support this activity, again the field SI and removal data is used to obtain ground truth information about the health state of the fleet. Documented failures, particularly those with a conclusive root cause assessment are extremely valuable in establishing system response at degraded health states. The fleet service history not belonging to known fault cases can be used to establish baseline system performance. Laboratory test results can be used to supplement field data experience, particularly in cases where practical fault experience limited. The validation of prognostic approaches is a far more complex topic and has a more significant set of input requirements (Byington, Roemer, Kalgren, & Vachtsevanos, 2005). Given the long timescale of component life, it is generally impractical to complete significant validation prior to algorithm deployment. However, it is of critical importance to establish the accuracy and uncertainty bounds of the models used to assess and predict system health progression."
10.36001_phmconf.2016.v8i1.2555,"supplementary data, data",130,,0,"Decision support systems aim to improve the quality of services and help operators perform their duties faster, more accurately and more efficiently by providing an immense amount of knowledge. As human operators cannot convey their complete understanding of the situation to the system, decision support systems face the challenge of interpreting human intent based on operator inputs, which introduces a high level of uncertainty into the system. In this paper, a decision support system is used for determining system health status as a decision aid to the operator. The goal of this work is to supplement sensor data with human inputs in a prognostic health management environment while minimizing the effects of uncertainty and to provide situational awareness for the operator."
10.36001_phmconf.2017.v9i1.2426,"used dataset, dataset provided, dataset, data repository, data",118,,1,"itoring (PHM) community. The synthetic problem is the nextstep prediction task for an AutoRegressive Fractionally Integrated Moving Average (ARFIMA) process, which has a long memory. This is an example of a time series prediction problem. For the fault detection, we use the bearing vibration dataset provided by Machinery Failure Prevention Technology (MFPT) Society.1 For the fault prognosis problem, we solve the Remaining Useful Life (RUL) problem using the Turbofan Engine Degradation Simulation dataset. Finally, the Li-Ion battery charge-discharge dataset is used as an example of the battery capacity predicting problem. These datasets are both provided by Prognostics Data Repository of NASA.2"
1401.8008,"code, package, code available, github, code package",56,2022-04-27,2,"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github."
1401.8008,data,103,2022-05-13,0,"Comparison data also results when considering subjective human evaluations of pairs of items. For example, if each item is a movie, a person might say that Les Misérables is better than Star Wars, and The Empire Strikes Back is as good as Star Wars. Another example is rating food items such as wine, in which a person may prefer one wine to another, but not be able to perceive a diﬀerence between two other wines. In this context, it is important to use a model which can predict no diﬀerence between two items."
1401.8008,data,11,2022-05-13,0,4. Comparison to SVMrank in sushi and simulated data sets
1401.8008,data,112,2022-05-13,0,"Fig. 6. Test AUC for each model used after training on the ﬁrst 4 months of match data in the 8 different 12-month periods. All SVM model AUCs are shown in addition to AUC of the ELO, Glicko scores and trivial model. The plots in black show the trivial AUC calculated by predicting the most common positive label. The plots in dark blue are AUC values obtained from using Glicko or ELO scores only. The plots in light blue show the AUC distribution from models using only ELO and Glicko features and plots in white are AUC values from models using all computed features."
1401.8008,data,113,2022-05-13,0,"The goal of learning to compare is to accurately predict a test set of labeled pairs (2), which includes equality yi = 0 pairs. We test the SVMcompare algorithm alongside two baseline models that use SVMrank (Joachims 2002). We chose SVMrank as a baseline because of its similar large-margin learning formulation, to demonstrate the importance of directly modeling the equality yi = 0 pairs. SVMrank does not directly model the equality yi = 0 pairs, so we expect that the proposed SVMcompare algorithm makes better predictions when these data are present. The diﬀerences between the algorithms are summarized in Table 2:"
1401.8008,data,115,2022-05-13,0,"Lemma 3.1 establishes the fact that one can learn a ranking function r and a corresponding comparison function c1, deﬁned in (5), by solving either the LP (7) or the QP (12). To make corresponding learning problems for non linearly-separable data as deﬁned by the linear separability test in this section, one can add slack variables to either the QP or the LP. In the next subsection, we pursue only the QP, since it leads to a dual problem with a sparse solution that can be solved by any standard SVM solver such as libsvm (Chang & Lin 2011)."
1401.8008,data,138,2022-05-13,0,"In this subsection, we assume the data are not linearly separable, and want to learn a nonlinear ranking function. We deﬁne a positive deﬁnite kernel κ : Rp × Rp → R, which implicitly deﬁnes an enlarged set of features Φ(x) (middle panel of Figure 1). As in (9), we learn a function f (x) = β + u⊺Φ(x) which is aﬃne in the feature space. Let α, α′ ∈ Rm be coeﬃcients such that u = i), and so we have m i=1(αiκ(˜xi, x) + α′ f (x) = β + i, x)). We then use Lemma 3.1 to deﬁne the ranking function"
1401.8008,data,15,2022-05-13,0,"Overall from the sushi data, it is clear that the proposed SVMcompare model performs"
1401.8008,data,15,2022-05-13,0,"generalizes to a test set of data, as measured by the zero-one loss:"
1401.8008,data,161,2022-05-13,0,"In Figure 5 we ﬁxed the number of training pairs n = 400 and varied the proportion ρ of equality pairs for the three simulated squared norm ranking functions r. We select the model with maximum area under the validation set ROC curve, then use test set AUC to evaluate the learned models. All methods perform close to the optimal true ranking function when r(x) = ||x||2 2. For the other patterns, it is clear that all the methods perform similarly when there are mostly inequality pairs (ρ = 0.1), since SVMrank was designed for this type of training data. In contrast, when there are mostly equality pairs (ρ = 0.9), the compare and rank2 methods clearly outperform the rank method, which ignores the equality pairs. It is also clear that the rank2 and compare methods perform similarly in terms of test AUC."
1401.8008,data,19,2022-05-13,0,"Joachims, T. (2002), Optimizing search engines using clickthrough data, in ‘KDD’."
1401.8008,data,199,2022-05-13,0,"In the supervised learning to rank problem (Li 2011), we are given labeled pairs of items x, x′, where the label y ∈ {−1, 1} indicates which item in the pair should be ranked higher. The goal is to learn a ranking function r(x) ∈ R which outputs a real-valued rank for each item. In this paper we consider a related problem in which the expanded label space y ∈ {−1, 0, 1} includes the y = 0 label which indicates that there should be no rank diﬀerence. In this context the goal is to learn a comparison function c(x, x′) ∈ {−1, 0, 1}. Comparison data naturally arise from competitive two-player games in which the space of possible outcomes includes a draw (neither player wins). In games such as chess, draws are a common result between highly skilled players (Elo 1978). To accurately predict the outcome of such games, it is thus important to learn a model that can predict a draw."
1401.8008,data,2,2022-05-13,0,training data
1401.8008,data,23,2022-05-13,0,"predicted (Shashua & Levin 2003). In this article we propose SVMcompare, a support vector algorithm for these data."
1401.8008,data,29,2022-05-13,0,"Overall, our analysis of the chess match data suggests that the proposed SVMcompare model performs with a higher AUC than the existing state-of-the-art ELO and Glicko results."
1401.8008,data,31,2022-05-13,0,"Overall from the simulations, it is clear that when the data contain equality pairs, it is advantageous to use a model such as the proposed SVMcompare method which learns"
1401.8008,data,32,2022-05-13,0,"Taken together, these imply ˆµ = −1/β. Now, by deﬁnition of the ﬂipped data (8), we can re-write the max margin QP (9) as"
1401.8008,data,51,2022-05-13,0,"These data are geometrically represented in the top panel of Figure 1. Pairs with equality labels yi = 0 are represented as line segments, and pairs with inequality labels yi = {−1, 1} are represented as arrows pointing to the item with the higher rank."
1401.8008,data,57,2022-05-13,0,"For future work, it will be interesting to see if the same results are observed in learning to rank data from search engines. For scaling to these very large data sets, we would like to try algorithms based on smooth discriminative loss functions, such as stochastic gradient descent with a logistic loss."
1401.8008,data,67,2022-05-13,0,"The optimal decision boundaries r(x) ∈ {−1, 1} and margin boundaries r(x) ∈ {−1 ± µ, 1 ± µ} are drawn in Figure 2. Note that ﬁnding a feasible point for this LP is a test of linear separability. If there are no feasible points then the data are not linearly separable."
1401.8008,data,7,2022-05-13,0,3.2. Kernelized QP for non-separable data
1401.8008,data,77,2022-05-13,0,"Fig. 2. The separable LP and QP comparison problems. Left: the difference vectors x′ − x of the original data and the optimal solution to the LP (7). Middle: for the unscaled ﬂipped data ˜x′ − ˜x (8), the LP is not the same as the QP (9). Right: in these scaled data, the QP is equivalent to the LP."
1401.8008,data,89,2022-05-13,0,"1 where x ∈ R2. Left: the training data Fig. 3. Application to a simulated pattern r(x) = ||x||2 are n = 100 pairs, half equality (segments indicate two points of equal rank), and half inequality (arrows point to the higher rank). Others: level curves of the learned ranking functions. The rank model does not directly model the equality pairs, so the rank2 and compare models recover the true pattern better."
1401.8008,data,89,2022-05-13,0,"Our experimental results on simulated and real data clearly showed the importance of directly modeling the equality pairs, when they are present. We showed in Figure 5 that when there are few equality pairs, as is the usual setup in learning to rank problems, the baseline SVMrank algorithm performs as well as our proposed SVMcompare algorithm. However, when there are many equality pairs, it is clearly advantageous to use a model such as SVMcompare which directly learns from the equality pairs."
1401.8008,data,94,2022-05-13,0,"3.1. LP and QP for separable data In our learning setup, the best comparison function is the one with maximum margin. We will deﬁne the margin in two diﬀerent ways, which correspond to the linear program (LP) and quadratic program (QP) discussed below. To illustrate the diﬀerences between these max-margin comparison problems, in this subsection we assume that the training data are linearly separable. Later in Section 3.2, we propose an algorithm for learning a nonlinear function from non linearly-separable data."
1401.8008,"data, database",181,2022-05-13,0,"Ranking data sets are often described not in terms of labeled pairs of inputs (xi, x′ i, yi) but instead single inputs xi with ordinal labels yi ∈ {1, . . . , k}, where k is the number of integral output values. Support Vector Ordinal Regression (Chu & Keerthi 2005) has a large-margin learning formulation speciﬁcally designed for these data. Another approach is to ﬁrst convert the inputs to a database of labeled pairs, and then learn a ranking model such as the SVMcompare model we propose in this paper. Van Belle et al. (2011) observed that directly using a regression model gives better performance than ranking models for survival data. However, in this paper we limit our study to models for labeled pairs of inputs, and we focus on answering the question, “how can we adapt the Support Vector Machine to exploit the structure of the equality yi = 0 pairs when they are present?”"
1401.8008,"data, dataset",101,2022-05-13,0,"The notation and organization of this article is as follows. We use bold uppercase letters for matrices such as X, K, and bold lowercase letters for their row vectors xi, ki. In Section 2 we discuss links with related work on classiﬁcation and ranking, then in Section 3 we propose a new algorithm: SVMcompare. We show results on 3 illustrative simulated data sets and 2 real by learning to rank a sushi data set and a chess dataset in Section 4 and 5. We then discuss future work in Section 6."
1401.8008,"data, dataset",159,2022-05-13,0,"In ranking problems, the goal is to learn a ranking function r(x) ∈ R from Summary. labeled pairs x, x′ of input points. In this paper, we consider the related comparison problem, where the label y ∈ {−1, 0, 1} indicates which element of the pair is better (y = −1 or 1), or if there is no signiﬁcant difference (y = 0). We cast the learning problem as a margin maximization, and show that it can be solved by converting it to a standard SVM. We use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed SVMcompare algorithm outperforms SVMrank when there are equality y = 0 pairs. In addition, we show that SVMcompare outperforms the ELO rating system when predicting the outcome of chess matches."
1401.8008,"data, dataset",68,2022-05-13,0,"Fig. 5. Area under the ROC curve (AUC) for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. For each data set we picked n = 400 pairs, varying the proportion ρ of equality pairs. We plot mean and standard deviation of AUC over 4 test sets."
1401.8008,"data, dataset",78,2022-05-13,0,"Another way to formulate the comparison problem is by ﬁrst performing a change of variables, and then solving a binary SVM QP. The idea is to maximize the margin between signiﬁcant diﬀerences yi ∈ {−1, 1} and equality pairs yi = 0. Let Xy, X′ y be the |Iy| × p matrices formed by all the pairs i ∈ Iy. We deﬁne a new “ﬂipped” data set with"
1401.8008,"data, dataset",86,2022-05-13,0,"2.2. SVMrank for comparing In this subsection we explain how to apply the existing SVMrank algorithm to a comparison data set. The goal of SVMrank is to learn a ranking function r : Rp → R. When r(x) = w⊺x (where ⊤ denotes the transpose) is linear, the primal problem for some cost parameter C ∈ R+ (where R+ is a set of all non-negative real numbers) is the following quadratic program (QP):"
1401.8008,"data, dataset",87,2022-05-13,0,"Fig. 4. Test error for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. We randomly generated data sets with ρ = 1/2 equality and 1/2 inequality pairs, then plotted test error as a function of data set size n (a vertical line shows the data set which was used in Figure 3). Lines show mean and shaded bands show standard deviation over 4 test sets."
1401.8008,"data, dataset",95,2022-05-13,0,"For each experiment, there are train, validation, and test sets each drawn from the same data set of examples. We ﬁt a 10 × 10 grid of models to the training set (cost parameter C = 10−3, . . . , 103, Gaussian kernel width 2−7, . . . , 24), and select the model using the validation set. We use two evaluation metrics to judge the performance of the models: zero-one loss and area under the ROC curve (AUC)."
1401.8008,"data, dataset, data https",164,2022-05-13,1,"5.1. Data source and processing We downloaded the chess match dataset from Chessmetrics (http://www.chessmetrics.com/cm/), containing 1.8 million games played over the 11year period from 1999–2009 by 54205 chess players. For each of the years 1999–2006, we consider the ﬁrst four months (Jan–Apr) as a train set, and the last eight months as a test set (May–Dec). We removed all matches containing a player who had less than 10 matches against other players in the train set, to prevent our data set from containing players with very little information. We also removed all matches that contained a player’s ﬁrst match from the train set as we would have no information about this player. Before pre-processing, 30.9% of matches were a draw and 69.1% of matches resulted in a win or loss. After pre-processing, the median percentage of draws and win-loss results"
1401.8008,"data, dataset, data https",233,2022-05-13,1,"4.2. Learning to rank sushi data We downloaded the sushi data set of Kamishima et al. (2010) from kamishima (http://www.kamishima.net/sushi). We used the sushi3b.5000.10.score from kamishima, which consist of 100 diﬀerent sushis rated by 5000 diﬀerent people. Each person rated 10 sushis on a 5 point scale, which we convert to 5 preference pairs, for a total of 17,832 equality yi = 0 and 7,168 inequality yi ∈ {−1, 1} pairs. For each pair i we have features i ∈ R14 consisting of 7 features of the sushi and 7 features of the person. Sushi xi, x′ features are style, major, minor, oily, eating frequency, price, and selling frequency. Person features are gender, age, time, birthplace and current home (we converted Japanese prefecture codes to latitude/longitude coordinates). As in the simulations of Section 4.1, we picked train, validation, and test sets, each with the same number of pairs n and the same proportion ρ of equality pairs. We ﬁt a grid of models to the training set, select the model with minimal zero-one loss on the validation set, and then use the test set to estimate the generalization ability of the selected model."
1401.8008,"dataset, package",127,2022-05-13,0,"was 44.7% and 55.3% respectively over each of the 8 datasets. For each match i, we i ∈ R16 consisting of ELO scores, Glicko scores, if the player had computed features xi, x′ the initial move, the percentage of instances where a player either lost to a lower ranked player, or won against a higher ranked player, the average score diﬀerence of opponents, win/loss/draw/games played raw values and percentages in addition to various other statistics. ELO scores were initially set at 1200 for all players and FIDE rules were applied to score calculations. ELO and Glicko scores were updated after every match using the PlayerRatings R package (Stephenson & Sonas 2016)."
1401.8008,github,3,2022-05-13,0,https://github.com/tdhock/compare-paper
1401.8008,package,13,2022-05-13,0,"Player Ratings Estimation (R package version 1.0-1)’, CRAN ."
1401.8008,package,26,2022-05-13,0,"Karatzoglou, A., Smola, A., Hornik, K. & Zeileis, A. (2004), ‘kernlab – an S4 package"
1605.07495,code,14,2022-05-13,0,Algorithm 1: Pseudo code of selection method based on non-dominated relative crowding distance
1605.07495,code,16,2022-05-13,0,Algorithm 3: Pseudo code of calculation methods of CR and LR in (15)
1605.07495,code,233,2022-05-13,0,"(i.e., ξcd) and the relative crowding distance (i.e., ξrcd) of all the 6 solutions are given out in TABLE II. Then, with nondominated CDV, the Θ2, Θ3 and Θ4 are qualiﬁed to be the candidates. Thus, the global best solution will be selected out according to their ξrcd (i.e., the selected ones will be marked with circle). For instance, since with the largest ξrcd, the Θ4 has the highest priority to be selected as global best in any case, while the Θ2 is with the lowest priority (only when Y = 3) for its smallest ξrcd. Finally, we assume there are 100 particles in total and we give out the result of particle dividing when Y = 1, 2, 3 (i.e., the numbers of particles in the group assigned to each selected solution are given out after each circle). We notice that the Θ5 is not be selected for any case though its relative crowding distance higher than that of the Θ2, it is because that the CDV of the Θ5 is dominated by the Θ3, which excluding its possibility to be selected as global best. We also give out the pseudo code of this selection method in Algorithm 1."
1605.07495,code,65,2022-05-13,0,"more non-dominated solutions. Meanwhile, each sub-swarm aims to ﬁnd better values for its assigned objective function and all sub-swarms work together in order to get a bigger PF . The pseudo code of MOPSO-NRCD is given at Algorithm 2 (the calculation method of objective functions is given in Algorithm 3, whose detail will be given in the next section)."
1605.07495,code,7,2022-05-13,0,Algorithm 2: Pseudo code of MOPSO-NRCD
1606.01039,data,10,2022-05-13,0,(b) Signal used for ﬁlling missing-data gaps.
1606.01039,data,106,2022-05-13,0,Results using (16) are presented in Figures 5(e)-5(f). We see that in Figure 5(f) the posterior mean describes properly the periodic behaviour and amplitude envelope smooth evolution of the modelled signal. We observe that prediction on the decay gap using (16) is closer to the actual data (red dots) than the results obtained with (15) as well as (14). This is reﬂected in the smallest RMS error in table 2. This is because (16) allows to describe periodic functions that
1606.01039,data,119,2022-05-13,0,"To face the issue of modelling time dynamics we modiﬁed the previous covariance function (15), by multiplying it with an exponentiated quadratic kernel (14). This allows to “smooth” the strictly periodic behaviour of (15). The resulting kernel corresponds to (16). From Figure 4(b) we see that although the posterior mean of the predictive distribution does not exactly ﬁt the data, the model is able to learn the pitch of each of the three sound events with a smaller RMS error (Table 1), as well as the time dynamics or variations in the amplitude envelope of the signal."
1606.01039,data,131,2022-05-13,0,"Experiments were done over real audio. We evaluated diﬀerent kernel conﬁgurations on a pitch estimation task, and on a missing data imputation task. All experiments assume we previously know the number of change-windows and its locations. In the pitch estimation task all the parameters of the covariance function are known, except those related with the fundamental frequency of each sound event, i.e. the value of ωm in (15) and (16) when using these kernels in the general model (10). Thus, we focus on optimizing only these model hyperparameters from the data. In the missing data imputation task the score of the modelled piece of music audio is used for tuning manually the model hyperparameters."
1606.01039,data,132,2022-05-13,0,"In music information research, the aim of audio content analysis is to estimate musical concepts which are present but hidden in the audio data [17]. With this purpose, diﬀerent signal processing techniques are applied to music signals for extracting useful information and descriptors related to the musical concepts. Here, musical concepts refers to parameters related to written music, such as pitch, melody, chords, onset, beat, tempo and rhythm. Then, perhaps the most general application is one which involves the prediction of several musical dimensions, that of recovering the score of a music track given only the audio signal [10]. This is known as automatic music transcription (AMT) [5]."
1606.01039,data,132,2022-05-13,0,"We performed regression on the signal shown in Figure 3(a) using the kernel (15). Figure 4(a) shows the posterior mean of the predictive distribution after training (blue continuous line). The black circle points correspond to observed data. We see the trained model is able to estimate the pitch for each sound event with a RMS error of 0.6282 semitones (Table 1). On the other hand, the amplitudeenvelope evolution of the signal is beyond the scope of the structure that this kernel can model. This is because this covariance function can only describe constant amplitude-envelope, periodic signals, with a fundamental frequency and several harmonics (see Figure 2(f))."
1606.01039,data,142,2022-05-13,0,"In this study we used two short audio excerpts, in order to explore the method, so that we can eﬃciently ﬁt models and search in the hyperparameter space. The excerpt used for pitch estimation experiments corresponds to 0.7 seconds of the song Black Chicken 37 by Buena Vista Social Club. This segment of audio contains three notes of a bass melody (Figure 3(a)). In the missing data imputation task we used polyphonic audio corresponding to 1.14 seconds of Chopin’s Nocturne Op. 15 No. 1, where more than one note occur at the same time. The segments of signal in red in Figure 3(b) represent gaps of missing data. We reduced the sample frequency of both audio excerpts from 44.1KHz to 8KHz."
1606.01039,data,144,2022-05-13,0,"In this article we discussed a Gaussian processes regression framework for modelling music audio. We compared diﬀerent models in pitch estimation as well as in prediction of missing data. We showed which kernels were more appropriate for describing properties of music signals, speciﬁcally: nonstationarity, dynamics, and spectral harmonic content. The advantage of this approach is that by designing a proper kernel we can introduce prior knowledge and beliefs about the properties of music signals, and use all that prior information to improve prediction. The presented work could be extended using eﬃcient representations of GPs in order to model larger audio signals. Other kernels could be studied, as the spectral mixture for modelling harmonic content [2], and Latent Force models [1] for describing mechanistic characteristics of the signal."
1606.01039,data,153,2022-05-13,0,"We compared three diﬀerent models predicting missing-data gaps. We studied kernels (14), (15), and (16). In Figure 3(b) ﬁrst gap (red segment) contains the transient (onset and attack [3]) of a sound event, whereas the second gap is located in a more stable segment of the data (smooth decay). Figures 5(a)5(b) depict the prediction using (14). These ﬁgures correspond to zoom in small sections of the signal where the gaps occur (Figure 3(b)). We see that the model using this kernel overﬁts the data, i.e. the posterior mean (blue line) ﬁts all the observed data (black dots) with high conﬁdence (grey shaded area), but the"
1606.01039,data,183,2022-05-13,0,"Figures 5(c)-5(d) show the prediction using covariance function (15). In the transient gap (Figure 5(c)) the posterior mean (blue line) does not follows the data, this is because transients are short intervals during which the signal evolves in a nonstationary, nontrivial and unpredictable way [3]. opposite to this, the model using kernel (15) can only describe the behaviour of constant amplitudeenvelope periodic stochastic functions. In the second gap (Figure 5(d)) the posterior mean describes properly the periodic behaviour of the data, but it does not follow the amplitude-envelope of the observations. This is because this covariance function is able to describe periodic functions that have several harmonic components. The drawback of this kernel is that it assumes constant the amplitude of the periodic stochastic functions that describes. These diﬀerent performance on the prediction is reﬂected on the RMS error obtained for each gap (Table 2)."
1606.01039,data,2,2022-05-13,0,3.1 Data
1606.01039,data,219,2022-05-13,0,"R where the time input variable t ∈ R, we model the whole function f (t) as a GP. That is, instead of putting a prior over the function parameters η, we introduce a prior over the function f (t) itself [16]. Learning in GP regression corresponds to computing the posterior distribution over the function f (t) conditioned on the observed data y = [y1, · · · , yN ] [15, 13]. The underlying idea in GP regression is that the correlation function introduces dependences between function f (t) values at diﬀerent inputs. Thus, the function values at the observed points give information also of the unobserved points [14]. The structure of the kernel (1) captures high-level properties of the unknown function f (t), which in turn determines how the model generalizes or extrapolates to new test time instants [9]. This is quite useful because we can introduce prior knowledge about what we believe the proprieties of music signals are, by choosing a proper kernel that reﬂects those characteristics. In section 2.2 we study in more detail the design of kernels."
1606.01039,data,264,2022-05-13,0,"In [20] GPs are used for time-frequency analysis as probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct timefrequency components, with the analytic goal being to infer these components by applying Bayes’ rule [20]. GPs have also been used for underdetermined audio source separation. In [8] the mixture signal is modelled as a linear combination of independent convolved versions of latent GPs or sources. The model splits the mixture signal in frames also considered independent, by using weightfunctions. Thus each source is modelled as a series of concatenated locally stationary frames, each one with its corresponding covariance function. With this assumption the resulting signal is supposed to be non-stationary [8]. On the other hand, despite the approach we present also assumes the latent GPs fm in (10) as non-correlated, the observed signal is not framed into independent segments. Instead of using weight-functions that act over the observed data, we introduce change-windows φm inﬂuencing each latent GP ending up with latent processes representing speciﬁc sound events that happen at certain segments of time. Therefore the proposed model keeps the correlation between the observations throughout all the signal. That is what allows to make prediction in gaps of missing data (section 4.2). GPs have been used also for estimating spectral envelope and fundamental frequency of singing voice [21], and for time-domain audio source separation [22]."
1606.01039,data,42,2022-05-13,0,"Figure 3: (a) analysed audio (blue line), change-windows (dashed lines). (b) observed data (blue line), missing-data gaps (red line), change-windows (dashed lines)."
1606.01039,data,45,2022-05-13,0,"Figure 5: Zoom in a portion of missing-data gaps. In each ﬁgure the continuous blue line represent the posterior mean, grey shaded areas correspond to the posterior variance, red dots are missing data, whereas black dots are observed data."
1606.01039,data,8,2022-05-13,0,4.2 Filling gaps of missing data in audio
1606.01039,data,8,2022-05-13,0,"and Data Analysis. Wiley, 1988."
1606.01039,data,91,2022-05-13,0,"The covariance function (1) used for computing the prior distribution (5) allows us to introduce in the model all the knowledge and beliefs we have about the properties of the data. We are trying to model music signals, and some of the broad properties of audio signals are non-stationarity, rich spectral content, dynamics (locally periodic, non constant amplitude envelope), mechanistic behaviour, and music structure. Therefore we seek covariance functions that can describe or reﬂect these properties."
1606.01039,"data, data available",57,2022-05-13,0,"conﬁdence decreases and the prediction is quite poor in the input space zones where the data is not available (red dots). Also, we see that the model using (14) does not expect any periodic behaviour in the gaps. The RMS error for both gaps is presented in Table 2."
1606.01039,"data, dataset",117,2022-05-13,0,"The regression problem concerns the prediction of a continuous quantity [12], here a function f (t), given a data set D = {(ti, yi)}N i=1, where yi are assumed as noisy measurements of f (t) at typically regularly-spaced time instants ti (though GP regression framework allows for irregular sampling or missing data), i.e. yi = f (ti)+ǫi, where ǫi ∼ N (0, σ2 noise). In GP regression for mono channel audio signals, instead of estimating parameters η of ﬁxed-form functions f (t, η) : R 7→"
1608.04885,code,13,2022-05-13,0,in the synthesis of code which mimics proposed service behaviour replacing the real
1608.04885,code,13,2022-05-13,0,"– First line speciﬁes HTTP version, three-digit code and a text string"
1608.04885,code,14,2022-05-13,0,Listing A.2 contains the Java code for the symmetric ﬁeld identiﬁcation class which can
1608.04885,code,14,2022-05-13,0,indicates the operation code that the client requests to or to which the server
1608.04885,code,14,2022-05-13,0,"interact with distributed software systems, while it interacts with local code in lieu"
1608.04885,code,15,2022-05-13,0,The java source code of the implementation of modifying the centroid response is listed in
1608.04885,code,15,2022-05-13,0,the code of mock objects are typically coupled with the code of the software under
1608.04885,code,17,2022-05-13,0,for both identiﬁed symmetric ﬁelds are shown in Table 5.3. The java source code of the
1608.04885,code,2,2022-05-13,0,Java Code
1608.04885,code,27,2022-05-13,0,Listing A.3 contains the Java code for the ﬁeld substitution method that performs sym metric ﬁeld substitution to modify a recorded response for generating a response.
1608.04885,code,3,2022-05-13,0,A Java Code
1608.04885,code,4,2022-05-13,0,APPENDIX A. JAVA CODE
1608.04885,"code, code available",14,2022-05-13,0,environment with the code under investigation is only possible if the code itself is
1608.04885,"code, code package, code available",13,2022-05-13,0,Listing A.1 contains the Java code for describing the symmetric ﬁeld information.
1608.04885,data,1,2022-05-13,0,data
1608.04885,data,10,2022-05-13,0,Table 7.8: Response Accuracy for Clusters with Noisy Data
1608.04885,data,10,2022-05-13,0,deﬁnes temporal rules expressing data dependencies among exchanged messages.
1608.04885,data,10,2022-05-13,0,signiﬁcant diﬀerence between this form and ASN.1 on-the-wire data representation
1608.04885,data,11,2022-05-13,0,EncodingDecodingApplication data representationmessageApplication data representationmessagemessageCHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,11,2022-05-13,0,attach location data to tweets and discover tweets & locations.
1608.04885,data,11,2022-05-13,0,"hierarchical data structure, for instance, a tree structure."
1608.04885,data,12,2022-05-13,0,approach. This approach adopted clustering techniques and data mining techniques to
1608.04885,data,13,2022-05-13,0,A reordered dissimilarity matrix image indicates cluster tendency in the data by dark
1608.04885,data,13,2022-05-13,0,One of the most common transformation of network data is from the representation
1608.04885,data,13,2022-05-13,0,and management of data in distributed applications. The middleware service protocol is
1608.04885,data,13,2022-05-13,0,databases. In Proceedings of the 20th International Conference on Very Large Data
1608.04885,data,13,2022-05-13,0,"twitter data, rather than changing/updating data. Although the current client program"
1608.04885,data,13,2022-05-13,0,"– Compound data types can be constructed by nesting primitive types, shown"
1608.04885,data,13,2022-05-13,0,• Built on a client-server architecture and uses separate control and data connections
1608.04885,data,14,2022-05-13,0,3. A variable-length SMB Data contain 2-byte length ﬁeld that indicates the size
1608.04885,data,14,2022-05-13,0,Textual protocols are built around the notion of message ﬁelds encoded with text data
1608.04885,data,14,2022-05-13,0,and transmitting structured data over a network connection. It is used primarily to
1608.04885,data,14,2022-05-13,0,"application integration, data integration, message oriented middleware (MOM), object"
1608.04885,data,14,2022-05-13,0,binary and textual methods. The binary encoding method targets to a standardized data
1608.04885,data,14,2022-05-13,0,"command type identiﬁer, and value represents data for the command. The most"
1608.04885,data,14,2022-05-13,0,how primitive data types and compound data structure are encoded so they can be
1608.04885,data,14,2022-05-13,0,individual machines. The same issues of complex setup and data availability exist.
1608.04885,data,14,2022-05-13,0,primitive data types and compound data structures. Both X DR and ASN.1 specify
1608.04885,data,14,2022-05-13,0,"speciﬁc data representation, we need to design speciﬁc rules to obtain the embedded"
1608.04885,data,14,2022-05-13,0,the data it wants to transmit from representation it uses internally into a message
1608.04885,data,14,2022-05-13,0,"the oﬄine processing, utilising data mining techniques. It can dramatically reduce the"
1608.04885,data,14,2022-05-13,0,"transmit data between a server and web applications, serving as an alternative of"
1608.04885,data,14,2022-05-13,0,• Uses ASN.1 to deﬁne the data types used to build an SNMP message
1608.04885,data,15,2022-05-13,0,"In this chapter, we have utilised data mining techniques in a large enterprise software"
1608.04885,data,15,2022-05-13,0,"data representations, where ASN.1 and XDR are the most popular ones. For a"
1608.04885,data,15,2022-05-13,0,library into clusters of interactions of the same type. The data mining techniques were
1608.04885,data,15,2022-05-13,0,"schema, which is a speciﬁcation for what JSON data is required for a given"
1608.04885,data,15,2022-05-13,0,• Streaming: Give developers low latency access to Tweet data and other events have
1608.04885,data,16,2022-05-13,0,External Data Representation (XDR) [22] is the network format used to transfer
1608.04885,data,16,2022-05-13,0,JavaScript Object Notation (JSON) [21] is a text-based data interchange format.
1608.04885,data,16,2022-05-13,0,Most of Twitter API operations are used to provide the twitter data for 3rd party Twitter
1608.04885,data,16,2022-05-13,0,Network Data Management Protocol (NDMP) [14] is used to transport data between
1608.04885,data,16,2022-05-13,0,"data on that directory. Once the server received an unbind request, it must unbind"
1608.04885,data,16,2022-05-13,0,"data reorganization by a clustering technique. Operations Research, 20(5):993–1009,"
1608.04885,data,16,2022-05-13,0,"than the Whole Library approach, even though the latter uses all the available data points"
1608.04885,data,16,2022-05-13,0,"• XML deﬁnes a basic syntax for mixing markup with data text, but the designer"
1608.04885,data,17,2022-05-13,0,[82] M. K. Jiawei Han. Data Mining: Concepts and Techniques. Morgan Kaufmann
1608.04885,data,17,2022-05-13,0,"name in their request, followed by a payload, containing the data the service is expected"
1608.04885,data,17,2022-05-13,0,"visual cluster analysis. In Data Mining, 2008. ICDM’08. Eighth IEEE International"
1608.04885,data,18,2022-04-21,0,"[122] A. A. Sofokleous and A. S. Andreou. Automatic, Evolutionary Test Data Generation"
1608.04885,data,18,2022-05-13,0,and semantics of the data. The model step is the ﬁnal stage where we specify how virtual
1608.04885,data,18,2022-05-13,0,"encode data, which is intended or expected to be read by a machine rather than a human"
1608.04885,data,18,2022-05-13,0,"fast as a real service can. To answer this question, we propose and implement a data"
1608.04885,data,19,2022-05-13,0,"2For each LDAP operation, we demonstrate its raw data, as well as the textual representation, which"
1608.04885,data,19,2022-05-13,0,"Data Units (PDU)), exchange of interactions (via service primitives) with service users at"
1608.04885,data,19,2022-05-13,0,"following, we ﬁrst describe how we collect data of six case study protocols in Section. 7.3.1."
1608.04885,data,19,2022-05-13,0,"that can be transmitted over the network; that is, the data is encoded into a message."
1608.04885,data,19,2022-05-13,0,"– Represents each data item with a triple of the form <tag, length, value>,"
1608.04885,data,20,2022-05-13,0,"[62] S. Elbaum, G. Rothermel, S. Karre, and M. F. II. Leveraging user-session data"
1608.04885,data,20,2022-05-13,0,work in pattern matching sequences of data. One is the n − gram approach [72] and the
1608.04885,data,21,2022-05-13,0,VAT [35][148] is a technique that exists in data mining for the visual assessment of cluster
1608.04885,data,21,2022-05-13,0,approach (cf. Section 5.4.2 in Chapter 5) to data sets of all six case study protocols. Having
1608.04885,data,22,2022-05-13,0,"Given a trace library (cf. Deﬁnition 5 in Chapter 4.2), shown as Table 6.1, a data clustering"
1608.04885,data,22,2022-05-13,0,"[147] D. Yuan, Y. Yang, X. Liu, and J. Chen. A data placement strategy in scientiﬁc"
1608.04885,data,23,2022-05-13,0,"protocols, they can be further divided based on the multi-byte order and data representa tions, which are illustrated as follows:"
1608.04885,data,24,2022-05-13,0,7.8 Response Accuracy for Clusters with Noisy Data . . . . . . . . . . . . . . . 118
1608.04885,data,25,2022-05-13,0,Abstract Syntax Notation One (ASN.1) [20] is an standard that deﬁnes a rep resentation for data sent over a network.
1608.04885,data,26,2022-05-13,0,"[97] G. J. McLachlan, K.-A. Do, and C. Ambroise. Analyzing Microarray Gene Expres sion Data. Wiley-Interscience, 2004."
1608.04885,data,27,2022-05-13,0,3.2 Encoding and decoding application data . . . . . . . . . . . . . . . . . . . . 34
1608.04885,data,28,2022-05-13,0,"Most application-level protocols deﬁne message structures containing some form of oper ation or service name in their requests, followed by a payload on what data this service"
1608.04885,data,28,2022-04-21,0,"interaction models. Moreover, by utilizing data mining techniques, the eﬃciency of re sponse generation in the emulation environment has been greatly improved. However the"
1608.04885,data,29,2022-05-13,0,"integrated with many other systems for managing and interpreting data from many busi ness activities. The other systems (called services) include a legacy mainframe program,"
1608.04885,data,3,2022-05-13,0,of data.
1608.04885,data,30,2022-05-13,0,"Binary protocols rely on speciﬁc data structure; and hence, transmitted messages usu ally resort to ﬁxed-length ﬁelds or to a special notation to indicate the length of variable"
1608.04885,data,30,2022-05-13,0,"There are two popular network data representations (i.e. External Data Represen tation (XDR) and Abstract Syntax Notation One (ASN.1)), proposed to encode"
1608.04885,data,31,2022-05-13,0,"• XML syntax provides for a nested structure of tag/value pairs, which is equiv alent to a tree structure for the represented data. This is similar to XDR and"
1608.04885,data,34,2022-05-13,0,"[104] R. T. Ng and J. Han. Eﬃcient and eﬀective clustering methods for spatial data min ing. In Proceedings of the 20th International Conference on Very Large Data Bases,"
1608.04885,data,34,2022-05-13,0,"[140] L. Wang, X. Geng, J. Bezdek, C. Leckie, and R. Kotagiri. Enhanced visual anal ysis for cluster tendency assessment and data partitioning. Knowledge and Data"
1608.04885,data,35,2022-05-13,0,"[138] L. Wang, J. C. Bezdek, C. Leckie, and R. Kotagiri. Selective sampling for approxi mate clustering of very large data sets. International Journal of Intelligent Systems,"
1608.04885,data,43,2022-05-13,0,"as AgileLoad [1] and Selenium [10]. AgileLoad [39] features functions like auto matic modeling, real time data correlation, anomaly diagnostic and recommenda tions, enabling automatic identiﬁcation of performance bottlenecks in the systems."
1608.04885,data,5,2022-05-13,0,2. Network data representations
1608.04885,data,7,2022-05-13,0,• External Data Representation (XDR)
1608.04885,data,8,2022-05-13,0,3.2.2 Network Data Management Protocol (NDMP)
1608.04885,data,8,2022-05-13,0,Figure 3.2: Encoding and decoding application data
1608.04885,data,8,2022-05-13,0,data between diﬀerent kinds of computer systems.
1608.04885,data,83,2022-05-13,0,Protocol Taxonomy      NFS NDMP SNMP LDAP SMB HTTP FTP SMTP POP3 IRC JMS SOAP Message Format Textual   √ √ √ √ √ √ Binary √ √ √ √ √   √ Multi-byte Transmission Order Big-Endian √ √ √ √     Little-Endian   √    Data Representation XDR √ √      ASN.1  √ √     Stateful Stateful v4 √ √ √ √ √ √ √  Stateless v1-v3 √  √    Layer Low √ √ √ √ √ √ √ √ √ √  High      √ √ CHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,9,2022-04-21,0,data is transformed into messages sent and received.
1608.04885,data,9,2022-05-13,0,"– Unstructured body data follows, with speciﬁed size"
1608.04885,data,9,2022-05-13,0,• All requests are four characters followed by data
1608.04885,"data, data available",12,2022-05-13,0,require physical relocation and manual reconﬁguration. Real system data may not
1608.04885,"data, data https",14,2022-05-13,0,[22] XDR: External Data Representation Standard. 2006. http://tools.ietf.org/
1608.04885,"data, data https",17,2022-05-13,0,[14] NDMP: Network Data Management Protocol. Network Working Group. 1996. http:
1608.04885,"data, dataset",16,2022-05-13,0,"As shown in Figure 5.4, we randomly partitioned the original interactions’ data set into"
1608.04885,"data, dataset",16,2022-05-13,0,a statistical analysis will generalise to an independent data set. For the purpose of our
1608.04885,"data, dataset",16,2022-05-13,0,results of a statistical analysis will generalise to an independent data set. For the purpose
1608.04885,"data, dataset",18,2022-05-13,0,"similar search requests in our data set, some of them resulting in responses with zero or one"
1608.04885,database,12,2022-05-13,0,widely utilised in distributed database systems for the vertical partition of large
1608.04885,database,13,2022-05-13,0,of relationships between proposed services and a database. This information is used
1608.04885,database,15,2022-05-13,0,which is a database term for a speciﬁcation of how to interpret a collection of
1608.04885,database,16,2022-05-13,0,[130] M. Tamer ˝Ozsu and P. Valduriez. Principles of Distributed Database Systems.
1608.04885,dataset,13,2022-05-13,0,Our experimental results using the 6 message trace datasets demonstrate that our approach
1608.04885,dataset,13,2022-05-13,0,has two additional datasets: a dataset with textual representation converted from the
1608.04885,dataset,13,2022-05-13,0,• The datasets were obtained by randomly generating client requests for services of
1608.04885,dataset,14,2022-05-13,0,accuracy overall for the datasets tested. The combined approach achieves 100% accuracy
1608.04885,dataset,14,2022-05-13,0,datasets. The Accuracy Ratio column is calculated by dividing the number of valid
1608.04885,dataset,15,2022-05-13,0,client-to-server load. They require detailed knowledge of target protocols and suit able datasets.
1608.04885,dataset,15,2022-04-21,0,"represent the average times spent generating requests, for all the requests in the datasets"
1608.04885,dataset,15,2022-05-13,0,than those of our datasets. Further testing on real system interactions are warranted.
1608.04885,dataset,15,2022-05-13,0,• Our evaluation was performed on six datasets from four protocols. Given the great
1608.04885,dataset,16,2022-05-13,0,The impact of the entropy weightings can only be observed for the LDAP binary dataset.
1608.04885,dataset,18,2022-05-13,0,higher accuracy than f = 1. For the other datasets the threshold had no impact on the
1608.04885,dataset,18,2022-05-13,0,the LDAP (binary) dataset the thresholds of f = 0.5 and f = 0.8 produced signiﬁcantly
1608.04885,dataset,19,2022-05-13,0,(i.e. 0% noise) of interaction messages by operation type for the six datasets tested.
1608.04885,dataset,19,2022-05-13,0,"for four of the datasets, and 99.95% and 99.34% for the remaining two (LDAP binary"
1608.04885,dataset,20,2022-05-13,0,"binary dataset (denoted by LDAP text (1)), and another textual dataset that was used in"
1608.04885,dataset,21,2022-05-13,0,"accuracy stays above 97% for all datasets, when the noise ratio is 5%. As the noise ratio"
1608.04885,dataset,29,2022-05-13,0,"datasets, no impact from the weightings can be observed, as the consensus sequence pro totype by itself (Consensus Only) already produces 99-100% accuracy."
1608.04885,dataset,31,2022-05-13,0,"responses, indeed much faster than the real services being emulated. The response genera tion time is comparable to the Cluster Centroid approach, being faster for some datasets,"
1608.04885,dataset,35,2022-05-13,0,"from the trace library (for three datasets Consensus+Weighting is signiﬁcantly more ac curate, for two it has the same accuracy, for one it is slightly lower). The reason for the"
1608.04885,dataset,4,2022-05-13,0,the other datasets.
1608.04885,dataset,6,2022-05-13,0,7.5 Sample protocol message trace datasets
1608.04885,dataset,8,2022-05-13,0,Table 7.5: Sample protocol message trace datasets
1608.04885,"dataset, used dataset",17,2022-05-13,0,"We have used one message trace dataset for each of these protocols. In addition, LDAP"
1608.04885,github,9,2022-05-13,0,[9] Mockery. https://github.com/padraic/mockery.
1608.04885,open-source,13,2022-05-13,0,ActiveMQ is an open source message broker which fully implements the Java Message
1608.04885,open-source,22,2022-05-13,0,"[142] T. Wang, G. Yin, X. Li, and H. Wang. Labeled topic detection of open source"
1608.04885,package,12,2022-05-13,0,1 package com . ca . calabs . bilby . substitution ;
1608.04885,publicly available,13,2022-05-13,0,"However, the disadvantage is that the communication contract is not always publicly"
1608.04885,publicly available,15,2022-05-13,0,publicly available so that we can use this knowledge to deﬁne criteria for validation.
1608.04885,python,20,2022-05-13,0,"including Java, C#, Groovy, Perl, PHP, Python and Ruby so that the tests can"
1701.07853,code,6,2022-05-13,0,"Elisa Mussumecia, Fl´avio Code¸co Coelhoa"
1701.07853,data,10,2022-05-13,0,empirical data. The adjacency matrix A is given by
1701.07853,data,133,2022-05-13,0,"In this work we decided to look at the spread of news stories over the internet characterizing the resulting spread network and the dynamics of the spread. We start by looking at an actual case of news spread, and estimate the spread network by applying ideas of temporal networks and topic Modeling, connecting similar articles within the bounds of temporal window of inﬂuence. Then we postulate that the spread dynamics approximates an epidemic process and model it using a Network SIR model[3]. The spread of ideas as an epidemic process is not a new idea[4], but here we Propose new tools to estimate the spread network from data and compare it with simulated networks produced by an SIR epidemic model."
1701.07853,data,28,2022-05-13,0,"From the simulation (ﬁgure 11) we obtain the state matrix, which we use to compare the simulated infection distribution with the original data. Then"
1701.07853,data,4,2022-05-13,0,2.1. Data sources
1701.07853,"data, database",91,2022-05-13,0,"The data used for this study was obtained from the Media Cloud Brasil project (MCB) which collects news articles from thousands of sources in the Brazilian Internet since 2013. From the MCB database we obtained 2129 articles talking about the Charlie Hebdo terrorist attack in February 2015. The articles span from the day of the attack to the end of march of 2015. The data include the full text of the article, the URL of publication and the date and time of the publication."
1701.07853,dataset,52,2022-05-13,0,"Figure 10: Total number of articles infected between 0 < λ < 0.00005. The blue area is the area where the peak of the simulation is the same as the peak of the dataset distribution, threfore is the area where the λ values were tested for our simulation."
1701.07853,dataset,63,2022-05-13,0,"where NXY is the number of times an article from publisher X (the publisher of article i), has infected an article from publisher Y (the publisher of article j) and NY Is the total number of articles from publisher Y that have been infected, regardless of publisher. These counts are derived from the empirical dataset."
1701.07853,"dataset, database",51,2022-05-13,0,"The dataset used is the result of a very speciﬁc search on a news articles database, therefore we can expect to the articles to display a great similarity among themselves. Figure 4, shows the distribution of pairwise similarities that were used to construct the empirical inﬂuence network."
1803.06456,code,61,2022-05-13,0,"Here, the baselines are the same as the TE schema. However, we are not able to compare the PAN winner methods with our model as they have not published their code and many implementation details are left unknown to us in their reports to reimplement their methods. We call our second model PRNN in our reports."
1803.06456,data,140,2022-05-13,0,"methods as long as we keep the test and training sets the same as theirs. TE methods: We perform the Transformation Encoder (TE) on a problem P = (DS, DT ) that its documents are represented under one feature set with one feature value assignment to compute the transformation error. We then leverage the error rates taking from (at most) F = 7 feature sets (Section 3.2) of TE to form the ﬁnal TE feature vector (V ). Indeed, Each of the dimensions captures the transformation loss of one feature set. We apply the TE to both training and test data. Two well-known GNB and DT classiﬁers are used for veriﬁcation. We indicate them as TE+GNB and TE+DT respectively in our experiments."
1803.06456,data,176,2022-05-13,0,"where zs ∈ Rd is the reconstructed input and must be transformed into the target (zs ≈ xt). This can be done by setting TE’s objective function as the minimization of the transformation loss. We set the TE transformation loss Er to be the cross-entropy between reconstructed input (zs) and the target input (xt) as: Er(xt, zs) = − (cid:80)d i + (1 − xt Now, we assign our authorship veriﬁcation problem into the proposed Transformation Encoder. It is intuitively expected that TE shows diﬀerent manner when it transforms the source into the target while both having many features in common compared to the case where they have less common features. Here, the goal is to utilize TE for the AV problems that suﬀer from restricted labeled data. So, we put a document expansion method on top of TE as an initial step to overcome the restriction to some extent."
1803.06456,data,236,2022-05-13,0,"In this paper we deﬁne two diﬀerent schemas to study the AV problem. Under the ﬁrst schema we address the following challenges: 1- writing samples of available authors are quite limited during the training step as the length of the given text documents is short (a few hundred to a few thousand words) and size of the training set is so small (from 10 to 200 examples). So, it is quite hard to infer the same or diﬀerent-authorship status of given pairs. 2- The test and train documents are from diﬀerent genera and/or topics which makes the learning and prediction process much harder as the word distribution might diﬀer considerably. 3- No writing samples of the future authors is speciﬁed to us during the training and we may have seen no samples by the future authors at all. Under the second schema the scale of the training data is larger compared to the ﬁrst schema. However, we address the problem of identifying the diﬀerence in documents from identical domains in two ways: 1- authorship diversity in similar contents by utilizing Amazon reviews from 300 distinct authors. 2- Scientiﬁc documents from the same area of research by diﬀerent authors who have almost identical level of expertise in the ﬁeld. It also can be considered as an application of plagiarism detection."
1803.06456,data,4,2022-05-13,0,1 http://pan.webis.de/data.html
1803.06456,data,43,2022-05-13,0,"of the two classes decreases as the transformation encoder updates its weights in each epoch. However, there is a diﬀerence between the transformation loss of the positive and negative data in both diagrams. The loss of negative data is less"
1803.06456,data,44,2022-05-13,0,"Fig. 4: t-SNE plot of two folds of output of the fusion layer for PAN2015 in 5-fold CV. +: positive training data, ×: positive test data, ◦: negative training data, •: negative test data"
1803.06456,"data, dataset",188,2022-05-13,0,"Document Expansion for Small Scale Datasets Neural networks need suﬃcient amounts of data during their learning process to avoid the over-ﬁtting problem to produce the desired output. So, we propose a document expansion method to make use of the existing labeled training data of small scale datasets such as PAN to a great extent. A sliding window with the length of l sentences moves forward through each text document by one sentence per step making a smaller document each time. More speciﬁcally, a document with n sentences will be distributed into n − l + 1 smaller documents. New line characters, as well as empty sentences, are ignored here. So, using this expansion technique each problem P = (S, T ) in the small datasets will be converted to P = (DS, DT ) j}lT where DS = {ds j=1 are the set of all shorter documents after expansion of S and T (source and target). lS and lT denote the size of DS and DT respectively."
1803.06456,"data, dataset",330,2022-05-13,0,"However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy. The best accuracy belongs to the Amazon dataset where we have the largest dataset. It can be inferred that when the scale of the underlying dataset is large enough, the network learns the relation between the two language models of its given inputs well. It should be noted that for the two PAN2013 and PAN2014E even after CV the network cannot converge and the validation loss increases after each epoch. To avoid it we increase the total number of document pairs by splitting each document into two smaller ones with an equal number of sentences and making new pairs. This technique decreases the validation loss during training. However, it still suﬀers from lack of labeled examples and causes weakest results compared to the other larger datasets. To illustrate how PRNN discriminate writing styles we provide the t-SNE plot of the output of the fusion layer in a 5-fold CV classiﬁcation for two folds of PAN2015 (Figure 4). According to Figure 4, both classes have almost similar distribution in the test and training data. But, in some rare parts, the positive and negative points are close. They are probably the portion of the data that mislead the classiﬁer during the training step or will be misclassiﬁed in predictions."
1803.06456,"data, dataset",344,2022-05-13,0,"than the positive’s. In other words, it is easier to transform one document into the other while they are a diﬀerent-authorship pair (negative pair) compared to a same-authorship pair (positive pair). It makes the results of reconstruction loss to be counterintuitive. The reason is that we represent both documents of each problem under vector space model and only based on the vocabulary of the source document. So, the exclusive features of the target document, the features that only belong to the target but no to the source document, will be ﬁltered under this document representation model. Moreover, it is expected that the documents written by diﬀerent authors have fewer features in common and have more exclusive features than the ones written by the same author. This fact makes the target document of diﬀerent-authorship pair sparser than that of the same-authorship pair. And transforming the source document into a sparse document (its vector is sparse) makes less error than to a dense document (its vector is dense). This feature diﬀerentiates the positive and negative data and exists for both training and test sets and makes the transformation loss a distinctive feature for the veriﬁcation. A more direct way to classify the documents (instead of using classiﬁers) is to simply thresholding the reconstruction error. However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy."
1803.06456,"data, dataset",46,2022-05-13,0,"Fig. 3: Transformation loss for 50 epochs: averaged over all problems in the PAN2015 dataset. (A): training data (100 problems), (B): test data (500 problems), Feature set: unigram."
1803.06456,"data, dataset",84,2022-05-13,0,"Neural Network (PRNN) for small and large scale datasets. TE transforms one document of the pair into the other and observes the transformation loss as a distinctive feature for classiﬁcation. PRNN investigates the diﬀerence between the language models of documents. Experiments show that TE can achieve stable results in all four PAN datasets with various size, genre and/or topics. Also, PRNN beats almost all baselines avoiding over-ﬁtting problem by a reasonable amount of training data."
1803.06456,"data, dataset, dataset provided",344,2022-05-13,0,"For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes. The underlying feature set is unigram. The ﬁgures show that in both training and test sets the TE loss"
1803.06456,"data, dataset, dataset provided",349,2022-05-13,0,"TE schema The classiﬁcation results are compared in Table 3. The highest accuracy is indicated in bold and the second highest is underlined. For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes."
1803.06456,dataset,10,2022-05-13,0,Experimental results on evaluation datasets show that both methods achieve
1803.06456,dataset,153,2022-05-13,0,"Let P = (S, T ) denotes a pair of documents, indicating S as the source and T as the target. Here, the task is to investigate whether S and T are written by the same author. We map this problem into a binary classiﬁcation paradigm. Accordingly, if S and T are authored by the same person, P belongs to the positive class. Nevertheless (S and T have diﬀerent authors) P belongs to the negative class. In the ﬁrst step, we explain the Transformation Encoder which is a feature extraction-based method designed for the small-scale datasets with 200 labeled samples at most. However, many AV problems might have a larger scale with much more examples. So, we introduce the Parallel Recurrent Neural Network (PRNN) for large scale datasets in the second step."
1803.06456,dataset,167,2022-05-13,0,"We analyze authorship veriﬁcation on several datasets with binary structure. To our knowledge this amount of analysis has not been done in authorship veriﬁcation on diverse types of datasets. Two models are proposed. First, a Transformation Encoder (TE) to model error feature vectors for classiﬁcation inspired by the idea of autoencoders. TE is compatible with the AV problems with small-scale training sets. Giving a pair of input documents, TE transforms one input into the other. In this process, the transformation loss is observed as a reasonable measure of closeness of the two inputs to be used by a classiﬁer. The second model is a parallel recurrent neural network (PRNN) that is inspired by the popular similarity measures in Statistical Machine Learning (ML). Being based on language models, it is mostly applicable for relatively larger datasets. PRNN compares the proximity of the language model of its two input sequences"
1803.06456,dataset,2,2022-05-13,0,3.1 Dataset
1803.06456,dataset,2,2022-05-13,0,Dataset Train
1803.06456,dataset,222,2022-05-13,1,"Abstract. We propose two models for a special case of authorship veriﬁcation problem. The task is to investigate whether the two documents of a given pair are written by the same author. We consider the authorship veriﬁcation problem for both small and large scale datasets. The underlying small-scale problem has two main challenges: First, the authors of the documents are unknown to us because no previous writing samples are available. Second, the two documents are short (a few hundred to a few thousand words) and may diﬀer considerably in the genre and/or topic. To solve it we propose transformation encoder to transform one document of the pair into the other. This document transformation generates a loss which is used as a recognizable feature to verify if the authors of the pair are identical. For the large scale problem where various authors are engaged and more examples are available with larger length, a parallel recurrent neural network is proposed. It compares the language models of the two documents. We evaluate our methods on various types of datasets including Authorship Identiﬁcation datasets of PAN competition, Amazon reviews and machine learning articles. Experiments show that both methods achieve stable and competitive performance compared to the baselines."
1803.06456,dataset,234,2022-05-13,1,"case no writing samples of a questioned author are speciﬁed and they are unknown to us. No general solution has been oﬀered for the veriﬁcation problem under this assumption till 2014 [7]. Since then, a few works can be found in the literature: Koppel and Winter [7] propose an almost unsupervised method for the blog corpus dataset using “impostors” method. Optimized Classiﬁcation Trees, the winner method of PAN2014 Essays dataset, optimizes a decision tree based on various types of features and diﬀerent comparison methods including cosine similarity, correlation coeﬃcient and euclidean distance [8]. Multi-headed RNN is a character-level RNN and contains a common recurrent state among all authors with an independent softmax output per author [9]. Fuzzy C-Means clustering, the winner of the PAN2014 competition for novels dataset, adopts C-Means clustering and lexical features for the task [10]. Recently, an approach based on the compression models has been evaluated on PAN datasets [11]. Their method achieves promising results for the two years of PAN competitions but not for the other two datasets. Our methods is similar to these methods and considers the problems with the binary structure but we examine them on all PAN small-scale datasets as well as two large scale datasets."
1803.06456,dataset,260,2022-05-13,0,"for PAN datasets. All other parameters are selected based on pilot experiments. We report accuracy, the Area Under Receiver Operating Characteristic (ROC) curve [4] (AUC) and Score=AUC× Acc in TE experiments. The higher AUC and Score indicate more eﬀective classiﬁcation. PRNN schema The plain text of each document is used as the input of PRNN. The features sets for the baselines are the same as the TE baselines. However, we did not use the original training and test sets of the PAN datasets as the size of the training set is too small to be used for PRNN. To avoid overﬁtting problem we perform 5-fold Cross Validation (CV) for the PAN2015, Amazon and MPLA* where we have suﬃcient amount of examples in training folds. And for the PAN2013, PAN2014E and PAN2014N datasets that are relatively smaller we perform 10-fold CV to increase the size of the training folds. This setting is applied for PRNN as well as the baselines. We use Theano to implement PRNN. All classiﬁer’s parameters are the same as the TE schema. The back-propagation is done using stochastic gradient descent with learning rate=0.001, batch size=1, and dropout rate=0.2. We use the Glove pre-trained vectors5 as an initial value for the embedding vectors when there is a match. Otherwise, a random vector from a continuous uniform distribution over [0, 1) is used."
1803.06456,dataset,3,2022-05-13,0,Dataset Positive Negative
1803.06456,dataset,30,2022-05-13,0,Table 4: Classiﬁcation accuracy for PRNN schema using 5 and 10-fold CV across different datasets. The input for the baselines are empowered by the proposed similarity vector.
1803.06456,dataset,334,2022-05-13,0,"Here, the comparison methods are presented in three categories: baseline, PAN winners and our TE method. The details are provided as follows. Baseline: We connect several Machine Learning reliable classiﬁers widely used in the area with the seven similarity measures to set strong baselines (Table2). Since each example in our underlying dataset structure comprises two documents, we need to adapt it to the structure of an ordinary classiﬁer input by converting them to one single entity. A simple direct way is to concatenate their feature vectors. However, our experiments show it provides weak results mostly equal to the random label assignment. So, we deﬁne the summary vector as a single unit representative of each example/problem P = (DS, DT ) by utilizing several similarity measures. The summary vector comprises a class of several metrics each measures one aspect of the closeness of the two documents (DS and DT ) of the pair for all underlying feature sets. For any two feature vector documents x, y their summary vector is sum(x, y) = [simj i (x, y)1≤i≤M,1≤j≤F computes the ith similarity metric of M metrics in Table 2 under jth of F = 7 feature sets (Section 3.2) between x, y. Then, we use a classiﬁer including SVM, Gaussian Naive Bayes (GNB), K-Nearest Neighbor (KNN), Logistic Regression (LR), Decision Tree (DT) and Multi-Layer Perception (MLP) to predict the class label. PAN winners: We compare our method with the top methods of PAN AV competition between 2013 and 2015. The results of each method for one year of the competition are available and we report them here. So, our comparisons are not impacted by diﬀerent parameter setting and implementation details of these"
1803.06456,dataset,348,2022-05-13,1,"PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random. On the other hand, the negative candidate set is made of all unique and possible combinations of review pairs having diﬀerent authors. The negative class having equal size with the positive class is created by random selection from the negative candidate set."
1803.06456,dataset,348,2022-05-13,1,"for PAN2013), and literally the second document includes one piece of writing. Two documents of a pair might be from signiﬁcantly various genres and topics. The length of a document changes from a few hundred to a few thousand words. PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random."
1803.06456,dataset,5,2022-05-13,0,Table 1: Datasets information
1803.06456,dataset,86,2022-05-13,1,TE schema To evaluate the Transformation Encoder we use all available authorship identiﬁcation datasets released by PAN 1 (Table 1). Each PAN dataset consists of a training and test corpus and each corpus has a various number of distinct problems. One problem is a pair of two documents: the ﬁrst document of a problem composed of up to ﬁve writings (even as few as one) by a single person (implicitly disjoint For PAN2014 and PAN2015 and explicitly disjoint
1803.06456,dataset,88,2022-05-13,0,"PRNN is designed to solve the AV problem for relatively large scale datasets. The structure of the problem is the same as TE’s. We model a pair of documents using a simple parallel recurrent architecture. The overall model is shown in Figure 2. In general, PRNN consists of three components: two parallel columns of identical layers, one shared fusion layer and a SoftMax layer as the output. We proceed to describe the network in the following paragraphs."
1803.06456,dataset,93,2022-05-13,0,"to investigate their authorship. We also propose the summary vector to adapt our problem to a common binary classiﬁcation style to create strong baselines as there are limited studies in authorship veriﬁcation according to the literature. Applying this adaptation we are able to employ the recognized classiﬁers as well as similarity measures that are widely used in ML to build our baselines. Besides, the two pre-existing datasets, Amazon reviews and MPLA-400, are mapped to the binary structure to be used for our large scale AV problem."
1803.06456,"dataset, github",17,2022-05-13,0,2 https://github.com/dainis-boumber/MLP-400-datasets 3 we use scikit-learn software for all linguistic features 4 http://deeplearning.net/software/theano/
1803.06456,python,106,2022-05-13,0,"3. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. Journal of Machine Learning Research 12 (2011) 2825–2830 4. Egan, J.P.: Signal detection theory and {ROC} analysis. Academic Press (1975) 5. Japkowicz, N., Myers, C., Gluck, M., et al.: A novelty detection approach to"
1807.01857,code,147,2022-05-13,0,"Fig. 1 shows the schematic diagram of our proposed approach for IDE-based web search. Once the developer selects an exception from the Error Log or Console View of Eclipse IDE, our approach collects necessary information about it such as error message, stack trace and source code context. Then, it collects the results from three reliable search engines (e.g., Google, Bing and Yahoo) and one programming Q & A site (e.g., StackOverﬂow) through API endpoints against the error message and develops the corpus. The proposed approach then considers the context of the occurred error or exception, popularity and search engine recommendation of the collected results and calculates the proposed metrics to determine their acceptability and relevance with the target exception. Once the ﬁnal scores are calculated from those metrics, the results"
1807.01857,code,148,2022-05-13,0,"A sites, forums or discussion boards in their program directly or with minor modiﬁcations. Therefore, a result link containing source code snippet similar to the surrounding code block of the selected error or exception location is likely to discuss relevant issues that the developer needs. We consider three lines before and after the affected line in the source ﬁle as the source code context of the error or exception and extract the code snippets from result links though HTML scrapping. Then, we apply SimHash Algorithm on both code contexts and generate their SimHash values. We use equation (1) to determine Source Code Context Matching Score for each result link. The score values from zero to one and it indicates the relevance of the result link with the target error in terms of the context of source code."
1807.01857,code,252,2022-05-13,0,"To summarize, we propose a novel IDE-based web search solution that (1) exploits the search and ranking capabilities of three reliable search engines and a programming Q & A site through their API endpoints, (2) considers not only the content of the search (i.e., query keywords) but also the problem context such as stack trace and source code context, link popularity and link recommendation from the search engines, and (3) provides search result within the context of IDE with web browsing capabilities. We conduct an experiment with 25 runtime errors and exceptions related to Eclipse plugin development. Our approach recommended solutions with 96% accuracy which necessarily outperforms the traditional keyword-based search. In order to validate the results, we conduct a user study involving ﬁve prospective participants which gave a response agreement of 64.28%. Given that the relevance checking of a solution against the selected error is completely a subjective process, the preliminary results are promising. However, the proposed approach needs to be further validated with more errors and exceptions followed by an extensive user study to establish itself as a complete IDE-based web search solution. We also have plans to enable multiprocessing for the application and host it as a web service API so that others can readily use it with real time experience and also can use the API in their own IDEs rather than Eclipse."
1807.01857,code,47,2022-05-13,0,"[2] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer. Two studies of opportunistic programming: interleaving web foraging, learning, and writing code. In Proc. SIGCHI, pages 1589–1598, 2009."
1807.01857,code,66,2022-05-13,0,"Title Matching Score measures the content similarity between search query and result title. Stack Trace Matching Score and Source Code Context Matching Score determine the relevance of the result link based on its contextual similarity with that of the selected error or exception; therefore, they constitute the Context Relevance Score, Scxt. We get this score using equation (5)."
1807.01857,code,81,2022-05-13,0,"[5] M. Goldman and R. C. Miller. Codetrail: Connecting source code and web resources. J. Vis. Lang. Comput., 20(4):223–235, August 2009. [6] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns. What makes a good code example?: A study of programming Q & A in Stack Overﬂow. In Proc. ICSM, pages 25–34, 2012."
1807.01857,code,86,2022-05-13,0,"4) Source Code Context Matching Score (Scc): Sometimes, stack trace may not be enough for problem ﬁxation and developers post related source code in forums and discussion boards for clariﬁcation. We are interested to check if the source code contexts of the discussed errors or exceptions in the result links are similar to that of the selected exception from IDE. The code contextual similarity is possible; because, the developers often reuse code snippets from programming Q &"
1807.01857,code,98,2022-05-13,0,"In our experiment, we select 25 runtime errors and exceptions related to Eclipse plug-in development, and collect associated information such as error or exception messages, stack traces and source code context. We then use those information (e.g., error content and context) to search for solution using our approach. We also perform extensive web search manually with different available search engines and ﬁnd out the solutions for all errors and exceptions. We should note that we choose the most appropriate solution as the accepted one for each exception or error"
1807.01857,"code, package",187,2022-05-13,0,"In this paper, we propose an Eclipse IDE-based search solution called SurfClipse to the encountered errors or exceptions which addresses the concerns identiﬁed in case of existing approaches. We package the solution as an Eclipse plug-in which (1) exploits the search and ranking algorithms of three reliable web search engines (e.g., Google, Bing and Yahoo) and a programming Q & A site (e.g., StackOverﬂow) through their API endpoints, (2) provides a content (e.g., error message), context (e.g., stack trace and surrounding source code of the subject error), popularity and search engine recommendation (of result links) based ﬁltration and ranking on the extracted results of step one, (3) facilitates the most recent solutions, accesses the complete and extensible solution set and pulls solutions from numerous forums, discussion boards, blogs, programming Q & A sites and so on, and (4) provides a real web surﬁng experiences within the IDE context using Java based browser."
1807.01857,data,102,2022-05-13,0,"discussion boards and Q & A sites with the help of search enginies, but also ensures the access to the most recent content of StackOverﬂow through API access. However, the existing approaches by Cordeiro et al. [4] and Ponzanelli et al. [7] provide results from a single and ﬁxed sized data dump of StackOverﬂow and therefore, the results do not contain the most recent posts (i.e., discussing the most recent errors or exceptions) from StackOverﬂow as well as the promising solutions from other programming Q & A sites."
1807.01857,data,103,2022-05-13,0,"8) Search Trafﬁc Rank Score (Sstr): The amount of search trafﬁc to a site can be considered as an important indicator of its popularity. In this research, we consider the relative popularity of the result links found in the corpus. We use the statistical data from two popular site trafﬁc control companiesAlexa and Compete through their provided APIs and get the average ranking for each result link. Then, based on their ranks, we provide a normalized Search Trafﬁc Rank Score between zero and one considering minimum and maximum search trafﬁc ranks found."
1807.01857,data,216,2022-05-13,0,"Abstract—Traditional web search forces the developers to leave their working environments and look for solutions in the web browsers. It often does not consider the context of their programming problems. The context-switching between the web browser and the working environment is time-consuming and distracting, and the keyword-based traditional search often does not help much in problem solving. In this paper, we propose an Eclipse IDE-based web search solution that collects the data from three web search APIs– Google, Yahoo, Bing and a programming Q & A site– StackOverﬂow. It then provides search results within IDE taking not only the content of the selected error into account but also the problem context, popularity and search engine recommendation of the result links. Experiments with 25 runtime errors and exceptions show that the proposed approach outperforms the keyword-based search approaches with a recommendation accuracy of 96%. We also validate the results with a user study involving ﬁve prospective participants where we get a result agreement of 64.28%. While the preliminary results are promising, the approach needs to be further validated with more errors and exceptions followed by a user study with more participants to establish itself as a complete IDE-based web search solution."
1807.01857,data,282,2022-05-13,0,"Traditional web search forces the developers to leave the working environment (i.e., IDE) and look for the solution in the web browsers. In contrast, if the developer chooses SurfClipse, it allows to check the search results within the context of IDE (e.g., Fig. 1-(b)). Once she selects an error message using context menu option (e.g., Fig. 1-(a)), the plugin pulls results from three reliable search engines and one programming Q & A site against that error message. Then, it calculates the proposed metrics for each result related to the error content, error context, popularity and search engine recommendation to determine its relevance with the occurred error or exception, and then sorts and displays the results. Moreover, the plug-in allows the developer to browse the solution on a Java-based web browser (e.g., Fig. 1-(c)) without leaving the context of the IDE which makes it time-efﬁcient and ﬂexible to use. The plug-in by Cordeiro et al. [4] also shows the results within the context of the IDE; however, (1) the result set is limited (i.e., only from StackOverﬂow and does not consider the whole web), (2) cannot address newly introduced issues (i.e., ﬁxed corpus and subjected to the availability of StackOverﬂow data dump), (3) only considers stack trace information as problem context, and (4) the developer cannot enjoy the web browsing experience."
1807.01857,data,318,2022-05-13,0,"1) Search Engine Weight Based Score (Ssew): According to Alexa1, one of the widely recognized web trafﬁc data providers, Google ranks second, Yahoo ranks fourth and Bing ranks sixteenth among all websites in the web this year. While these ranks indicate their popularity (e.g., site trafﬁc) and reliability (i.e., users’ trust) as information service providers, it is reasonable to think that search results from different search engines of different ranks have different levels of acceptance. We conduct an experiment with 75 programming task and exception related queries2 against those search engines and a programming Q & A site (e.g., StackOverﬂow) to determine the relative weights or acceptance. We collect the top 15 search results for each query from each search tool and get their Alexa ranks. Then, we consider the Alexa ranks of all result links provided by each search tool and calculate the average rank for a result link provided by them. The average rank for each search tool is then normalized and inversed which provides a value between 0 and 1. We get a normalized weight of 0.41 for Google, 0.30 for Bing, 0.29 for Yahoo and 1.00 for StackOverﬂow. The idea is that if a result link against a single query is found in all three search engines, it gets the search engine scores (i.e., conﬁdence) from all three of them which sum to 1.00. StackOverﬂow has drawn the attention of a vast community (1.7 million3) of programmers and software professionals, and it also has a far better average Alexa rank than that of the search engines; therefore, the results returned from StackOverﬂow are provided a search engine score (i.e., conﬁdence) of 1.00."
1807.01857,"data, code",117,2022-05-13,0,"Existing studies related to our research focus on integrating commercial-off-the-shelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying within IDE environment [4, 7], embedding web browser inside the IDE [3] for code example recommendation and so on. In this paper, we propose a novel approach that exploits result data from the state of art web search APIs and provides ﬁltered and ranked search results taking problem content, context, result link’s popularity and search engine recommendation about the result links into consideration. Our proposed approach not only collects solution posts from a large set of forums,"
1807.01857,"data, code, dataset provided",293,2022-05-13,0,"for the solution in the web browsers. The context-switching between IDE and the web browser is distracting and timeconsuming. Moreover, checking relevance from hundreds of search results is a cognitive burden on the novice developers. Existing studies focus on integrating commercial-off-theshelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying them within the IDE environment [4], embedding web browser inside the IDE [3] and so on. Cordeiro et al. [4] propose an IDE-based recommendation system for runtime exceptions. They extract the question and answer posts from StackOverﬂow data dump and suggest posts relevant to the occurred exceptions considering the context from the stack trace information generated by the IDE. They also suggest a nice solution to the context-switching issue through visualization of the solution within the IDE. However, the proposed approach suffers from several limitations. First, they consider only one source (e.g., StackOverﬂow Q & A site) rather than the whole web for information and thus, their search scope is limited. Second, the developed corpus cannot be easily updated and is subjected to the availability of the data dump. For example, they use the StackOverﬂow data dump of September 2011, that means it cannot provide help or suggestions to the recently introduced software bugs or errors after September 2011. Third, the visualization of the solutions is not efﬁcient as it uses plain text to show the post contents such as source code, stack trace and discussion. Thus the developers do not really experience the style and presentation of a web page."
1807.04488,code,115,2022-04-21,0,"Baseline Query Selection: We select the title of a bug report as the baseline query for our experiments, as was also selected by earlier studies [21, 28, 49]. However, we discard such queries that (i.e., in verbatim titles) already return their ﬁrst correct results within the Top-10 positions, they possibly do not need query reformulation [21]. Finally, we ended up with a collection of 1,675 baseline queries. We perform the same preprocessing steps as were done on the source documents (Section II-C), on the queries before using them for code search in our experiments."
1807.04488,code,117,2022-04-21,0,"Answering RQ3–Do Document Structures Matter? While most of the earlier reformulation techniques miss or ignore the structural aspect of a source document, we consider such aspect as an important paradigm of our technique. We consider a source document as a collection of structured entities (e.g., signatures, methods, ﬁelds) [38] rather than a regular text document. Thus, we make use of method signatures and ﬁeld signatures rather than the whole source code for query reformulation given that they are likely to contain more salient terms and less noise [23]. Fig. 4 demonstrates how incorpora Method signature Field signature Both signatures Both signatures"
1807.04488,code,13,2022-05-13,0,"Code Elements in Informal Documentation. ICSE, pages 832–841, 2013."
1807.04488,code,132,2022-05-13,0,"earlier [32], we apply a heuristic threshold of 0.0001 for the convergence checking. The algorithm captures importance of a source term not only by estimating its local impact but also by considering its global inﬂuence over other terms. For example, the term, “Classpath”, Fig. 1, occurs in multiple structured tokens (Listing 1), complements the semantics of ﬁve other terms, and thus is highly important within the term graph (i.e., Fig. 1). Once the iterative computation is over, each of the terms from the graph is found with a numeric score. We consider these scores as the relative weight or importance of the corresponding terms from the source code."
1807.04488,code,146,2022-04-21,0,"To summarize, we propose a novel technique–ACER–for improved query reformulation for concept location. It takes an initial query as input, identiﬁes appropriate search terms from the source code using a novel term weight, and then suggests the best reformulation to the initial query using document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight systems report that our technique can improve 71% of the baseline queries and preserve 26% of them, which are highly promising. Comparison with ﬁve closely related approaches including the state-of-the-art not only validates our empirical ﬁndings but also demonstrates the high potential of our technique. In future, we plan to apply our term weighting method, CodeRank, to other SE text retrieval tasks involving source code such as bug localization and traceability recovery."
1807.04488,code,146,2022-04-21,0,"source subject systems show that our technique can improve 71% (and preserve 26%) of the baseline queries which are highly promising according to relevant literature [13, 21, 34]. Our suggested queries return correct results for 64% of the queries in the Top-100 results. Our ﬁndings report that CodeRank is a more effective term weighting method than the traditional methods (e.g., TF, TF-IDF) for search query reformulation in the context of source code. Our ﬁndings also suggest is an important paradigm for both term weighting and query reformulation. Comparison with ﬁve closely related existing approaches [13, 21, 23, 43, 49] not only validates our empirical ﬁndings but also demonstrates the superiority of our technique. Thus, the paper makes the following contributions:"
1807.04488,code,149,2022-05-13,0,"In this paper, we propose a novel technique–ACER–for automatic query reformulation for concept location in the context of software change tasks. We (1) ﬁrst introduce a novel graph-based term weight –CodeRank– for identifying important terms from the source code, and then (2) apply that term weight and source document structures (e.g., method signatures) to our technique for automatic query reformulation. CodeRank identiﬁes important terms not only by analyzing salient structured entities (e.g., camel case tokens), but also by exploiting the co-occurrences among the terms across various entities. Our technique–ACER–accepts a natural language query as input, develops multiple candidate queries from two different important contexts, (1) method signatures and (2) ﬁeld signatures of the source documents independently using CodeRank, and then suggests the best reformulation ( based"
1807.04488,code,163,2022-05-13,0,"[14] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker. Mining Source Code to Automatically Split Identiﬁers for Software Analysis. In Proc. MSR, pages 71–80, 2009. [15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The Vocabulary Problem in Human-system Communication. Commun. ACM, 30(11):964–971, 1987. [16] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the Use of Relevance Feedback in IR-based Concept Location. In Proc. ICSM, pages 351–360, 2009. [17] S. Haiduc and A. Marcus. On the Use of Domain Terms in Source Code. In Proc. ICPC, pages 113–122, 2008. [18] S. Haiduc and A. Marcus. On the Effect of the Query In Proc. ICPC, pages"
1807.04488,code,167,2022-05-13,0,"CodeRank: PageRank [10] is one of the most popular algorithms for web link analysis which was later adapted by Mihalcea and Tarau [32] for text documents as TextRank. In this research, we adapt our term weighting method from TextRank [9, 32, 41] for source code, and we call it CodeRank. To date, only traditional term weights (e.g., TF, TFIDF [21, 43, 49]) are applied to source code which were originally proposed for regular texts [26] and are mostly based on isolated frequencies. On the contrary, CodeRank not only analyzes the connectivity (i.e., incoming links and outgoing links) of each source term, but also the relative weight of the connected terms from the graph recursively, and calculates the term weight, S(Vi), as follows (Step 6, Fig. 2):"
1807.04488,code,168,2022-05-13,0,"There exist a number of studies in the literature that reformulate a given query for concept location in the context of software change tasks. Existing studies apply relevance feedback from developers [16], pseudo-relevance feedback from IR tools [21], partial phrasal matching [23, 44], and machine learning [21, 34] to query reformulation. They also make use of context of query terms from source code [25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Hill et al. [23] consider the presence of query terms in the method or ﬁeld signatures as an indicator of their relevance, and suggest natural language phrases from them as reformulated queries. Sisman and Kak [49] choose such terms for query reformulation that frequently co-occur with query terms within"
1807.04488,code,17,2022-05-13,0,Fig. 1. An example term graph generated by CodeRank for source code of Listing 1
1807.04488,code,17,2022-05-13,0,Listing 1. Source code used for automatic query reformulation (abridged from [3])
1807.04488,code,179,2022-05-13,0,"In order to suggest meaningful reformulations to an initial query, feedback on the query is required. Gay et al. [16] ﬁrst reformulate queries based on explicit feedback from the developers. Although such feedback could be useful, gathering them is often time-consuming and sometimes infeasible. Hence, a number of recent studies [13, 21, 40, 41] apply pseudorelevance feedback as a feasible alternative. The top ranked results returned by the code search tool for an initial query are considered as the pseudo-relevance feedback for the query. We ﬁrst reﬁne an initial query by removing the punctuation marks, numbers, special symbols and stop words (Step 1, Fig. 2). Then we collect the Top-K (i.e., K = 10, best performing heuristic according to our experiments) search results returned by the query, and use them as the source for our candidate terms for query reformulation (Steps 2, 3, Fig. 2)."
1807.04488,code,185,2022-04-21,0,"Abstract—During software maintenance, developers usually deal with a signiﬁcant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique –ACER– that takes an initial query, identiﬁes appropriate search terms from the source code using a novel term weight –CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with ﬁve closely related existing techniques in query reformulation not only validates our empirical ﬁndings but also demonstrates the superiority of our technique."
1807.04488,code,190,2022-05-13,0,"Stop word and Keyword Removal: Since our structured tokens comprise of natural language terms, we discard stop words from them as a common practice (Step 4, Fig. 2). We use a standard list [6] hosted by Google for stop word removal. Programming keywords can often be considered as the equivalence of stop words in the source code which are also discarded from our analysis. Since we deal with Java source code, the keywords of Java are considered for this step. As suggested by earlier study [21], we also discard insigniﬁcant source terms (i.e., having word length< 3) from our analysis. Stemming: It extracts the root (e.g., “send”) out of a word (e.g., “sending”). Although existing studies suggest contradictory [28, 45] or conﬂicting [24] evidences for stemming with the source code, we investigate the impact of stemming with RQ4 where Snowball stemmer [24, 37] is used for stemming."
1807.04488,code,219,2022-05-13,0,"Thus, to answer RQ1, the reformulation of ACER improves the baseline queries signiﬁcantly both in terms of query effectiveness and retrieval performance. ACER improves 71% of the baseline queries with 64% Top-100 retrieval accuracy. Answering RQ2–CodeRank vs. Traditional Term Weighting Methods: Table VII shows the comparative analysis between CodeRank and two traditional term weights– TF and TF-IDF– which are widely used in the text retrieval contexts [13, 28, 43]. While TF estimates the importance of a term based on its occurrences within a document, TF-IDF additionally captures the global occurrences of the term across all the documents of the corpus [26]. On the contrary, CodeRank employs a graph-based scoring mechanism that determines the importance of a term based on its co-occurrences with other important terms within a certain context. From Table VII, we see that CodeRank performs signiﬁcantly better than both TF (i.e., paired t-test, p-value=0.005<0.05) and TF-IDF (i.e., p-value<0.001) in identifying important search terms from source code, especially from the method signatures. Considering the whole source code rather than signatures improves the performance of both TF (i.e., 56% query improvement) and"
1807.04488,code,228,2022-05-13,0,"Fig. 3 shows how CodeRank and traditional term weights perform in reformulating the baseline queries with their (a) Top-10 and (b) Top-30 terms. We see that TF reaches its peak performance pretty quickly (i.e., K = 3), and then shows a stationary or irregular behaviour. That means, TF identiﬁes frequent terms for query reformulation, and few of them (e.g., Top-3) could be highly effective. On the contrary, our method– CodeRank– demonstrates a gradual improvement in the performance up to Top-12 terms (i.e., K=12, Fig. 3-(b)), and crosses the performance peak of TF with a large margin (i.e., paired t-test, p-value=0.004<0.05, Cohen’s D=3.77>1.00 (large)), for K=10 to K=15). CodeRank emphasizes on the votes from other important terms (i.e., by leveraging co-occurrences) for determining weight of a term, and as demonstrated in Fig. 3, this weight is found to be more reliable than TF. TF-IDF is found relatively less effective according to our investigation. Thus, to answer RQ2, CodeRank performs signiﬁcantly better than traditional methods in identifying effective terms for query reformulation from the source code."
1807.04488,code,23,2022-04-21,0,• RQ3: Does employment of document structure improve ACER’s suggestion on good quality search terms from the source code?
1807.04488,code,231,2022-05-13,0,"Candidate Token Mining: Developers often express their intent behind the code and encode domain related concepts in the identiﬁer names and comments [17]. However, code comments are often inadequate or outdated [51]. All identiﬁer types also do not have the same level of importance. For example, while the signature of a method encodes the high level its body focuses on granular level implementation details and thus possibly contains more noisy terms [23]. In fact, Hill et al. [23] ﬁrst analyze method signatures and ﬁeld signatures to suggest natural language phrases as queries for code search. In the same vein, we thus also consider method signatures (msig) and ﬁeld signatures (f sig) as the source for our candidate reformulation terms. We extract structured identiﬁer names from these signatures using appropriate regular expressions [42] (Step 4, Fig. 2). Since different contexts of a source document might convey different types or levels of semantics (i.e., developers’ intent), we develop a separate candidate token set (CTsig) for each of the two signature types (sig ∈ {msig, f sig}) from the feedback documents (∀d ∈ DRF ) as follows: CTsig ="
1807.04488,code,248,2022-05-13,0,"Candidate Reformulation Selection: Algorithms 1 and 2 show the pseudo-code of our query reformulation technique– ACER–for concept location. We ﬁrst collect pseudo-relevance feedback for the initially provided query (Q) where Top-K source documents are returned (Lines 3–5, Algorithm 1). Then we collect method signatures and ﬁeld signatures from each of the documents (∀d ∈ DRF ), and extract structured tokens from them. We prepare three token sets–CTmsig, CTf sig and CTcomb from these signatures (Lines 6–12, Algorithm 1, Step 4, Fig. 2) where CTcomb combines tokens from both signatures. Then we perform limited natural language preprocessing on each token set where Samurai algorithm [14] is used for token splitting. We develop separate term graph for each of these token sets where individual terms are represented as vertices, and term co-occurrences are encoded as connecting edges (Lines 3–7, Algorithm 2, Step 5, Fig. 2). We apply CodeRank term weighting to each of the graphs which provides a ranked list of terms based on their relative importance. Then we select Top-K (e.g., K = 10) important terms from each of the three graphs, and prepare three reformulation candidates (Lines 8– 12, Algorithm 2, Steps 6, 7, 8, Fig. 2). Algorithm 1 ACER: Proposed Query Reformulation"
1807.04488,code,26,2022-05-13,0,[3] Example code snippet. URL https://goo.gl/WSZHiC. [4] Samurai preﬁx and sufﬁx list. URL https://hiper.cis.udel.
1807.04488,code,263,2022-05-13,0,"Once candidate tokens are extracted from method signatures and ﬁeld signatures, and are splitted into candidate terms, we develop source term graphs (e.g., Fig. 1) from them (Step 5, Fig. 2). Developers often encode their intent behind the code and domain vocabulary into the carefully crafted identiﬁer names where multiple terms are concatenated. For example, the method name–getChatRoomBots–looks like a natural language phrase–“get chat room bots”–when splitted properly. Please note that each of these three terms–“chat”, “room” and “bots”– co-occur with each other to convey an important concept– a robotic technology, and thus, they are semantically connected. On the other hand, the remaining term–“get”– cooccurs with them due to a temporal relationship (i.e., develops a verbal phrase). Similar phrasal representations (reﬁned with lexical matching) were directly returned by Hill et al. for query reformulation. However, their approach could be limited due to the added constraint (e.g., warrants query terms in signatures). We thus perform further analysis on such phrases, and exploit the co-occurrences among the terms for our graph based term weighting. In particular, we encode the term co-occurrences into connecting edges (E) in the term graph (G(V, E)) where the individual terms (Vi) are denoted as vertices (V )."
1807.04488,code,281,2022-05-13,0,"Studies show that about 80% of the total efforts is spent in software maintenance [36] where developers deal with a signiﬁcant number of software issues [35, 45, 52]. Software issue reports (a.k.a., change requests) discuss both unexpected (or erroneous features such as bugs) and expected but nonexistent features (e.g., new functionality). For both bug resolution and new feature implementation, a developer is required to map the concepts discussed in the issue report to appropriate source code within the project which is widely known as concept location [29, 31, 40]. Developers generally choose one or more important keywords from the report texts, and then use a search method (e.g., regular expression) to locate the source code entities (e.g., classes, methods) that need to be changed. Unfortunately, as the existing studies [28, 30] report, developers regardless of their experience perform poorly in choosing appropriate search terms for software change tasks. According to Kevic and Fritz [28], only 12.20% of the search terms chosen by the developers were able to locate relevant source code entities for the change tasks. Furnas et al. [15] also suggest that there is a little chance (i.e., 10%–15%) that developers guess the exact words used in the source code. One way to assist the developers in this regard is to automatically suggest helpful reformulations (e.g., complementary keywords) to their initially chosen queries."
1807.04488,code,29,2022-05-13,0,"• A novel term weighting method –CodeRank– for source code that identiﬁes the most important terms from a given code entity (e.g., class, method)."
1807.04488,code,29,2022-05-13,0,"• RQ2: Does CodeRank perform better than traditional term weighting methods (e.g., TF, TF-IDF) in identifying effective search terms from the source code?"
1807.04488,code,3,2022-04-21,0,Preprocessing Code search
1807.04488,code,31,2022-05-13,0,"[49] B. Sisman and A. C. Kak. Assisting Code Search with Automatic Query Reformulation for Bug Localization. In Proc. MSR, pages 309–318, 2013."
1807.04488,code,311,2022-05-13,0,"Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]. Thus, they might also fail to identify the appropriate terms from the structured source documents for query reformulation."
1807.04488,code,320,2022-05-13,0,"We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig. 5 shows how (a) Top-10 and (b) Top-30 reformulation terms improve the baseline queries. We see that"
1807.04488,code,324,2022-05-13,0,"Table I shows an example change request [2] submitted for eclipse.jdt.debug system, and it refers to “debugger source lookup” issue of Eclipse IDE. Let us assume that the developer chooses important keywords from the request title, and formulates a generic initial query–“debugger source lookup.” Unfortunately, the query does not perform well, and the 79th position of the returns the ﬁrst correct result at result list. Further extension–“debugger source lookup work variables”–also does not help, and returns the result at the 77th position. The existing technique – RSV [13]– extends the query as follows–“debugger source lookup work variables launch conﬁguration jdt java debug”–where the new terms are collected from the project source using TF-IDF based term weight. This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig."
1807.04488,code,328,2022-05-13,0,"This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig. 1) based on term co-occurrences from each signature type, apply CodeRank term weighting, and extract multiple candidate reformulations with the highly weighted terms (e.g., orange coloured, Fig. 1). Then we analyze the quality of the candidates using their quality measures [19], apply machine learning, and suggest the best reformulation to the initial query. Thus, our technique (1) ﬁrst captures salient terms from the source documents by analyzing their structural aspects (i.e., unlike bag of words approaches [46]) and an appropriate term weight–CodeRank, and (2) then suggests the best query reformulation using document structures (i.e., multiple candidates derived from various signatures), query quality analysis and machine learning [19]. Experiments using 1,675 baseline queries from eight open"
1807.04488,code,33,2022-05-13,0,"[38] M. M. Rahman and C. K. Roy. On the Use of Context in Recommending Exception Handling Code Examples. In Proc. SCAM, pages 285–294, 2014."
1807.04488,code,335,2022-05-13,0,"* = Statistically signiﬁcant difference between two measures from the same signature, MRD = Mean Rank Difference between ACER and baseline queries tion of document structures into a technique could be useful for query reformulations. We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig."
1807.04488,code,342,2022-05-13,0,"reformulation tasks. They also make use of context of query terms from source code [23, 25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]."
1807.04488,code,35,2022-04-21,0,"Fig. 2 shows the schematic diagram of our proposed technique–ACER–for automatic query reformulation. We use a novel graph-based metric of term importance–CodeRank– for source code, and apply source document structures, query"
1807.04488,code,36,2022-05-13,0,"[23] E. Hill, L. Pollock, and K. Vijay-Shanker. Automatically Capturing Source Code Context of NL-queries for Software Maintenance and Reuse. In Proc. ICSE, pages 232–242, 2009."
1807.04488,code,37,2022-04-21,0,"[25] M. J. Howard, S. Gupta, L. Pollock, and K. VijayShanker. Automatically Mining Software-based, Semantically-Similar Words from Comment-Code Mappings. In Proc. MSR, pages 377–386, 2013."
1807.04488,code,38,2022-05-13,0,"[31] A. Marcus, A. Sergeyev, V. Rajlich, and J.I. Maletic. An Information Retrieval Approach to Concept Location in Source Code. In Proc. WCRE, pages 214–223, 2004."
1807.04488,code,4,2022-05-13,0,C. Source Code Preprocessing
1807.04488,code,4,2022-05-13,0,URL https://code.google.com/p/
1807.04488,code,49,2022-05-13,0,"[22] S. Haiduc, G. De Rosa, G. Bavota, R. Oliveto, A. De Lucia, and A. Marcus. Query Quality Prediction and the Refoqus Reformulation for Source Code Search: Tool. In Proc. ICSE, pages 1307–1310, 2013."
1807.04488,code,61,2022-04-21,0,"[43] J.J. Rocchio. The SMART Retrieval System—Experiments in Automatic Document Processing. Prentice-Hall, Inc. [44] M. Roldan-Vega, G. Mallet, E. Hill, and J. A. Fails. CONQUER: A Tool for NL-based Query Reﬁnement and In Proc. ICSM, Contextualizing Code Search Results. pages 512–515, 2013."
1807.04488,code,7,2022-05-13,0,B. Corpus Indexing & Source Code Search
1807.04488,code,7,2022-05-13,0,Tasks to Source Code. 2014.
1807.04488,code,7,2022-04-21,0,that structure of a source code document
1807.04488,code,77,2022-05-13,0,"[51] C. Vassallo, S. Panichella, M. Di Penta, and G. Canfora. CODES: Mining Source Code Descriptions from Developers Discussions. In Proc. ICPC, pages 106–109, 2014. [52] I. Vessey. Expertise in Debugging Computer Programs: An Analysis of the Content of Verbal Protocols. TSMC, 16(5):621–637, 1986. [53] J. Yang and L. Tan."
1807.04488,code,87,2022-05-13,0,"Here sig(d) extracts all tokens from method signatures or ﬁeld signatures, and structured(t) determines whether the token t ∈ Tsig is structured or not. Although we deal with Java source code in this research where the developers generally use camel case tokens (e.g., MessageType) or occasionally might use same case tokens (e.g., DECIMALTYPE), our approach can be easily replicated for snake case tokens (e.g., reverse traversal) as well."
1807.04488,data,130,2022-05-13,0,"Threats to internal validity relate to experimental errors and biases [55]. Although CodeRank and document structures play a major role, the data resampling step (Section II-F, Step 9, Fig. 2) has a signiﬁcant role behind the high performance of our technique. Unfortunately, to the best of our knowledge, Refoqus [21] does not have such a step. Thus, the performance comparison might look like a bit unfair. Besides, models based on data resampling are sometimes criticized for intrinsic biases [5]. However, we apply data resampling to Refoqus as well (i.e., Refoqussampled), and demonstrate that our technique still performs better in terms of worsening ratio."
1807.04488,data,19,2022-05-13,0,"Index Terms—Query reformulation, CodeRank, term weight ing, query quality analysis, concept location, data resampling"
1807.04488,data,328,2022-05-13,0,"Haiduc et al. [19] suggest that quality of a query with respect to the corpus could be determined using four of its statistical properties– speciﬁcity, coherency, similarity and term relatedness–that comprise of 21 metrics [11]. They apply machine learning on these properties, and separate high quality queries from low quality ones. We thus also similarly apply machine learning on our reformulation candidates (and their metrics), and develop classiﬁer model(s) where Classiﬁcation And Regression Tree (CART) is used as the learning algorithm [19]. Since only the best of the four reformulation candidates (i.e., including baseline) is of our interest, the training data was inherently skewed. We thus perform bootstrapping (i.e., random resampling) [27, 50] on the data multiple times (e.g., 50) with 100% sample size and replacement (Step 9, Fig. 2), train multiple models using the sampled data, and then record their output predictions. Then, we average all the predictions for each test instance from all models, and determine their average probability of being the best candidate reformulation. Thus, we identify the best of the four candidates using our models, and suggest the best reformulation to the initial query (Lines 16–20, Algorithm 1, Steps 10, 11, Fig. 2). Bassett and Kraft [8] suggest that repetition of certain query terms might improve retrieval performance of the query. If none of the candidates is likely to improve the initial query according to the quality model (i.e., baseline itself is the best), we repeat all the terms from the initial query as the reformulation. Algorithm 2 getQRCandidate: Get a candidate reformulation (cid:46) CTsig: extracted"
1807.04488,data,33,2022-05-13,0,"[54] J. Yao, B. Cui, L. Hua, and Y. Huang. Keyword Query Reformulation on Structured Data. In Proc. ICDE, pages 953–964, 2012."
1807.04488,data,36,2022-05-13,0,"[50] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online Defect Prediction for Imbalanced Data. In Proc. ICSE, volume 2, pages 99–108, 2015."
1807.04488,data,37,2022-04-21,0,"[7] A. Bachmann and A. Bernstein. Software Process Data Quality and Characteristics: A Historical View on Open and Closed Source Projects. In Proc. IWPSE, pages 119– 128, 2009."
1807.04488,data,4,2022-04-21,0,Quality metric data resampling
1807.04488,"data, code",319,2022-05-13,0,"a ﬁxed size of window in the code. Rocchio [43] and RSV [13] determine importance of a term using TF-IDF based metrics. Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source."
1807.04488,"data, code",338,2022-05-13,0,"Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source. Thus, while earlier studies adopt various methodologies or information sources, our technique not only employs a novel and promising term weight –CodeRank, but also exploits structures of the source documents for identifying the best reformulation to a given query for improved concept location."
1807.04488,"data, data https",32,2022-04-21,0,[1] ACER experimental data. URL https://goo.gl/ZkaNvd. [2] Debbugger source lookup does not work with variables. URL https://bugs.eclipse.org/bugs/show bug.cgi? id=31110.
1807.04488,"data, data https",8,2022-04-21,0,Replication: All experimental data and relevant materials
1807.04488,"data, dataset",308,2022-04-21,0,"From Table IX, we see that RSV and Refoqus perform better than the other existing approaches. They improve about 55% and about 53% of the baseline queries respectively. Such ratios are also pretty close to the originally reported performances by Haiduc et al. on a different dataset, which possibly validates the correctness of our implementation. While 55% query improvement is the maximum performance provided by any of the existing approaches, our technique–ACER–improves about 70% of the baseline queries (i.e., 1% difference between Table V and Table IX due to rounding error) which is signiﬁcantly higher, i.e., paired t-test, p-value=6.663e-06<0.05, Cohen’s D=2.43>1.00 (large). Refoqus adopts a similar methodology like ours. Unfortunately, the approach is limited due to possibly the low performance of its candidate reformulations. One might argue about the data resampling step (i.e., Step 9, Fig. 2) of ACER for the high performance. However, we also apply data resampling to Refoqus using the same settings as ours for further investigation. We see that Refoqussampled has a similar improvement ratio like ours, but it still worsens a signiﬁcant amount of queries, 29%, compared to our 3.40%. Thus, our technique still performs better than Refoqus in the equal settings. Our quantile measures and mean ranks are more promising than those from the baseline or competing methods as reported in Table IX. Table V and RQ1 also suggest that our queries have high potential for reducing human efforts. We also experiment with an extended dataset (i.e., 1,755=1,675 + 8x10) containing 80 very good queries. As reported in Table"
1807.04488,"data, dataset, open-source",55,2022-05-13,0,"Data Collection: We collect a total of 1,675 bug reports from eight open source subject systems (i.e., ﬁve Eclipse systems and three Apache systems) for our experiments. Table III shows the experimental dataset. We ﬁrst extract resolved bug reports (i.e., marked as RESOLVED) from BugZilla and"
1807.04488,database,32,2022-05-13,0,"[55] T. Yuan, D. Lo, and J. Lawall. Automated Construction of a Software-speciﬁc Word Similarity Database. In Proc. CSMR-WCRE, pages 44–53, 2014."
1807.04488,dataset,13,2022-04-21,0,TABLE III EXPERIMENTAL DATASET System #CR #Classes ecf–279.279 log4j–1.2.18 sling–9.0 tomcat70–7.0.73
1807.04488,dataset,4,2022-05-13,0,A. Experimental Dataset
1807.04488,"dataset, code, github",133,2022-04-21,0,"JIRA repositories, and then collect corresponding bug-ﬁxing commits from GitHub version control histories of these eight systems. Such approach was regularly adopted by the relevant literature [8, 21, 41, 49], and we also follow the same. In order to ensure a fair evaluation or validation, we discard the bug reports from our dataset for which no source code ﬁles (e.g., Java classes) were changed or no relevant source ﬁles exist in the system snapshot collected for our study. We also discard such bug reports that contain stack traces using appropriate regular expressions [33]. They do not represent a typical change request (i.e., mostly containing natural language texts) from the regular software users."
1807.04488,open-source,7,2022-05-13,0,from eight open source subject systems.
1807.04488,publicly available,103,2022-04-21,0,"that their implementations are not publicly available. In the case of Refoqus, we implement 27 metrics (20 pre-retrieval [19] and 7 post-retrieval [21]) that estimate query difﬁculty. We develop a machine learning model using CART algorithm (i.e., as used by them) and 10-fold cross validation. Then, we use the model to return the best reformulation out of four candidates of Refoqus– query reduction, Dice expansion, Rocchio’s expansion and RSV expansion–for each baseline query. Table IX and Fig. 6 summarize our comparative analyses."
1810.03977,data,100,2022-05-13,0,The images are normalized and then given to the model for training.First convolution layer the kernel size used is 3×3 with input shape 32×56×56 with RELU (Rectiﬁed Linear Unit) activation function in the ﬁrst convolution layer and then with max pooling layer of size 32×27×27 we are down sampling the data to half of the original dimension and subsequent layers follow the similar pattern.The brief description of the CNN layers architecture along with the output shape is described in table 1.drop out is 0.25 which means we randoms abandon some of the weights to avoid the over ﬁtting
1810.03977,data,173,2022-05-13,0,"Support vector machines(SVM’s) are the most used machine learning algorithms for the image spam detection and because of high accuracy and robustness to misclassiﬁcations is the reason researcher prefer SVM[6]. SVM is a supervised learning algorithm used for the classiﬁcation of data.It consists of support vectors which divide and classiﬁes the data.It classiﬁes the non linear data using kernel trick in which the non linear data is projected to higher dimensions to make it linearly separable by a plane which is generally referred as hyper plane.It does this using a kernel function and their are diﬀerent types of kernel functions like linear,polynomial,radial basis function(RBF) and GausImage spam detection is a binary classian kernels. siﬁcation problem and two classes are spam and not spam.Using the training data that is collected and labeled according to respective classes model is trained and then the model is tested by giving the test data and performance of model is evaluated."
1810.03977,data,39,2022-05-13,0,"[8] R. Vinayakumar, P. Poornachandran, K. P. Soman, Scalable Framework for Cyber Threat Situational Awareness Based on Domain Name Systems Data Analysis, Springer Singapore, Singapore, 2018, pp. 113–142."
1810.03977,data,50,2022-05-13,0,"Initially, neural networks are used for image spam detection and then now research has shifted focus on applying the deep learning algorithms. Deep learning consists of neural network layers which automatically extracts the features from the data in hierarchical pattern and then predicts and classiﬁes the data."
1810.03977,data,81,2022-05-13,0,Convolutional neural networks (CNN’s) are one of the highly eﬃcient deep learning algorithms used for classifying data (particularly image data) using supervised learning technique. They consist of an Input layer and convolution layer followed by pooling layer and again convolution and pooling layers alternatively based on the size and architecture of the network. The ﬁnal layer is a fully connected layer. Fully connected layer converts the ﬁnal scalar outputs of individual classes
1810.03977,"data, dataset",10,2022-05-13,0,Table 2: Results metrics evaluated on test data set
1810.03977,"data, dataset",144,2022-05-13,0,In this research we have used the convolutional neural network(CNN) which is a deep learning network architecture for image spam detection.The deep learning approach gives better accuracy when compared with the machine learning and other conventional image processing based methods and also avoids the manual feature extraction task by automatically identifying the features by itself reducing the time and eﬀort.Binary classiﬁcation of image is performed the model is trained with existing labelled data set and then tested with the test data then metrics are evaluated.Further research can be carried out by exploring other deep learning algorithms like RNN and LSTM and tuning the architecture and hyper parameters may provide interesting insights.Capsule networks can also be tested on the data set which are giving promising results recently when compared with the convolutional neural networks for image related techniques[10].
1810.03977,"data, dataset",24,2022-05-13,0,"features[5]. Features like sender,meta data,message header are extracted and training dataset is prepared and labeled."
1810.03977,"data, dataset",4,2022-05-13,0,4. Data set
1810.03977,"data, dataset",47,2022-05-13,0,"Total images are split in ratio of 80 percent training data and 20 percent testing data.The model is evaluated after the convolutional neural network is trained on training dataset.It is then tested on the testing data set and result metrics accuracy,precision,recall and f1score"
1810.03977,dataset,32,2022-05-13,0,Dataset is subdivided into both training and testing datasets. Training dataset consists of 742 spam images and 648 normal images.Testing dataset consists of 186 spam images and 162 normal images.
1810.03977,dataset,47,2022-05-13,1,The dataset used in the experiment consist of 928 spam images and 810 normal images which collected from diﬀerent sources[9] and all are RGB images in various dimensions which in preprocessing are reshaped to 56×56 images.The sample images are shown in ﬁgure.1 and ﬁgure.2
1810.03977,dataset,8,2022-05-13,0,Figure 1: Sample spam image from dataset
1810.03977,dataset,8,2022-05-13,0,Figure 2: Sample non-spam image from dataset
1810.03977,dataset,96,2022-05-13,0,Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classiﬁcation achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques.
1812.05067,code,133,2022-05-13,0,"Experimental evaluation We have used our implementation to typecheck a variety of examples, including all the examples from the RelCost paper. Some of the examples, such as the relational analysis of merge sort (msort), have rather complex paper proofs. However, in all cases, the total typechecking time (including existential elimination and SMT solving) is less than 1s, suggesting that the approach is practical. Table 1 shows the experimental results over a subset of our example programs (our appendix lists all our example programs, including their code and experimental results). A “-” indicates a negligible value. Our experiments were performed on a 3.20GHz 4-core Intel Core i5-6500 processor with 16 GB of RAM."
1812.05067,code,239,2022-05-13,0,"Heuristics illustrated with merge sort We explain how our implementation types one example—the standard merge sort function— relationally. The goal of this exercise is primarily to illustrate some of our heuristics. The merge sort function, msort, splits a list into two nearly equal-sized sublists using an auxiliary function we call bsplit, recursively sorts each sublist and then merges the two sorted sublists using another function merge. In their paper on RelCost, Çiçek et al. [14] show that the relative cost of two runs of msort with two input lists of length n that diﬀer in at most α positions is upper-bounded by Q(n, α) = ) · min(α, 2H −i ), where H = ⌈log2(n)⌉ and h is a speciﬁc linear function. This open-form expression lies in O(n · (1 + log2(α))).1 Next, we explain at a high-level how this relative cost is typechecked bidirectionally. We show below the code of the top-level merge sort function msort. ﬁx msort(_).Λ.Λ.λl .case l of nil → nil | h1 :: tl1 → case tl1 of nil → cons(h1, nil) | _ :: _ → let r = bsplit ()[ ] [ ] l in"
1812.05067,code,36,2022-05-13,0,"We do not show the code of the helper functions bsplit and merge, but they have the following types (these types are also checked with BiRelCost; we omit those details here):"
1812.05067,data,154,2022-05-13,0,"Next, we extend relSTLC in two steps inspired by the features of previously proposed relational type systems. Our ﬁrst step, named RelRef, adds relational reﬁnement types over lists (as an example of an inductive data type), and a comonadic type that represents syntactic equality of two values. Our second step, named RelRefU, adds to RelRef the possibility to relate arbitrary programs of possibly dissimilar syntactic structure, thanks to the possibility to switch to a complementary unary type system. Both these extensions add intrinsic nondeterminism to the type system to allow a programmer ﬂexibility in writing programs. The source of nondeterminism in both these systems is non-syntax-directed typing and subtyping rules. RelRef has such rules for relational reﬁnement types and for subtyping, while RelRefU has such a rule for switching to unary typing and more such rules for subtyping."
1812.05067,data,98,2022-05-13,0,"Relational eﬀects [8, 14, 16, 28, 39] are often of a quantitative nature and measure some quantitative diﬀerence between two executions of the two expressions. These relational eﬀects are similar in spirit to their standard unary counterpart [13, 31, 32, 34] but their interpretation is a relation between the eﬀects of the two executions. For example, in diﬀerential privacy, a relational eﬀect is used to measure the level of indistinguishability between the observable outputs on two inputs diﬀering in one data element."
1904.11228,data,112,2022-05-13,0,"The feature matrix of data in the vth view is denoted as Xv = [xv N ]T ∈ RN ×dv , xv 1 ∈ Rdv×1, dv is the dimension of feature in the vth view, N is the number of data samples. We pack the feature matrices in V views {Xv}V v=1 and the overall feature matrix of data can be represented as X = [X1, X2, ..., XV ] ∈ RN ×d, (cid:80)V v=1 dv = d. The objective of unsupervised multi-view feature selection is to identify l most valuable features with only X."
1904.11228,data,145,2022-05-13,0,"The selected features should preserve the dynamically learned similarity structure. Conventional approaches separate the similarity structure construction and feature selection into two independent processes, which will potentially lead to sub-optimal performance. In this paper, we learn the collaborative similarity structure dynamically and further integrate it with feature selection into a uniﬁed framework. Specifically, based on the collaborative similarity structure learning in Eq.(3), we employ sparse regression model to learn a projection matrix P ∈ Rd×k, so that the projected lowdimensional data XP can approximate the relaxed cluster indicator F. To select the features, we impose l2,1 norm penalty on P to force it with row sparsity. The importance of features can be measured by the l2 norm of each row feature in P. The overall optimization formulation can be derived as"
1904.11228,data,15,2022-05-13,0,"ture selection for big data analytics. 32(2):9–15, 2017."
1904.11228,data,173,2022-05-13,0,"N Sj = 1, Sj ≥ 0, WT where Sj ∈ RN ×1 characterizes the similarities between any data points with j, it should be subjected to the constraint that 1TSj = 1, Sj ≥ 0, Wj = [w1 j ]T ∈ RV ×1 is comj , w2 prised of view weights for the jth column of similarities, it is constrained with WT j 1V = 1, W = [W1, W2, ..., WN ] ∈ RV ×N is view weight matrix for all columns in the similarity structures. As indicated in recent work [Nie et al., 2014], a theoretically ideal similarity structure for clustering should have the property that the number of connected components is equal to the number of clusters. The similarity structure with such neighbor assignment could beneﬁt the subsequent feature selection. Unfortunately, the similarity structure learned from Eq.(1) does not have such desirable property."
1904.11228,data,176,2022-05-13,0,"hand, with multi-view features, the data could be characterized more precisely and comprehensively from different perspectives. On the other hand, high-dimensional multiview features will inevitably generate expensive computation cost and cause massive storage cost. Moreover, they may contain adverse noises, outlying entries, irrelevant and correlated features, which may be detrimental to the subsequent learning process [Zhu et al., 2016b; Zhu et al., 2016a; Zhu et al., 2017a]. Unsupervised multi-view feature selection [Wang et al., 2016; Li and Liu, 2017] is devised to alleviate the problem. It selects a compact subset of informative features from the original features by dropping irrelevant and redundant features with advanced unsupervised learning. Due to the independence on semantic labels, high computing efﬁciency and well interpretation capability, unsupervised multiview feature selection has received considerable attention in It becomes a prerequisite component in various literature. machine learning models [Li et al., 2017]."
1904.11228,data,204,2022-05-13,0,"The key problem of multi-view feature selection is how to effectively exploit the diversity and consistency of multi-view features to collaboratively identify the feature dimensions, which could retain the key characteristics of the original features. Existing approaches can be categorized into two major families. The ﬁrst kind of methods ﬁrst concatenates multiview features into a vector and then directly imports it into the conventional single-view feature selection model. The candidate features are generally ranked based on spectral graph theory. Typical methods of this kind include Laplacian Score (LapScor) [He et al., 2005], spectral feature selection (SPEC) [Zhao and Liu, 2007] and minimum redundancy spectral feature selection (MRSF) [Zhao et al., 2010]. Commonly, the pipeline of these methods follows two separate processes: 1) Similarity structure is constructed with ﬁxed graph parameters to describe the geometric structure of data. 2) Sparsity and manifold regularization are employed together to identify the most salient features. Although these methods are reported to achieve certain success, they treat features from different views independently and unfortunately neglect the important view correlations."
1904.11228,data,205,2022-05-13,0,"Another family of methods considers view correlation when performing feature selection. Representative works in       clude adaptive multi-view feature selection (AMFS) [Wang et al., 2016], multi-view feature selection (MVFS) [Tang et al., 2013] and adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013]. These methods ﬁrst construct multiple view-speciﬁc similarity structures1 and then perform the subsequent feature selection based on the collaborative (combined) similarity structure. These two processes are separate and independent. The collaborative similarity structure remains ﬁxed during feature selection. The latently involved data noises and outlying entries in the view-speciﬁc similarity structures will adversely reduce the reliability of the ultimate collaborative similarity structure for feature selection. Furthermore, conventional approaches generally employ knearest neighbors assignment to construct the view-speciﬁc similarity structures and the simple weighted combination for ultimate similarity structure generation. This strategy can hardly achieve the ideal state for clustering that the number of connected components in the ultimate similarity structure is equal to the number of clusters [Nie et al., 2014]. Thus, suboptimal performance may be caused under such circumstance."
1904.11228,data,206,2022-05-13,0,"the important correlation of different feature views. Another kind of methods directly tackles the multi-view feature selection. They consider view correlations when performing feature selection. Adaptive multi-view feature selection (AMFS) [Wang et al., 2016] is an unsupervised feature selection approach which is developed for human motion retrieval. It describes the local geometric structure of data in each view with local descriptor and performs the feature selection in a general trace ratio optimization. In this method, the feature dimensions are determined with trace ratio criteria. Adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013] addresses the feature selection problem for visual concept recognition. It employs l2,1 norm [Nie et al., 2010] based sparse regression model to automatically identify discriminative features. In AUMFS, data cluster structure, data similarity and the correlations of different views are considered for feature selection. Multi-view feature selection (MVFS) [Tang et al., 2013] investigates the feature selection for multi-view data in social media. A learning framework is devised to exploit the relations of views and help each view select relevant features."
1904.11228,data,38,2022-05-13,0,"[Tang et al., 2013] Jiliang Tang, Xia Hu, Huiji Gao, and Huan Liu. Unsupervised feature selection for multi-view data in social media. In SDM, pages 270–278, 2013."
1904.11228,data,69,2022-05-13,0,"[He et al., 2005] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS, pages 507–514, 2005. [Huang et al., 2015] Jin Huang, Feiping Nie, and Heng Huang. A new simplex sparse learning model to measure data similarity for clustering. In IJCAI, pages 3569–3575, 2015."
1904.11228,data,75,2022-05-13,0,"As mentioned above, the data points can be directly partitioned into k clusters if the number of components in the similarity structure S is exactly equal to k. Theorem 1 indicates that this condition can be achieved if the rank of Laplacian matrix is equal to n − k. With the analysis, we add a reasonable rank constraint in Eq.(1) to achieve the condition. The optimization problem becomes"
1904.11228,data,76,2022-05-13,0,"With the advent of big data, multi-view features with high dimensions are widely employed to represent the complex data in various research ﬁelds, such as multimedia computing, machine learning and data mining [Liu et al., 2016; Liu et al., 2017; Zhu et al., 2017b; Zhu et al., 2015; Cheng and Shen, 2016; Cheng et al., 2016]. On the one"
1904.11228,"data, dataset",295,2022-05-13,1,"4 Experiments 4.1 Experimental Datasets 1) MSRC-v1 [Winn and Jojic, 2005]. The dataset contains 240 images in 8 class as a whole. Following the setting in [Grauman and Darrell, 2006], we select 7 classes composed of tree, building, airplane, cow, face, car, bicycle and each class has 30 images. We extract 5 visual features from each image: color moment with dimension 48, GIST with 512 dimension, SIFT with dimension 1230, CENTRIST feature with 210 dimension, and local binary pattern (LBP) with 256 dimension. 2) Handwritten Numeral [van Breukelen et al., 1998]. This dataset is comprised of 2,000 data points from 0 to 9 digit classes. 6 features are used to represent each digit. They are 76 dimensional Fourier coefﬁcients of the character shapes, 216 dimensional proﬁle correlations, 64 dimensional Karhunen-love coefﬁcients, 240 dimensional pixel averages in 2 × 3 windows, 47 dimensional Zernike moment and 6 dimensional morphological features. 3) Youtube [Liu et al., 2009]. This real-world dataset is collected from Youtube. It contains intended camera motion, variations of the object scale, viewpoint, illumination and cluttered background. The dataset is comprised of 1,596 video sequences in 11 actions. 4) Outdoor Scene [Monadjemi et al., 2002]. The outdoor scene dataset contains 2,688 color images that belong to 8 outdoor scene categories. 4 visual features are extracted from each image: color moment with dimension 432, GIST with dimension 512, HOG with dimension 256, and LBP with dimension 48."
1904.11228,dataset,1,2022-05-13,0,Dataset
1905.12665,code,82,2022-05-13,0,"This work was ﬁnanced in part by the S˜ao Paulo Research Foundation (FAPESP) under grants No. 2016/199476 and No. 2017/16597-7, the Brazilian National Council for Scientiﬁc and Technological Development (CNPq) under grant No. 307425/2017-7, and the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior—Brasil (CAPES)—Finance Code 001. We acknowledge the support of NVIDIA Corporation for the donation of a Titan X Pascal GPU used in this research."
1905.12665,"code, code available",1,2022-05-13,0,Code
1905.12665,data,102,2022-05-13,0,"Abstract Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classiﬁcation, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings."
1905.12665,data,22,2022-05-13,0,Presented at the ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data Copyright 2019 by the author(s).
1905.12665,data,37,2022-05-13,0,"Berg, R. v. d., Kipf, T. N., and Welling, M. Graph convolutional matrix completion. ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), 2018."
1905.12665,data,49,2022-05-13,0,"Donnat, C., Zitnik, M., Hallac, D., and Leskovec, J. Learning structural node embeddings via diffusion wavelets. In ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), pp. 1320–1329. ACM, 2018."
1905.12665,data,73,2022-05-13,0,"Bai, Y., Ding, H., Bian, S., Chen, T., Sun, Y., and Wang, W. SimGNN: A neural network approach to fast graph similarity computation. In ACM Inter. Conf. Web Search Data Min. (WSDM), WSDM ’19, pp. 384–392, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-5940-5."
1905.12665,data,94,2022-05-13,0,"Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In Gangemi, A., Navigli, R., Vidal, M.-E., Hitzler, P., Troncy, R., Hollink, L., Tordai, A., and Alam, M. (eds.), Semantic Web Conf. (ESWC), pp. 593–607, Cham, 2018. Springer International Publishing."
1905.12665,dataset,140,2022-05-13,1,"In this work, we evaluate our model as an edge classiﬁer, and simulate its performance as a graph generator by inputting noise as features and predicting on them. We perform experiments on three synthetic datasets that consist of images with Geometric Figures for segmentation, 3D surface function, and Community dataset (see Appendices A.1, A.2, and A.3, respectively). For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. Our evaluation metric is the Maximum Mean Discrepancy (MMD) measure (You et al., 2018), which measures the Wasserstein distance over three statistics of the graphs: degree (Deg), clustering coefﬁcients (Clus), and orbits (Orb)."
1905.12665,dataset,147,2022-05-13,0,"For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. For both models, we use the following settings. Our activation functions, σl, are sigmoid for all layers, except for the Eq. 7 where σl is a hyperbolic tangent. We use L = 5 layers to extract the ﬁnal adjacency and embeddings. The feature dimension, dl, is 32 for all layers. The learning rate is set 10−5 for the Community dataset, and in the rest of datasets, the learning rate is set 5 × 10−6. Additionally, the number of epochs changes depending on the experiment. Thus in the experiments of Communities, Surfaces and Geometrical Figures we use 150, 200 and 150 times respectively and, the number"
1905.12665,dataset,2,2022-05-13,0,A. Datasets
1905.12665,dataset,39,2022-05-13,0,"Figure 2. Results of the dissimilarity (MMD) between the prediction and ground truth (smaller values are better) while varying the number of recurrent steps, on the 3D Surface dataset (Surf400)."
1905.12665,dataset,4,2022-05-13,0,A.3. Community Dataset
1905.12665,dataset,40,2022-05-13,0,"We made the Geometric Figures dataset for the task of image segmentation within a controlled environment. Segmentation is given by the connected components of the graph ground-truth. Here, we provide RGB images and their expected segmentations."
1905.12665,dataset,44,2022-05-13,0,"Additionally, in Table 2, we present an ablation analysis of our model’s loss functions and regularization components on the Geometric Figures dataset. We emphasize a stable training and a fast convergence when we minimize both loss functions simultaneously."
1905.12665,dataset,5,2022-05-13,0,A.1. Geometric Figures Dataset
1905.12665,dataset,5,2022-05-13,0,A.2. 3D Surfaces Dataset
1905.12665,dataset,52,2022-05-13,0,"The Geometric Figures dataset contains 3000 images of size n×n, that are generated procedurally.1 Each image contains circles, rectangles, and lines (dividing the image into two parts). We also add white noise to the color intensity of the images to perturb and mixed their regions."
1905.12665,dataset,56,2022-05-13,0,"We generated 200 versions of each surface by randomly applying a set of transformations (from scaling, translation, rotation, reﬂection, or shearing) to the curve, moreover, two versions of the Surface dataset were created, Surf100 and Surf400 that use 100 and 400 vertices per surface, respectively."
1905.12665,dataset,68,2022-05-13,0,"We perform experiments on a synthetic dataset (Community dataset) that comprises two sets with C = 2 and C = 4 communities with 40 and 80 vertices each, respectively, created with the caveman algorithm (Watts, 1999), where each community has 20 people. Besides, Community C = 4 and C = 2 have 500 and 300 samples respectively."
1905.12665,dataset,69,2022-05-13,0,"Table C.1. Comparison of GLN, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric are accuracy (Acc), intersection-over-union (IoU), Recall (Rec), and Precision (Prec) shown row-wise per method, where larger numbers denote better performance."
1905.12665,dataset,73,2022-05-13,0,"Finally, in Fig. F.1, we present an application, even fundamental, on segmentation where each of the connected components represents different objects. For this, we apply our GLN model on Geometric Image dataset, using size image of 20 × 20. Besides, the white edges represent correct predictions, and light blue dashed edges are false negatives (i.e., not predicted edges)."
1905.12665,dataset,78,2022-05-13,0,"Figure E.1. Results on Community dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,78,2022-05-13,0,"Knowing the depth of the recursive model (i.e., the number of iterations) is not a trivial task since we must ﬁnd a tradeoff between the efﬁciency and effectiveness of the model. In Fig. 2, we show the dissimilarity metrics (MMD) while varying the number of applications of our proposed block on the 3D Surface dataset. According to our experiment, using ﬁve recurrent steps provides the right trade-off."
1905.12665,dataset,79,2022-05-13,0,"Figure D.1. Results on 3D Surface dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,81,2022-05-13,0,"To evaluate our method we needed a highly structured dataset with intricate relations and with easily understandable features. Hence, we convert parts of 3D surfaces into a mesh by sampling them. Each point in the mesh is translated into a node of the graph, with its position as a feature vector. We have a generator2 that creates different conﬁgurations for this dataset based on a number of nodes per surface, and transformation on it."
1905.12665,dataset,83,2022-05-13,0,"In Fig. D.1, we show the qualitative result of GLN for the 3D Surface dataset. We show the prediction on the elliptic hyperboloid, elliptic paraboloid, torus, saddle, and ellipsoid, all using 100 nodes (Surf100). We normalized the graphs (w.r.t. scale and translation) for better visualization. Besides, the red edges represent false negatives (i.e., not predicted edges) and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"In Fig. E.1, we predict the adjacency matrix the of Community dataset on two and four communities, C = 2 and C = 4 respectively (even rows). Note, our node embedding obtained after apply the λl function, shows a good grouping of individuals in the hyperspace (odd rows). Furthermore, the red edges represent false negatives (i.e., not predicted edges), and black edges are correctly predicted ones."
1905.12665,dataset,83,2022-05-13,0,"structure given a set of points and their feature embeddings, respectively. (ii) A recurrent architecture that deﬁnes our iterative process and our prediction functions. (iii) An endto-end learning framework for predicting graphs’ structure given a family of graphs. (iv) Additionally, we introduce a synthetic dataset, i.e., 3D surface functions, that contains patterns that can be controlled and mapped into graphs to evaluate the robustness of existing methods."
1905.12665,dataset,84,2022-04-21,0,"Table 1. Comparison of GLN against deep generative models, GraphRNN (G.RNN), Kronecker (Kron.), and MMSB, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric is MMD for degree (D), cluster (C), and orbits (O) shown row-wise per method, where smaller numbers denote better performance."
1905.12665,github,4,2022-05-13,0,at https://gitlab.com/mipl/
1905.12665,supplementary data,10,2022-05-13,0,Graph Learning Network: A Structure Learning Algorithm SUPPLEMENTARY MATERIAL
1906.08554,data,105,2022-05-13,0,"The smart devices are increasing exponentially day by day in the whole world. They provide much more facility to the end users and also attach with their daily life. Smart devices can connect to the internet easily for sending and receiving data within the network. The smart devices are not just smart phones, it may be smart refrigerator, Smart home automation entry point, smart air conditioners, Smart hubs, Smart thermostat, Color changing smart LEDs, Smart Watches and smart Tablets etc. in internet of things framework they are connected to each other through internet."
1906.08554,data,119,2022-05-13,0,"The objective of this research is to create the new reliable communication framework for the smart cities using the Tactile Internet a next revolution of internet of things. This research is based on low-latency, ultra-high availability and high-performance concepts of Tactile Internet. The framework provides QoS through reducing the latency (1ms in round trip) also the variety of the quantity of smart devices. In this research we consider idle state in order to makes our examination more efficient, at that point the general execution regarding the overall performance of the framework is evaluated. The framework will monitor and analyze the real-time data collected from network and then taking the action."
1906.08554,data,12,2022-05-13,0,Table.2: Comparison of peak data rate and latency [3]
1906.08554,data,127,2022-05-13,0,"The proposed research entitled “Tactile Internet based reliable communication framework for Smart cities in 5G” is a step forward in wireless networking and IoT where we propose new reliable framework based on Tactile Internet. The Wireless communication is the key of Internet of things and Tactile Internet. It is expected to exceed 50 billion connected devices by 2020 and most of these nodes cannot be connected by wireline. In order to enable critical applications such as smart factories or smart buildings, the networking protocols have to deal with the non-deterministic nature of wireless links. In the 5th generation communication system, the secure and reliable data packets will rely on the network with high availability and low latency."
1906.08554,data,169,2022-05-13,0,"The proposed research plan builds research on extending the performance of communication in internet of things using tactile internet. The transfer data from one configuration to another using wireless networks starts from 1973 in the form of packets radio network. They were able to communicate with another same configuration devices. Recent work is continuing on a project called the Serval Project. It provides networks facility to android devices for communication in infrastructure less network. Whereas our research is concerned about the high-performance communication in internet of smart devices for smart cities. The main contribution of this research is the creation of the reliable communication framework and provide secure, reliable and fast communication using Tactile Internet among the internet of smart devices. The previous studies have been focused on the creation and optimization the framework for communication, but such research doesn’t perform the full framework for secure and reliable communication among internet of smart devices for smart cities."
1906.08554,data,270,2022-05-13,0,"The main contribution of this research is designing a framework for ultra-reliable, low latency and high availability communication in Internet of smart devices for future smart cities using the Tactile Internet. The proposed framework is specifically appropriate for applications in which data is periodically transmitted in internet of smart devices environment. In these applications, on one hand, packets are being produced based on a certain periodic time pattern. On the other hand, service time is always a random variable with general distribution. Therefore, service time might temporarily exceed the period time which, as an inevitable consequence some packets might encounter a busy channel and be dropped. We solve this problem by proposing the new communication framework. We demonstrate that proposed reliable framework, not only increases the throughput, but also the direct connection between the generation (sensors) and communication packet systems are eliminated which make the system far more stable. Moreover, in order to enhance the proposed model, we have employed retransmission scheme, variable packet length, and saturated traffic condition. The solution of this research is summarized as follows. The implementation of proposed framework for communication among internet of smart devices in 5G will be programmed to execute on to the internet of things using Tactile Internet concepts. The idea will focus into three main concepts, these concepts are Reliability, Security and availability. The proposed study supports the wireless networking technology to establish a reliable framework among internet of devices for smart cities."
1906.08554,database,110,2022-05-13,0,"[27]. Aljohani, Mohammed, and Tanweer Alam. ""An algorithm for accessing traffic database using wireless technologies."" In Computational Intelligence and Computing Research (ICCIC), 2015 IEEE International Conference on, pp. 1-4. IEEE, 2015. DOI: https://doi.org/10.1109/iccic.2015.7435818  [28]. Alam, Tanweer, and Mohammed Aljohani. ""Design a new middleware for communication in ad hoc network of android smart devices."" In Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies, p. 38. ACM, 2016. DOI: https://doi.org/10.1145/2905055.2905244"
1912.0313,data,125,2022-05-13,0,"We observe that the ST-DIM based pre-trained models can easily be ﬁne-tuned only with small amount of downstream data. In this classiﬁcation task, our model can classify a randomly chosen time-series as a sample of VAR or SVAR. Note, with very few samples, models based on the pre-trained encoder (FPT and UFPT) outperform supervised models. However, as the number of samples grows, the accuracy achieved with or without pre-training levels out. We also notice that autoencoder based self-supervised pretraining does not assist in VAR vs. SVAR classiﬁcation. Consequently, we use only ST-DIM based pre-training for all the real data experiments. Refer to Figure 4 for the results of simulation experiments."
1912.0313,data,212,2022-05-13,0,"Unsupervised pre-training is a well-known technique to get a head start for the deep neural network. It may be considered as a regularizer which compares to classical regularizers (i.e. L1/L2) may not vanish even with more data and could ﬁnd a robust local minima for better generalization [6]. Classical methods are Deep Beliefs Networks (DBMs) [7] and stacked denoising autoencoders (SDAE) [8]. Unsupervised pre-training has broad implications in ﬁelds such as computer vision [9], natural language processing (NLP) (GPT2 [10], BERT [11], Word2Vec [12]) and automatic speech recognition (ASR) (with SDAE [13], with DBN-HMMs [14]). However, this unsupervised pre-training is considered to be less popular in ﬁelds other than NLP [15]. Speciﬁcally, in computer vision, researchers usually use the model which is pre-trained in supervised fashion on Imagenet as a starting point for downstream tasks. Furthermore, given enough data and technical strategies, it is possible to achieve better results on COCO object detection without supervised pre-training on Imagenet [16]."
1912.0313,data,25,2022-05-13,0,Simulations Real Data 16 Training Batch Size 200 Validation Batch Size 200 Test Batch Size Initial Learning Rate 3e-4 Learning Rate Scheduler None Max Epochs
1912.0313,data,30,2022-05-13,0,Figure 8. ICA time courses are computed from the resting state fMRI data. Results contain statistically independent spatial maps (top) and their corresponding time courses.
1912.0313,data,36,2022-05-13,0,"[19] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019."
1912.0313,data,38,2022-05-13,0,"namics generalizes across different data distributions, as our model pre-trained on healthy adults shows improvements in children and elderly. The generality of the approach is also demonstrated in an application to the keyword detection problem."
1912.0313,data,42,2022-05-13,0,"To simulate the data, we generate multiple 10-node graphs with 10 × 10 stable transition matrices. Using these we generate multivariate time series with autoregressive (VAR) and structural vector autoregressive (SVAR) models [44]."
1912.0313,data,42,2022-05-13,0,"[2] Vince D Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adalı. The chronnectome: time-varying connectivity networks as the next frontier in fmri data discovery. Neuron, 84(2):262–274, 2014."
1912.0313,data,45,2022-05-13,0,"[28] Vince D Calhoun, Tulay Adali, Godfrey D Pearlson, and JJ Pekar. A method for making group inferences from functional MRI data using independent component analysis. Human brain mapping, 14(3):140–151, 2001."
1912.0313,data,48,2022-05-13,0,"[31] R Devon Hjelm, Eswar Damaraju, Kyunghyun Cho, Helmut Laufs, Sergey M Plis, and Vince D Calhoun. Spatio-temporal dynamics of intrinsic networks in functional magnetic imaging data using recurrent neural networks. Frontiers in neuroscience, 12:600, 2018."
1912.0313,data,51,2022-05-13,0,"application [33–35] is considered as a way to enable learning from data and thus improve results in downstream classiﬁcation. To achieve improved performance, another idea is the data generating approach [36] which uses synthetic data generator for pre-training, relieving the scarcity of data."
1912.0313,data,61,2022-05-13,0,"U. Mahmood1, M. M. Rahman1, A. Fedorov2 , Z. Fu1, V. D. Calhoun1, 2, 3, S. M. Plis1 Tri-institutional Center for Translational Research in Neuroimaging and Data Science: 1Georgia State University, 2Georgia Institute of Technology, 3Emory University Atlanta, GA, USA {umahmood1,mrahman21}@student.gsu.edu afedorov@gatech.edu"
1912.0313,data,70,2022-05-13,0,"[50] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer, Mirella Dapretto, et al. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular psychiatry, 19(6):659, 2014."
1912.0313,data,96,2022-05-13,0,"As we are more interested in subjects for classiﬁcation task, we feed each time series (ICA time courses) into the encoder in the form of a sequence of windows. The encoder encodes the windows of input data into latent representation. The latent representation of the entire time series is then concatenated and passed to a biLSTM with hidden dimension of size 200. The output of biLSTM is then used as input to a feed forward network of two linear layers with 200 and 2 units to perform binary classiﬁcation."
1912.0313,"data, data https",154,2022-05-13,0,"We preprocessed the fMRI data using statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) under MATLAB 2016 environment. A rigid body motion correction was performed using the toolbox in SPM to correct subject head motion, followed by the slice-timing correction to account for timing difference in slice acquisition. The fMRI data were subsequently warped into the standard Montreal Neurological Institute (MNI) space using an echo planar imaging (EPI) template and were slightly resampled to 3 × 3 × 3 mm3 isotropic voxels. The resampled fMRI images were ﬁnally smoothed using a Gaussian kernel with a full width at half maximum (FWHM) = 6 mm. After the preprocessing. We included subjects in the analysis if the subjects have head motion ≤ 3◦ and ≤ 3 mm, and with functional data providing near full brain successful normalization [52]."
1912.0313,"data, data repository",11,2022-05-13,1,"2These data were downloaded from the Function BIRN Data Repository,"
1912.0313,"data, data repository",61,2022-05-13,0,"[48] David B Keator, Theo GM van Erp, Jessica A Turner, Gary H Glover, Bryon A Mueller, Thomas T Liu, James T Voyvodic, Jerod Rasmussen, Vince D Calhoun, Hyo Jong Lee, et al. The function biomedical informatics research network data repository. Neuroimage, 124:1074–1079, 2016."
1912.0313,"data, data repository, data https",100,2022-05-13,1,"Data for Schizophrenia classiﬁcation was used in this study were downloaded from the Function BIRN Data Repository (http://bdr.birncommunity.org:8080/BDR/), supported by grants to the Function BIRN (U24-RR021992) Testbed funded by the National Center for Research Resources at the National Institutes of Health, U.S.A. and from the COllaborative Informatics and Neuroimaging Suite Data Exchange tool (COINS; http://coins.trendscenter.org) and data collection was performed at the Mind Research Network, and funded by a Center of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 from the NIH to Dr.Vince Calhoun."
1912.0313,"data, dataset",106,2022-05-13,0,"COBRE The dataset has total 157 subjects — a collection of 68 HC and 89 affected with SZ. Like FBIRN, each subject has 53 non-noise components in its ICA time courses with 140 time points. We use two hold-out sets of size 32 each respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Unlike FBIRN, it has been impossible to increase the number of subjects for downstream training due to insufﬁciency of data."
1912.0313,"data, dataset",111,2022-05-13,0,"15255075100Number of Subjects Per Class0.30.40.50.60.70.80.91.0AUCSpeech CommandsFPTUFPTNPTFBIRNCOBREABIDEOASISHCPpre-trainingapplyingautismschizophreniaADdatasets contain labeled Schizophrenia (SZ) and Healthy Control (HC) subjects. FBIRN The dataset has total 311 subjects consisting of 150 HC and 161 affected with SZ. Each subject has 53 non-noise components with 140 time points. We use two hold-out sets with sizes 32 and 64 respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with 50% overlap along time dimension resulting in 13 windows for each subject. The details of the results are shown in Figure 9."
1912.0313,"data, dataset",111,2022-05-13,0,"As we see, the AUC scores of all the three models for 15 subjects is ∼ 0.5, which can be treated as merely random guess. However, as the number of subjects increases, pretrained models gradually start performing better than NPT which, in fact, even with 120 subjects fails to learn from the data. We suspect that the reason why pre-trained models do not work for 15 subjects is that the data set is much different than HCP. The big age gap between subjects of HCP and OASIS is a major difference and 15 subjects are even not enough for pre-trained models."
1912.0313,"data, dataset",126,2022-05-13,0,"Figure 7. Datasets used for pre-training and classiﬁcation tasks. Healthy controls from the HCP [5] are used for encoder pre-training guided by data dynamics alone1. The pre-trained encoder is then used in downstream classiﬁcation tasks of 3 different diseases, 4 independently collected datasets, many of which contain data from a number of sites, and consist of populations with signiﬁcant age difference. The age distributions in the datasets have the following means, medians and standard deviations: HCP: 29.31, 29.00, 3.667; ABIDE: 17.04, 15.40, 7.29; COBRE: 37.96, 37, 12.90; FBIRN: 37.87, 38, 11.25; OASIS: 67.67, 68, 8.92."
1912.0313,"data, dataset",131,2022-05-13,0,"In this section we study the performance of our model on both, synthetic and real data. To compare and show the advantage of pre-training on large unrelated dataset we use three different kind of models — 1) FPT (Frozen Pre Encoder for simulation experiment consists of 4 1D convolutional layers with output features (32, 64, 128, 64), kernel sizes (4, 4, 3, 2) respectively, followed by ReLU [41] after each layer followed by a linear layer with 256 units. For real data experiments, we use 3 1D convolutional layers with output features (64, 128, 200), kernel sizes (4, 4, 3) re 3"
1912.0313,"data, dataset",136,2022-05-13,0,"Recent advances in unsupervised learning using selfsupervised methods with mutual information objectives have reduced the gap between supervised and unsupervised learning on standard computer vision classiﬁcation datasets [17– 21] and scaled pre-training to very deep convolutional networks (e.g., 50-layer ResNet). Furthermore, it inﬂuences the neuroimaging ﬁeld for classiﬁcation of progression to Alzheimer’s disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identiﬁcation [24]. Speciﬁcally, authors [19] have shown that contrastive based self-supervised pre-training can outperform supervised methods by a large margin in case of small data (e.g., 13 images per class in ImageNet [25])."
1912.0313,"data, dataset",164,2022-05-13,0,"Differentiating multivariate dynamic signals is a difﬁcult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m (cid:29) n problem. In this paper, we show that the source of the problem—signal dynamics—can be used to our advantage and noticeably improve classiﬁcation performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets."
1912.0313,"data, dataset",17,2022-05-13,0,data experiments. Refer to Figure 7 for the details of the datasets for disease classiﬁcation.
1912.0313,"data, dataset",217,2022-05-13,0,"subjects provides beneﬁts that transfer across datasets, collection sites, and multiple disease classiﬁcation with varying age gap. Learning dynamics of fMRI helps to improve classiﬁcation results for schizophrenia, autism, Alzheimer’s dieseases and speed up the convergence of the algorithm on small datasets, that otherwise do not provide reliable generalizations. Although the utility of these results is highly promising by itself, we conjecture that direct application to spatio-temporal data will warrant beneﬁts beyond improved classiﬁcation accuracy in the future work. Working with ICA components is a smaller and thus easier to handle space that exhibits all dynamics of the signal, in future we will move beyond ICA pre-processing and work with fMRI data directly. We expect model introspection to yield insight into the spatio-temporal biomarkers of schizophrenia. In future work, we will use the same analogously pre-trained encoder on datasets with various other mental disorders such as MCI and bipolar. We are optimistic about the outcome because the proposed pre-training is oblivious of the downstream use and is done in a manner quite different from the classiﬁer’s work. It may indeed be learning crucial information about dynamics that might contain important clues into the nature of mental disorders."
1912.0313,"data, dataset",221,2022-05-13,0,"Mental disorders manifest in behavior that is driven by disruptions in brain dynamics [1, 2]. Functional MRI captures the nuances of spatio-temporal dynamics that could potentially provide clues to the causes of mental disorders and enable early diagnosis. However, the obtained data for a single subject is of high dimensionality m and to be useful for learning, and statistical analysis, one needs to collect datasets with a large number of subjects n. Yet, for any kind of a disorder, demographics or other types of conditions, a single study is rarely able to amass datasets large enough to go out of the m (cid:29) n mode. Traditionally this is approached by handcrafting features [3] of much smaller dimension, effectively reducing m via dimensionality reduction. Often, the dynamics of brain function in these representations vanishes into proxy features such as correlation matrices of functional network connectivity (FNC) [4]. Efforts that pull together data from various studies and increase n do exist, but it is difﬁcult to generalize to study of smaller and more speciﬁc disease populations that cannot be shared to become a part of these pools or are too different from the data in them."
1912.0313,"data, dataset",71,2022-05-13,0,"points. We use two hold-out sets of size 100 each respectively for validation and test purpose. The remaining data are used for downstream training i.e., autism vs. HC classiﬁcation. Like COBRE dataset, with 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Refer to Figure 11 for the details of the experimental results."
1912.0313,"data, dataset",93,2022-05-13,0,"For experiments on the downstream tasks, a hold out is selected for testing and is never used through the training/validation phase. For each downstream task, the number of subjects used for supervised training is gradually increased within a range to observe the effectiveness of pretraining in downstream task with varied number of training subjects . For each experiment, 10 trials are performed to ensure random selection of training subjects and, in each case, the performance is evaluated on the hold out dataset (test data)."
1912.0313,"data, dataset",94,2022-05-13,0,"For brain data, each of the models (FPT, UFPT, NPT) yields the best results based on its respective gain value of Xavier [43] initialization used for biLSTM. To ﬁnd the best gain value for each model, 20 values in the range 0 − 1 are tried with an increment of 0.05. For each value, 10 experiments are performed and best value is chosen based on the results on validation dataset. Refer to Table 1 for more parametric details of the models."
1912.0313,"data, dataset",98,2022-05-13,0,"15306090120Number of Subjects Per Class0.30.40.50.60.70.80.9AUCSTDIM vs AutoencoderNPTUFPT_STDIMFPT_STDIMUFPT_AutoencoderFPT_AutoencoderLibriSpeech+Mel-SpectrogramCoffee ShopBackgroundSpeech Commands    (Cat)Coffee ShopBackground+Mel-SpectrogramPre-TrainingTraining00.511.52Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dB+0 dB1111TTTT/2051015Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dBResearch Excellence) [49] project, from the release 1.0 of ABIDE (Autism Brain Imaging Data Exchange3) [50] and from release 3.0 of OASIS (Open Access Series of Imaging Studies4) [51]. Written informed consent was obtained from all participants of each dataset under protocols approved by the institutional review board (IRB)."
1912.0313,"data, dataset provided",53,2022-05-13,1,"Data for Alzheimer’s was provided by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, UL1TR000448, R01AG043434, R01EB009352. AV-45 doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly."
1912.0313,"data, dataset provided",60,2022-05-13,1,"Data for healthy subjects was provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University."
1912.0313,"data, dataset provided",82,2022-05-13,1,"15255075100120Number of Subjects Per Class0.400.450.500.550.600.650.70AUCOASISFPTUFPTNPTAutism data was provided by ABIDE. We acknowledge primary support for the work by Adriana Di Martino provided by the (NIMH K23MH087770) and the Leon Levy Foundation and primary support for the work by Michael P. Milham and the INDI team was provided by gifts from Joseph P. Healy and the Stavros Niarchos Foundation to the Child Mind Institute, as well as by an NIMH award to MPM (NIMH R03MH096321)."
1912.0313,"data, dataset, publicly available, data available",158,2022-05-13,1,"Our goal is to enable the direct study of systems dynamics in smaller datasets. In the case of brain data it, in turn, can enable an analysis of brain function. In this paper, we show how one can achieve signiﬁcant improvement in classiﬁcation directly from dynamical data on small datasets by taking advantage of publicly available large but unrelated datasets. We demonstrate that it is possible to train a model in a self-supervised manner on dynamics of healthy control subjects from the Human Connectome Project (HCP) [5] and apply the pre-trained encoder to a completely different data collected across multiple sites from healthy controls and patients. We show that pre-training on dynamics allows the encoder to generalize across a number of datasets and a wide range of disorders: schizophrenia, autism, and Alzheimer’s disease. Importantly, we show that learnt dy 1"
1912.0313,"data, dataset, used dataset",70,2022-05-13,1,"Next, we apply the same unsupervised pre-training method to brain imagining data. For encoder pre-training, we use HCP [5] consortium dataset and apply the pre-trained encoder for further downstream tasks. We apply the same pre-trained encoder for three different types of diseases spanning four datasets to classify schizophrenia, autism and Alzheimer’s diseases. We use resting fMRI data for all brain"
1912.0313,database,11,2022-05-13,0,"L. Fei-Fei. Image Database. In CVPR09, 2009."
1912.0313,dataset,10,2022-05-13,0,"Learnt dynamics generalizes across tasks, datasets, and populations"
1912.0313,dataset,101,2022-05-13,0,"The dataset OASIS [51] has total 372 subjects with equal number (186) of HC and AZ patients. We use two holdout sets each of size 64 respectively for validation and test purpose. The remaining are used for supervised training. Unlike other datasets described earlier, it has only 120 time points though the number of non-noise componets is same (53) as other datasets. With 120 time points, we use six 53 × 20-sized non-overlapping windows for each subject. Refer to Figure 12 for the details of the experiments."
1912.0313,dataset,104,2022-05-13,0,"Figure 5. Left: For pre-training, we combine audio ﬁles from LibriSpeech dataset with background noise of coffee shop. T is the length of audio ﬁles which ranges from 1 to 20 seconds. Right: For training, we superimpose a T/2 length audio of word ""cat"" padded with T/2 zeros onto a background noise of coffee shop of length T (T = 2). For both pre-training and training, we calculate the mel-spectrogram of the combined audio ﬁles that results in a matrix of size components × time courses for each audio ﬁle."
1912.0313,dataset,111,2022-05-13,0,"First, we generate 50 VAR times series with size 10 × 20000. Then we split our dataset to 50 × 10 × 14000 samples for training, 50 × 10 × 4000 —for validation and 50 × 10 × 2000 — for testing. Using these samples, We pre-train an encoder to learn consecutive windows (positive examples) from the same VAR time series. As mentioned in Section 3.1.2, we also use autoencoder for pre-training the same encoder and show the effectiveness of ST-DIM to learn time-series dynamics in self-supervised manner. After pretraining, we use our pre-trained encoder for complete-time series classiﬁcation."
1912.0313,dataset,141,2022-05-13,0,"In most cases, due to practical reasons, researchers in brain imaging are constrained to work with small datasets. In addition, earlier work [26, 27] in brain imaging have been based on unsupervised methods to learn the dynamics and structure of the brain while supervised approaches are used to perform predictions at individual level. Such unsupervised methods include models as linear ICA [28], HMM framework [29]. Moreover, some other nonlinear approaches are also proposed to capture the dynamics. Examples include using Restricted Boltzman Machines (RBMs) [30], RNN modiﬁcation of ICA [31], and reconstructions by recurrent U-Net architecture [32]. In some cases, where dataset is very small, transfer learning as observed in some neuroimaging"
1912.0313,dataset,142,2022-05-13,0,"Let D = {(ut, vs) : 1 ≤ t, s ≤ N, t (cid:54)= s} be a dataset of pairs of values at time point t and s sampled from sequence with length N . Then D+ = {(ut, vs) : 1 ≤ t ≤ N − 1, s = t + 1} is called a dataset of positive pairs and D− = {(ut, vs) : 1 ≤ t, s ≤ N, s (cid:54)= t + 1} — of negative pairs. The dataset D+ refers to a joint distribution and D− — a marginal distribution. Eventually, the lower bound with InfoNCE estimator [37] If (D+) is deﬁned as:"
1912.0313,dataset,19,2022-05-13,0,[46] Pete Warden. Speech commands: A dataset for limited vocabulary speech recognition. 2018.
1912.0313,dataset,2,2022-05-13,0,4.4.1 Datasets
1912.0313,dataset,23,2022-05-13,1,"For schizophrenia classiﬁcation, we conduct experiments on two different datasets, FBIRN [48] and COBRE [49]. The"
1912.0313,dataset,30,2022-05-13,1,"Four datasets used in this study are collected from the FBIRN (Function Biomedical Informatics Research Network2) [48] project, from the COBRE (Center of Biomedical"
1912.0313,dataset,32,2022-05-13,0,"As we have demonstrated, self-supervised pre-training of a spatiotemporal encoder gives signiﬁcant improvement on the downstream tasks in both keyword detection and brain imaging datasets. Pre-training on fMRI of healthy"
1912.0313,dataset,34,2022-05-13,0,"Figure 11. AUC scores for all the three models on ABIDE dataset. Like experiments on FBIRN and COBRE, it is evident that the pre-trained models consistently perform better than NPT."
1912.0313,dataset,34,2022-05-13,0,"The dataset ABIDE has total 569 subjects, of which, 255 are HC and 314 are affected with autism. Like other datasets, each subject has 53 non-noise components with 140 time"
1912.0313,dataset,43,2022-05-13,0,"Our method is two fold. We ﬁrst pre-train our encoder on large unrelated dataset to learn improved representation of the latent factors, and then use the pre-trained encoder for downstream task. We explain both steps in the following sections."
1912.0313,dataset,49,2022-05-13,0,"Figure 10. AUC scores for all the three models on COBRE dataset. It is obvious that even with 15 subjects for training, FPT outperforms NPT noticeably, that is, the difference between two median AUC scores is remarkable ((cid:39) 0.15)."
1912.0313,dataset,51,2022-05-13,0,"Figure 9. AUC scores for all the three models (Refer to Figure 3) on FBIRN dataset. It is noticeable that even with only 15 subjects for supervised training, the median AUC scores of FPT and NPT differ by a large margin (10%)."
1912.0313,dataset,53,2022-05-13,0,"Figure 12. AUC scores for all the models on OASIS dataset. As we continue increasing the number of subjects, the pre-trained models start learning and thus improve their respective scores. However, notice that the NPT model even with 120 subjects didn’t signiﬁcantly improve its predictability."
1912.0313,dataset,56,2022-05-13,0,"Figure 4. Area Under Curve (AUC) scores for VAR vs. SVAR time-series classiﬁcation using ST-DIM and autoencoder based pre-training methods. ST-DIM based pre-training greatly improves the performance of downstream task with small datasets. On the other side, autoencoder based pre-training fails to learn dynamics and thus exhibits poor performance."
1912.0313,dataset,59,2022-05-13,0,"Out of all the available subjects, we select 416 which have large number of time points (N (cid:62) 20k). We use 300 subjects for training and 116 for validation. For pretraining, we use the algorithm as described in section 3.1.1 and achieve accuracy of ∼ 0.95 on the validation dataset."
1912.0313,dataset,61,2022-05-13,0,15255075100Number of Subjects Per Class0.500.550.600.650.700.750.80AUCFBIRNFPTUFPTNPT152540Number of Subjects Per Class0.40.50.60.70.8AUCCOBREFPTUFPTNPT15255075100150Number of Subjects Per Class0.350.400.450.500.550.600.650.70AUCABIDEFPTUFPTNPT7.29 years. Refer to Figure 7 for the demographic information of all the datasets. The dissimilarity in the age range is supposed to cause signiﬁcant difference between these two datasets as the structure of brain and thought process of children is obviously different than adults.
1912.0313,dataset,69,2022-05-13,0,"For each dataset, 100 ICA components as shown in Figure 8 are acquired using the same procedure described in [52]. However, only 53 non-noise components as determined per slice (time point) are used in pre-training of encoder and on downstream task. For experiments, including both pretraining and classiﬁcation the fMRI sequence is divided into windows of 20 time points."
1912.0313,dataset,89,2022-05-13,0,"Trained): The pre-trained encoder is not further trained on the dataset of downstream task, 2) UFPT (Unfrozen PreTrained): The pre-trained encoder is further trained on the dataset of downstream task and 3) NPT (Not Pre-trained): The encoder is not pre-trained at all and only trained on the small dataset of downstream task. The models are shown in Figure 3. In each experiment, we compare all three models to demonstrate the effectiveness of unsupervised pre-training."
1912.0313,dataset,94,2022-05-13,0,"To show the broad implications of unsupervised pretraining, we ﬁrst apply it to a simple problem of keyword detection in audio ﬁles. We choose this problem as it has many practical applications (e.g., virtual assistants in smart phones, robots). We use LibriSpeech ASR corpus [45] for pre-training and Speech Commands Dataset [46] for supervised training. The audio ﬁles of both datasets are combined with a background noise of coffee shop collected from [47] to make pre-training and classiﬁcation harder."
1912.0313,"dataset, used dataset",108,2022-05-13,0,"As seen in the ﬁgure, same pre-trained encoder performs reasonably better than NPT for autism vs. HC classiﬁcation and thus reinforces our hypothesis that unsupervised pretraining learns signal dynamics useful for downstream tasks. Note the difference between age ranges of ABIDE and HCP datasets. The age range of ABIDE is much lower than that of HCP dataset used for pre-training. HCP dataset contains subjects of different ages with means 30.01 and 28.48, medians 30 and 28, and standard deviations 3.522 and 3.665 years respectively for female and male, whereas ABIDE dataset has overall mean 17.04, median 15.40 and standard deviation"
1912.09621,"code, code available",88,2022-04-21,0,"Images from digital microscopy are captured under different illumination conditions. To aid our classification architecture, we preprocess the images using a color constancy technique [10, 11] to maintain the color contrast across all images. Later, we resize the images to match with the ResNet and GoogLeNet architectures. Figures 4 and 5 present the results obtained using the color constancy preprocessing for different images [10]. The code utilized for color constancy is available at [11]."
1912.09621,data,156,2022-05-13,0,"Computer-aided detection has been a research area attracting great interest in the past decade. Machine learning algorithms have been utilized extensively for this application as they provide a valuable second opinion to the doctors. Despite several machine learning models being available for medical imaging applications, not many have been implemented in the real-world due to the uninterpretable nature of the decisions made by the network. In this paper, we investigate the results provided by deep neural networks for the detection of malaria, diabetic retinopathy, brain tumor, and tuberculosis in different imaging modalities. We visualize the class activation mappings for all the applications in order to enhance the understanding of these networks. This type of visualization, along with the corresponding network performance metrics, would aid the data science experts in better understanding of their models as well as assisting doctors in their decision-making process."
1912.09621,data,206,2022-05-13,0,"This paper describes a study that supports the apprehension of the results predicted by deep neural networks applied towards medical imaging analysis. Several machine learning and deep learning architectures have been proposed in the literature for automated Computer-Aided Detection (CAD) tools for various applications [1-4]. In the past few years, deep learning networks have been used widely in medical imaging applications [1, 3]. Residual Networks (ResNet) [5] and GoogLeNet [6] are some of the most popular networks used in this field. The availability of a vast variety of networks raises the question of choosing the optimal network for a given disease/condition. In a data science perspective, optimal results could be measured in terms of overall accuracy, confusion matrix, precision, recall, Receiver Operating Characteristics (ROC) curve, or any other performance metric. However, these optimal results might not be satisfactory for the doctors if the results are not interpretable. Determining the Region of Interest (ROI) that contributed to the decision making of the network will enhance the understanding for both data science experts and clinicians."
1912.09621,"data available, dataset",3,2022-05-13,0,5.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,2.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,3.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,2022-05-13,0,4.1. Dataset
1912.09621,"data, dataset",213,2022-05-13,0,"In addition, we have presented a comprehensive study of these algorithms based on their CAM results. This type of CAD system would let the medical expert analyst choose the algorithm based on their discretion in terms of CAM results, overall accuracy, AUC, or any other performance metric along with computation time and memory consumption. CAM results could be utilized by data science experts to further optimize their respective models in terms of architecture and/or preprocessing techniques. This type of CAM study would also assist the researchers in understanding the discriminative regions determined by various architectures in different imaging modalities. For instance, if an architecture achieves a high CAD detection accuracy for a particular dataset but the discriminative region determined using CAM is present in an unrelated area, performance metrics might mislead both data science researchers and medical expert analysts. Hence, CAM visualization would provide the necessary documentation for the medical expert analysts to enhance the trust in CAD system. This research can be further extended by studying the ROI determined by CAM by changing different hyper-parameters and how they evolve throughout the epochs. Another way to extend this research is by fusing CAM regions determined using different architectures."
1912.09621,"data, dataset",59,2022-05-13,0,"For this application, we perform a hold-out validation study. We split the dataset into groups of 80% and 20% for training and testing respectively. We utilize a subset of 10% from our training data for validation purpose in order to fine-tune our hyperparameters. Table 1 presents the distribution of the dataset."
1912.09621,"data, dataset, publicly available",98,2022-05-13,1,"To maintain the homogeneity across all these applications, we solely study the performance of transfer learning approaches using GoogLeNet and ResNet. Figure 1 presents the top-level block diagram of the transfer learning methodology adopted in this study. We implement these techniques for publicly available datasets thereby setting a new benchmark for each application. Results presented for the publicly available datasets would grant the capability for researchers to replicate and enhance them further. This type of analysis would assist the doctors and data science experts in selecting the optimal model of their choice."
1912.09621,dataset,115,2022-05-13,0,"For this application, due to the limited availability of images, we perform a 10-fold validation study. We believe 10-fold validation would give a better estimate of our performance. We train 10 different networks based on 9 folds and test on the remaining fold in each iteration. For each fold, we train and tune our hyper-parameters solely based on the images from training fold. We make sure to exclude the testing fold in any manner to conduct a rigorous study. Note that we utilize the same set of cases in each fold for the architectures implemented. Overall distribution of the dataset is presented in Table 5."
1912.09621,dataset,135,2022-05-13,0,"Figures 21 and 22 present the CAM results obtained for two different cases from the brain tumor dataset. Figure 21 presents the results for the case marked as ‘positive brain tumor’ by the trained clinician and our algorithm accurately predicts the same. The discriminative region is near the tumor portion which would help the doctors pinpoint important features, spatially. Figure 22 presents the results for the case marked as ‘negative brain tumor’ and the visualization behind algorithm’s prediction. CAM visualization for 26 (one test fold) different cases using our approach is available at [17]. This type of automated CAD technology for brain tumor detection would assist the doctors in providing a valuable second opinion and enhancing their workflow."
1912.09621,dataset,147,2022-05-13,0,"In this section, we study the CAM results obtained using GoogLeNet and ResNet. Figures 8 and 9 present the CAM results obtained for two different cases from the malaria dataset using our proposed approach. Figure 8 presents the results for the case marked as parasitized by the expert reader, and our algorithm not only accurately predicts the same but also presents the discriminative region that contributed the most to its decision. The ROI is around the red spot containing plasmodium. Figure 9 presents the results for the case marked as uninfected. CAM visualization for 100 different cases using our presented approach is available at [12]. Typically, ResNet CAM converges to a smaller ROI in comparison to GoogLeNet. This type of automated CAD technology for malaria detection would assist the microscopists and enhance their workflow."
1912.09621,dataset,155,2022-05-13,0,"Figures 14 and 15 present the CAM results obtained for two different test cases from the APTOS dataset using our proposed approach. Figure 14 presents the results for a case marked as ‘positive DR’ by the expert clinicians. It is also interesting to note that CAM results presented by GoogLeNet and ResNet for DR detection have minimal intersection despite exhibiting similar performance, which could affect an expert clinician’s choice of network. Figure 15 presents the results for a case marked as ‘negative DR’ by an expert clinician and the discriminative region for this case is near the retinal portion. CAM visualization for 60 different cases using our presented approach is available at [15]. This type of automated CAD technology for DR detection could be implemented for immediate solutions and could be applied in places with scarcity of such expert clinicians."
1912.09621,dataset,3,2022-05-13,0,Type of Dataset
1912.09621,dataset,49,2022-05-13,0,"Similar to brain tumor detection, due to the limited availability of images, we perform a 10-fold validation study. Overall distribution of the dataset is presented in Table 7. There is no additional preprocessing except converting these images into the input size for the network."
1912.09621,dataset,6,2022-05-13,0,Table 7: Tuberculosis dataset distribution
1912.09621,dataset,66,2022-05-13,0,"Similar to malaria detection (Section 2), we perform a hold-out validation study. We split the dataset into groups of 72%, 8%, and 20% for training, validation, and testing respectively. Table 3 presents the distribution of each dataset. There is no preprocessing except converting these images into the input size of the network."
1912.09621,dataset,67,2022-05-13,0,"The remainder of this paper is organized as follows. Sections 2-5 present the results obtained for CAD of malaria, DR, brain tumor, and tuberculosis respectively. In each of these sections, we describe the dataset along with the experimental results obtained in terms of both performance metrics and CAM results. Finally, discussions and conclusions are offered in Section 6."
1912.09621,dataset,7,2022-05-13,0,Table 1: Malaria dataset distribution.
1912.09621,dataset,7,2022-05-13,0,Table 3: DR dataset distribution.
1912.09621,dataset,8,2022-05-13,0,Table 5: Brain Tumor dataset distribution.
1912.09621,dataset,92,2022-05-13,0,"MRI scans contain text information for some cases, which are not essential for classification and might mislead our deep neural networks. Hence, we preprocess MRI scans by cropping ROI of brain and removing any additional text from the image using simple morphological operations. In addition, we perform histogram equalization to enhance and maintain the contrast across the dataset. Later, we resize the images to match with the input of ResNet and GoogLeNet architectures. Figure 18 presents the results obtained using these preprocessing techniques."
1912.09621,"dataset, data https",17,2022-05-13,1,"[16] Brain Tumor Dataset, https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumordetection, Accessed December 7, 2019."
1912.09621,"dataset, data https",18,2022-05-13,1,"[13] Kaggle Diabetic Retinopathy Dataset, https://www.kaggle.com/c/diabetic-retinopathydetection/overview, Accessed December 7, 2019."
1912.09621,"dataset, data https",21,2022-05-13,1,"[8] Malaria Dataset, National Institutes of Health, https://ceb.nlm.nih.gov/repositories/malariadatasets/ , Accessed December 8, 2019."
1912.09621,"dataset, publicly available, dataset provided, used dataset",48,2022-05-13,1,We make use of the publicly available Shenzhen dataset [19] provided for the classification of chest radiographs. This dataset contains a total of 662 images. Figures 23 and 24 present sample images marked as ‘Normal’ and ‘Tuberculosis’ by radiologists.
1912.09621,"dataset, publicly available, used dataset",136,2022-05-13,1,"We make use of a publicly available dataset provided by the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 on Kaggle [14] to detect DR in retinal images. In this dataset, 3662 retinal images are graded by expert clinicians at Aravind Eye Hospital, India into 5 different categories: (i) Negative DR, (ii) Mild DR, (iii) Moderate DR, (iv) Proliferative DR, and (v) Severe DR. In this research, we solely focus on the detection of DR, hence, we merge all the mild, moderate, proliferative and severe cases into a single category ‘positive DR’. Figures 10 and 11 present sample images marked in different categories by expert clinicians."
1912.09621,"dataset, publicly available, used dataset",68,2022-05-13,1,We make use of a publicly available dataset provided on Kaggle [16] for the classification of MRI scans. The dataset contains a total of 253 images classified into two different categories ‘positive’ and ‘negative’ brain tumor. Figures 16 and 17 present sample images marked as ‘positive brain tumor’ and ‘negative brain tumor’ by trained clinicians.
1912.09621,"dataset, publicly available, used dataset",84,2022-05-13,1,"We make use of a publicly available dataset provided by the National Institutes of Health (NIH) for the classification of cell images [8, 9]. Any cell image that contains plasmodium is marked as ‘parasitized’ by expert analysts and ‘uninfected’ otherwise. The dataset contains a total of 27,558 images with equal distribution of parasitized and uninfected cells. Figures 2 and 3 present sample images marked as parasitized and uninfected by expert readers."
2002.00512,code,55,2022-05-13,0,"[32] M. Torrent, F. Jollet, F. Bottin, G. Zérah;, and X. Gonze, Implementation of the projector augmentedwave method in the ABINIT code: Application to the study of iron under pressure, Computational Materials Science, 42 (2008), pp. 337 – 351."
2002.00512,code,94,2022-05-13,0,"The numerical results using a Julia [1] homemade code are summarized in the following ﬁgures with Z = 3, R = 1 and L = 5. The atomic PAW function φk are the eigenfunctions of the hydrogenoid atom. For the pseudo atomic function (cid:101)φk, continuity of the function and of the ﬁrst four derivatives are enforced (i.e. d = 5). The lowest eigenvalue is computed using a conjugate-gradient algorithm stopped when the norm of the residual is less than 10−5."
2002.00512,data,43,2022-05-13,0,"[17] F. Jollet, M. Torrent, and N. Holzwarth, Generation of Projector Augmented-Wave atomic data: A 71 element validated table in the XML format, Computer Physics Communications, 185 (2014), pp. 1246–1254."
2002.04279,code,15,2022-05-13,0,"● Software that checks text-based documents, source code, or both (Chowdhury &"
2002.04279,data,126,2022-05-13,0,"Since the analysis and interpretation of the data are quite sensitive, we approached this period with the utmost care. As suggested by Guba and Lincoln (1989), member check is an effective technique for establishing the trustworthiness criteria in qualitative studies. Therefore, having analyzed and reported the data, we sent a preprint of the results to the vendors. Team members closely evaluated the issues raised by the vendors. Not all of them were able to be addressed in this paper, but as many as possible were incorporated. Because of the rigorous efforts to establish the validity of the results and the reliability of the study in this process, this study was further delayed."
2002.04279,data,18,2022-05-13,0,"Journal of Data Mining and Knowledge Discovery, 2(2), 50–53. doi: 10.11648/j.ajdmkd.20170202.12"
2002.04279,data,21,2022-05-13,0,This research did not receive any external funding. HTW Berlin provided funding for openly publishing the data and materials.
2002.04279,data,36,2022-05-13,0,"5. Because of European data privacy laws, for higher education institutions in the EU it must be certain that the companies are only using servers in the EU if they are storing material."
2002.04279,data,5,2022-05-13,0,Availability of data and materials
2002.04279,data,64,2022-05-13,0,"Sorokina, D., Gehrke, J., Warner, S., & Ginsparg, P. (2006, December). Plagiarism detection in arXiv. In J. Liu & B. W. Wah (Eds.), Proceedings of the Sixth International Conference on Data Mining (ICDM'06). Hong Kong. (pp. 1070–1075). doi: 10.1109/ICDM.2006.147"
2002.04279,"data, data available",125,2022-05-13,0,"Our testing took place between Nov 2018 and May 2019. During this time, we tested both coverage and usability. An additional test of multi-source document took place between August and November 2019. Since the present research did not benefit from any funding, the researchers were expected to fulfill their institutional workloads during the research period. Considering the size of the project team from various countries, we could make significant progress only during semester breaks, which explains the length of the testing process. It should be noted that we tested what the systems offered at the time of data collection. We used features that were allowed by the access given to us by the vendors."
2002.04279,"data, publicly available, data available",14,2022-05-13,1,Data and materials used in this project are publicly available from http://www.academicintegrity.eu/wp/wg-testing/
2002.04279,database,105,2022-05-13,0,"As has been shown in other investigations (Weber-Wulff et al., 2013) translation plagiarism is very seldom picked up by software systems. The worst performance of the systems in this test was indeed the translation plagiarism, with one notable exception—Akademia. This system is the only one that performs semantic analysis and allows users to choose the translation language. Unfortunately, their database—with respect to the languages of our testing—is much smaller than the database of other systems. However, the performance drop between copypaste and translation plagiarism is much smaller for Akademia than for the other systems."
2002.04279,database,107,2022-05-13,0,"PlagScan presents itself as a plagiarism checker. It is operated by the German company PlagScan GmbH and was launched in 2009. They state that they have more than 1,500 organizations as customers. Although they focus on higher education, high schools, and businesses, PlagScan is also available for single users. They search the internet using MS Bing, published academic articles, their so-called “Plagiarism Prevention Pool”, and optionally a customer’s own database. PlagScan offers multiple pricing plans for each type of customer, there are apparently also now options for a free trial."
2002.04279,database,108,2022-05-13,0,"One aspect of Wikipedia sources that is not adequately addressed by the text-matching software systems is the proliferation of Wikipedia copies on the internet. As discussed in Weber-Wulff et al. (2013), this can lead to the appearance of many smallish text matches instead of one large one. In particular, this can happen if the copy of the ever-changing Wikipedia in the database of the software system is relatively old and the copies on the internet are from newer versions. A careless teacher may draw false conclusions if they focus only on the quantity of Wikipedia similarities in the report."
2002.04279,database,13,2022-05-13,0,"2. Text-matching systems that maintain a database of potential sources, employ"
2002.04279,database,139,2022-05-13,0,"The Croatian researchers Birkić, Celjak, Cundeković, and Rako (2016) tested four tools that are widely used in Europe and have the possibility to be used at the national and institutional level. They compared such criteria as the existence of an API (application programming interface) and the possibility to integrate it as a plug-in for learning management systems, database scope, size of the user community, and other criteria. The researchers tested the tools using two papers for each type of submission: journal articles, conference papers, master’s and doctoral theses, and student papers. However, they did not include different types of plagiarism and evaluated the checking process with a focus on quote recognition, tool limitations, and interface intuitiveness."
2002.04279,database,151,2022-05-13,0,"Turnitin was founded in 1999 by four students and grew to be an internationally known company. In 2014, they acquired the Dutch system Ephorus and “joined forces” (Ephorus, 2015). In 2019 they themselves were taken over by a US investment company, Advance (Turnitin, 2019). With a focus on institutional users only, they are used by 15,000 institutions in 150 countries. Turnitin uses its own crawler to search the web including also an archive of all previously indexed web pages (Turnitin, n.d.). Turnitin further compares the texts against published academic articles, as well as their own database of all assignments which have ever been submitted to the system, and optionally institutional databases. They are also developing many additional software tools for educators to use in teaching and giving feedback."
2002.04279,database,156,2022-05-13,0,"Luparenko (2014) tested 22 tools that were selected as popular ones based on an analysis of scientific literature and web sources. She considered many criteria related to functional specification (such as type, availability of free trial mode, need for mandatory registration at a website, number of users that have access to the program, database, acceptable file formats, etc.) and also checked the performance of the tools using one scientific paper in the Ukrainian language and another one in English. Moreover, the checking was done using three different methods: entering the text in the field of website, uploading a file, and submitting the URL of the article. She measured the checking time and evaluated the quality of the report provided by tools, as well as reported the percentage of unique text found in each of the articles."
2002.04279,database,166,2022-05-13,0,"As for the general testing, the results are highly consistent with the Wikipedia results which contributes the validity of the single-source and multi-source testing. Again, in single-source documents, Urkund obtained the highest score, while PlagAware is the best performing system in multi-source documents. Dupli Checker, DPV and intihal.net obtained the least scores in both categories. Most of the systems demonstrated better performance for multi-source documents than for single-source ones. This is most probably explained by the chances the systems had for having access to a source. If one source was missing in the tool’s database, it had no chance to identify the text match. The use of multiple sources gave the tools multiple chances of identifying at least one of the sources. This points out quite clearly the issue of false negatives: even if a text-matching tool does not identify a source, the text can still be plagiarized."
2002.04279,database,249,2022-05-13,0,"Viper presents itself as a plagiarism checker. It was founded in 2007. Viper focuses on all types of customers; the pricing is based on the pay-as-you-go principle. Currently, it is owned by All Answers Limited (2019), which according to the information at the website, gives an impression of an essay mill. It is interesting to see the progress in the way Viper uses the uploaded content on their “Terms and conditions” page. In 2016 the page stated ""[w]hen you scan a document, you agree that 9 months after completion of your scan, we will automatically upload your essay to our student essays database which will appear on one of our network of websites so that other students may use it to help them write their own essays"" (Viper, 2016). The time span was shortened to 3 months some time afterwards (Viper, 2019a). These paragraphs have been removed from the current version of the page (Viper, 2019b). On a different page, it is noted that ""[w]hen you scan your work for plagiarism using Viper Premium it will never be published on any of our study sites"" (Viper, 2019c). In e-mail communication, Viper claims that they are not using any essay without the author's explicit consent."
2002.04279,database,25,2022-05-13,0,Innovation Centre Kosovo (2018). An Albanian Academic Database and a Qualitative AntiPlagiarism System created by Akademia. Retrieved from https://ickosovo.com/news/post/an-albanian-academic-database-and-a-qualitative-antiplagiarism-system-crea
2002.04279,database,90,2022-05-13,0,"Source-based coverage testing was made using four types of sources; Wikipedia, openaccess papers, a student thesis and online articles. For many students, Wikipedia is the starting point for research (Howard & Davies, 2009), and thus can be regarded as one of the primary sources for plagiarists. Since a Wikipedia database is freely available, it is expected that Wikipedia texts should easily be identifiable. Testing the tools with Wikipedia texts demonstrates the fundamental ability to catch text matches."
2002.04279,database,92,2022-05-13,0,"Copyscape declares itself to be a plagiarism checker. The primary aim is to provide a tool for owners of websites to check if their original content was not used by others. They also provide a service of regular checks and email alerts. Copyscape, which started in 2004 (Greenspan, 2019), is operated by a private company, Indigo Stream Technologies Ltd., which is apparently based in Gibraltar. It does not have its own database but uses Google services to crawl the web."
2002.04279,download,138,2022-05-13,0,"The presentation and understandability of the results reported by the systems were evaluated in a second usability criteria group. Since the systems cannot determine plagiarism, the results must be examined by one or more persons in order to determine if plagiarism is present and a sanction warranted. It must be necessary to download the result reports and to be able to locate them again in the system. Some systems rename the documents, assigning internal numbering to them, which makes it extremely difficult to find the report again. Many systems have different formats for online and downloadable reports. It would be useful for the report review if the system kept the original formatting and page numbers of the document being analyzed in order to ease the load of evaluation."
2002.04279,download,88,2022-05-13,0,"None of the systems was able to get the highest score in the usability group related to the test results. Two systems (PlagScan and Urkund) support almost all features, but six systems support half or fewer features. The most supported features are the possibility to download result reports and highlighting matched passages in the online report. Less supported features are a side-by-side demonstration of evidence in the downloaded report and in the online report, as well as keeping document formatting."
2002.04279,publicly available,11,2022-05-13,0,4–5 pages from a publicly available source in the given language
2002.04279,publicly available,125,2022-05-13,0,"Table 2 shows the aggregated results of the language comparisons based on the language sets. It can be seen that most of the systems performed better for English, Italian, Spanish, and German, whereas the results for Latvian, Slovak, Czech, and Turkish languages are poorer in general. The only system which found a Czech student thesis from 2010 which is publicly available from a university webpage, was StrikePlagiarism.com. The Slovak paper in an open-access journal was not found by any of the systems. Urkund was the only system that found an open-access book in Turkish. It is worth noting that a Turkish system, intihal.net, did not find this Turkish source."
2002.04279,publicly available,16,2022-05-13,0,● 4–5 pages from any publicly available source in a given language with 1/3 copy &
2005.10539,data,109,2022-05-13,0,"The second approach was intended to increase the coordination between each instrument, by combining time (i. e., horizontal) and harmony (i. e., vertically) information. This was achieved by changing the data extraction phase from obtaining separately each complete instrument part to obtain every instrument part at every beat. This allowed us to train a set of instruments at the same time from a concrete set of symphonies. This way, the generated parts present a considerable increment of coordination and it is easier to differentiate each musical phrase, as each instrument part respects or accompanies the others."
2005.10539,data,131,2022-05-13,0,"The goal of this work is to generate music, based on Beethoven’s compositional model, obtaining the conductor’s score with all the orchestra instrument’s parts. Two approaches to the goal were established; ﬁrstly, the system was trained with each instrument individually, to generate all the different instrument’s parts, and then put them all together in a conductor’s score. Nevertheless training each part individually lead to a lack of coordination, so a new approach was addressed. This second approach consisted in training the system with information from different instruments at the same time, extracting the data vertically, to maintain the harmony (vertical) and time (horizontal) information."
2005.10539,data,135,2022-05-13,0,"Introduction Romantic composer Ludwig van Beethoven wrote his Symphonies from 1799 to 1824, when he ﬁnished the No. 9 (Cooper 2000). Although there is no constancy of the existence of the 10th Symphony score, there exists some manuscripts found in Beethoven’s house after his death that are thought to be part of the upcoming Symphony. In 1988 Barry Cooper tried to ﬁnish it, building from 50 of those fragments the ﬁrst movement of the Symphony. Those manuscripts are kept in the museum dedicated to his life in his natal city, Bonn, although they can be seen online 1. The public manuscript is not easy to read and understand, so that existing data will not be used in this paper."
2005.10539,data,138,2022-05-13,0,"Ludwig van Beethoven composed his symphonies between 1799 and 1825, when he was writing his Tenth symphony. As we dispose of a great amount of data belonging to his work, the purpose of this paper is to investigate the possibility of extracting patterns on his compositional model from symbolic data and generate what would have been his last symphony, the Tenth. A neural network model has been built based on the Long Short-Therm Memory (LSTM) neural networks. After training the model, the generated music has been analysed by comparing the input data with the results, and establishing differences between the generated outputs based on the training data used to obtain them. The structure of the outputs strongly depends on the symphonies used to train the network."
2005.10539,data,147,2022-05-13,0,"Since the main goal of this paper is to obtain a score including every orchestra instrument’s part, the input ﬁles format were changed to symbolic data in mxl. This extension refers to a compressed music score, which Music21 easily processes. Mxl ﬁles are the compressed format of the so called MusicXML (Good 2001). In order to represent the output of the training, i.e. the weights of the different notes and durations, the model also returns a HDF5 ﬁle, i. e., Hierarchical Data Format version 5, commonly used to store big quantities of data. After the prediction process, given the obtained weights, Music21 allows us to generate the ﬁnal output in MIDI or MusicXML, formats accepted by Musescore (so the score can be visualised and played)."
2005.10539,data,153,2022-05-13,0,"Our ﬁnal network is composed 3 different types of layers. The most relevant ones are the LSTM layers, which take the sequences and return new ones. Then, the Dropout layers prevent overﬁtting, ignoring randomly selected neurons during the training, setting those inputs to 0. The Dense (Density) layer serves as a full connection mechanism. This layer is the last one, so the system returns the same number of outputs as the different numbers of tuples (note name, note duration) the input data had. Finally, the activation function used for every layer is set, and it determines how each node’s output is represented. In this case, the softmax function (i. e., linear activation) is used, allowing the output to be interpreted as a probability between 0 and 1."
2005.10539,data,160,2022-05-13,0,"Technical background Deep learning: LSTM Networks Included in the ﬁeld of Machine Learning, Deep Learning involves the use of artiﬁcial neural networks (Gulli and Pal 2017). There exists several types of neural networks, such as Deep Neural, Deep Brief and Recurrent Neural Networks (RNN). In this paper we work with the last ones, since we need to process sequential data, assuming that each event depends on previous ones. The most accurate RNN variant is the LSTM. As proved with Figure 1, we need the memory that this type of networks own. On it we ﬁnd the sequence F - F - F, a predictor without memory would return another F, although by learning from the notes before, it can extract that after three equal notes, it is probable that the upcoming note is two tones below the last one."
2005.10539,data,247,2022-05-13,0,"Proposed in 1997, LSTM neural networks can learn longterm dependencies, improving the cells or neurons in the RNN graph. They have the ability to connect previous knowledge to a present task. Each cell has memory, and it decides to store or forget a data based on a given priority (i. e., represented as weights), assigned by the algorithm after the learning process. Figure 3 shows a LSTM cell or neuron. The top line represents the ﬂow of the cell state, which can be altered up to three times. The ﬁrst layer, sigmoid (σ), takes information from the previous state and determines if it is useful or not, returning a value between 0 and 1. As it is shown with the vertical arrow, it directly affects to the ﬂow of the cell state. The second layer is composed of the combination of the sigmoid (σ) and tanh functions, which chooses the data to be updated from the previous state, and creates a vector of candidate values to be added to the current cell state. The ﬁnal sigmoid (σ) layer determines the output, by deciding which parts of the state are more relevant. Those will be combined with a tanh function, converting the current state into values between 1 and -1 (Gulli and Pal 2017)."
2005.10539,data,328,2022-05-13,0,"References [Biles 1994] Biles, J. 1994. Genjam: A genetic algorithm In Proc. of the International for generating jazz solos. Computer Music Conference, 131–137. Aarhus, Denmark: ICMC. [Cohen 1995] Cohen, H. 1995. The further exploits of aaron, painter. Stanford Humanities Review 4(2):141–158. [Colton 2012] Colton, S. 2012. The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04."
2005.10539,data,344,2022-05-13,0,"The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04. [Graves, Mohamed, and Hinton 2013] Graves, A.; Mohamed, A.; and Hinton, G. E. 2013. Speech recognition the with deep recurrent neural networks. International conference on acoustics, speech and signal processing, 6645–6649. Vancouver, Canada: IEEE. [Gulli and Pal 2017] Gulli, A., and Pal, S. 2017. Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow. Birmingham, UK: Packt Publishing Ltd."
2005.10539,data,347,2022-05-13,0,"2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor? AI Magazine 30(3):71–71. [Schwanauer and Levitt 1993] Schwanauer, S., and Levitt, D. 1993. Musact: A connectionist model of musical harmony. In Machine Models of Music. Cambridge, MS, USA: The MIT Press. 497–510. [Tao Li 2011] Tao Li, Mitsunori Ogihara, G. T. 2011. Music Data Mining. Boca Ratn, FL, USA: CRC Press. [Thiemel 2001] Thiemel, M. 2001. Dynamics. New Grove online."
2005.10539,data,35,2022-05-13,0,"Once the model is built and the input and output data are ready, it gets trained, generating an hdf5 ﬁle containing the weights (i. e., input notes’ priorities)."
2005.10539,data,35,2022-05-13,0,"Results The system output differs from the information given to the training, although once with the same trained data, the system predicts the same score, which denotes a lack of variability."
2005.10539,data,350,2022-05-13,0,"[Haynes and Cooke 2001] Haynes, B., and Cooke, P. 2001. Pitch. New Grove online. [Hiley 2001] Hiley, D. 2001. Clef (i). New Grove online. [Hiller and Isaacson 1958] Hiller, Jr., L. A., and Isaacson, L. M. 1958. Musical composition with a high-speed digital computer. J. Audio Eng. Soc 6(3):154–160. [Liang and Gotham 2017] Liang, F. T., and Gotham, M. e. a. 2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor?"
2005.10539,data,43,2022-05-13,0,"Training Finally, we can generate the training data (i. e., sequence input and output). By establishing a certain sequence length, the output for each input sequence will be the ﬁrst note that comes after that sequence."
2005.10539,data,76,2022-05-13,0,"For example, setting a sequence length equal to two, the ﬁrst stage of the system’s work ﬂow (i. e., data extraction) for the Figure 5 input would be the shown in Table 1. It is important to take into account that in case of establishing a big sequence length, the model may generalise, while setting a small sequence length may lead to an overlearning problem."
2005.10539,data,84,2022-05-13,0,"Data representation Several ways of representing the Beethoven Symphonies’ scores have been studied for this paper. Firstly, we used MIDI ﬁles as an input for our system, as it is a data ﬁle which contains information about the sounds: what note is played, when, and how long or loud. Figure 4 shows the problem we boarded with the MIDI input. Music21 was unable to differentiate between the different string instrument’s MIDI channels."
2005.10539,"data, dataset",137,2022-05-13,0,"This paper is structured as follows. Previous work on using Artiﬁcial Intelligence in computational creativity and concretely in music generation is exposed in the State of the art section. After that, musical deﬁnitions needed to understand requirements, limitations and characteristics found in the results are introduced. Later, the work developed for this paper is explained in detail, presenting the Deep Learning technique used, the needed toolkits, and how the data was represented. Then, the Music generation subsection is divided in: dataset creation, training, and prediction. The results section is focused on explaining the reason why the system returns a certain output when trained with a speciﬁc set of symphonies. The conclusions and future work are described in the last sections."
2005.10539,"data, dataset, python",314,2022-05-13,0,"Dataset creation As previously mentioned, the Beethoven symphonies have been converted to an mxl ﬁle, which constitutes the dataset or corpus that we have used to obtain the desired results. Also, the instrument or instruments with which the system works has to be established, so the Python module music21 can divide the mxl score into all the present instruments, and take only the desired parts. This way, in the ﬁrst approach, where the goal is to obtain each instrument’s part individually, the note names and durations are stored in an independent ﬁle, being the different tuples of note names and durations the training data. Nevertheless, as the second approach trains the model with a set of chosen instruments at the same time, we need to store, besides the note name and duration, the offset (i. e., time data relating to the moment in which the note is being played regarding the score) and the name of the instrument that plays it. The offset information will be used to sort the data. After making sure that the events are sorted in a time-line as they are in the original score, it can be removed from the dataset, in order to reduce the data dimensionality and to avoid an overlearning problem in the model. This way, the training data will be composed of the different tuples of note names, note durations and the instrument’s name playing it (introduced in the second approach). At this point, a dictionary to encode each data tuple as a number is created, so the neural network can work with it. This dictionary will be also used for the decoding phase, after the prediction."
2005.10539,"data, python, open-source",129,2022-05-13,0,"This project has been developed in Python. Data extracting and processing from the scores has been performed using the python’s library Music21 (Cuthbert and Ariza 2010), which allows parsing and generating scores in different formats. Furthermore, every musical action and representation that we needed to perform, was made possible using that library. For the Deep Learning engine we have used Keras 3 (Gulli and Pal 2017). Finally, in order to manage the score formats, Musescore 4 (i. e., open source program available for every platform) brought us the possibility to import and export the symphonies, so we could see the score and listen to it at the same time."
2005.10539,dataset,114,2022-05-13,0,"Prediction For this task, the network input is generated again, as in the previous process (see Table 1). Since it needs to work over the same model, it is created again, with the same parameters, but now, instead of training the model, it loads the generated weights from the previous process (i. e., the hdf5 ﬁle). It is important at this point that the network input shapes and the loaded weights have the same dimensions. Once the model is ready, the encoding dictionary built during the dataset creation phase is inverted, for decoding the prediction results."
2005.10539,python,123,2022-05-13,0,"In case of the input, reshaping into a 3 dimension matrix is needed so it is compatible with the LSTM layers, using Python’s numpy module. The ﬁrst dimension or shape of the network is the number of unique different sequences (i. e., sequence in in Table 1) obtained in the last step, the second one is the previously established sequence length and ﬁnally the last dimension is forced to be 1, so it has just one input information per sequence length. After that, the software normalises the input into sequential values, from 0 to 1. In case of the output, it is converted into a categorical model."
2010.02554,"code, github",45,2022-05-13,0,The dependencies of parameters in our Pytorch implementation (https://github.com/pmorenoz/RecyclableGP) are clearly shown and evident from the code structure oriented to objects. It is also amenable for the introduction of new covariance functions and more structured variational approximations if needed.
2010.02554,"code, publicly available, github, code available",11,2022-05-13,2,1The code is publicly available in the repository: github.com/pmorenoz/RecyclableGP/.
2010.02554,data,109,2022-05-13,0,"Stationarity and expressiveness. We assume that the non-linear function f is stationary across subsets of data. If this assumption is relaxed, some form of adaptation or forgetting should be included to match the local GPs. Other types of models can be considered for the ensemble, as for instance, with several latent functions (Lázaro-Gredilla and Titsias, 2011) or sparse multi-output GPs (Álvarez and Lawrence, 2011). The model also accepts GPs with increased expressiveness. For example, to get multi-modal likelihoods, we can use mixture of GP experts (Rasmussen and Ghahramani, 2002)."
2010.02554,data,146,2022-05-13,0,"The data D is assumed to be partitioned into an arbitrary number of K subsets that we aim to observe and process independently, that is, {D1, D2, . . . , DK}. There is not any restriction on the amount of subsets or learning nodes. The subsets {Dk}K k=1 do not need to have the same size, and we only restrict them to be Nk<N . However, since we treat with a huge number of observations, we still consider that Nk for all k ∈ {1, 2, . . . , K} is sufﬁciently large for not accepting exact GP inference due to temporal and computational demand. Notice that k is an index while k(·, ·) refers to the kernel."
2010.02554,data,151,2022-05-13,0,"where, once again, φ∗ = {µ∗, S∗} are the global variational parameters that we aim to learn. One important detail of the sum of expectations in (16) is that it works as an average contrastive indicator that measures how well the global q(u∗) is being ﬁtted to the local experts qk(uk). Without the need of revisiting any distributed subset of data samples, the GP predictive qC(uk) is playing a different role in contrast with the usual one. Typically, we assume the approximate posterior ﬁxed and ﬁtted, and we evaluate its performance on some test data points. In this case, it goes in the opposite way, the approximate variational distribution is unﬁxed, and it is instead evaluated over each k-th local subset of inducing-inputs Zk."
2010.02554,data,161,2022-05-13,0,"To obtain multiple independent approximations to the posterior distribution p(f |D) of the GP function, we introduce K variational distributions qk(f ), one per distributed partition Dk. In particular, each variational distribution factorises as qk(f ) = p(f(cid:54)=uk |uk)qk(uk), with qk(uk) = N (uk|µk, Sk) and p(f(cid:54)=uk |uk) being the standard conditional GP prior distribution given the hyperparameters ψk of each k-th kernel. To ﬁt the local variational distributions qk(uk), we build lower bounds Lk on the marginal log-likelihood (ELBO) of every data partition Dk. Then, we use optimisation methods, typically gradient-based, to maximise the K objective functions Lk, one per distributed task, separately. Each local ELBO is obtained as follows"
2010.02554,data,173,2022-05-13,0,"Heterogeneous single-output GP. Extensions to GPs with heterogeneous likelihoods, that is, a mix of continuous and discrete variables yi, have been proposed for multi-output GPs (Moreno-Muñoz et al., 2018). However, there are no restrictions in our single-output model to accept different likelihoods p(yi|f (xi)) per data point {xi, yi}. An inconvenience of the bound in (1), is that, each i-th expectation term could be imbalanced with respect to the others. For example, if mixing Bernoulli and Gaussian variables, binary outputs could contribute more to the objective function than the rest, due to the dimensionality. To overcome this issue, we ﬁt a local GP model to each heterogeneous variable. We join all models together using the ensemble bound in (6) to propagate the uncertainty in a principled way. Although, data-types need to be known beforehand, perhaps as additional labels."
2010.02554,data,20,2022-05-13,0,DATA ST. (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55)
2010.02554,data,214,2022-05-13,0,"4.3 Heterogeneous tasks. We analysed how ensembles of recyclable GPs can be used if one of the local tasks is regression and the other a GP classiﬁer. (viii) London household data: We have two subsets of input-output variables: the binary contract of houses (leasehold vs. freehold) and the price per latitude-longitude coordinate in the London area. Three quadrants (Q) of the city {Q2, Q3, Q4} are trained with a GP classiﬁer and Q1 as regression. To clarify, Q1 is the right-upper corner given the central axes. Our purpose is to combine the local latent uk, learned with the binary data on {Q2, Q3, Q4} and the uk learned on Q1 via regression. Then, we search the global f to be predict with a Bernoulli likelihood in Q1. The ensemble shows a test NLPD of 7.94 ± 0.01 in classiﬁcation while the recyclable task predicts with an NLPD of 8.00 ± 0.01 in the Q1. We asses that the heterogeneous GP prediction is better in Q1 than the local GP classiﬁer. The mean GP of regression is passed through the sigmoid function to show the multimodality."
2010.02554,data,22,2022-05-13,0,where yt and ft are the true output target and function values. Nt is the number of test data points.
2010.02554,data,27,2022-05-13,0,"J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in Artiﬁcial Intelligence (UAI), pages"
2010.02554,data,290,2022-05-13,0,"A priori, the ensemble GP bound is agnostic with respect to the likelihood model. There is a general derivation in Matthews et al. (2016) of how stochastic processes and their integral operators are affected by projection functions, that is, different linking mappings of the function f (·) to the parameters θ. In such cases, the local lower bounds Lk in (1) might include expectation terms that are intractable. Since we build the framework to accept any possible data-type, we propose to solve the integrals via Gaussian-Hermite quadratures as in Hensman et al. (2015); Saul et al. (2016) and if this is not possible, an alternative would be to apply Monte-Carlo methods. Computational cost and connections. The computational cost of the local models is O(NkM 2 k ), while the global GP reduces to O(((cid:80) k Mk)M 2) and O(M 2) in training and prediction, respectively. The methods in Table 1 typically need O((cid:80) k ) for global prediction. A last theoretical aspect is the link between the global bound in (6) and the underlying idea in Tresp (2000); Deisenroth and Ng (2015). Distributed GP models are based on the application of CI to factorise the likelihood terms of subsets. To approximate the posterior predictive, they combine local estimates, divided by the GP prior. It is analogous to (6), but in the logarithmic plane and the variational inference setup."
2010.02554,data,3,2022-05-13,0,DATA SIZE →
2010.02554,data,339,2022-05-13,0,"All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−5, ηL = 10−7, ηψ = 10−8, ηZ = 10−7} and we used M = 25 inducing-inputs."
2010.02554,data,34,2022-05-13,0,"In our experiments with toy data, we used two versions of the same sinusoidal function, one of them with an incremental bias. The true expressions of f (·) are"
2010.02554,data,343,2022-05-13,0,"viii) London household data: Based on the large scale experiments in Hensman et al. (2013), we obtained the register of properties sold in the Greater London county during the 2017 year (https://www.gov.uk/government/collections/ price-paid-data). All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information."
2010.02554,data,349,2022-05-13,0,"In terms of distributed inference for scaling up computation, that is, the delivery of calculus operations across parallel nodes but not data or independent models, we are similar to Gal et al. (2014). Their approach can be understood as a speciﬁc case of our framework. Alternatively, if we look to the property of having nodes that contain usable GP models (Table 1), we are similar to Deisenroth and Ng (2015); Cao and Fleet (2014) and Tresp (2000), with the difference that we introduce variational approximation methods for non-Gaussian likelihoods. An important detail is that the idea of exploiting properties of full stochastic processes (Matthews et al., 2016) for substituting likelihood terms in a general bound has been previously considered in Bui et al. (2017) and Moreno-Muñoz et al. (2019). Whilst the work of Bui et al. (2017) ends in the derivation of expectation-propagation (EP) methods for streaming inference in GPs, the introduction of the reparameterisation of Gal et al. (2014) makes our inference and performance different from Moreno-Muñoz et al. (2019). There is also the inference framework of Bui et al. (2018) for both federated and continual learning, but focused on EP and the Bayesian approach of Nguyen et al. (2018). A short analysis of its application to GPs is included for continual learning settings but far from the large-scale scope of our paper. Moreover, the spirit of using inducing-points as pseudo-approximations of local subsets of data is shared with Bui and Turner (2014), that comments its potential application to distributed setups. More oriented to dynamical modular models, we ﬁnd the work by Velychko et al. (2018), whose factorisation across tasks is similar to Ng and Deisenroth (2014) but oriented to state-space models."
2010.02554,data,45,2022-05-13,0,"We highlight several use cases for the proposed framework. The idea of recycling GP models opens the door to multiple extensions, with particular attention to the local-global modelling of heterogeneous data problems and the adaptation of model complexity in a data-driven manner."
2010.02554,data,48,2022-05-13,0,"Then, if we have (10), which is the ﬁrst version of our ensemble lower bound LE , we can use the augmented likelihood term p(y|f∞) to introduce the local approximations to f instead of revisiting the data. This is,"
2010.02554,data,52,2022-05-13,0,"We introduced a novel framework for building global approximations from already ﬁtted GP models. Our main contribution is the construction of ensemble bounds that accept parameters from regression, classiﬁcation and heterogeneous GPs with different complexity without revisiting any data. We analysed its performance on synthetic and real data with"
2010.02554,data,67,2022-05-13,0,"Model recycling and use cases. The ability of recycling GPs in future global tasks have a signiﬁcant impact in behavioral applications, where ﬁtted private-owned models in smartphones can be shared for global predictions rather than data. Its application to medicine is also of high interest. If one has personalized GPs for patients, epidemiologic surveys can be built without centralising private data."
2010.02554,data,70,2022-05-13,0,"In this paper, we investigate a general framework for recycling distributed variational sparse approximations to GPs, illustrated in Figure 1. Based on the properties of the Kullback-Leibler divergence between stochastic processes (Matthews et al., 2016) and Bayesian inference, our method ensembles an arbitrary amount of variational GP models with different complexity, likelihood and location of pseudo-inputs, without revisiting any data."
2010.02554,data,8,2022-05-13,0,Appendix C. Combined Ensemble Bounds with Unseen Data
2010.02554,data,85,2022-05-13,0,"A common theme in the previous approaches is the idea of model memorising and recycling, i.e. using the already ﬁtted parameters in another problem or joining it with others for an additional global task without revisiting any data. If we look to the functional view of this idea, uncertainty is still much harder to be repurposed than parameters. This is the point where Gaussian process (GP) models (Rasmussen and Williams, 2006) play their role."
2010.02554,data,89,2022-05-13,0,"Data-driven complexity and recyclable ensembles. One of the main advantages of the recyclable GP framework is that it allows data-driven updates of the complexity. That is, if an ensemble ends in a new variational GP model, it also can be recycled. Hence, the number of global inducing-variables M can be iteratively increased conditioned to the amount of samples considered. A similar idea was already commented as an application of the sparse order-selection theorems by Burt et al. (2019)."
2010.02554,data,9,2022-05-13,0,"Data Privacy and Conﬁdentiality at NeurIPS, 2019."
2010.02554,data,9,2022-05-13,0,Figure 2: Recyclable GPs with synthetic data.
2010.02554,"data, code, data available",178,2022-05-13,2,"In this section, we evaluate the performance of our framework for multiple recyclable GP models and data access settings. To illustrate its usability, we present results in three different learning scenarios: i) regression, ii) classiﬁcation and iii) heterogeneous data. All experiments are numbered from one to nine in roman characters. Performance metrics are given in terms of the negative log-predictive density (NLPD), root mean square error (RMSE) and mean-absolute error (MAE). We provide Pytorch code that allows to easily learn the GP ensembles.1 It also includes the baseline methods. The syntax follows the spirit of providing a list of recyclable_models = [GP1, GP2, GP3], where each GPk contains exclusively parameters of the local approximations. Further details about initialization, optimization and metrics are in the appendix. Importantly, we remark that data is never revisited and its presence in the ensemble plots is just for clarity in the comprehension of results."
2010.02554,"data, code, python, github",121,2022-05-13,2,"The code for the experiments is written in Python 3.7 and uses the Pytorch syntax for the automatic differentiation of the probabilistic models. It can be found in the repository https://github.com/pmorenoz/RecyclableGP, where we also use the library GPy for some algebraic utilities. In this section, we provide a detailed description of the experiments and the data used, the initialization of both variational parameters and hyperparameters, the optimization algorithm for both the local and the global GP and the performance metrics included in the main manuscript, e.g. the negative log-predictive density (NLPD), the root mean square error (RMSE) and the mean absolute error (MAE)."
2010.02554,"data, data available",121,2022-05-13,0,"Local likelihood reconstruction. The augmented likelihood distribution is perhaps, the most important point of the derivation. It allows us to apply conditional independence (CI) between the subsets of distributed output targets. This gives a factorized term that we will later use for introducing the local variational experts in the bound, that is, log p(y|f∞) = (cid:80)K k=1 log p(yk|f∞). To avoid revisiting local likelihood terms, and hence, evaluating distributed subsets of data that might not be available, we use the Bayes theorem but conditioned to the inﬁnite-dimensional augmentation. It indicates that the local variational distributions can be approximated as"
2010.02554,"data, dataset",108,2022-05-13,0,"As we already mentioned in the manuscript, there might be scenarios where it could be not necessary to distribute the whole dataset D in K local tasks or, for instance, a new unseen subset k + 1 of observations might be available for processing. In such case, it is still possible to obtain a combined global solution that ﬁts both to the local GP approximations and the new data. For clarity on this point, we rewrite the principal steps of the ensemble bound derivation in section A but without substituting all the log-likelihood terms by its Bayesian approximation, that is"
2010.02554,"data, dataset",166,2022-05-13,0,"One of the most desirable properties for any modern machine learning method is the handling of very large datasets. Since this goal has been progressively achieved in the literature with scalable models, much attention is now paid to the notion of efﬁciency. For instance, in the way of accessing data. The fundamental assumption used to be that samples can be revisited without restrictions a priori. In practice, we encounter cases where the massive storage or data centralisation is not possible anymore for preserving the privacy of individuals, e.g. health and behavioral data. The mere limitation of data availability forces learning algorithms to derive new capabilities, such as i) distributing the data for federated learning (Smith et al., 2017), ii) observe streaming samples for continual learning (Goodfellow et al., 2014) and iii) limiting data exchange for private-owned models (Peterson et al., 2019)."
2010.02554,"data, dataset",230,2022-05-13,1,"v) Pixel-wise MNIST classiﬁcation: We took images of ones and zeros from the MNIST dataset. To simulate a pixel-wise unsupervised classiﬁcation problem, true labels of images were ignored. Instead, we threshold the pixels to be greater or smaller than 0.5, and labeled as yi = 0 or yi = 1. That is, we turned the grey-scaled values to a binary coding. Then, all pixels were described by a two-dimensional input in the range [−1.0, 1.0], that indicates the coordinate of each output datum. In the case of the zero image, we splitted the data in four areas, i.e. the four corners, as is shown in the subﬁgure (A) of Figure 4. Each one of the local tasks was initialized with an equally spaced grid of Mk = 16 inducing-inputs. The ensemble GP required M = 25 in the case of the number zero and M = 16 for the one. The plotted curves correspond to the test GP predictive posterior at the probit levels [0.2, 0.5, 0.8]. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,"data, dataset",311,2022-05-13,0,"1.1 Background. The ﬂexible nature of GP models for deﬁnining prior distributions over non-linear function spaces has made them a suitable alternative in many probabilistic regression and classiﬁcation problems. However, GP models are not immune to settings where the model needs to adapt to irregular ways of accessing the data, e.g. asynchronous observations or missings input areas. Such settings, together with GP model’s well-known computational cost for the exact solutions, typically O(N 3) where N is the data size, has motivated plenty of aproaches focused on parallelising inference. Regarding the task of distributing the computational load between learning agents, GP models have been inspired by local experts (Jacobs et al., 1991; Hinton, 2002). Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al."
2010.02554,"data, dataset",320,2022-05-13,0,"ii) Distributed GPs: In this second experiment, our goal is to compare the performance of the recyclable framework with the distributed GP methods in the literature (Tresp, 2000; Ng and Deisenroth, 2014; Cao and Fleet, 2014; Deisenroth and Ng, 2015). To do so, we begin by generating toy samples from the sinusoidal function f (x). The comparative experiment is divided in two parts, in one, we observe N = 103 and in the other, N = 104 input-output data points. In the ﬁrst case, we splitted the dataset D in K = 50 tasks with Nk = 200 and Mk = 3 per partition. Any of these distributed subsets were overlapping, and their corresponding input-spaces concatenated perfectly in the range x ∈ [0.0, 5.5]. For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm."
2010.02554,"data, dataset",322,2022-05-13,0,"Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al. (2014) presented a new re-parameterisation of the lower bounds that allows to distribute the computational load accross nodes, also applicable to GPs with stochastic variational inference (Hensman et al., 2013) and with non-Gaussian likelihoods (Hensman et al., 2015; Saul et al., 2016). Out of the sparse GP approach and more inspired in Tresp (2000) and product of experts (Bordley, 1982), the distributed GPs of Deisenroth and Ng (2015) scaled up the parallelisation mechanism of local experts to the range of N >106. Their approach is focused on exact GP regression, not considering classiﬁcation or other non-Gaussian likelihoods. Table 1 provides a description of these different methods and their main properties, also if each distributed node is a GP model itself."
2010.02554,"data, dataset",330,2022-05-13,0,"Ideally, to obtain a global inference solution given the GP models included in the dictionary, the resulting posterior distribution should be valid for all the local subsets of data. This is only possible if we consider the entire data set D in a maximum likelihood criterion setting. Speciﬁcally, our goal now is to obtain an approximate posterior q(f ) ≈ p(f |D) by maximising a lower bound LE under the log-marginal likelihood log p(D) without revisiting the data already observed by the local models. We begin by considering the full posterior distribution of the stochastic process, similarly as Burt et al. (2019) does for obtaining an upper bound on the KL divergence. The idea is to use inﬁnite-dimensional integral operators that were introduced by Matthews et al. (2016) in the context of variational inference, and previously by Seeger (2002) for standard GP error bounds. The use of the inﬁnite-dimensional integrals is equivalent to an augment-and-reduce strategy (Ruiz et al., 2018). It consists of two steps: i) we augment the model to accept the conditioning on the inﬁnite-dimensional stochastic process and ii) we use properties of Gaussian marginals to reduce the inﬁnite-dimensional integral operators to a ﬁnite amount of GP function values of interest. Similar strategies have been used in continual learning for GPs (Bui et al., 2017; Moreno-Muñoz et al., 2019). Global objective. The construction considered is as follows. We ﬁrst denote y as all the output targets {yi}N i=1in the dataset D and f∞ as the augmented inﬁnite-dimensional GP. Notice that f∞ contains all the function values taken by f (·), including that ones at {xi}N k=1 for all partitions. The augmented log-marginal expression is therefore"
2010.02554,"data, dataset",350,2022-05-13,0,"For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm. The setup of the VEM algorithm during the ensemble ﬁtting was {VE = 30, VM = 10, ηm = 10−3, ηL = 10−6, ηψ = 10−8, ηZ = 10−8}. As in the previous experiment with toy data, we set M = 35 inducing-inputs. iii) Recyclable ensembles: For simulating potential scenarios with at least N = 106 input-output data points, we used the setting of the previous experiment, but with K = 5 · 103 tasks of Nk = 800 instead. However, as explained in the paper, its performance was hard to evaluate in the baseline methods, due to the problem of combining bad-ﬁtted GP models. Then, based on the experiments of Deisenroth and Ng (2015) and the idea of building ensembles of ensembles, we set a pyramidal way for joining the distributed local GPs. It was formed by two layers, that is, we joined ensembles twice as shown in the Figure 5 of this appendix."
2010.02554,"data, dataset, data https, data available",327,2022-05-13,1,"The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g. K(cid:29)103, it is problematic for baseline methods, due to partitions must be revisited for building predictions and if one-of-many GP fails, performance decreases. Then, we repeated the experiment in a pyramidal way. That is, building ensembles of recyclable ensembles, inspired in Deisenroth and Ng (2015). Our method obtained {NLPD=4.15, RMSE=2.71, MAE=2.27}. The results in Table 3 indicate that our model is more robust under the concatenation of approximations rather than overlapping them in the input space. (iv) Solar physics dataset: We tested the framework on solar data (available at https://solarscience.msfc.nasa.gov/), which consists of more than N =103 monthly average estimates of the sunspot counting numbers from 1700 to 1995. We applied the mapping log(1 + yi) to the output targets for performing Gaussian regression. Metrics are provided in Table 2, where std. values were small, so we do not include them. The perfomance with 50 tasks is close to the baseline solutions, but without storing all distributed subsets of data."
2010.02554,"data, dataset, data https, data available",332,2022-05-13,0,"4.1 Regression. In our ﬁrst experiments for variational GP regression with distributed models, we provide both qualitative and quantitative results about the performance of recyclable ensembles. (i) Toy concatenation: In Figure 2, we show three of ﬁve tasks united in a new GP model. Tasks are GPs ﬁtted independently with Nk=500 synthetic data points and Mk=15 inducing variables per distributed task. The ensemble ﬁts a global variational solution of dimension M =35. Notice that the global variational GP tends to match the uncertainty of the local approximations. (ii) Distributed GPs: We provide error metrics for the recyclable GP framework compared with the state-of-the-art models in Table 3. The training data is synthetic and generated as a combination of sin(·) functions (in the appendix). For the case with 10K observations, we used K=50 tasks with Nk=200 data-points and Mk=3 inducing variables in the sparse GP. The scenario for 100K is similar but divided into K=250 tasks with Nk=400. Our method obtains better results than the exact distributed solutions due to the ensemble bound searches the average solution among all recyclable GPs. The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g."
2010.02554,"data, dataset, data https, used dataset",206,2022-05-13,1,"iv) Solar physics dataset: We used the solar physics dataset (https://solarscience.msfc.nasa.gov/) which consists of N = 3196 samples. Each input-output data point corresponds to the monthly average estimate of the sunspot counting numbers from 1700 to 1995. The output targets y were transformed to the real-valued domain via the mapping log(yi + 1) to use a normal likelihood distribution. We also scaled the input area to the range x ∈ [0, 100] and normalized the outputs to be zero-mean. The number of tasks was K = 50 and 20% of the data observations were reserved for test. The initial values of kernel and likelihood hyperparameters was {(cid:96) = 0.2, σ2 n is the initial likelihood variance, that we also learn. In this case, the setup of the VEM algorithm was {VE = 20, VM = 20, ηm = 10−5, ηL = 10−8, ηψ = 10−10, ηZ = 10−10}. The number of global inducing-inputs used for the ensemble was M = 90, whilst we used Mk = 6 for each distributed approximation."
2010.02554,"data, dataset, dataset provided, data available",109,2022-05-13,0,"In practice, it might not be necessary to distribute the whole dataset D in parallel Recyclable GPs and new data. tasks, with some subsets Dk available at the global ensemble. It is possible to combine the samples in Dk with the dictionary of local GP variational distributions. In such cases, we would only approximate the likelihood terms in (3) related to the distributed subsets of samples. The resulting combined bound would be equivalent to (6) with an additional expectation term on the new data. We provide the derivation of this combined bound in the supplementary material."
2010.02554,dataset,156,2022-05-13,0,"We consider a supervised learning problem, where we have an input-output training dataset D = {xi, yi}N i=1 with x ∈ Rp. We assume i.i.d. outputs yi, that can be either continuous or discrete variables. For convenience, we will refer to the likelihood term p(y|θ) as p(y|f ) where the generative parameters are linked via θ = f (x), being f (·) a non-linear function drawn from a zero-mean GP prior f ∼ GP(0, k(·, ·)), and k(·, ·) is the covariance function or kernel. Importantly, when non-Gaussian outputs are considered, the GP output function f (·) might need an extra deterministic mapping Φ(·) that transforms it to the appropriate parametric domain of θ."
2010.02554,dataset,20,2022-05-13,0,Table 2: Performance metrics for distributed GP regression with the solar physics dataset. (std. ×102)
2010.02554,dataset,205,2022-05-13,1,"vii) Banana dataset: The banana experiment is perhaps one of the most used datasets for testing GP classiﬁcation models. We followed a similar strategy as the one used in the MNIST experiment. After removing the 33% of samples for testing, we partitioned the input-area in four quadrants, i.e. as is shown in Figure 4. For each partition we set a grid of Mk = 9 inducing-inputs and later, the maximum complexity of the global sparse model was set to M = 25. The baseline GP classiﬁcation method also used M = 25 inducing-inputs and obtained an NLPD value of 7.29 ± 7.85 × 10−4 after ten trials with different initializations. Our method obtained a test NLPD of 7.21 ± 0.04. As we mentioned in the main manuscript, the difference is understandable as the recyclable GP framework used a total amount of 4 × 16 inducing-inputs, that capture more uncertainty than the 16 of the baseline method. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,dataset,97,2022-05-13,0,"The construction of ensemble variational bounds from recyclable GP models is based on the idea of augmenting the marginal likelihood to be conditioned on the inﬁnite-dimensional GP function f∞. Notice that f∞ contains all the function values taken by f (·) over the input-space Rp, including the input targets {xi}N k=1 and the global ones Z∗. Having K partitions of the dataset D with their corresponding outputs y = {y1, y2, . . . , yK }, we begin by augmenting the marginal log-likelihood as"
2010.02554,"dataset, used dataset",218,2022-05-13,1,"of Van der Wilk et al. (2017), we threshold images of zeros and ones to black and white pixels. Then, to simulate a pixel-wise learning scenario, we used each pixel as an input-output datum whose input xi contains the two coordinates (x1 and x2 axes). Plots in Figure 4 illustrate that a predictive ensemble can be built from smaller pieces of GP models, four corners in the case of the number zero and two for the number one. (v) Compositional number: As an illustration of potential applications of the recyclable GP approach, we build a number eight predictor using exclusively two subsets of the approximations learned in the previous experiment with the image of a zero. The trick is to shift Zk to place the local approximations in the desired position. (vi) Banana dataset: We used the popular dataset in sparse GP classiﬁcation for testing our method with M =25. We obtained a test NLPD= 7.21 ± 0.04, while the baseline variational GP test NLPD was 7.29 ± 7.85×10−4. The improvement is understandable as the total number of inducing points, including the local ones, is higher in the recyclable GP scenario."
2010.09647,code,123,2022-05-13,0,"ing unconstrained. The base measure problem occurs, in a sense, but the remedy is both local and internal to the implementation of Stan. The latter Stan pushes on the user: if a user writes a Stan model with a parameter x and a transformed parameter f (x), and wishes to code a density on f (x) that corresponds to the pushforward of some known density p(x), then that user must think about base measures and partial derivatives themselves. Stan does not help; but then again, it also does not claim it would help, so Stan doesn’t compute anything visibly “incorrect”."
2010.09647,code,89,2022-05-13,0,"We have named the Base Measure Problem and provided a solution to it. Implementing the solution in probabilistic programming systems should cause negligible loss of performance for cases that were already correctly handled, and expand the set of models in which the system can compute correct probability densities. Implementation does carry a code complexity cost, but that cost is minimized by using two-argument dispatch, or emulating it with a Visitor pattern. Despite correctly accounting for measures, no non-local information is required."
2010.09647,data,80,2022-05-13,0,"The root of the base measure problem is that we didn’t want to compute with measures directly, but lost the base measure when representing probability distributions with densities. It’s not actually possible to infer the correct base measure from the data type representing the sample: a point on the unit circle in R2 is represented with two ﬂoating-point numbers, but using Lebesgue measure on R2 as the base is not helpful."
2010.09647,package,88,2022-05-13,0,"The Jacobian-determinant correction 1/| det(Jfx)| accounts for the possibility that f changes the volume of an inﬁnitesimal volume element near x. The Jacobian determinant can be computed by forming the Jacobian of f , for example with automatic diﬀerentiation; but for many functions f , it’s available more eﬃciently. Thus a conventional choice is to package such f , together with their inverse f −1, in a Bijector class with a method for computing said Jacobian determinant."
2011.00242,code,100,2022-05-13,0,"Therefore, in all applications, there are indications, in the form of source code, comments, and issues, that developers demand an effort to understand and improve cache keys, reasoning about alternative better ways to organize and identify content. We identiﬁed common content properties used to build cache keys, which are: (i) content ids, (ii) method signatures, and (iii) a tag-based identiﬁcation, in which the type of content, the class, the module or hierarchy are used (Evidence 19)."
2011.00242,code,11,2022-05-13,0,Category (Acronym) Code Scattering and Tangling (CST)
2011.00242,code,12,2022-05-13,0,Complex Naming Conventions (CNC) Additional Caching Code (ACC)
2011.00242,code,120,2022-05-13,0,"Given that implementing cache is challenging, we noticed that in six applications developers made use of supporting libraries and frameworks. This was done to prevent adding much cache-related code to the base code, because such components raise the abstraction level of caching, providing some ready-to-use features (Evidence 22). Examples of such external components are distributed cache systems, e.g. Redis [27] and Memcached [26], and libraries that can act locally, e.g. Spring Caching [51], EhCache [52], Inﬁnispan [53], Rails low-level caching [54], Google Guava [55] and Caffeine [56]."
2011.00242,code,152,2022-05-13,0,"First, we assigned concepts to pieces of extracted text (open coding), each representing application-level caching characteristics. Figure 2 exempliﬁes different codes identiﬁed during the analysis of an application. For instance, Code Tangling is created from the observation of cache-related implementation spread all over the application base code (underlined). Then, for each new concept, we veriﬁed whether they are connected somehow with existing ones, in order to generate categories (selective coding). Thus, the name assigned to a particular category aims at representing, at a higher abstraction level, all concepts related to it. Regarding the Code Tangling example, if Code Scattered were found afterwards, we could establish a relationship between the former and the latter to create a category, given that both are related to lack of separation of concerns."
2011.00242,code,161,2022-05-13,0,"[7] M. P. Robillard, W. Coelho, and G. C. Murphy, “How effective developers investigate source code: An exploratory study,” IEEE Transactions on Software Engineering, vol. 30, no. 12, pp. 889–903, 2004. [Online]. Available: http://dx.doi.org/10.1109/TSE.2016. 2532873 J. Sillito, G. C. Murphy, “Asking and answering questions during a programming change task,” in IEEE Transactions on Software Engineering, vol. 34, [Online]. Available: no. 4. http://dx.doi.org/10.1109/TSE.2008.26 S. Nadi, T. Berger, C. K¨astner, and K. Czarnecki, “Where do conﬁguration constraints stem from? An extraction approach and an empirical study,” IEEE Transactions on Software Engineering, vol. 41, no. 8, pp. 820–841, aug 2015. [Online]. Available: http://dx.doi.org/10.1109/TSE.2015.2415793"
2011.00242,code,17,2022-05-13,0,"to implementation issues of application-level caching, by providing solutions and guidance at the code level."
2011.00242,code,19,2022-05-13,0,A code comment before an expensive operation: TODO cache the global properties to speed this up??
2011.00242,code,21,2022-05-13,0,Labels of Evidence Sources: SC-Source Code (without comments); COM-Code Comments; IS-Issues; DOC-Documentation; DEV-Developers.
2011.00242,code,23,2022-05-13,0,A code comment: Is hibernate taking care of caching and not hitting the db every time? (hopefully it is)
2011.00242,code,236,2022-05-13,0,"As can be seen in Figure 3, caching implementation and associated design decisions are much more discussed and revised by developers than maintenance decisions. Caching implementation, which is spread in the code and involves the choice for appropriate locations to add and remove elements from the cache, is error-prone and can compromise code legibility. Consequently, many issues are associated with bug ﬁxes, technological details and code refactorings. Moreover, despite being less frequent, caching design is time-consuming and challenging, given that it requires understanding of the application behavior, as well as limitations, conditions and restrictions of content being cached. In applications analyzed, the mean (M) and standard deviation (SD) values of cache-related implementation issues are M = 47.45% and SD = 5.76%, while design issues achieve M = 37.92% and SD = 7.11%. Finally, because ﬁnegrained conﬁgurations require empirical analysis such as cache proﬁling, and there is little evidence that this was performed in investigated applications, maintenance decisions often result in the choice for default settings. Consequently, a lower number of issues is associated with such decisions, speciﬁcally M = 14.61% and SD = 3.44%. This issue analysis allowed us to understand the aspects of caching that require more effort from developers."
2011.00242,code,30,2022-05-13,0,"Code comments. Given that caching is an orthogonal concern in the application, unrelated to the business logic, but interleaved with its code, code comments are often"
2011.00242,code,36,2022-05-13,0,"reasoning, time, and modiﬁcations to the code (Evidence 14). We discussed our ﬁndings regarding cache design decisions and we next discuss observations made associated with how to implement design decisions."
2011.00242,code,38,2022-05-13,0,"We performed an analysis of the issues available in issue platforms to investigate the primary sources of cacherelated problems, typically bugs, in the applications. Based on user messages, code reviews and commit messages that"
2011.00242,code,43,2022-05-13,0,"A code snippet exposing a test of caching logic: it ”can set and get false values when return cache nil” do @store.set :test, false expect(@store.get(:test)).to be false end"
2011.00242,code,46,2022-05-13,0,"Before addressing each of our research questions, we show an objective analysis of the impact of caching in the investigated applications by identifying all code and issues related to them. This gives a broad sense of how caching is implemented in target systems."
2011.00242,code,48,2022-05-13,0,"Source code. Application source code is our core source of information. Since we focus on application-level caching, our analysis is concentrated in the core of the application (i.e. the business logic), which is where the caching logic is typically implemented."
2011.00242,code,49,2022-05-13,0,A code snippet: cache id = ’objectmodel ’ . $entity defs[’classname’] . ’ ’ . (int)$id . ’ ’ . (int)$id shop . ’ ’ . (int)$id lang;
2011.00242,code,56,2022-05-13,0,"Keep the cache API simple. (Evidence 15), (Evidence 17), (Evidence 20) and (Evidence 22) Caching logic tends to be spread all over the application, and a good solution should be employed to avoid writing messy code at the cost of high maintenance efforts."
2011.00242,code,59,2022-05-13,0,"Furthermore, in cases where updates in the base code are not an option (due to time or technical restrictions), a transparent and automatic caching component can provide fast results. These solutions address layers before and after application boundaries, and require only a few adaptations to the application needs (Evidence 24)."
2011.00242,code,60,2022-05-13,0,"All these caching design options may become complex and difﬁcult to understand. Indeed, identifying caching opportunities and ensuring consistency can add much code and may not be trivial to implement and understand. Due to the nature of application-level caching, such logic is spread all over the system. We noticed in 90% of the applications"
2011.00242,code,60,2022-05-13,0,"Such supporting libraries and frameworks not only provide partial ready-to-use features but also reduce the amount of additional effort required to guarantee that the cache is working. We observed in all applications that the cache includes code dedicated to test, debug and conﬁgure cache components, which can be expensive in some scenarios (Evidence 23)."
2011.00242,code,63,2022-05-13,0,"A developer quote: At ﬁrst glance, the cache code hinders the understanding of the business logic. Also, the cache logic itself it is not easy to get. A bug report on an issue platform: When you import categories with a parent category which does not exist, it prevents from duplicate it because of the cache."
2011.00242,code,87,2022-05-13,0,"Although this problem is present in the code, there are indications that developers know about it and express they are willing to improve the provided solution. In order to reduce the impact of an infrastructure component to the system business logic, we identiﬁed cases where there are suggestions to design more extensible classes and modules, refactoring and reducing cache-related code, and reusing components (Evidence 17). This acknowledgment of technical debt was observed in 90% of the applications."
2011.00242,code,91,2022-05-13,0,"such as design patterns, third-party libraries or aspects? 22 How is the caching logic mixed with the application code? 23 Is this extra cache logic tested? 24 What is the required format to cache? 25 How are objects translated to the cache? 26 How are names (keys) deﬁned for cached objects? 27 Do developers use another caching layer besides application-level? 28 Is any transparent or automatic caching component being used? 29 Do developers rely on automatic caching components? 30"
2011.00242,code,96,2022-05-13,0,"Presence of complex constructs such as batch processing and asynchronous communication, which require extra effort and reasoning from developers to be implemented. Indication of use of third-party caching solutions to help the implementation, raising the level of abstraction of some caching aspects. Choice for complex keys of caching content, causing developers to spend time and effort to elaborate and understand such keys. the Code implemented to support caching logic, such as implemented caching tests, logic to monitor cache statistics and additional interfaces to support available caching providers."
2011.00242,"code, github",72,2022-05-13,0,"Issues. An issue can represent a software bug, a project task, a help-desk ticket, a leave request form, or even user messages about the project in general. Usually, changes in the code are due to registered issues. Thus, implementation and design decisions are better explained by associated issues in issue platforms, such as GitHub Issue Tracker, JIRA, and Bugzilla."
2011.00242,"code, open-source",68,2022-05-13,0,"To identify patterns of application-level caching adopted by developers and understand what kinds of caching implementations and decisions can be automatically inferred, characterize and evaluate application-level caching-related design and implementation from a perspective of the researcher as they are implemented in the source code and described in issues of web-based applications in the context of 10 software projects, obtained from open-source repositories and software companies."
2011.00242,data,102,2022-05-13,0,"The high number of occurrences related to ensuring consistency refers to the expiration process of cached content, which requires extra reasoning from developers, because they should track which changes cause data content to become outdated, and be aware for how long the cache can provide stale data, in case the data source has been updated. In fact, consistency approaches have been widely investigated [4], [6], and the typical way of dealing with it is to analyze data dependency, from which conditions and constraints for consistency can be derived."
2011.00242,data,106,2022-05-13,0,"Avoid caching per-user data. (Evidence 4) and (Evidence 26) It is recommended to avoid caching per-user data unless the user base is small and the total size of the cached data does not require an excessive amount of memory; otherwise, it can cause a memory bottleneck. However, if users tend to be active for a while and then go away again, caching per-user data for short-time periods may be an appropriate approach. For instance, a search engine that caches query results by each user, so that it can page through results efﬁciently."
2011.00242,data,107,2022-05-13,0,"Regarding implementation choices, we observed common practices. The ﬁrst is associated with how to name cached data. In order to use in-memory caching solutions, there is no prescribed way to organize data. Typically, unique names are assigned to each cached content, thus leading to a key-value model—and this was the case in all investigated applications. Given that cache stores lots of data, the set of possible names must be large; otherwise, two names (keys) can conﬂict with each other and, thus stale (or even entirely wrong) data can be retrieved from"
2011.00242,data,11,2022-05-13,0,RQ1. What and when is data cached at the application
2011.00242,data,110,2022-05-13,0,"Despite choosing where and what to cache, cached values are valid only as long as the sources do not change, and when sources change, a consistency policy should be employed to ensure that the application is not serving stale data. Therefore, in nine analyzed applications, there are indications that developers demand an effort to design consistency approaches, reasoning about the lifetime of cached data, as well as eviction conditions and constraints. We identiﬁed common approaches to keep consistency, which are: (i) a less efﬁcient and straightforward approach is to invalidate cached values based on mapping actions that"
2011.00242,data,127,2022-05-13,0,"The theoretical categories are described in Table 5, which presents their description, an original piece of content (example of evidence) and sources of data classiﬁed in each category. Moreover, categories are also shown in Figure 5a with the associated number of occurrences in the applications analyzed and classiﬁed according to each research question. Figure 5b shows the percentage of contribution of each application to the categories emerged from the study. These ﬁgures also present categories identiﬁed in RQ2 (described in Table 6) and RQ3 (described in Table 7), which are discussed in the following sections. Acronyms used in Figures 5a and 5b are introduced in Tables 5, 6, and 7."
2011.00242,data,13,2022-05-13,0,Is there a relationship between the data cached and the application domain?
2011.00242,data,13,2022-05-13,0,RQ1. What and when is data cached at the application level?
2011.00242,data,131,2022-05-13,0,"Do not discard small improvements. (Evidence 3), (Evidence 8) and (Evidence 9) The user perceived latency is reduced by any caching solution employed. This means that even not obvious scenarios should be target of caching, i.e. it is not true that solely data that is frequently used and expensive to retrieve or create should considered for caching. Furthermore, data that is expensive to retrieve and is modiﬁed on a periodic basis can still improve performance and scalability when properly managed. Caching data even for a few seconds can make a large difference in high volume sites. If the data is handled more often than it is updated, it is also a candidate for caching."
2011.00242,data,136,2022-05-13,0,"Following the introduced phases of data analysis, we performed mainly a subjective analysis of the data, collecting: (i) typical caching design, implementation and maintenance strategies; (ii) motivations, challenges and problems behind caching, and (iii) characteristics of caching decisions. These collected data were evaluated to conceptualize how the open codes were related to each other as a set of hypotheses in accounting for resolving the primary concern. Furthermore, we also made a broad analysis of the target systems in order to investigate how application-level caching was conceived in them. All the research phases were performed manually, as the collected data (most of them expressed in natural language) analysis is associated with the interpretation of caching approaches."
2011.00242,data,14,2022-05-13,0,4.2 RQ1: What and when is data cached at the application level?
2011.00242,data,141,2022-05-13,0,"7 DISCUSSION AND CONCLUSION Application-level caching has been increasingly used in the development of web applications, in order to improve their response time given that they are becoming more complex and dealing with larger amounts of data over time. Caching has been used in different locations, such as proxy servers, often as seamless components. Applicationlevel caching allows caching additional content taking into account application speciﬁcities not captured by off-theshelf components. However, there is limited guidance to design, implement and manage application-level caching, which is often implemented in an ad-hoc way. In this paper, we presented a qualitative study performed to understand how developers approach application-level caching. The study consisted of the selection ten web applications and investigation of caching-related aspects, namely design, implementation and maintenance practices."
2011.00242,data,141,2022-05-13,0,"performance via adaptive content the Proceedings of Computing and Applications. [Online]. Available: http://dx.doi.org/10.1109/NCA.2011.55 [20] G. Soundararajan and C. Amza, “Using semantic information to improve transparent query caching for dynamic content web sites,” in Proceedings of the International Workshop on Data Engineering Issues in E-Commerce, vol. 2005. IEEE, 2005, pp. 132– 138. [Online]. Available: http://dx.doi.org/10.1109/DEEC.2005.25 [21] C. Amza, G. Soundararajan, and E. Cecchet, “Transparent caching with strong consistency in dynamic content web sites,” in Proceedings of international conference on Supercomputing. New York, New York, USA: ACM Press, jun 2005, p. 264. [Online]. Available: http://dx.doi.org/10.1145/ 1088149.1088185"
2011.00242,data,150,2022-05-13,0,"From all analyzed applications, we observed that none of them use a proactive approach to cache content. Content is always cached after it requested (i.e. reactive approach) and, as a consequence, the ﬁrst request always results in a cache miss. Due to this, prefetching techniques can be used in order to populate the cache and prevent misses by predicting and caching data that will potentially be requested in the future. It can be based on heuristics, usage observations or even with the use of complex prediction algorithms [15], [32], [58], [59]. However, the design and implementation of a reactive cache component already requires signiﬁcant effort and reasoning to be properly done, and a proactive approach increases the complexity of the caching solution even more."
2011.00242,data,16,2022-05-13,0,Deﬁne naming conventions Perform cache actions asynchronously Do not use cache as data storage Perform measurements
2011.00242,data,18,2022-05-13,0,"Design of some kind of consistency approach such as expiration policies and invalidation, preventing stale data."
2011.00242,data,183,2022-05-13,0,"the content being cached should be considered when using size-limited caches. In this case, an adequate trade-off between popularity (hits) and size of the items must be achieved. Keeping small popular items in the cache tends to optimize hit-ratio; however, a hit in a large item may be more beneﬁcial for an application than many hits on small items. At the same time, ﬁlling the cache with few large items may turn the cache performance dependent on a good replacement policy. Evaluate staleness and lifetime of cached data. (Evidence 2), (Evidence 3) and (Evidence 12) Every piece of cached data is already potentially stale, it is important to rethink the degree of integrity and potential staleness that the application can compromise for increased performance and scalability. Many cache implementations adopted an expiration policy to invalidate cached data based on a timeout since weak consistency is easier than deﬁning a hard-to-maintain, but more robust, invalidation process. In short, developers must"
2011.00242,data,186,2022-05-13,0,"Even though these approaches focus on providing an adaptive behavior to application-level cache, none of them takes application speciﬁcities into account to autonomously manage their target. They attempt only to optimize cache decisions based on cache statistics like hit-ratio and access patterns at a higher level, thus, ignoring cache meta-data expressed by application-speciﬁc characteristics, which are closely related to application computations and could help to reach an optimal performance of the caching service on time. Addressing this issues, content-aware approaches have been proposed [45], [46] for admitting and replacing items in the cache by exploring descriptive caching hits in the application model or spatial locality (relationships among web objects). Though adaptive caching is in general new and innovative, it is far from being adopted as standard practice in software industry; many improvements must be achieved before this can happen, such as reducing the overhead in terms of resource consumption and processing time of the learning process, and providing easy ways to integrate application and caching method."
2011.00242,data,189,2022-05-13,0,"The last issue is related to the maintenance of the cache system, which involves several aspects such as determining replacement policies and the size of the cache. According to Radhakrishnan [31], a traditional approach to maintain cache is to set up strategies based on initial assumptions or well-accepted characteristics of workload and access patterns. Thus, cache statistics such as hit ratio, the number of objects cached, and average object size can be collected by observing the application and cache at run-time. These data, compared in the context of desired values of these parameters, can be used to decide whether to change the ﬁrst choices. Usually, this process is repeated until an acceptable trade-off between conﬁguration and performance is reached, and then cache strategies are ﬁxed for the lifetime of the product or at least for several release cycles. However, its performance may decay over time due to changes in the workload characteristics and access patterns. Therefore, achieving acceptable application performance requires constantly tuning cache decisions, which implies extra time"
2011.00242,data,19,2022-05-13,0,Fig. 6. Cacheability ﬂowchart: intuitive process to decide whether to cache or not particular data.
2011.00242,data,19,2022-05-13,0,Guideline Evaluate different abstraction levels to cache Stack caching layers Separate dynamic static data Evaluate boundaries Specify selection criteria
2011.00242,data,198,2022-05-13,0,"issues about whether to cache some speciﬁc data or not, showing that the connection between observed bottlenecks in the application and opportunities for caching is not straightforward and requires a deeper analysis (Evidence 1). Therefore, selection criteria based on common sense or past experiences are initial assumptions to ease such decisions. Despite these criteria are usually unjustiﬁed, they guide developers while selecting content. We observed in 90% of the applications the deﬁnition of selection criteria to make the distinction of cacheable from uncacheable content easier. These criteria were observed in explanations of cache design choices in comments, issues, and documentation. We identiﬁed common criteria used to determine whether to cache or not a speciﬁc content, which are: (i) content change frequency (Evidence 2), (ii) content usage frequency (Evidence 3), (iii) content shareability (Evidence 4), (iv) content retrieval complexity (Evidence 5), (v) content size (Evidence 6), and (vi) size of the cache (Evidence 7)."
2011.00242,data,2,2022-05-13,0,Data Expiration
2011.00242,data,21,2022-05-13,0,A sentence in documentation: We use Redis [a thirdparty solution] as a cache and for transient data.
2011.00242,data,221,2022-05-13,0,"Abstract—Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users’ demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on speciﬁc details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications."
2011.00242,data,257,2022-05-13,0,"Specify selection criteria. (Evidence 1), (Evidence 2), (Evidence 3), (Evidence 5), (Evidence 6) and (Evidence 7) Selecting the right data to cache involves a great reasoning effort given that data manipulated by web applications range in dynamicity, from being completely static to changing constantly. To optimize this selection process, there are four primary selection criteria used by developers while detecting cacheable content, which should be used in decisions regarding whether to cache. These criteria are described below, ordered according to their importance; i.e. the higher the inﬂuence level, the earlier it is presented. Data change frequency. Developers should seek for data that have some degree of stability, i.e. those that are more used than changed. Even if data are volatile and change in time intervals, caching still brings a beneﬁt. This is the ﬁrst factor to be considered since caching volatile data implies the implementation of consistency mechanisms, which is not trivial and requires an extra effort and reasoning from developers. In short, the cost of consistency approaches cannot be higher than the beneﬁt of caching. Besides, when stale data is not a critical issue, an approach of weak consistency can be employed, such as time-to-live (TTL) eviction, where data is expired after a time in cache, regardless of possible changes."
2011.00242,data,29,2022-05-13,0,"Data retrieval complexity. Data that is expensive to retrieve, compute, or render, regardless of its dynamicity, is always considered a good caching opportunity."
2011.00242,data,35,2022-05-13,0,"order to avoid this, we followed a systematic analysis of our data, and conclusions are all founded on these data. Moreover, cross-checks were performed using our different sources of evidence."
2011.00242,data,39,2022-05-13,0,"ensure that the expiration policy matches the pattern of access to applications that use the data, which is based on determining how often the cached information is allowed to be outdated, and relaxing freshness when possible."
2011.00242,data,44,2022-05-13,0,7 What kinds of bottlenecks are addressed by developers? 8 What motivated the need for explicit caching manipulation? 9 What is the granularity of the cached objects? 10 What is the importance of the cached data to the application? 11
2011.00242,data,44,2022-05-13,0,"Separate dynamic from static data. (Evidence 3) and (Evidence 2) Content can be distinguished in static, dynamic, and user-speciﬁc. By partitioning the content, it is easier to select portions of the data to cache."
2011.00242,data,46,2022-05-13,0,Evaluate staleness and lifetime of cached data Avoid caching per-user data (Evidence 4) and (Evidence 26) DG-07 Avoid caching volatile data (Evidence 2) and (Evidence 3) DG-08 DG-09 Do not discard small provements Keep the cache API simple
2011.00242,data,5,2022-04-21,0,Data usage frequency. Frequent
2011.00242,data,55,2022-05-14,0,"[4] D. R. K. Ports, A. T. Clements, I. Zhang, S. Madden, and B. Liskov, “Transactional Consistency and Automatic Management in an Application Data Cache,” Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation, pp. 279–292, oct 2010."
2011.00242,data,56,2022-05-14,0,"Occurrences of multiple caching solutions, which can involve different third-party components or different application layers. As a consequence, the same content can be cached at different places in varying forms. Occurrences of caching content being added to the cache because they retrieve data from frameworks, libraries, or external applications."
2011.00242,data,57,2022-05-14,0,"6 THREATS TO VALIDITY We now analyze the possible threats to the validity of this study, and how we mitigated them. Researcher bias is a typical threat to qualitative research because the results are subject to the researcher interpretation of the data. In our study, prior knowledge might have inﬂuenced results. In"
2011.00242,data,59,2022-05-14,0,1 What are the motivations to employ cache? 2 What are the typical use scenarios? 3 Where and when data is cached? 4 What are the constraints related to data cached? 5 What are the selection criteria adopted to detect cacheable content? 6 Do developers adopt a pattern to decide which content should be
2011.00242,data,59,2022-05-14,0,"[29] K. Nguyen and G. Xu, “Cachetor: detecting cacheable data the 9th Joint Meeting on to remove bloat,” in Proceedings of Foundations of Software Engineering. New York, New York, USA: ACM Press, aug 2013, p. 268. [Online]. Available: http://dx.doi.org/10.1145/2491411.2491416"
2011.00242,data,59,2022-05-14,0,"of the application-level caching is not trivial and demands high effort, because it involves manual implementation by developers. Its design and maintenance involve four key challenging issues: determining how to cache the selected data, what data should be cached, when the selected data should be cached or evicted, and where the cached data"
2011.00242,data,6,2022-05-14,0,Controller layer. Caching data at
2011.00242,data,64,2022-05-14,0,"Due to the increasing demand and complexity involved with caching, developers try to decrease cache-related effort by adopting simple design solutions. We observed in seven of the ten analyzed applications design choices such as managing consistency based on expiration time, keeping default parameters, selecting data without any criteria, or even adopting external solutions, which do not claim extra"
2011.00242,data,68,2022-05-14,0,"Finally, determining maintenance strategies to manage efﬁciently where data is placed, such as replacement policies or size of the cache, requires additional knowledge and reasoning from application developers. Nevertheless, there are no foundations to make this choice adequately. Therefore, RQ3 complements the analysis above, questioning how application speciﬁcities are leveraged to provide the desired performance to the cache system."
2011.00242,data,69,2022-05-14,0,"Avoid caching volatile data. (Evidence 2) and (Evidence 3) Data should be cached when it is frequently used and is not continually changing. Developers should remember that caching is most effective for relatively stable data, or data that is frequently read. Caching volatile data, which is required to be accurate or updated in real time, should be avoided."
2011.00242,data,72,2022-05-14,0,"time-consuming, error-prone and, consequently, a common source of bugs [25]. Gupta et al. [6] and Ports et al. [4] both address these implementation issues by providing high-level caching abstractions, in which developers can simply designate application functions as cacheable, and the proposed system automatically caches their results and invalidates the cached data when the underlying source changes."
2011.00242,data,75,2022-05-14,0,"operations, requests, queries and shared content (accessed by multiple users) must be identiﬁed, focusing on recomputation avoidance. Even if some processing can be fast enough at a glance, it can potentially become a bottleneck when being invoked many times. Despite being frequently used, user-speciﬁc data cannot be shared and may not bring the beneﬁt of caching, being usually left out of the cache."
2011.00242,data,77,2022-05-14,0,"Do not use cache as data storage. (Evidence 27) An application can modify data held in a cache, but the cache should be considered as a transient data store that can disappear at any time. Therefore, developers should not save valuable data only in the cache, but keep the information where it should be as well, minimizing the chances of losing data if the cache unexpectedly becomes unavailable."
2011.00242,data,8,2022-05-14,0,Business or service layer. Caching data at
2011.00242,data,8,2022-05-14,0,Size of the data. The size of
2011.00242,data,81,2022-05-14,0,"[25] W. Wang, Z. Liu, Y. Jiang, X. Yuan, and J. Wei, “EasyCache: a transparent in-memory data caching approach for internetware,” in Proceedings of the 6th Asia-Paciﬁc Symposium on Internetware on Internetware. New York, New York, USA: ACM Press, nov 2014, pp. 35–44. [Online]. Available: http://dx.doi.org/10.1145/ 2677832.2677837 [26] “Memcached,” 2016."
2011.00242,data,84,2022-05-14,0,"Our study was designed based on comparative and interactive principles of grounded theory [47]. The purpose of grounded theory is to construct theory grounded in data by identifying general concepts, develop theoretical explanations that reach beyond the known, and offer new insights into the area of study. The systematic procedures of grounded theory enable qualitative researchers to generate ideas. In turn, these ideas can be later studied and veriﬁed through traditional quantitative forms of research."
2011.00242,data,88,2022-05-14,0,"Determining the cacheable content and the right moment of caching or clearing the cache content are a developer’s responsibility and might not be trivial in complex applications, motivating RQ1. In this research question, we aim to identify what data is selected to be cached, and the criteria used to detect such cacheable data. Furthermore, this question also explores when data should be evicted, as well as constraints, consistency conditions, and the rationale for all these choices."
2011.00242,data,91,2022-05-14,0,"By using grounded theory, we aimed to construct a wellintegrated set of hypotheses that explain how the concepts operated. Thus, the selective coding involves identifying the core category that best explains how study data refers to a large portion of the variation in a pattern and is considered the primary concern or problem related to the study, integrating closely related concepts. Finally, theoretical codes conceptualize how the codes may relate to each other as hypotheses to be incorporated into the theory [47]."
2011.00242,data,99,2022-05-14,0,"12 How much memory does the cached data consume? 13 What data is most frequently accessed? 14 How often is the cached data going to be used and changed? 15 What data is expensive? 16 What data depends on user sessions? 17 How up to date does the data need to be? 18 How is consistency assurance implemented? Why was it chosen? 19 Where and when is consistency assurance employed? 20 Which kind of operation/behavior affects cache consistency? 21 Do developers employ any technique to ease caching implementation,"
2011.00242,"data, code",107,2022-05-14,0,"4 ANALYSIS AND RESULTS This section details the results of our study and their analysis, according to the research questions we aim to answer. Our collected data consist mainly of source code and issues (expressed in natural language) and, as these are qualitative data, we have undertaken a subjective analysis of the application-level caching (hereafter referred to simply as “caching”) aspects represented in the target systems. Note that we labeled some ﬁndings with “Evidence X,” so that we can later refer to them to support the guidelines we derived from this analysis."
2011.00242,"data, code",108,2022-05-14,0,"We observed a higher number of categories, which are classes of observations made based on the analysis of these applications, associated with caching design and implementation than those associated with maintenance. Furthermore, the number of occurrences of each category, design and implementation categories also have higher numbers. This phenomenon was expected since the most representative portion of our qualitative data consists of source code and issues. Moreover, simple maintenance tasks and conﬁgurations are already executed and provided by external components, being commonly adopted. However, the number of occurrences do not reﬂect the importance of a category."
2011.00242,"data, code",135,2022-05-14,0,"The second and third issues refer to deciding the right content to cache and the best moment of caching or clearing the cache in order to avoid cache thrashing and stale content. These decisions involve (i) choosing the granularity of cache objects, (ii) translating between raw data and cache objects, and (iii) maintaining cache consistency, which are all tasks to be accomplished by developers and might not be trivial in complex applications. Della Toffola et al. [5], Nguyen and Xu [29] and Infante [30] address part of these issues by identifying and suggesting caching opportunities. However, developers should still review the suggestions and refactor the code, integrating cache logic into the application."
2011.00242,"data, code",139,2022-05-14,0,"To understand how developers integrate application-level caching logic in their applications, we analyzed the cache implementations from two perspectives. The ﬁrst consists of examining the explicit caching logic present in the application code, focusing on analyzing where the caching logic is placed, for what this logic is responsible, and when it is executed. The second evaluates the integration and communication between the application and an external caching component, which is usually used to relieve the burden on developers, easing cache implementation, raising abstraction levels, or even taking full control of caching. For this question, the most valuable data comes from source code and comments, which express implementation details. The theoretical categories referring to RQ2 are described in Table 6 and shown in Figure 5."
2011.00242,"data, code",158,2022-05-14,0,"should be placed and maintained. Although applicationlevel caching is commonly being adopted, it is typically implemented in an ad hoc way, and there are no available practical guidelines for developers to appropriately design, implement and maintain caching in their applications. To ﬁnd caching best practices, developers can make use of conventional wisdom, consult development blogs, or simply search online for tips. Nevertheless, this empirical knowledge is unsupported by concrete data or theoretical foundations that demonstrate its effectiveness in practice. Despite there are existing tool-supported approaches that can help developers to implement caching with minimal impact on the application code [4], [5], [6], they do not consider all the aforementioned issues and do not take application speciﬁcities into account, letting most part of the reasoning, as well as the integration with the tool, to developers."
2011.00242,"data, code",165,2022-05-14,0,"As previously described, this study is mainly based on development information of web applications. To investigate different caching constructs, we followed a set of steps to perform our qualitative study: (i) selection of a set of suitable web applications; (ii) speciﬁcation of a set of questions to guide us in the data analysis and (iii) analysis of each web application using the speciﬁed questions. The collected data consists of six different sources of information, explained as follows. Information about the application. Our goal is to identify caching patterns or decisions, which possibly depend on the application domain. Therefore, we collected general details of the applications to characterize them. The collected application data is (i) its description; (iii) programming languages and technologies involved; and (iii) size of the application in terms of the number of lines of code."
2011.00242,"data, code",243,2022-05-14,0,"We followed the analytical process of coding in our analysis [47], which makes it easier to search the data and identify patterns. This process combines the data for themes, ideas and categories, labeling similar passages of text with a code label, allowing them to be easily retrieved at a later stage for further comparison and analysis. We used what we learned from the analysis to adapt our evaluation approach and observation protocols. Insights we had while coding the data and clustering the codes were captured in memos. There are three coding phases in classical grounded theory: open coding, selective coding, and theoretical coding. Open coding generates concepts from the data that will become the building blocks for the theory [47]. The process of doing grounded theory is based on a concept indicator model of constant comparisons of incidents or indicators to incidents [50]. Indicators are actual data, such as behavioral actions and events observed or described in documents and interviews. In this case, an indicator may be an architectural style or design pattern adopted to implement the cache, a data structure, a class, a control ﬂow logic, a comment, a discussion in the issues platform, a paragraph in the documentation, or any other evidence we can get from the data being analyzed."
2011.00242,"data, code",50,2022-05-14,0,"We observed in eight from the ten analyzed applications that developers indicated that they had uncertainty in cache design, with respect to deciding what data should be cached and the right moment to do it, causing missed caching opportunities. There are comments in the source code and"
2011.00242,"data, code",80,2022-05-14,0,"We observed in eight from the ten analyzed applications that they present code scattering and tangling, on caching logic, causing low cohesion and high coupling in the code. Caching control code, responsible for caching data in particular places, was spread all over the base code, being invoked when application requests were processed. Consequently, there is a lack of separation of concerns, leading to increased maintenance effort (Evidence 15)."
2011.00242,"data, code",82,2022-05-14,0,"In addition to design issues, as shown in Figure 1, the cache system and the underlying source of data are not aware of each other, and the application must implement ways to interact with the cache. Therefore, our goal with RQ2 is to characterize patterns of how this implementation occurs in the application code; for example, ways to assigning names to cached values, performing lookups, and keeping the cache up to date."
2011.00242,"data, code",92,2022-05-14,0,"In this question, we focus on going beyond the facts exposed by source code and analyze the reasoning behind caching decisions such as what and why data is cached or evicted, when and where caching is done, and what are the conditions and constraints involved with this. Therefore, issues, code comments, and documentation about the cache of applications were the primary source of information to ﬁnd answers to this question, because they convey (in natural language) the rationale behind implementation decisions."
2011.00242,"data, code",93,2022-05-14,0,"Based on data presented in Table 4, we can observe a signiﬁcant amount of lines of code dedicated to implementing caching, ranging from 0.27% to 3.02%. It shows the importance of the caching in the project, considering that caching is a solution for a non-functional requirement (i.e. scalability and performance). Furthermore, caching logic is presented in a substantial amount of ﬁles, from 2.06% to 10.76%, which indicates the caching nature of being spread all over the application."
2011.00242,"data, code, database",210,2022-05-14,0,"Model or database layer. At the model or database layer, a large amount of data can be cached, for lengthy periods. It is useful to cache data from a database when it demands a long time to process queries, avoiding unnecessary round-trips. Stack caching layers. (Evidence 9) and (Evidence 10) It is reasonable to say that the more data cached, the lower the chance of being hit without any content already loaded. Caching might be at the client, proxy server, inside the application in presentation, business, and model logics, or database. Despite the same data may be cached in multiple locations, when the cache expires in one of them, the application will not be hit with an entirely uncached content, avoiding processing and network round trips. However, it is important to keep in mind that caching layers imply a range of responsibilities, such as consistency conditions and constraints, and extra code and conﬁguration. Due to this, it is important to consider many caching layers but, at the same time, achieve a good trade-off between caching beneﬁts and implementation effort."
2011.00242,"data, code, database",237,2022-05-14,0,"A code comment detailing the motivation for caching: Fetch all dashboard data. This can be an expensive request when the cached data has expired, and the server must collect the data again. A user announcement on issue platform: Caching has now landed: Fragment caching for each product; Fragment caching for the lists of products in home/index and products/index; Caching in the ProductsController, using expires in which caches for 15 minutes. A sentence in the documentation: Most stores spend much time serving up the same pages over and over again. [...] In such cases, a caching solution may be appropriate and can improve server performance by bypassing time-consuming operations such as database access. A user message on issue platform detailing the invalidation approach adopted: Ideally we cache until the topic changes (a new post is added, or a post changed) [...] less ideally we cache for N minutes A user message on issue platform: i can see that it may speed up the query responses, but is the saved time substantial enough to be worth the effort? A code snippet deﬁning a default setting: <defaultCache maxElementsInMemory=“1000” eternal=“false” timeToIdleSeconds=“60” timeToLiveSeconds=“0” overﬂowToDisk=“false” diskPersistent=“false”/>"
2011.00242,"data, code, database, data available",282,2022-05-14,0,"As already discussed in the introduction, the development of application-level caching involves four key issues: determining how, what, when, and where to cache data. The main problem is that solutions for all of these issues usually depend on application-speciﬁc details, and are manually designed and implemented by developers, as shown in the example presented in Listing 1. In this example, we assume that an e-commerce application where a class named ProductsRepository is responsible for loading products from database. To reduce database workload, caching logic is inserted in this class within the methods getProducts, updateProduct, and deleteProduct, which retrieves, updates and deletes content from the database, respectively. The ﬁrst issue is related to the cache-aside implementation and the fact that the cache system and the underlying source of data are not aware of each other (as illustrated in Figure 1). The cache system itself is a passive component, and developers must implement, directly in the application code, ways to assign names to cached values, perform lookups, and keep the cache up to date. The extra logic also requires additional testing and debugging time, which can be expensive in some scenarios. The implementation and maintenance of application-level caching are a challenge because developers must refactor all the data access logic to encapsulate cache data into the proper application-level object, and its direct management leads to a concern spread all over the system—mixed with business code—which leads to increasing complexity and maintenance time. Thus, this on-demand and manual caching process can be very"
2011.00242,"data, data available",115,2022-05-14,0,"Developers. In addition to the manual inspection of the above data, we asked developers to which we had access about caching-related implementation, decisions, challenges, and problems. To guide the extraction of the information needed from the above data for our qualitative analysis, we derived sub-questions for each research question, which convey characteristics that should be observed and evaluated while looking for answers to the main research questions, i.e. sub-questions were used to extract data. Results presented in Section 4 are derived from answers to those questions. Table 2 depicts the sub-questions derived, which serve as a checklist while analyzing our data."
2011.00242,"data, data available",120,2022-05-14,0,"Moreover, we observed that all analyzed web applications did not adopt caching since their conception. As they increased in size, usage and performance analysis were performed and led to requests for improvements. Thus, developers had to refactor data access logic to encapsulate cache data into the proper application-level object, which is a task that can be very time-consuming, error-prone and, consequently, a common source of bugs. As result, we found a signiﬁcant number of issues speciﬁcally related to caching, achieving the maximum of 4.99% of the Pencilblue issues, which can express in numbers the impact of caching in the entire project (Evidence 15)."
2011.00242,"data, data https, data available",32,2022-05-14,0,"caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 859–874, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2005.89"
2011.00242,"data, data https, data available",48,2022-04-21,0,"[38] Q. Yang and H. H. Zhang, “Web-log mining for predictive web caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 1050–1053, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2003.1209022"
2011.00242,"data, data https, database, data available",56,2022-05-14,0,"[24] P. Larson, J. Goldstein, and J. Zhou, “MTCache: Transparent mid-tier database caching in SQL server,” Proceedings of the International Conference on Data Engineering, vol. 20, pp. 177–188, mar 2004. [Online]. Available: http://dx.doi.org/10.1109/ICDE. 2004.1319994"
2011.00242,"data, database",138,2022-05-14,0,"Evaluate different abstraction levels to cache. (Evidence 9) and (Evidence 11) It is important to cache data where it reduces the most processing power and round trips, choosing locations that support the lifetime needed for the cached items, despite where it is located in the application. Different levels of caching provide different behavior, and possibilities must be analyzed. For instance, caching in the model or database level offers higher hit ratios, while caching in presentation layer can reduce the application processing overhead signiﬁcantly in the application in case of a hit. However, in the latter case, hit ratios are in general lower. It is possible to cache data at various layers of an application, according to the following layer-by-layer considerations."
2011.00242,"data, database",74,2022-05-14,0,"the business layer should be considered if an application needs to process requests from the presentation layer or when the data cannot be efﬁciently retrieved from the database or another service. It can be implemented by using hash tables, library or framework. However, at this level, a large amount of data tends to be manipulated and caching it can consume more memory and leads to memory bottlenecks."
2011.00242,"data, database",84,2022-05-14,0,"layer should be considered when data needs to be frequently displayed to the user and is not cached on a peruser basis. At this level, controllers usually work by serving parameterized content, which can be used as an identiﬁer in the cache. For example, if a list of states is presented to the user, the application can load these once from the database and then cache them, according to the parameters passed in the ﬁrst request."
2011.00242,"data, database",89,2022-05-14,0,"Recently, latency and cost of Internet-based services are driving the proliferation of application-level caching, which is placed on the server-side and typically uses a key-value inmemory store system to cache frequently accessed or expensive to compute data that remain not cached in other caching levels, lowering the load on database or services that are difﬁcult to scale up [3], [4], [5]. Therefore, application-level caching has become a popular technique for enabling highly scalable web applications."
2011.00242,"data, database, data available",237,2022-05-14,0,"Figure 1 illustrates an example where an applicationlevel cache is used to lower the application workload. First, the application receives an HTTP request (Step 1) from the web infrastructure. This HTTP request is eventually treated by an application component called M1, which in turn depends on M2. However, M2 can be an expensive operation (i.e. request the database, call a service or process a large amount of data). Therefore, M1 implements a caching layer, which veriﬁes whether the necessary processing is already in the cache before calling M2 (Step 2). Then, the cache component performs a look up for the requested data and returns either the cached result or a not found error (Step 3). If data is found, it means a cache hit and M1 can continue its execution without calling M2. If, however, a not found error is returned, it means a cache miss happened, then M2 needs to be invoked (Steps 4 and 5). The newly fetched result of M2 can be cached to serve future requests faster (Step 6) and eventually a response is sent to the user (Step 7). Furthermore, other caching layers can be deployed outside the application, at the web infrastructure."
2011.00242,"data, database, data https",67,2022-05-14,0,"[16] K. S. Candan, W.-S. Li, Q. Luo, W.-P. Hsiung, and D. Agrawal, “Enabling dynamic content caching for database-driven web sites,” Proceedings of the ACM SIGMOD International Conference on Management of Data, vol. 30, no. 2, pp. 532–543, jun 2001. [Online]. Available: http://dx.doi.org/10.1145/376284.375736"
2011.00242,"data, database, dataset provided",156,2022-05-14,0,"Regarding design choices, we observed common practices. The ﬁrst is associated with the lack of a speciﬁc approach to cache data. To process a client request, application components of distinct layers and other systems (databases, web services, and others) are invoked, and each interaction results in data transformation, which is likely cacheable. Due to this, nine analyzed applications present multiple caching solutions by not specifying cacheable layers, com ponents or data, and employing cache wherever can potentially provide performance and scalability beneﬁts (Evidence 8), no matter which is the application layer, component or data (Evidence 9). As a consequence, the same content can be cached at different places, from the database to controllers, in varying forms such as query results, page fragments or lists of objects (Evidence 10)."
2011.00242,"data, database, dataset provided",185,2022-05-14,0,"2 BACKGROUND AND RELATED WORK Focused on server-side, caching of dynamic web content can be implemented at several locations across a web-based system architecture [15]. Depending on where caching is implemented, different types of content can be stored, such as the ﬁnal HTML page [16], intermediate HTML or XML fragments [17], [18], [19], database queries [20], [21], [22], [23], or database tables [24]. These alternatives are conceived out of application boundaries and can cache dynamic data automatically, which provide transparent caching components for developers. However, they do not take application speciﬁcities into account, providing good results in general but are less effective where complex logic and personalized web content are processed within the application [25]. Therefore, application-level caching is an appealing technique to improve performance, reduce workload and make the overall user experience more pleasurable by reducing communication delays of web-based systems."
2011.00242,"data, dataset provided",200,2022-05-14,0,"Classiﬁcation: Design Intent: provide an intuitive process to decide whether to cache or not particular data. Problem: cache has limited size, so it is important to use the available space to cache data that maximizes the beneﬁts provided to the application. Otherwise, it can end up reducing application performance instead of improving it, consuming more cache memory and at the same time suffering from cache misses, where the data is not getting served from cache but is fetched from the source. Solution: even though there are many criteria that contribute for identifying the level of data cacheability, there is a subset that would conﬁrm this decision regardless of the values of the other criteria. Changeability is the ﬁrst criterion that should be analyzed while selecting cacheable data, then usage frequency, shareability, retrieval complexity, and cache properties should be considered. Figure 6 expresses a ﬂowchart of the reasoning process to decide whether to cache data, based on the observation of data and cache properties. All criteria are tightly related to the application speciﬁcities and should be speciﬁed by the developer."
2011.00242,"data, dataset provided",37,2022-05-14,0,"asynchronously with caching. Provide an intuitive process decide to whether to cache or not particular data. Given cacheable content, provide an intuitive process to choose a consistency management approach based on data speciﬁcities."
2011.00242,"data, dataset provided",85,2022-05-14,0,"Deﬁne naming conventions. (Evidence 19) and (Evidence 18) To deﬁne appropriate names for cached data, it is important to assign a name that is related to its context, the data itself, and the caching location. It can provide two direct beneﬁts: (a) prevention of key conﬂicts, and (b) guidance of cache actions such as updates and deletes of stale data in case of changes in the source of information."
2011.00242,"data, dataset, dataset provided",314,2022-05-14,0,"Rules of thumb: (a) Despite being frequently used, user-speciﬁc data are not shareable and may not bring the beneﬁt of caching, being usually avoided by developers. In this case, a speciﬁc session component is used to keep and retrieve user sessions. (b) If the data changes frequently, it should not be immediately discarded from cache. An evaluation of the performance beneﬁts of caching against the cost of building the cache should be done. Caching frequently changing data can provide beneﬁts if slightly stale data is allowed. (c) Expensive spots (when much processing is required to retrieve or create data) are bottlenecks that directly affect application performance and should be cached, even though it can increase complexity and responsibilities to deal with. Methods with high latency or that consists of a large call stack are some examples of this situation and opportunities for caching. In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects."
2011.00242,"data, dataset, dataset provided",338,2022-05-14,0,"In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects. Unless the size of the cache is large enough, do not cache large objects, it will probably result in a cache trashing problem, where the caching component is ﬂushed or replaced many times in a short period. Example: we list some typical scenarios where data should be cached and also give explanations based on the criteria presented. (a) Headlines. In most cases, headlines are shared by multiple users and updated infrequently. (b) Dashboards. Usually, much data need to be gathered across several application modules and manipulated to build a summarized information about the application. (c) Catalogs. Catalogs need to be updated at speciﬁc intervals, are shared across the application, and manipulated before sending the content to the client. (d) Metadata/conﬁguration. Settings that do not frequently change, such as country/state lists, external resource addresses, logic/branching settings and tax deﬁnitions. (e) Historical datasets for reports. Costly to retrieve or create and does not need to change frequently."
2011.00242,database,150,2022-05-14,0,"caching at a granularity best suited to the application, providing a way to cache entire HTML pages, page fragments, database queries or even computed results; rather than laying in front of the application servers (e.g., a proxy-level cache) or between the application and the database (e.g., a query or database cache). For example, many websites have highly-personalized content, thus rendering wholepage web caches is mostly useless; application-level caches can be used to separate shared content from customized content, and then the shared content can be cached and reused among users [4]. Memcached [26] and Redis [27] are popular solutions and are a critical web infrastructure component for some big players such as Amazon, Facebook, LinkedIn, Twitter, Wikipedia, and YouTube [28]."
2011.00242,database,36,2022-05-14,0,"components is a common bottleneck and, consequently, an opportunity for caching. Consider caching for database queries, remote server calls and requests to web services, which are made across a network."
2011.00242,database,76,2022-05-14,0,"The second design choice is associated with the concern of reducing the communication latency between the application and other systems, which increases the overall response time of a user request. Therefore, we noticed caching in application boundaries in nine of the analyzed applications, addressing remote server calls and requests to web services, database queries, and loads dependent on ﬁle systems, which are common bottlenecks (Evidence 11)."
2011.00242,"database, code, github",162,2022-05-14,0,"GitHub, the widely known common code hosting service. We selected GitHub projects that match the following criteria: (i) projects with some popularity (at least 350 stars); (ii) projects containing application-level caching implementation and issues (at least 50 occurrences of cache-related aspects); (iii) projects written in different programming languages; and (iv) projects of different domains. The ﬁrst criterion indicates that projects are interesting and were possibly evolved by people other than the initial developers. The second ensures that selected projects would present caching-related implementation and issues, which would contribute more to the study. The inclusion of applications was not restricted to any particular technology or programming language given that the study is focused on identifying language-independent caching patterns and guidelines expressed in the source code. Furthermore, we found applications that use cache in other architectural (e.g. database"
2011.00242,github,19,2022-05-14,0,"[55] “Google Guava,” 2016. [Online]. Available: https://github.com/"
2011.00242,github,9,2022-05-14,0,[Online]. Available: https://github.com/
2011.00242,open-source,113,2022-05-14,0,"Therefore, we analyzed systems with a broad range of characteristics. Aiming at reducing the inﬂuence of particular software features on the results, we selected systems of different sizes (from 21 KLOC to 250 KLOC, without comments and blank lines), written in different programming languages, adopting different frameworks and architectural styles, and spanning different domains. We studied ten systems in total, of which nine are open-source software projects, and one is from the industry. Due to intellectual property constraints, we will refer to the commercial system as S1. Table 3 summarizes the general characteristics of each target system."
2011.00242,open-source,119,2022-05-14,0,"are described in the issues, we classiﬁed them into three different categories, which follow the main topic of our research questions: Design (e.g. changes in the selection of content to be put in the cache), Implementation (e.g. bugs in the implementation) and Maintenance (e.g. performance tests, adjustments in replacement algorithms or size limits of the cache). Results of the performed analysis are presented in Figure 3, in which applications are ordered ascending by the number of cache-related issues, which is shown next to each application name. Only open source projects with issues related to caching are detailed in this ﬁgure."
2011.00242,open-source,134,2022-05-14,0,"is implemented, the number of LOC associated speciﬁcally with caching (without comments and blank lines), and the number of issues related to it. This analysis is shown in Table 4, in the columns #Cache Files, Cache LOC and #Cache Issues, respectively. This table also gives an overview of further information of each application to which we had access. They are some form of documentation and access to developers. It is important to note that available documentation about caching, in most cases, was limited to an abstract or general description of how caching was adopted. This documentation was available in some open source systems, and we only had access to developers of our system from the industry."
2011.00242,open-source,89,2022-05-14,0,"In order to investigate different aspects of caching, it was important to select representative systems that make extensive use of application-level caching. To obtain applications that employ application-level caching, we searched through open-source repositories, from which information can be easily retrieved for our study. Based on text search mechanisms, we assessed how many occurrences of cache implementation and issues were present in applications to ensure they would contribute to the study. The more caching-related implementation and issues, the better, so"
2011.00242,"open-source code, open-source",6,2022-05-14,0,The open-source applications were selected from
2011.00242,python,2,2022-04-21,0,PHP Python
2011.05411,data,10,2022-05-14,0,"Original training data End-users Service Provider Service Provider, Third-parties"
2011.05411,data,101,2022-05-14,0,"Along with the privacy-preserving techniques such as Secure Aggregation, diﬀerential privacy, and Homomorphic Encryption designated for protecting local ML parameters, the FL client application installed at end-users’ devices must be secure to prevent from unauthorised access, cyber-attack, or data breach directly from the devices or from the communications between the users’ devices and a coordination server. This precondition is same as any other systems in which a variety of security and privacy techniques are readily integrated into FL applications, as well as secure communications protocols such as IPSec, SSL/TLS and HTTPS"
2011.05411,data,102,2022-05-14,0,"[124] Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F., 2015. Is feature selection secure against training data poisoning?, in: International Conference on Machine Learning, pp. 1689–1698. [125] Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., Beaufays, F., 2018. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903 ."
2011.05411,data,103,2022-05-14,0,"Furthermore, the requirements of purpose limitation and data minimisation are not always feasibly carried out in MLbased systems. The majority of ML algorithms heavily rely on data quality and quantity, thus researchers tend to collect as much related data as possible. Therefore, determining 1) the purposes of data collection as well as 2) what data is adequate, limited, and relevant only to the claimed purposes before executing such ML algorithms are problematic challenges. These requirements overly restrict the natural operations of ML-based services and applications to a smaller range than ever before."
2011.05411,data,105,2022-05-14,0,"The purpose of this principle is to ensure that a Data Controller should keep personal data correctly, updated, and not misleading any matter of fact. In centralised FL settings, a coordination server does not store any individual locally trained ML model parameters except the aggregated results from a batch of participants, and the anonymised global ML model. This information is stored and processed (i.e., for updating global model) in its original form without any changes, and updated for every training round. For these reasons, FL systems automatically satisfy the GDPR accuracy principle."
2011.05411,data,107,2022-05-14,0,"Dr. Kai Sun is the Operation Manager of the Data Science Institute at Imperial College London. She received the MSc degree and the Ph.D degree in Computing from Imperial College London, in 2010 and 2014, respectively. From 2014 to 2017, she was a Research Associate at the Data Science Institute at Imperial College London, working on EU IMI projects including U-BIOPRED and eTRIKS, responsible for translational data management and analysis. She was the manager of the HNA Centre of Future Data Ecosystem in 20172018. Her research interests include translational research management, network analysis and decentralised systems."
2011.05411,data,11,2022-05-14,0,to protect data in transit between clients and the server.
2011.05411,data,112,2022-05-14,0,"ing set without having access to the original data. For instance, Hitaj et al. based on GANs have developed an attack at user-level which allows an insider to infer information from a victim just by analysing the shared model parameters in some consecutive training rounds [57]. This attack can be accomplished at client-side without interfering the whole FL procedure, even when the local model parameters are obfuscated using DP technique. A malicious coordination server can also recover partial personal data by inspecting the proportionality between locally trained model parameters sent to the server and the original data samples [4, 122]."
2011.05411,data,116,2022-05-14,0,"In most of the real-world scenarios, data, particularly personal data, is generated and stored in data silos, either end-users’ devices or service providers’ data centres. Most of conventional ML algorithms are operated in a centralised fashion, requiring training data to be fused in a data server. Essentially, collecting, aggregating and integrating heterogeneous data dispersed over various data sources as well as securely managing and processing the data are non-trivial tasks. The challenges are not only due to transporting highvolume, high-velocity, high-veracity, and heterogeneous data across organisations but also the industry competition, the complicated administrative procedures, and essentially, the"
2011.05411,data,116,2022-05-14,0,"We are now living in a data-driven world where most of applications and services such as health-care and medical services, autonomous cars, and ﬁnance applications are based on artiﬁcial intelligence (AI) technology with complex data-hungry machine learning (ML) algorithms. AI has been showing advances in every aspect of lives and expected to ""change the world more than anything in the history of mankind. More than electricity.” 1. However, the AI technology is yet to reach its full potential, also the realisation of such AI/ML-based applications has been still facing longstanding challenges wherein centralised storage and computation is one of the critical reasons."
2011.05411,data,119,2022-05-14,0,"Dr. Nguyen B.Truong is currently a Research Associate at Data Science Institute, Imperial College London, United Kingdom. He received his Ph.D, MSc, and BSc degrees from Liverpool John Moores University, United Kingdom, Pohang University of Science and Technology, Korea, and Hanoi University of Science and Technology, Vietnam in 2018, 2013, and 2008, respectively. He was a Software Engineer at DASAN Networks, a leading company on Networking Products and Services in South Korea in 2012-2015. His research interest is including, but not limited to, Data Privacy, Security, and Trust, Personal Data Management, Distributed Systems, and Blockchain."
2011.05411,data,119,2022-05-14,0,"[78] Lindell, Y., Pinkas, B., 2000. Privacy preserving data mining, in: Annual International Cryptology Conference, Springer. pp. 36–54. [79] Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, l-diversity: Privacy beyond k-anonymity. ACM Trans M., 2007. actions on Knowledge Discovery from Data (TKDD) 1, 3–es. [80] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017a. Communication-eﬃcient learning of deep networks from decentralized data, in: Artiﬁcial Intelligence and Statistics, pp. 1273– 1282."
2011.05411,data,120,2022-05-14,0,"Large-scale data collection, aggregation and processing at a central server in such ML-based systems not only entail the risks of severe data breaches due to single-point-offailure but also intensify the lack of transparency, data misuse and data abuse because the service providers are in full control of the whole data lifecycle [118]. In addition, as ML algorithms operate in a black-box manner, it is also challenging to provide insightful interpretation of how the algorithms execute and how certain decisions are made [83, 91]. Consequently, most of the ML-based systems ﬁnd it diﬃcult to satisfy the requirements of transparency, fairness, and automated decision-making in the GDPR."
2011.05411,data,127,2022-05-14,0,"ML is a disruptive technology for designing and building intelligent systems that can automatically learn and improve from experience to accomplish a task without being explicitly programmed. For this purpose, an ML-based system builds up a mathematical model (i.e., model training process) based on a sample set (i.e., training data) whose parameters are to be optimised during this training process. As a result, the system can perform better predictions or decisions on a new, unseen task. Typically, an ML task can be formulated as a mathematical optimisation problem whose goal is to ﬁnd the extremum of an objective function. Thus, an optimisation method is of paramount importance in any ML-based systems."
2011.05411,data,13,2022-05-14,0,Keywords: Federated Learning Data Protection Regulation GDPR Personal Data Privacy Privacy Preservation
2011.05411,data,133,2022-05-14,0,"Depending on encryption schemes and classes of computational operations that can be performed on an encrypted form, homomorphic encryption techniques are divided into diﬀerent categories such as partial, somewhat (SWHE), and fully homomorphic encryption (FHE)[2]. Some classic encryption techniques, including Rivest–Shamir–Adleman (RSA), is SWHE wherein simple addition and multiplication oper ations can be executed [2]. FHE, ﬁrstly proposed by Graig et al. in [45, 46], enables any arbitrary operations (thus, enables any desirable functionality) over cipher-text, yielding results in encrypted forms. In FHE, computation on the original data or the cipher-text can be mathematically transferred using a decryption function without any conﬂicts."
2011.05411,data,135,2022-04-21,0,"3. An assessment of the data security and privacy risks that might be induced by each operation, along with the technical measures implemented to mitigate and manage the risks. For instance, in an FL system, the operation of sending local ML model parameters to a coordination server for global ML model update might be the target of inference attacks, thus, inducing privacy leakage. The measures called Secure Aggregation and Homomorphic Encryption mechanisms are implemented along with the technical report. Even though such privacy-preserving methods are implemented to strengthen FL systems, there exist some risks that can be exploited for illegitimate purposes such as model poisoning with back-door sub-tasks. These possible attacks, which lead to non-compliance with the GDPR, should be addressed."
2011.05411,data,136,2022-05-14,0,"As depicted in Table 2, in FL settings, personal data is regarded as local model parameters, not the original data samples as in traditional cloud-based ML systems. A service provider, who implements an FL system, is Data Controller and Data Processor combined as the service provider (i) dictates end-users (i.e., Data Subject) to train an ML model using their local training data and to share such locally trained model, (ii) processes the local model parameters sent from end-users (i.e., aggregates and updates the global model), and (ii) disseminates the global models to all end-users and requests the end-users to update their local models. Furthermore, in centralised FL settings, a service provider can only"
2011.05411,data,136,2022-05-14,0,"This purpose limitation principle can be interpreted that an FL service provider needs to clearly inform clients about the purpose of a global ML model training as well as how clients’ local personal data and devices’ computation are used to locally train a requested ML model provided by the service provider. The principle also states that the service provider can further process the data for other compatible purposes. In this respect, FL systems fully satisfy with the principles if suﬃcient privacy-preserving mechanisms such as Secure Aggregation and diﬀerential privacy are readily implemented into the systems. This is because locally trained ML models from clients are aggregated only for the global model updates and cannot be individually extracted and exploited (by the coordination server) for other purposes."
2011.05411,data,138,2022-05-14,0,"clarify the whole data management processes along with the necessity and proportionality of these processes. Such assessments are important tools for accountability and essential to eﬃciently manage the data security and privacy risks, to demonstrate the compliance, as well as to determine the measure have been taken to address the risks. However, carrying out a DPIA or PIA is not mandatory for every data processing operation. It is only required when the operation is ""likely to result in a high risk to the rights and freedoms of natural persons"" (Article 35(1)). The guideline for the criteria on the DPIA/PIA obligatory is described under Article 35(3), 35(4) which are adopted by DPAs to carry out such assessments."
2011.05411,data,139,2022-05-14,0,"Attackers might carry out model inversion (MI) attack to extract sensitive information contained in training data samples, for instance, by reconstructing representatives of classes which characterising features in classiﬁcation ML models [38]. MI attacks do not require the attacker to actively participate in the training process (i.e., black-box or passive attacks). For example, it is possible to recover images from a facial recognition model for a particular person (i.e., all class members depict this person) using MI by deriving a correct weighted probability estimation for the target feature vectors [112, 43]. In this scenario, the experiment results show that this MI attack can reconstruct images that are visually similar to the victim’s photos [38]."
2011.05411,data,142,2022-05-14,0,"One of the fundamentals of FL is eﬃcient optimisation algorithms for federated settings wherein training data is nonIID, massively and unevenly distributed across local nodes, ﬁrst introduced by Konečn`y et al. in 2016 [70]. The distributed settings for the federated optimisation is formulated as follows. Let 𝐾 be the number of local nodes, ℙ𝑘 be the set of data samples stored on node 𝑘 ∈ {1, 2, .., 𝐾}, and be the number of data samples stored on node 𝑛𝑘 = |ℙ𝑘| 𝑘. As personal data in each local node is diﬀerent, we can assume that ℙ𝑘 ∩ ℙ𝑙 = ∅ if 𝑘 ≠ 𝑙 and ∑𝐾 𝑘=1 𝑛𝑘 = 𝑛. The distributed problem formulation for the minimisation objective is deﬁned as:"
2011.05411,data,142,2022-04-21,0,"sonal data, any previous infringement, and the nature, gravity, and duration of the current infringement. For instance, Facebook and Google were hit with a collective $8.8 billion lawsuit (Facebook, 3.9 billion euro; Google, 3.7 billion euro) by Austrian privacy campaigner, Max Schrems, alleging violations of GDPR as it pertains to the opt-in/opt-out clauses. Speciﬁcally, the complaint alleges that the way these companies obtain user consent for privacy policies is an ""all-ornothing"" choice, asking users to check a small box allowing them to access services. It is a clear violation of the GDPR’s provisions per privacy experts and the EU/UK. A list of ﬁnes and notices (with non-compliance reasons) issued under the GDPR can be found on Wikipedia12"
2011.05411,data,146,2022-05-14,0,"rameters updates and aggregation between local nodes and a central coordination server are strengthened by privacypreserving and cryptography techniques, which enhance data security and privacy [48, 123, 14, 15, 96]. The FL capability could potentially inaugurate new opportunities for service providers to implement some sorts of ML algorithms for their applications and services without acquiring clients’ personal data, hence naturally complying with data protection regulations like the GDPR. Unfortunately, despite the distributed collaborative learning model of FL empowered by privacy-preserving measures, personal information can be stealthily extracted from local training parameters [4, 96, 130, 57, 86]. As a consequence, FL-based service providers still stay within the regulatory personal data protection framework and are still liable for implementing GDPR-compliant mechanisms when dealing with EU/UK citizens."
2011.05411,data,148,2022-05-14,0,"for the aggregation of independently trained neural networks in [95]. Since then, this technique has been improved to return statistically indistinguishable results among participants while ensuring that such noise-added model parameters do not aﬀect much on the accuracy of the global model in FL settings [111, 4, 48, 1, 114, 82]. As a consequence, adversaries cannot distinguish individual records in the FL training process and do not know whether or not a targeted client participating in the training; thus, preserving data privacy and protecting against the inference attacks. Generally, there are two types of employing diﬀerential privacy techniques for local nodes in FL settings: batch-level and userlevel where random noise is added by measuring parameters’ sensitivity from data points in a mini-batch and users themselves, respectively."
2011.05411,data,15,2022-05-14,0,"Data Poisoning [11, 84, 124, 68, 23, 63]"
2011.05411,data,150,2022-05-14,0,"Furthermore, local nodes not only passively contribute local training results but also get updated about intermediate stages of a global training model from a coordination server. This practice enables the opportunity for malicious participants to manipulate the training process by providing arbitrary updates in order to poison the global model [41, 9], which calls for an investigation on security models along with insightful analysis of privacy guarantees for a centralised FL framework. Accordingly, the FL framework then needs to be strengthened by employing further privacy and security mechanisms to protect personal data eﬀectively and to comply with intricate data protection legislation like the GDPR. A summary of related articles in terms of attack models with associated privacy preservation methods in centralised FL is depicted in Table 1. Detailed descriptions along with analysis are carried out in the following subsections."
2011.05411,data,151,2022-05-14,0,"According to the ﬁrst principle, a service provider providing an FL application, as a Data Controller, must specify its legal basis in order to request end-users to participate in the FL training. There are total six legal bases required by the GDPR namely (1) Consent, (2) Contract, (3) Legal Obligation, (4) Vital Interest, (5) Public Task, and (6) Legitimate Interest (deﬁned in Article 6 of the GDPR in detail). These lawful bases might need to come along with other separate conditions for lawfully processing some special cate gory data including healthcare data, biometric data, racial and ethnic origin. Depending on speciﬁc purposes and context of the processing, the most appropriate one should be determined and documented before starting to process personal data."
2011.05411,data,154,2022-05-14,0,"One of the privacy-preserving objectives of centralised FL is that a coordination server is unable to inspect the data or administer the training process at a local node. This, however, prohibits the transparency of the training process; thus, imposes a new vulnerability of a new type of attack called model poisoning [12, 87, 22, 41, 9, 6]. Generally, model poisoning attacks aim at manipulating training process by feeding poisoned local model updates to a coordination server. This type of attack is diﬀerent from data poisoning [11, 84, 124, 68, 23, 63], which is less eﬀective in FL settings [9, 6] because the original training data is never shared with a server. Thus, this section is mainly dedicated to analysing the model poisoning attacks in FL."
2011.05411,data,16,2022-05-14,0,"[44] Gentry, C., 2010. Computing arbitrary functions of encrypted data."
2011.05411,data,166,2022-05-14,0,"In this paper, we examine the centralised FL in which there exists a centralised server (i.e., service provider) requests to coordinate the whole training process. Speciﬁcally, this coordination server (i) determines a global model to be trained, (ii) selects participants (i.e., local nodes) for each training round, (iii) aggregates local training results sent by the participants, (iv) updates the global model based on the aggregated results, (v) disseminates the updated model to the participants, and (vi) terminates the training when the global model satisﬁes some requirements (e.g., accurate enough). Local nodes passively train the model over their local data as requested, and send the training results back to the server whenever possible. The workﬂow cycle in a centralised FL framework consists of four steps (illustrated in Fig. 2) as follows:"
2011.05411,data,169,2022-05-14,0,"Nevertheless, both centralised and decentralised architectures are required to acquire model consistency, particularly when data parallelism is employed. There are numerous strategies to update parameters in order to maintain the consistency of a global model, respected to a synchronisaIn this regard, Asyntion model among compute nodes. chronous Parallel (ASP) [99, 29], Bulk Synchronous Parallel (BSP) [47], and Stale Synchronous Parallel (SSP) [58] are the most common approaches to update parameters in a distributed learning system. The BSP and the ASP update parameters once receiving all gradients from a bulk of compute nodes (barrier synchronisation) and from just any node (no synchronisation), respectively. Generally, the BSP is relatively slow due to the stall time of waiting whereas ASP is faster as it does not perform any synchronisation; as a tradeoﬀ, the convergence in BSP is guaranteed but uncertain in"
2011.05411,data,171,2022-05-14,0,"can be generated by injecting a hidden backdoor model intentionally, as illustrated in Fig. 4. Compromised participants analyse the targeted global model; the poisoned model is then trained on backdoor data samples using dedicated techniques such as constrain-and-scale accordingly, and feed the parameters to a coordination server as other honest participants. The objective of this attack is that the global model is replaced by a joint model consisting of both original task and the injected backdoor sub-task while retaining high accuracy on the two. The backdoor training at the adversary can be empowered by modifying minimisation strategies such as constrain-and-scale, which optimises both gradients of the loss and the backdoor objective [6]. A parameter estimation mechanism is then used for generating parameters submitted to the coordination server for honest participants’ updates. As secure aggregation is used for preventing the server from inspecting individual models, this poisoning model is unable to detect [9, 6]."
2011.05411,data,177,2022-05-14,0,"To ensure privacy, an FL system is designed in a way that does not let the service provider (i.e., the coordination server) to directly access and obtain either original training data or locally trained ML models at end-users’ devices. Instead, end-users, as participants in the FL system, will only send the results back to the coordination server when they are ready. An FL client-side application should oﬀer several options for clients to participate in the training process proactively that allows a client to fully control the local training as well as of the sending/receiving ML model updates to/from a coordination server. Furthermore, FL systems only process data (i.e., local ML model parameters) for an explicit purpose (i.e., aggregates results and updates a global model), which is in ways that clients would reasonably expect whilst having minimal privacy impact. For these reasons, either Consent or Legitimate Interest legal basis can be appropriate for an FL application10."
2011.05411,data,180,2022-05-14,0,"In this federated setting, minimising the number of iterations in the optimisation algorithms is paramount of importance as there is limited communication capability of the local nodes. In the same paper, Konečn`y et al. proposed a novel distributed gradient descent by combining the Stochastic Variance Reduced Gradient (SVRG) algorithm [64, 72] with the Distributed Approximate Newton algorithm (DANE) [110] for distributed optimisation called Federated SVRG (FSVRG) [70]. The FSVRG computes gradients based on data on each local node 𝑘, obtains a weighted average of ℙ𝑘 the parameters from all the 𝐾 local nodes, and updates new parameters for each node after round. This algorithm is then experimented based on public Google+ posts, clustered by about 10, 000 users as local nodes, for predicting whether a post will receive any comments. The results show that the FSVRG outperforms the native gradient descent algorithm as it converges to the optimum within only 30 iterations."
2011.05411,data,181,2022-04-21,0,"4. Privacy-Preservation in Centralised Federated Learning Framework As an ML model can be cooperatively trained while retaining training data and computation on-device, FL naturally oﬀers privacy-guarantee advantages compared to the traditional ML approaches. Unfortunately, although personal data is not directly sent to a coordination server in its original form, the local ML model parameters still contain sensitive information because some features of the training data samples are inherently encoded into such models [5, 81, 4, 96, 86]. For example, authors in [5] have shown that during the training process, correlations implied in the training data are concealed inside the trained models, and personal information can be subsequently extracted. Melis et al. have also pointed out that modern deep-learning models conceal internal representations of all kinds of features, and some of them are not related to the task being learned. Such unintended features can be exploited to infer some information about the training data samples. FL systems, consequently,"
2011.05411,data,186,2022-05-14,0,"Model poisoning attacks are always inherent in collaborative learning including FL. As shown by Bagdasaryan et in [6], just by controlling less than 1% Byzantine paral. ticipants, an adversary can successfully insert a backdoor functionality into a global model without reducing much accuracy, preventing the coordination server from detecting the attack. Solutions to mitigate model poisoning attack at server-side have to detect and ﬁlter out poisoned model updates from malicious clients (i.e., model anomaly detection) [41, 63]. For this purpose, the server needs to access either participants’ training data or parameter model updates, which breaks the privacy-preservation catalyst of FL. Besides, Secure Aggregation protocol is assumed to be implemented at both client- and server-side, which prevents the server from inspecting individual model updates; consequently, ruling out any solutions for model poisoning attacks [41]. Indeed, no resolutions have been proposed that eﬀectively tackle model poisoning attacks at server-side, which imposes as a critical research topic for FL."
2011.05411,data,194,2022-05-14,0,"The data minimisation principle in the GDPR necessitates a Data Controller (e.g., a service provider) to collect and process personal data that is adequate, limited, and relIn traditional centralised evant only to claimed purposes. ML algorithms, this data minimisation requirement is a challenge as it is not always possible to envision what data and the minimal amount of data are necessary for training an ML model. In this regard, FL appears as a game-changer as an FL system does not need to collect and process original training data; instead, a service provider only needs to gather local ML models from participants for assembling the global model. Generally, with privacy-preserving techniques introduced in Section 4, an FL system can assure that the coordination server obtains aggregated local model parameters from participants for global model updates only (i.e., the claimed purposes) while acquiring nothing about individual’s contribution. The aggregation mechanism also assures that the global model itself contains no individual sensitive features that can be exploited by adversaries to extract or infer any personal information."
2011.05411,data,196,2022-05-14,0,"The challenge to provide this right to Data Subjects is that the GDPR demands the Data Controller to concisely, intelligibly, and speciﬁcally specify what and how the local ML model is used in the FL training, along with expected outputs of the mechanism11. Same as many complex ML mechanisms, FL is as a black-box model; thus, it cannot be precisely interpreted of how it works and predicting the outcomes. The GDPR supervisory board recognises the challenges and relaxes the requirement for AI/ML mechanisms by accepting a general explanation as an indication of how and what personal data is going to be processed. As a result, for an FL system, the right to be informed is achieved as privacy information including purposes for processing local ML model (i.e., to build a global ML model), retention periods (i.e., no longer in use after each training round), and who it will be shared with (only the coordination server) can be determined as in Terms and Conditions when a client accepts to participate in an FL system."
2011.05411,data,20,2022-05-14,0,"[24] Chen, X.W., Lin, X., 2014. Big data deep learning: challenges and"
2011.05411,data,204,2022-05-14,0,"Federated Learning Systems FL emerges a new approach to tackle data privacy challenges in ML-based applications by decoupling of data storage and processing (i.e., local model training) at end-users’ devices (i.e., local nodes) and the aggregation of a global ML model at a service provider’s server (i.e., a coordination server). The privacy-preservation advantage of FL compared to the traditional centralised ML approaches is undeniable: It enables to train an ML model whilst retaining personal training data on end-users’ devices. Only locally trained model parameters, which contain the essential amount of information required to update the global model, are shared with a coordination server. Nevertheless, such model parameters still enclose some sensitive features that can be exploited to reconstruct or to infer related personal information as depicted in Section 4. Subsequently, an FL system still retains within the GDPR and is liable for complying with obligatory requirements. This section closely examines whether a GDPR requirement should be complied or inapplicable and should be waived in FL settings. Unsolved challenges on fully complying with the GDPR are also determined and discussed."
2011.05411,data,205,2022-05-14,0,"In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with diﬃculties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not suﬃcient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR."
2011.05411,data,210,2022-04-21,0,"number of data samples and data distributions among personal mobile devices. Training over non-IID data has been shown to be much less accurate as well as slower convergence than IID data in federated settings [128]. Konečn`y with his colleagues at Google went further on improving the eﬃciency of the FSVRG algorithms in distributed settings by minimising the information in parameter update to be sent to an orchestration server [71]. Two types of updates are considered called structured updates and sketched updates in which the number of variables used in an ML model is minimised as many as possible, along with the compression of the information in the full model updates. Another ambitious federated optimisation approach is that local nodes are independently trained diﬀerent ML models as a task in a multi-learning objective simultaneously [113]. Generally, local nodes generate data under diﬀerent distributions which naturally ﬁt separate learning models; however, these models are structurally similar resulting in the ability to model the similarity using a multi-tasking learning (MTL) framework. Therefore, this approach improves performance when dealing with non-IID data as well as guarantees the learning convergence [113]."
2011.05411,data,22,2022-05-14,0,"[59] Horvitz, E., Mulligan, D., 2015. Data, privacy, and the greater good."
2011.05411,data,234,2022-04-21,0,"A Data Subject is assumed to have the right ""not to be subject to a decision based solely on automated processing, including proﬁling"" - Article 22(1), the GDPR. Therefore, an FL client, as a Data Subject, has the right to receive meaningful information and explanation about whether the result of the processing (i.e., a global ML model) used in an automated decision-making system will produce legal eﬀects concerning the client or similarly signiﬁcantly aﬀects the client. Unfortunately, due to the black-box operation model and the limitation of the transparency in ML, including FL, training process, results (e.g., a global ML model in FL) are generally generated without any proper explanation [119]. Thus, it is infeasible to predict whether outcomes of an ML model might aﬀect the legal status or legal rights of the Data Subject, or negatively impact on its circumstances, behaviour or choices. Consequently, any FL system fails to comply with the GDPR requirements of the data subject’s right in control of automated decision making. Fortunately, this requirement can be neglected if a Data Controller explicitly mentions the lack of automated decision making and proﬁling right when asking for Data Subject’s consent to process personal data."
2011.05411,data,242,2022-05-14,0,"data; instead, inferring attributes or membership of the original trained data from local model parameters can also induce serious privacy leakage [42, 85, 86, 93, 94] (e.g., an attacker can ﬁgure out whether a speciﬁc data sample (of a patient) is used to train a model of a disease). This is the baseline for the membership attack. Authors in [85, 86, 94] have investigated membership attacks in FL and demonstrated the capability of these attacks in both passive and active approaches. For instance, the gender of a victim can be inferred with a very high accuracy of 90% when conducting this attack in a binary gender classiﬁer on the FaceScrub dataset7. Other features, which are uncorrelated with the main task, can also be inferred such as race and facial appearance (e.g., whether a face photo is wearing glasses) [86]. Nasr et al. proposed an active attack approach called gradient ascent by exploiting the privacy vulnerabilities of SGD optimisation algorithms. This attack based on the correlation between the local gradients of the loss and the direction and the amount of changes of model parameters when minimising the loss to ﬁt a model to train data samples in the SGD algorithms. This active membership attack was conducted on the CIFAR100"
2011.05411,data,279,2022-05-14,0,"The nature of decoupling between data storage and processing at client-side and global ML model aggregation at server-side in centralised FL leads to the unnecessity of providing the (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure, (5) Right to restrict processing, (6) Right to data portability, and (7) Right to object. For instance, regarding the ""Right to erasure"", if a user requests to delete its data (i.e., local ML model parameters sent to an FL server), literally, the only way to fulﬁl the user’s request is to thoroughly re-train the global model without using user’s data from the round that the user ﬁrst participates [50]. This is unnecessary and impractical in FL settings as only local ML model parameters (possibly privacy guarantee-strengthened with diﬀerential privacy) in aggregated encrypted forms (by using Secure Aggregation and other advanced cryptography techniques) are shared with a coordination server. Consequently, it is worthless for a Data Subject to have control over its local ML model as (i) the model parameters are protected by privacy-preserving techniques from inference attacks; (ii) the server is unable to separate the user’s data from the others, the server also does not store the model once it is aggregated to update the global model; and (iii) the global model is wholly anonymised and cannot be exploited to extract or infer any individual information."
2011.05411,data,283,2022-05-14,0,"Standing on these federated optimisation research works, McMahan et al. proposed a variation of the SGD called FederatedSGD along with the Federated Averaging algorithm that can train a deep network at 100 times fewer communications compared to the naive FSVRG [81, 80]. The catalyst of such algorithms is to leverage the increasingly powerful processors in modern personal mobile devices to perform high-quality updates than simply calculating gradient steps. Speciﬁcally, each client not only calculates the gradients but also computes the local model for multiple times; the coordination server only performs aggregation of the local models from the clients. This results in fewer training rounds iterations (thus fewer communications) while producing a decent global model. These proposed algorithms well suited for scenarios that are highly limited communication bandwidth with high jitter and latency. In these scenarios, the naive FSVRG algorithms proposed in [70, 71] are not eﬃcient enough. Indeed, the algorithms are utilised for a realworld application for text prediction in Google keyboard in Android smartphones (i.e., G-board)5 [125]. In this system setting, the FederatedSGD is executed locally on the smartphone to compute gradient descent using local data. The gradient is then sent to an aggregation server. This server performs the FederatedAveraging algorithm which randomly selects a fraction of smartphones for each training round, and takes the average of all gradients sent from the selected participants to update the global model. This updated global model is distributed to all participants; the local nodes will then update their local models accordingly."
2011.05411,data,293,2022-05-14,0,"As FL is in the early stage, a fruitful area of multi-disciplinary research is commenced in order to ﬂourish the technology and to comply with the GDPR fully. Firstly, eﬃcient cryptographic and privacy primitives for decentralised collaborative learning must be further developed, particularly for counteracting model poisoning and inference attacks. Furthermore, as these privacy-preserving techniques such as SMC impose non-trivial performance overheads, further eﬀort on how to eﬃciently utilise such techniques on FL applications are required. Secondly, research on transparency, interpret-ability and algorithm fairness in FL systems should be profoundly carried out. Even though a sub stantial amount of research has been conducted in centralised AI/ML settings, there is still an open question whether these approaches could be employed and how to sensibly adapt them to the decentralised settings where training data is highly skewed non-IID and unevenly distributed across sources. The sampling constraints should be investigated to see how much extend they aﬀect and how to mitigate the bias of the global training model. For instance, the agnostic FL framework introduced in [89] naturally yields good-intent fairness as it modelled the target distribution as an unknown mixture of the distributions instead of the uniform distribution in typical FL training algorithms. This agnostic FL framework, as a result, can control for bias in the training objective. Thirdly, it requires more research on interpretable and unbiased ML models and algorithms that can be employed over encrypted settings to well consolidate with advanced encryption schemes in FL systems. Besides, the trade-oﬀs between privacy utility, accuracy, interpretability, and fairness in an FL framework need to be thoroughly explored."
2011.05411,data,3,2022-04-21,0,11https://ico.org.uk/for-organisations/guide-to-data-protection/guideto-the-general-data-protection-regulation-gdpr/individual-rights/right-tobe-informed
2011.05411,data,31,2022-05-14,0,This research was supported by the HNA Research Centre for Future Data Ecosystems at Imperial College London and the Innovative Medicines Initiative 2 IDEA-FAST project under grant agreement No 853981.
2011.05411,data,316,2022-05-14,0,"This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]. We present a systematic summary of the threat models, possible attacks, and the privacy-preserving techniques in FL systems, along with the analysis of how these techniques can mitigate the risk of privacy leakages. Furthermore, insightful analysis of how an FL-system complies with the GDPR is also provided. Obligations and appropriate measures for a service provider to implement a GDPR-compliant FL system are examined in details following the rational guidelines of the GDPR six principles."
2011.05411,data,32,2022-05-14,0,"member of the Academia Europaea. His research interests are in the areas of data mining for largescale scientiﬁc applications including distributed data mining methods, machine learning and informatics systems."
2011.05411,data,330,2022-05-14,0,"AI/ML-based applications and services are high on the agenda in most sectors. However, the unregulated use or misuse of personal data is dramatically spreading, resulting in severe concerns of data privacy. A series of severe personal data breaches such as Facebook’s Cambridge Analytica scandal, along with urgent mobile applications during the SARS-CoV2 pandemic for large-scale contact tracing and movement tracking [61] trigger worldwide attention respecting to a variety of privacy-related aspects including algorithm bias, ethics, implications of politic settings, and legal responsibility. This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]."
2011.05411,data,35,2022-05-14,0,"[128] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V., arXiv preprint Federated learning with non-iid data."
2011.05411,data,37,2022-05-14,0,"It is worth noting that standard distributed ML algorithms are generally designed to train independent identically-distributed (IID) data, and this assumption does not hold in federated settings due to the signiﬁcant diﬀerences of the"
2011.05411,data,37,2022-05-14,0,"[108] Sarwate, A.D., Chaudhuri, K., 2013. Signal processing and machine learning with diﬀerential privacy: Algorithms and challenges for continuous data. IEEE signal processing magazine 30, 86–94."
2011.05411,data,38,2022-05-14,0,• Provide insightful examination on pros and cons of the existing privacy-preserving techniques as well as prospective solution approaches in order for a FL-based service to comply with the EU/UK General Data Protection Regulation (GDPR).
2011.05411,data,4,2022-04-21,0,10https://ico.org.uk/for-organisations/guide-to-data-protection/guide to-the-general-data-protection-regulation-gdpr/lawful-basis-forprocessing/
2011.05411,data,4,2022-04-21,0,2.2.1. Data Anonymisation
2011.05411,data,4,2022-04-21,0,5.2.3. Data Minimisation
2011.05411,data,40,2022-05-14,0,"[23] Chen, X., Liu, C., Li, B., Lu, K., Song, D., 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 ."
2011.05411,data,42,2022-05-14,0,"[36] Dwork, C., Smith, A., Steinke, T., Ullman, J., 2017. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application 4, 61–84."
2011.05411,data,45,2022-05-14,0,"[50] Ginart, A., Guan, M., Valiant, G., Zou, J.Y., 2019. Making ai forget you: Data deletion in machine learning, in: Advances in Neural Information Processing Systems, pp. 3518–3531."
2011.05411,data,46,2022-05-14,0,"[31] Du, W., Han, Y.S., Chen, S., 2004. Privacy-preserving multivariate statistical analysis: Linear regression and classiﬁcation, in: Proceedings of the 2004 SIAM international conference on data mining, SIAM. pp. 222–233."
2011.05411,data,53,2022-05-14,0,"The natural advantage of FL compared to the traditional cloud-centric ML approaches is the ability to reassure data privacy and (presumably) comply with the GDPR because personal data is stored and processed locally, and only model parameters are exchanged. In addition, the processes of pa 2https://gdpr-info.eu/"
2011.05411,data,54,2022-04-21,0,"1. A systematic description of data processing operations, associated purposes, along with clariﬁcation and justiﬁcation of the operations. For instance, the operation of asking Data Subject’s consent for local ML training and sending the ML model parameters to a coordination server should be documented in detail."
2011.05411,data,54,2022-05-14,0,"Even though homomorphic encryption oﬀers rigorous privacy-guarantee to individuals as the original data in plaintext has never been disclosed, there is a practical limitation in performing computation over cipher-text due to the tremendous computational overhead. As a consequence, employing homomorphic encryption in large-scale data training remains impractical [49]."
2011.05411,data,55,2022-05-14,0,"[14] Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K., 2016. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482 ."
2011.05411,data,55,2022-05-14,0,"[49] Gilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M., Wernsing, J., 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy, in: International Conference on Machine Learning, pp. 201–210."
2011.05411,data,56,2022-05-14,0,"[34] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M., 2006. Our data, ourselves: Privacy via distributed noise generation, in: Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer. pp. 486–503."
2011.05411,data,56,2022-05-14,0,"[5] Ateniese, G., Mancini, L.V., Spognardi, A., Villani, A., Vitali, D., Felici, G., 2015. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classiﬁers. International Journal of Security and Networks 10, 137–150."
2011.05411,data,62,2022-05-14,0,"Diﬀerential privacy technique has been widely employed in various ML algorithms such as linear and logistic regression [19], Support Vector Machine (SVM) [102] and deep learning [20, 1], as well as in ML-based applications such as data mining [39] and signal processing with continuous data [108]."
2011.05411,data,62,2022-05-14,0,"This principle obligates Data Controllers to implement appropriate measures in place to eﬀectively protect personal data. Thus, in order to comply with this principle, a centralised FL system requires to implement security and privacy mechanisms not only at a coordination server but also at end-users’ devices as the FL system itself does not guarantee security and privacy."
2011.05411,data,66,2022-04-21,0,The GDPR deﬁnes 6-core principles as rational guidelines for service providers to manage personal data as illustrated in Fig. 7 (The GDPR Articles 5-11). These principles are broadly similar to the principles in the Data Protection Act 1998 with the accountability that obligates Data Controllers to take responsibility for complying with the principles and implementing appropriate measures to demonstrate the compliance.
2011.05411,data,68,2022-05-14,0,"Basically, this principle ensures that a Data Controller does not keep personal data for longer if the data is no longer needed for the claimed purposes. In this case, data should be erased or anonymised. There is an exception for data retention only if the Data Controller keeps the data for public interest archiving, scientiﬁc or historical research, or statistical purposes."
2011.05411,data,75,2022-05-14,0,"Mr. Siyao Wang is a PhD student of the Data Science Institute at Imperial College London. He received the BSc degree in Computer Science and Technology from the University of Chinese Academy of Sciences in 2018. He received the MRes degree in Medical Robotics and ImageGuided Intervention from Imperial College London in 2019. His research interests include machine learning, deep learning, computer vision and artiﬁcial intelligence applications in healthcare."
2011.05411,data,75,2022-05-14,0,"[61] Ienca, M., Vayena, E., 2020. On the responsible use of digital data to tackle the covid-19 pandemic. Nature medicine 26, 463–464. [62] Jagannathan, G., Wright, R.N., 2005. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 593–599."
2011.05411,data,77,2022-04-21,0,"3. Federated Learning: A Distributed Collaborative Learning Approach In many scenarios, the traditional cloud-centric ML approaches are no longer suitable due to the challenges of complying with strict data protection regulations on vast aggregation and processing personal data. By nature, most personal data is generated at the edge by end-users’ devices (e.g., smart phones, tablets, and wearable devices) which are equipped with increasingly powerful computing capability"
2011.05411,data,8,2022-05-14,0,Personal Data Data Subject Data Controller Data Processor
2011.05411,data,81,2022-05-14,0,"[39] Friedman, A., Schuster, A., 2010. Data mining with diﬀerential privacy, in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 493–502. [40] Fung, B.C., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: A survey of recent developments. ACM Computing Surveys (Csur) 42, 1–53."
2011.05411,data,85,2022-05-14,0,"[118] Truong, N.B., Sun, K., Lee, G.M., Guo, Y., 2019. Gdpr-compliant personal data management: A blockchain-based solution. IEEE Transactions on Information Forensics and Security 15, 1746–1761. [119] Wachter, S., Mittelstadt, B., Floridi, L., 2017. Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7, 76– 99."
2011.05411,data,87,2022-05-14,0,"As illustrated in Fig. 8, the investigation of non-compliance and decision of punishment are carried out by DPAs once there is a suspicion or a claim ﬁled by a customer. The compliance inspection will conduct some analysis to see whether a suspicious organisation follows the legal requirement of Privacy&Security-by-design approach and satisﬁes some standard assessments such as Data Protection Impact Assessment (DPIA) and Privacy Impact Assessment (PIA), which are essential parts of the GDPR accountability obligations."
2011.05411,data,87,2022-05-14,0,"data protection regulations and restrictions such as the EU General Data Protection Regulation (GDPR)2 [59]. In traditional ML algorithms, large-scale data collection and processing at a powerful cloud-based server entails the singlepoint-of-failure and the risks of severe data breaches. Foremost, centralised data processing and management impose limited transparency and provenance on the system, which could lead to the lack of trust from end-users as well as the diﬃculty in complying with the GDPR [118]."
2011.05411,data,94,2022-05-14,0,"The GDPR requires Data Controllers to provide the following rights for Data Subjects if capable (The GDPR Articles 12-23): (1) Right to be informed, (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure (Right to be forgotten), (5) Right to restrict processing, (6) Right to data portability, (7) Right to object, and (8) Rights in relation to automated decision making and proﬁling."
2011.05411,data,94,2022-05-14,0,"methods. Frontiers in Applied Mathematics and Statistics 3, 9. [73] Li, N., Li, T., Venkatasubramanian, S., 2007. t-closeness: Privacy beyond k-anonymity and l-diversity, in: 2007 IEEE 23rd International Conference on Data Engineering, IEEE. pp. 106–115. [74] Li, O., Liu, H., Chen, C., Rudin, C., 2017. Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions. arXiv preprint arXiv:1710.04806 ."
2011.05411,data,95,2022-05-14,0,"Dr. Yike Guo (FREng, MAE) is the director of the Data Science Institute at Imperial College London and the Vice-President (Research and Development) of Hong Kong Baptist University. He received the BSc degree in Computing Science from Tsinghua University, China, in 1985 and received the Ph.D in Computational Logic from Imperial College London in 1993. He is a Professor of Computing Science in the Department of Computing at Imperial College London since 2002. He is a fellow of the Royal Academy of Engineering and a"
2011.05411,data,95,2022-05-14,0,"Mr. Florian Guitton received a BSc in Software Engineering from Epitech (France) in 2011 and a MSc in Advanced Computing from the University of Kent (United Kingdom) in 2012. In 2012 he joined the Discovery Sciences Group at Imperial College London where he became Research Assistant working on iHealth, eTRIKS and IDEA-FAST EU programs. He is currently a PhD candidate at Data Science Institute, Imperial College London working on distributed data collection and analysis pipeline in mixed-security environments with the angle of optimising user facing experiences."
2011.05411,data,98,2022-05-14,0,"share a global ML model, which can be considered as anonymous information, with third-parties as it does not possess any other personal data (e.g., original training data as in traditional ML systems). Therefore, Data Processors in FL settings are also the service providers, but not other players (i.e., third-parties). The processing mechanisms in FL are also uncomplicated compared to the traditional ones as they are only related to the aggregation of the local ML models as well as the update of the global ML model."
2011.05411,"data, data available",150,2022-05-14,0,"Normally, DPAs might require a variety of information with a detailed explanation from Data Controller to perform the analysis including documents of organisational and technical measures related to the implementation the GDPR requirements as well as independent DPIA and PIA reports frequently conducted by the Data Controller. DPAs may also require to be given access to data server infrastructure and management system including personal data that is being processed. In this respect, besides the legal basis such as consents from end-users, an FL service provider can only provide documentation of how FL-related mechanisms are implemented along with privacy-preserving technical measures such as secure aggregation, diﬀerential privacy, and homomorphic encryption. Other inquiries from DPAs such as direct access to the FL model training operations and inspection of individual local model parameters from a particular end-user are technically infeasible for any FL systems."
2011.05411,"data, data available",85,2022-05-14,0,"FL settings. Training data in FL is unbalanced and non-IID, which is scattered across millions of personal mobile devices with signiﬁcant higher-latency, lower-throughput connections compared to the traditional techniques working on a cloud-centric data server. In addition, the data and computing resources in personal devices are only intermittently available for training. Therefore, to actualise FL, optimisation algorithms must be well adapted and eﬃciently performed for federated settings (i.e., federated optimisation [70])."
2011.05411,"data, dataset",114,2022-05-14,0,"Another approach to preserve data privacy and security in ML is to utilise homomorphic encryption techniques, particularly in centralised systems, e.g., cloud servers, wherein data is collected and trained at a server without disclosing the original information. Homomorphic encryption enables the ability to perform computation on an encrypted form of data without the need for the secret key to decrypt the ciphertext [44]. Results of the computation are in encrypted form and can only be decrypted by the requester of the computation. In addition, homomorphic encryption ensures that the decrypted output is the same as the one computed on the original unencrypted dataset."
2011.05411,"data, dataset",120,2022-05-14,0,"As aforementioned, a trained ML model contains unintended features that can be utilised to extract personal information. Thus, local ML model parameters from a federated optimisation algorithm can be exploited by an adversary to infer personal information, particularly when combining with related information such as model data structure and meta-data. This information can be either original training data samples (i.e., reconstruction attack) [38, 111, 81, 4, 57, 96, 112, 6, 93, 130, 43] or membership tracing (i.e., to check if a given data point belongs to a training dataset) [15, 112, 86]."
2011.05411,"data, dataset",136,2022-05-14,0,"The authors in [1] have proposed an SGD algorithm integrated with diﬀerential privacy performing over some batches (a group) of data samples. This algorithm estimates the gradient of the group by taking the average of the gradient loss of these batches and adds noise (generated by Gaussian mechanism) to the group to protect the privacy. This algorithm is implemented to train on the MNIST and CIFAR-10 datasets showing sensible results as it achieves only 1.3% and 7% less accurate compared to the non-diﬀerentially private conventional baseline algorithms on the same datasets, respectively. Similar to the mechanism proposed by Shokri and Shmatikov in [111], the authors have proposed a mechanism to monitor the total privacy budget (i.e., privacy accounting)"
2011.05411,"data, dataset",161,2022-05-14,0,"Generally, there are three gradient descent methods that are categorised based on the amount of training data used in the gradient calculation of the objective function 𝑓 (𝜃) [103]. The ﬁrst category is batch gradient descent, in which the gradients are computed over the entire training dataset  for one update. The second category is stochastic gradient descent (SGD), that, in contrast to batch gradient descent, randomly selects a sample (or a subset) from  and performs the parameters update based on the gradient of this sample only (one sample per step, the whole process sweeps through the entire dataset). The third one is mini-batch gradient descent in which the dataset is subdivided into mini-batches of 𝑛 training samples (𝑛 is the batch-size); the parameters update is then performed on every mini-batch (single minibatch per step)."
2011.05411,"data, dataset",172,2022-05-14,0,"Furthermore, the local nodes can leverage the perturbation method to prevent a coordination server and other adversaries from disclosing model parameters updates and original training dataset. The idea of employing perturbation technique to FL is that a local node adds random noise to its local model parameters in order to obscure certain sensitive attributes of the model before sharing. As a result, adversaries, in case it can successfully derive such model parameters, is unable to accurately reconstruct the original training data or infer some related information. In other words, the perturbation method could prevent adversaries from carrying out inference attacks on a local model trained by a particular client. This privacy-preservation method typically adopts diﬀerential privacy technique that adds random noises to either training dataset or model parameters, oﬀering statistical privacy guarantees for individual data [35, 33, 7]. Indeed, before the proposal of FL, diﬀerential privacy with SMC has been suggested as a privacy-preserving technique"
2011.05411,"data, dataset",178,2022-05-14,0,"Data anonymisation or de-identiﬁcation is a technique to hide (e.g., hashing) or remove sensitive attributes, such as personally identiﬁable information (PII), so that a data subject cannot be identiﬁed within the modiﬁed dataset (i.e., the anonymous dataset) [92]. As a consequence, data anonymisation has to balance well between privacy-guarantee and utility because hiding or removing information may reduce the utility of the dataset. Furthermore, when combined with auxiliary information from other anonymous datasets, a data subject might be re-identiﬁed, subjected to a privacy attack called linkage attack [40]. To prevent from linkage attack, numerous techniques have been proposed such as k-anonymity [116], l-diversity [79], a k-anonymity-based method, and tcloseness - a technique built on both k-anonymity and l-diversity that preserves the distribution of sensitive attributes in a dataset so that it reduces the risk of re-identifying a data subject in a same quasi-identiﬁer group [73]."
2011.05411,"data, dataset",186,2022-05-14,0,"To overcome such challenges, Federated Learning (FL), proposed by Google researchers in 2016, has appeared as a promising solution and attracted attention from both industry and academia [70, 71, 81, 80]. Generally, FL is a technique to implement an ML algorithm in decentralised collaborative learning settings wherein the algorithm is executed on multiple local datasets stored at isolated data sources (i.e., local nodes) such as smart phones, tablet, PCs, and wearable devices without the need for collecting and processing the training data at a centralised data server. FL allows local nodes to collaboratively train a shared ML model while retaining both training dataset and computation at internal sites [70]. Only results of the training (i.e., parameters) are exchanged at a certain frequency, which requires a central server to coordinate the training process (centralised FL) or utilises a peer-to-peer underlying network infrastructure (i.e., decentralised FL) to aggregate the training results and calculate the global model."
2011.05411,"data, dataset",199,2022-05-14,0,"In this regard, FL is an alternative for the cloud-centric ML technique that facilitates an ML model to be trained collaboratively while retaining original personal data on their devices, thus potentially mitigates data privacy-related vulnerabilities. It is a cross-disciplinary technique covering multiple computer science aspects including ML, distributed computing, data privacy and security that enables end-users’ devices (i.e., local nodes) to locally train a shared ML model on local data. Only parameters in the training process are exchanged for the model aggregation and updates. The diﬀerence between FL and the standard distributed learning is that in distributed learning, local training datasets in compute nodes are assumed to be independent and identically distributed data (IID) whose their sizes are roughly the same. FL is, thus, as an advancement of distributed learning as it is designed to work with unbalanced and non-independent identically-distributed data (non-IID) whose sizes may span several orders of magnitude. Such heterogeneous datasets are resided at a massive number of scattering mobile devices under unstable connectivity and limited communication bandwidth [81, 80, 65]."
2011.05411,"data, dataset",200,2022-05-14,0,"SMC, also known as multi-party computation (MPC) or privacy-preserving computation, was ﬁrstly introduced by Yao in 1986 [126] and further developed by numerous researchers. Its catalyst is that a function can be collectively computed over a dataset owned by multiple parties using their own inputs (i.e., a subset of the dataset) so that any party learns nothing about others’ data except the outputs [51, 18, 27]. Speciﬁcally, 𝑛 parties 𝑃1, 𝑃2, .., 𝑃𝑛 own 𝑛 pieces of pri, respectively to collectively comvate data 𝑋1, 𝑋2, ..., 𝑋𝑛 pute a public function 𝑓 (𝑋1, 𝑋2, .., 𝑋𝑛) = (𝑌1, 𝑌2, .., 𝑌𝑛). The only information each party can obtain from the computation is the result (𝑌1, 𝑌2, .., 𝑌𝑛) and its own inputs 𝑋𝑖 . Classical secret sharing such as Shamir’s secret sharing [109, 17] and veriﬁable secret sharing (VSS) schemes [26] are the groundwork for most of the SMC protocols."
2011.05411,"data, dataset",217,2022-05-14,0,"Regarding the Fairness and Transparency requirements, as AI/ML algorithms like deep learning are normally operated in a black-box fashion, it is limited of transparency of how certain decisions are made, as well as limited understanding of the bias in data samples and training process [30, 83, 3, 91]. An FL system is not an exception. Generally, if the training data is poorly collected or intentionally prejudicial and fed to an ML, including FL, system, the results apparently turn out to be biased. If the trained model is then utilised for an automated decision-making system, then it probably leads to discrimination and injustice. Furthermore, the nature of preventing service providers from accessing original training dataset as well as the inability to inspect individuals’ locally trained ML model due to Secure Aggregation mechanism ampliﬁes the lack of transparency and fairness in FL systems. As a result, an FL system ﬁnds it problematic to transparently execute the training operations as well as to ensure any automated decisions from the system are impartially performed. This, consequently, induces the impracticality for any FL systems and fails to fully comply with the GDPR requirements of fairness and transparency."
2011.05411,"data, dataset",226,2022-05-14,0,"Geyer et al. in [48] have developed another method to implement diﬀerential privacy for federated optimisation in FL settings that conceals the participation of a user in a training task; as a result, the whole local training dataset of the user is protected against diﬀerential attacks. This approach is diﬀerent from the batch-level one, which aims at protecting a single data point in a training task. The proposed method utilises a similar concept of privacy accounting from [1] that allows a coordination server to monitor the accumulated privacy budget by observing the moment accountant and privacy loss proposed in [1]. The training process is halted once the accumulated privacy budget reaches a pre-deﬁned threshold, implying that the privacy guarantee is no further tolerated. The Gaussian mechanism is also used to generate random noise which is then added to distort the sum of gradients updates to protect the whole training data. The proposed method has been experimented on MNIST dataset, and the results show that with a suﬃciently large number of participants (e.g., about 10,000 clients), the accuracy of the FL trained model almost achieves as high as the nondiﬀerential-privacy baseline while a certain level of privacy guarantee over the local training data still holds."
2011.05411,"data, dataset",67,2022-05-14,0,"In traditional ML approaches, this sort of algorithms performs a vast number of fast iterations over a large dataset homogeneously partitioned in data servers. Such algorithms require super low-latency and high-throughput connections to the training data [80]. Therefore, solving this optimisation problem in the context of FL is diﬀerent from the traditional ML approaches as such conditions do not hold in"
2011.05411,"data, dataset",71,2022-05-14,0,"Reconstruction attacks using MI and GANs are only feasible if and only if all class members in an ML model are analogous which entails a similarity between the MI/GANreconstructed outputs and the training data (e.g., facial recognition of a speciﬁc person, or MNIST dataset for handwritten digits6 used in [4]). Fortunately, this precondition is less practical in most of the FL scenarios."
2011.05411,"data, dataset",90,2022-05-14,0,"learning system target two main objectives: (i) privacy of the training dataset and (ii) privacy of the local model parameters (from an optimisation algorithm such as a gradient descent variant) which are exchanged with other nodes and/or a centralised server [111]. In this respect, prominent privacy-preserving techniques in ML include data anonymisation [92], diﬀerential privacy [34], secure multi-party computation (SMC) [126], and homomorphic encryption [44]."
2011.05411,"data, dataset",96,2022-05-14,0,"SMC is beneﬁcial to data privacy preservation in distributed learning wherein compute nodes collaboratively perform model training on their local dataset without revealing such dataset to others. Indeed, SMC has been employed in numerous ML algorithms such as secure two-party computation (S2C) in linear regression [31], Iterative Dichotomiser3 (ID3) decision tree learning algorithm [78], and k-means clustering algorithm for distributed data mining [62]. However,most of SMC protocols impose non-trivial overheads which require further eﬃciency improvements with practical deployment."
2011.05411,"data, dataset provided",139,2022-05-14,0,"The GDPR clearly diﬀerentiates three participant roles, namely: Data Subject, Data Controller and Data Processor, along with associated requirements and obligations under the EU/UK data protection law. While serving as a better privacy and security framework, the GDPR also aims at protecting data ownership by obligating Data Controllers to provide fundamental rights for Data Subjects to control over their data (""How?"" in Fig. 1). For these purposes, the GDPR introduces and sets high-standard for the consent lawful basis in which Data Controller shall obtain consent from Data Subject in order to process data. Data Controller takes full responsibility to regulate the purposes for which and the methods in which, personal data is processed under the Terms and Conditions deﬁned in the consent."
2011.05411,"data, dataset provided",142,2022-05-14,0,"In this article, we conduct a survey on existing FL studies with an emphasis on privacy-preserving techniques from the GDPR-compliance perspective. Firstly, we brieﬂy review the challenges on data privacy preservation in conventional centralised ML approaches (Section 2) and introduce FL as a potential approach to address the challenges (Section 3). Secondly, the state-of-the-art privacy-preserving techniques for centralised FL are described with the analysis of how these solutions can mitigate data security and privacy risks (Section 4). Thirdly, we provide an insightful deliberation with potential solution approaches of how an FL system can be implemented in order to comply with the EU/UK GDPR (Section 5). Unsolved challenges hindering an FL system from complying with the GDPR are also speciﬁed along with the future research directions."
2011.05411,"data, dataset provided",178,2022-05-14,0,"The GDPR diﬀerentiates three participant roles, namely Data Subject, Data Controller and Data Processor, and designates associated obligations for these roles under the EU data protection law. Data Controllers are subject to comply with the GDPR by determining the purposes for which, and the method in which, personal data is processed by Data Processors - who will be responsible for processing the data on behalf of Data Controllers. Furthermore, Data Controllers should take appropriate measures to provide Data Subjects with information related not only to how data is shared but also to how data is processed in the manner ensuring security and privacy of personal data. The GDPR also clearly speciﬁes rights of Data Subjects, giving data owners the rights to inspect information about how the personal data is being processed (e.g., Right to be informed and Right of access) as well as to fully control the data (e.g., Right of rectiﬁcation and erasure, and Right to restriction of processing)."
2011.05411,"data, dataset provided",6,2022-04-21,0,5.3. Rights of Data Subject
2011.05411,"data, dataset provided",81,2022-05-14,0,"To meet stringent requirements of the GDPR, conventional ML-based applications and services are required to implement measures that eﬀectively protect and manage personal data adhering to the six data protection principles in the GDPR, as well as to provide mechanisms for data subjects to fully control their data. Although ML-based systems are strengthened by several privacy-preserving methods, implementing these obligations in a centralised MLbased system is non-trivial, sometimes technologically impractical [119, 53]."
2011.05411,"data, dataset, data available",222,2022-05-14,0,"called compute nodes and grouped into clusters. For efﬁciency, the calculations in the training process should be parallelised using concurrency methods such as model parallelism and data parallelism [24]. Model parallelism distributes an ML model into diﬀerent computing blocks; available computing nodes are then be assigned to compute some speciﬁc blocks only. Model parallelism requires mini-batch data is replicated at computing nodes in a cluster, as well as regular communication and synchronisation among such nodes [29]. Data parallelism, instead, keeps the completeness of the model on each computing node but partitions the training dataset into smaller equal size shards (also known as sharding), which are then distributed to computing nodes in each cluster [8]. The computing nodes then train the model on their subset as a mini-batch, which is especially eﬀective for SGD variants because most operations over mini-batches are independent in these algorithms. Data parallelism can be found in numerous modern ML frameworks including TensorFlow3 and Pytorch4. The two parallelism techniques can also be combined (so-called Hybrid parallelism) to intensify the advantages while mitigating the drawbacks of each one; as a result, a hybrid system can achieve better eﬃciency and scalability [25]."
2011.05411,"data, publicly available",151,2022-05-14,0,"The new GDPR legislation has come into force from May 2018 in all European Union (EU) countries which is a major update to the EU Data Protection Directive (95/46/EC) (DPD-95) introduced in the year 1995. The GDPR aims to protect personal data (more comprehensive range depicted in ""Which?"" - Fig. 1) with the impetus that ""personal data can only be gathered legally, under strict conditions, for a legitimate purpose"". The full regulation is described in detail across 99 articles covering principles, and both technical and admin requirements around how organisations need to process personal data. The GDPR creates a legal data protection framework throughout the EU/UK member states which has impacted commercial and public organisations worldwide processing EU/UK residents’ data (""Global"" in Fig. 1)."
2011.05411,"data, publicly available",77,2022-05-14,0,"The GDPR establishes supervisory authorities in each member state which are independent public authorities called Data Protection Authorities (DPAs). DPAs are responsible for supervising and inspecting whether a Data Controller is compliant with the data protection regulations whilst the Data Controller is responsible for demonstrating the compliance. The questions are judiciously raised: How can an FL system be investigated and validated by DPAs, and how can it demonstrate the compliance?"
2011.05411,dataset,126,2022-05-14,0,"Proposed by Dwork et al. in 2006, diﬀerential privacy [34] is an advanced solution of the perturbation privacy-preserving technique in which random noise is added to true outputs using rigorous mathematical measures [40]. As a result, it is statistically indistinguishable between an original aggregate dataset and a diﬀerentially additive-noise one. Thus, a single individual cannot be identiﬁed as any (statistical) query results to the original dataset is practically the same regardless of the existence of the individual [34, 33, 35]. However, there is a trade-oﬀ between privacy-guarantee and utility as adding too much noise and improper random Nguyen Truong et al.: Preprint submitted to Elsevier"
2011.05411,dataset,141,2022-05-14,0,"where the training dataset is in form of a set of input-output pairs (𝑥𝑖, 𝑦𝑖), 𝑥𝑖 ∈ ℝ𝑑 and 𝑦𝑖 ∈ ℝ, ∀𝑖 ∈ {1, 2, .., 𝑛}. In Equation 2, 𝑛 is the number of samples in the dataset, 𝑤 ∈ ℝ𝑑 is the parameter vector, and 𝑓𝑖(𝑤) is a loss function. This formulation covers both linear and logistic regressions, support vector machines, as well as complicated non-convex problems in Artiﬁcial Neural Networks (ANN) including Deep Learning [70]. This problem requires an optimisation process that can be eﬃciently computed by using a gradient descent algorithm with back-propagation technique [105, 101] for minimising the overall loss with respect to each model parameters."
2011.05411,dataset,156,2022-05-14,0,"It is worth to emphasise that the separation of the four steps in the cycle is not a strict requirement in every training round. For instance, an asynchronous SGD algorithm can be used in which results of the local training can be immediately applied to update the local model before obtaining updates from other participants [21]. This asynchronous approach is typically utilised in distributed training for deep learning models on a large-scale dataset as it maximises the rate of updates [29, 25]. However, in FL settings, the synchronous approach, which requires the coordination from a centralised server, has substantial advantages over the asynchronous ones in terms of both communication eﬃciency and security because it allows advanced technologies to be integrated such as aggregation compression, secure aggregation with SMC, and diﬀerential privacy [80, 71, 55, 120]."
2011.05411,dataset,18,2022-05-14,0,"ness will signiﬁcantly depreciate reliability and usability of the dataset [33, 35, 40]."
2011.05411,dataset,247,2022-05-14,0,"Shokri and Shmatikov in [111] have proposed a communication eﬃcient privacy-preserving SGD algorithm for deep learning in distributed settings in which local gradient parameters are asynchronously shared among participants with an option of adding noise to such updates for the differentially private protection of the individual model parameters. In this algorithm, participants can choose a fraction of parameters (randomly selected or following a strategy) to be updated at each round so that their local optimal can converge faster while being more accurate. In order to integrate diﬀerential privacy technique into the algorithm, the 𝜀 total privacy budget parameter and the sensitivity of gradient are taken into account to control Δ𝑓𝑖 the trade-oﬀ between the diﬀerential privacy protection and the model accuracy. Laplacian mechanism is used to generate noise during both parameter selection and exchange processes based on the estimation of the Δ𝑓𝑖 sensitivity and the allocated 𝜀 privacy budget. The proposed algorithm has experimented on MNIST and SVHN datasets showing the trade-oﬀ between strong diﬀerential privacy guarantees and high accuracy of the training model. However, with a large number of participants sharing a large fraction of gradients, the accuracy of the proposed algorithm with diﬀerential privacy is better than the standalone baseline. It is worth noting that in this algorithm, local gradients can be exchanged directly or via a central server, which can feasibly be implemented in the FL settings."
2011.05411,dataset,38,2022-05-14,0,"as accumulated privacy loss by observing privacy loss random variables. Based on the experiment, the authors also indicate that privacy loss is minimal for large group size (with a large number of datasets)."
2011.05411,dataset,40,2022-05-14,0,"[92] Narayanan, A., Shmatikov, V., 2008. Robust de-anonymization of large sparse datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008), IEEE. pp. 111–125."
2011.05411,dataset,47,2022-05-14,0,"FL is well-suited for sorts of ML models that are formulated as minimisation of some objective functions (loss functions) on a training dataset for parameter estimation, particularly for gradient-based optimisation algorithms [70]. The minimisation objective can be formulated as follows:"
2011.05411,dataset,71,2022-05-14,0,"Although gradient descent-based optimisation methods were successfully engaged in various ML algorithms, they have recently re-gained much attention since the emergence of large-scale distributed learning, including FL [16, 29]. In these scenarios, a complex model, e.g., a deep neural network (DNN) with millions of parameters, is trained on a very large dataset across multiple nodes. These nodes are"
2011.05411,dataset,89,2022-04-21,0,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-users’ devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.0684,data,106,2022-05-14,0,"(cid:12) (cid:12) 2, 𝑥𝑛 is the 𝑛th data symbol (cid:205)𝑈 −1 where 𝐵1,𝑛 = (cid:12)ℎB,𝑛+𝑖 𝑁 (cid:12) 𝑖=0 (cid:12) (cid:12) (cid:205)𝑈 −1 (cid:12) is the 𝑛th noise symbol (cid:12)𝑣B,𝑛+𝑖 𝑁 at Bob, and 𝐵2,𝑛 = 1√ 𝑖=0 𝑈 component at Bob and where it is observed that 𝐵1,𝑛 ⊥⊥ 𝑥𝑛 ⊥⊥ 𝐵2,𝑛. As detailed in A-A and A-B, the components can respectively be derived as:"
2011.0684,data,106,2022-05-14,0,"Prior to the secure data transmission between Alice and Bob, a handshake protocol must take place. Depending on it, Eve may obtain different degrees of information regarding the channels, which leads to different decoding capabilities and so, different security performance. PLS performance highly depends on the availability of CSI at the communication parties. It is assumed that Alice knows Bob CSI but does not know Eve CSI who is assumed to be an external passive node of the network that tries to eavesdrop the data. Furthermore, Bob and Eve CSI’s are considered spatially independent."
2011.0684,data,109,2022-05-14,0,"In this paper, a new scheme is introduced in order to establish a secure communication at the physical layer between a base station, Alice, and a legitimate user, Bob, in the presence of a passive eavesdropper, Eve. Alice uses a time reversal precoder, implemented in the frequency domain with OFDM, to add to the transmitted data an artiﬁcial noise that lies in the null-space of Bob but degrade Eve’s channel. The proposed technique only requires a single transmit antenna and is therefore well suited for devices with limited capabilities, such as in IoT for instance."
2011.0684,data,11,2022-05-14,0,1) Data term: E (cid:2)|E𝑆𝐷𝑆 1
2011.0684,data,113,2022-05-14,0,"Equation (9) shows that the addition of AN in the FD TR SISO OFDM communication can secure the data transmission. The degree of security depends on G and the amount of data energy, 𝛼, that is injected into the communication, with respect to the amount of AN energy injected (via 1 − 𝛼), as explained in Section III. It is to be noted that, since w is generated from an inﬁnite set of possibilities, even if Eve knows its equivalent channel HEH∗ B and the spreading sequence, she cannot estimate the AN signal to try retrieving the data."
2011.0684,data,124,2022-05-14,0,"It has to be pointed out that lower bounds of the SINR at Bob and Eve were determined for the three investigated scenarios. From simulations, the closed form approximated SINR lower bounds, derived in (15), (22), (28), and (34), are observed to be very tight and are therefore used in the remaining as an approximation. By doing so, an analytical expression of the SR can be determined using (11) as a function of 𝛼. It is therefore straightforward to determine the amount of data energy to inject in the communication, with respect to AN, in order to maximize the ergodic SR."
2011.0684,data,131,2022-05-14,0,"and covariance matrix E (cid:2)(S𝐻 vB)(S𝐻 vB) 𝐻 (cid:3) = 𝜎2 V,BI𝑁 . In (6), each transmitted data symbol is affected by a real gain √ 𝛼 2 at the position of the legitimate receiver. 𝑈 This frequency diversity gain consequently increases the received useful signal power at Bob in fading environments and increases with the BOR value. Considering a ﬁxed bandwidth, the TR focusing effect is enhanced for higher BOR’s at the expense of the data rate. It is also observed that no AN contribution is present in (6) since (3) is respected. A ZF equalization is performed at the receiver leading to:"
2011.0684,data,137,2022-05-14,0,"where G is a 𝑁 × 𝑄 decoding matrix performed by Eve and vE is a complex AWGN. The nature of the decoding matrix is determined by the considered scenarios, which are presented in the next Section II-B. The noise variance is E (cid:2)|𝑣E,n|2(cid:3) = 𝜎2 V,E. The gain of the data component in (8) depends on G and does not necessarily provide a SNR enhancement due to a TR effect. Similarly, the AN component does not necessarily cancel out, depending on G. After ZF equalization, the estimated symbols are: BS(cid:1) −1 (cid:16)√ ˆxE = (cid:0)GHEH∗ √ √ 1 − 𝛼 (cid:0)GHEH∗ 𝛼x +"
2011.0684,data,14,2022-05-14,0,Eve can decode the data thanks to G = S𝐻 H∗ sequence is:
2011.0684,data,14,2022-05-14,0,"Therefore, the optimal amount of data energy to inject is given by:"
2011.0684,data,141,2022-05-14,0,"The scheme consists in data transmission onto OFDM blocks with 𝑄 subcarriers. Without loss of generality, it is considered that only one data block x is sent and is composed of 𝑁 symbols 𝑥𝑛 (for 𝑛 = 0, ..., 𝑁−1, with 𝑁 ≤ 𝑄). The symbol 𝑥𝑛 is a zero-mean random variable (RV) with variance E (cid:2)|𝑥𝑛|2(cid:3) = 𝜎2 𝑥 = 1, i.e., a normalized constellation is considered. The block is then spread in the FD by a factor 𝑈 = 𝑄/𝑁, called back-off rate (BOR), thanks to the spreading matrix S of size 𝑄 × 𝑁. S is designed in such a way not to increase the PAPR, as suggested in [46]."
2011.0684,data,142,2022-05-14,0,"Fig. 8 presents the SR enhancement thanks to the waterﬁlling optimization. The SR gain is deﬁned as the difference between the maximal SR obtained after and before optimization. As a reminder, before and after optimization, the mean energy radiated dedicated to the useful data remains unchanged, and the AN signal always remains in Bob’s null space. The optimal amount of data energy to inject is computed thanks to (36), (38), and (40) in order to ensure a maximal ergodic SR. The SR is then further increased via the waterﬁlling optimization procedure, as described in section III-E. As it can be observed, there is an increase of the SR gain for all three models and all BOR values thanks to the waterﬁlling."
2011.0684,data,16,2022-05-14,0,"From (7), a perfect data recovery is possible in high SNR scenarios."
2011.0684,data,164,2022-05-14,0,"Channel-based adaptation secrecy schemes were ﬁrst introduced in [17]–[19]. In these works, it was proven that positive secrecy rate (SR) can be obtained even if, on average, the channel between Alice and Bob is a degraded version of the one between Alice and Eve, by optimizing or adapting at the transmitter side the communication parameters. In doing so, the precoded signal is optimal for Bob’s channel but not for Eve’s one since they experience different fading. The concept of AN addition was ﬁrst established in [20]–[22]. The idea is to make Eve’s channel condition artiﬁcially degraded by intentionnaly adding an AN signal to the transmitter data. This AN signal is designed in such a way not to degrade Bob’s channel, therefore leading to a PLS enhancement, [1]."
2011.0684,data,177,2022-05-14,0,"AN is added either on all the channel taps or on a set of selected taps. While the condition for AN generation is given, its derivation is however not detailed. In [9], [41], [42], FD precoders using OFDM and AN injection are presented. In [9], the AN is injected in the null space of Bob but only limited decoding capacilities were attributed to Eve. In [41], [42], the idea is to use several OFDM subcarriers for dummy data transmission, i.e., several subcarriers are used for data obfuscation. However, the encryption information must be shared between the transmitter and the legitimate receiver, leading to more processing needed at the receiver. In addition, the security is enhanced when more subcarriers are used for data obfuscation, at the expense of the data rate. Furthermore, it is assumed that Eve has no knowledge about the legitimate link."
2011.0684,data,190,2022-05-14,0,"the optimal amount of transmitted data energy is derived thanks to eq.(36), (38), and (40). It leads to the coefﬁcient 𝛼𝐺 that maximizes the ergodic opt SR of the communication depending on G. It is a unique power coefﬁcient weighting the 𝑄 components of the useful transmitted data. Since the channel capacity is proportionnal to the subcarrier energy, and since Alice has access only to the instantaneous channel capacity at Bob, she can tune the amount of transmitted data energy at each subcarrier, i.e., she can apply a different weight at each subcarrier, to enhance the instantaneous capacity at Bob. In doing so, at each channel realization, she determines a new set of coefﬁcients, denoted w = [𝛼𝐺 𝜶𝐺 ]𝑇 , that enhances the instantaneous w,0 capacity at Bob. Because Bob and Eve channels are independent, enhancing the channel capacity at Bob does not change the ergodic capacity at Eve. This power allocation strategy is described below. 𝑤 = [𝛼𝐺 𝑤 ,0"
2011.0684,data,202,2022-05-14,0,"The left part of Fig.6 illustrates the values of 𝛼opt given by (36), (38), and (40) that maximize the ergodic SR determined from the closed-form approximations (35), (37), and (39), as well as obtained from the numerical simulations, as a function of the BOR. There is a slight discrepency between the analytical estimations of the optimal amount of data energy to inject using (36), (38), and (40), and its numerical estimation. However, the resulting analytical SR does not differ much from the maximal SR obtained in simulation, as it can be observed on the right part of Fig.6. Indeed, as observed in Fig.5, the SR is a function that varies slowly about its maximum, for all models. So, for a given BOR value, Alice can make a rough determination of 𝛼𝐺 opt depending on Eve decoding structure, and therefore the available SR, if 𝛿𝐵 and 𝛿𝐸 are known. One can also note that much more AN power should be injected to"
2011.0684,data,217,2022-05-14,0,"Abstract—A frequency domain (FD) time-reversal (TR) precoder is proposed to perform physical layer security (PLS) in single-input single-output (SISO) systems using orthogonal frequency-division multiplexing (OFDM) and artiﬁcial noise (AN) signal the data transmission to the legitimate receiver but degrades the decoding performance of the eavesdropper. This scheme guarantees the secrecy of a communication towards a legitimate user when the transmitter knows the instantaneous channel state information (CSI) of the legitimate link thanks to the channel reciprocity in time division duplex (TDD) systems, but does not know the instantaneous CSI of a potential eavesdropper. Three optimal decoding structures at the eavesdropper are considered in a fast fading (FF) environment depending on the handshake procedure between Alice and Bob. Closed-form approximations of the AN energy to inject the communication are derived. In addition, the required conditions at the legitimate receiver’s end to guarantee a given SR are determined when Eve’s signal-to-noise ratio (SNR) is inﬁnite. Furthermore, a waterﬁlling power allocation strategy is presented to further enhance the secrecy of the scheme. Simulation results are presented to demonstrate the security performance of the proposed secure system."
2011.0684,data,24,2022-05-14,0,"• The data and noise are independent of each other. • ℎ𝐵,𝑖 ⊥⊥ ℎ𝐵, 𝑗 , ∀𝑖 ≠ 𝑗,"
2011.0684,data,280,2022-05-14,0,"SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains. In [33], the information-theoretic secrecy capacity of an Offset-QAMbased ﬁlterbank multicarrier (FBMC-OQAM) communication over a wiretap frequency selective channel is studied. The authors compare the secrecy capacity of the FMBC-OQAM modulation with a cyclic preﬁx-orthogonal frequency-division multiplexing (CP-OFDM) modulation."
2011.0684,data,285,2022-05-14,0,"First, it can be seen that the analytical models given by (35), (37), and (39) well approximate the simulation curves and remain tight upper bounds for all scenarios. In addition, one can notice the importance of the AN addition on the SR. In fact, one can observe a SR enhancement with the addition of AN except for very high percentages of AN sent, i.e., when 1 − 𝛼 → 1, or for very low percentages of AN sent, i.e., 1 − 𝛼 → 0. Furthermore, for all three models, SR→ 0 when 1 − 𝛼 → 1 since the SINR at Bob and Eve drops to zero. As anticipated from sections III-B2a and III-B2c, high SR values are obtained, i.e., low decoding performance at Eve, when she has the same capabilities as Bob, and when she only knows her own channel. It is also observed that these two scenarios exhibit very similar behaviours except when 1 − 𝛼 → 0, as explained in section III-B2c. Finally, one can observe lower SR values when Eve implements a matched ﬁltering decoding structure. This can be understood from (23) where it is noticed that each transmitted data symbol is afffected by a real gain at Eve such that it beneﬁts from a frequency diversity gain, leading to higher decoding performances at Eve, and so, lower SR values. In fact, Eve SINR is about U times larger with the MF decoder compared to the SDS and the OC decoders."
2011.0684,data,31,2022-05-14,0,"In order to transmit secure data between Alice and Bob, the useful data is precoded and an AN signal w is added before transmission, as depicted in Fig.1."
2011.0684,data,335,2022-05-14,0,"While many works implement these schemes with multiple antennas at the transmitter, using for instance frequency diverse array beamforming [23], [24], directional modulation (DM) [25], antenna subset modulation (ASM) [26], nearﬁeld direct antenna modulation (NFDAM) [27], [28], spatial diversity [29]–[32], or waveform design [33], only few works perfom PLS using single-input single-output (SISO) systems [9], [34]–[42]. SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains."
2011.0684,data,4,2022-05-14,0,A. Data term
2011.0684,data,42,2022-05-14,0,"V,E + (𝑈 + 1) −𝑈𝜎2 V,B, and 𝑇4 = (𝑈 + 1)(𝑈 + 3)𝜎2 V,B, the optimal amount of data energy to transmit is:"
2011.0684,data,45,2022-05-14,0,"precoded data without pilot to Bob. From the FF assumption, Eve cannot learn the precoding performed by the transmitter. In this conﬁguration, Eve implements a decoding structure that takes beneﬁt of her own channel knowledge, denoted by OC."
2011.0684,data,48,2022-05-14,0,"However, if Alice sends a precoded pilot in addition to the precoded data to Bob, Eve is then able to evaluate her equivalent channel H∗ BHE, and therefore to implement a matched ﬁltering decoding structure, denoted by MF. This is depicted in Fig.3"
2011.0684,data,5,2022-05-14,0,1) Data term:
2011.0684,data,52,2022-05-14,0,"In doing so, each data symbol is transmitted onto 𝑈 different subcarriers with a spacing of 𝑁 subcarriers, introducing frequency diversity. The spread sequence is then precoded with the complex conjugate of Bob’s channel H∗ B, before addition of the AN signal w and transmission."
2011.0684,data,55,2022-05-14,0,"1) At the intended position: At Bob, a simple despreading operation is performed. Thanks to the precoding at the transmitter side, every received data symbol is affected by a real gain, as expressed in (6). The ergodic SINR for transmitted symbol 𝑛 is given by:"
2011.0684,data,57,2022-05-14,0,"The AN should not have any impact at Bob’s position but should corrupt the data everywhere else since Alice does not have any information about Eve’s instantaneous CSI, i.e., Eve is a passive node. Furthermore, this signal should not be guessed at the unintended positions to ensure the secure"
2011.0684,data,69,2022-05-14,0,"If Alice only transmits precoded data to Bob, Eve is not able to know anything but H𝐵𝐸 , the channel between Bob and Eve. In that situation, she cannot do better but to implement the same decoding structure as Bob, denoted by the abbreviation SDS. In that scenario, Eve only despreads the received sequence. This situation is presented in Fig.2."
2011.0684,data,72,2022-05-14,0,"where 𝐸 𝐺 3,𝑛 are respectively the data, noise and AN 𝑛th symbol components of the received signal at Eve’s position, for a particular decoding structure G. The expression of the SINR at Eve depends on the receiving structure G whose design depends on the amount of knowledge Eve can obtain. The expression (16) is therefore derived for the three considered scenarios."
2011.0684,data,75,2022-05-14,0,"Equations (45), and (46) are convex expressions that can be minimized as a function of 𝛼. Let’s denote 𝛼𝑆𝐷𝑆 ∞ , 𝛼𝑀 𝐹 ∞ , as the amount of data energy to inject, with respect to AN, in order to guarantee a desired communication SR when 𝛿𝐸 = ∞, respectively for the ﬁrst, second and third scenario. It can be shown that:"
2011.0684,data,76,2022-05-14,0,"PLS can be achieved by increasing the signal-plusinterference to noise ratio (SINR) at Bob and decreasing the SINR at Eve. This can be done by designing a suitable channel-based adaptive transmission scheme, and/or by injecting an artiﬁcial noise (AN) signal to the data. These time and/or techniques can be implemented in the space, frequency domains, [1], [15], [16]."
2011.0684,data,77,2022-05-14,0,"III. PERFORMANCE ASSESSMENTS The classical metric used to evaluate the degree of secrecy in a communication in the PLS ﬁeld is the secrecy channel capacity (SC). The SC is deﬁned as the maximum transmission rate that can be supported by the legitimate receiver’s channel while ensuring the impossibility for the eavesdropper to retrieve the data, [47]. In the ergodic sense, it can be expressed as:"
2011.0684,data,78,2022-05-14,0,"life. Wireless communication has become the dominant access for most of these services but it is intrisically unsecure due to its unbounded nature. Therefore, several issues have emerged and need to be urgently adressed such as data conﬁdentiality and integrity. The amount of leaked information is also an important feature that needs to be considered and minimized in order to guarantee secrecy of wireless transmissions, [1]–[3]."
2011.0684,data,8,2022-05-14,0,B. MF Decoder 1) Data term:
2011.0684,data,8,2022-05-14,0,C. Optimal amount of data energy to inject
2011.0684,data,87,2022-05-14,0,"This paper shows, consequently, with analytical and simulation results, that a scheme exploiting only frequency degrees of freedom can achieve a positive ergodic secrecy rate to considerably jeopardize any attempt of an eavesdropper to retrieve the data. This approach can be easily integrated into existing standards based on OFDM and does not necessitate extra hardware. However, a perspective of this work is to extend it to multiple antenna systems to assess the beneﬁt of the extra spatial degree of freedom."
2011.0684,data,89,2022-05-14,0,"the unintended position, the received signal before ZF equalization is given by √ (8). Let’s introduce E𝐺 𝛼GHEH∗ 2 = GvE and E𝐺 1 − 𝛼GHEw being respectively the data component, 3 = the noise component, and the AN component of the received signal at Eve for a particular decoding structure G. Using the Jensen’s inequality, an approximation of a lower-bound of the averaged SINR of the symbols 𝑛 at the unintended position can be derived as2:"
2011.0684,data,99,2022-05-14,0,"With the closed-form approximations of the SR (35), (37), and (39), it is possible to determine the SNR at Bob and the amount of data energy 𝛼 that guarantees a given SR, as a function of the communication parameters. Let’s introduce Δ being the targetted SR in bit per channel use, 𝛿𝑆𝐷𝑆 , 𝛿𝑀 𝐹 𝐵 , and 𝛿𝑂𝐶 being respectively Bob’s required SNR for the ﬁrst, 𝐵 second and third investigated scenario. Remembering that 𝜎2 V,B = 1"
2011.0684,"data, data available",86,2022-05-14,0,"[36] Y. Lee, H. Jo, Y. Ko, and J. Choi, “Secure index and data symbol modulation for ofdm-im,” IEEE Access, vol. 5, pp. 24 959–24 974, 2017. [37] J. M. Hamamreh, E. Basar, and H. Arslan, “Ofdm-subcarrier index selection for enhancing security and reliability of 5g urllc services,” IEEE Access, vol. 5, pp. 25 863–25 875, 2017."
2011.07981,code,35,2022-05-14,0,"[33] “Distribution code,” ESB Networks,” Grid code, April 2016. [34] W. H. Kersting, “Radial distribution test feeders,” IEEE Transactions on"
2011.07981,data,126,2022-05-15,0,"To examine the performance of the proposed data recovery approach, the interruption of the communication channels from the substation or one of the DERs is simulated, resulting in simultaneous loss of four signals in the former case or three signals in the latter cases. Without a recovery plan, considering the mean values in the training set for the missing signals seems like the most logical choice. Fig. 5 presents the performance of DA in this scenario. As noted, DA can no longer predict the correct conﬁguration, especially for the cases in which the communication from DER4, DER5, or DER6 is interrupted. This establishes the vulnerability of DA under the interruption of communication channels."
2011.07981,data,130,2022-05-15,0,"To simulate anomalous measurements, at each step, the measurements from one of the DERs are multiplied by 0.9, to examine a negative bias, and then 1.1, to examine a positive bias. α is calculated with the manipulated values, as well as the original values. Fig. 9 presents the percentage of the data points placed in each speciﬁed range of α. As noted, with the original values, for the majority of the data points α is below 60 (more than 92% of the data points, in all the scenarios), while with the manipulated values, α takes larger values (above 60 for over 95% of the data points, in all the"
2011.07981,data,133,2022-05-15,0,"While deploying information technologies streamlines the system management, it can cause more dependencies between the physical and cyber components, which makes the system more vulnerable to natural disasters and cyber-attacks [15]. In fact, proneness to loss of online data has undermined the popularity of the approaches that rely on online measurements in practice, especially when the system may face the loss of multiple signals, because of, e.g., interruption of communication channels, or cyber-attacks. Securing the power system management against cyber-attacks has been the subject of many recent studies, like [16] and [17]. This highlights the importance of resiliency in the development of a TI function. None of the reviewed TI approaches is resilience-oriented."
2011.07981,data,170,2022-05-15,0,"This paper is dedicated to developing a topology identiﬁcation function for distribution networks that relies only on the measurements available to DERMS. These measurements include the operating condition of the grid and DERs [1]. Considering the applicability of discriminant analysis (DA) as a widely-used statistical classiﬁer, DA is employed for this purpose. While originally developed for DERMS, this method is applicable for DMS too since DMS and DERMS share the DERs measurements [1]. To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,18,2022-05-15,0,It is worth emphasizing that the quadratic programming data recovery approach is applicable only when some mea 5
2011.07981,data,186,2022-05-15,0,"In addition to the loss of multiple signals, the TI approach should be resilient against malicious data. In this subsection, a benchmark is introduced to detect if one or a group of measurements contain anomalous values. Suppose the measurements from a speciﬁc meter, i.e., X s = [xs l ], are suspected to contain anomalous values. Considering (10), the information of the other measurements can be exploited to estimate the normal value of X s, i.e., X r = [xr l ]. To discern if X s is anomalous, the idea is to compare the likelihood of X r associating with any of the network topologies, i.e., ∪K i=1(ki) denotes the union of all the possible topologies. For this purpose, the likelihood ratio, denoted by α = Λ(X r : X s| ∪K i=1 (ki)), is employed as a benchmark to detect if X s is anomalous. The likelihood ratio is deﬁned as:"
2011.07981,data,188,2022-05-15,0,"Abstract—Network topology identiﬁcation (TI) is an essential function for distributed energy resources management systems (DERMS) to organize and operate widespread distributed energy resources (DERs). In this paper, discriminant analysis (DA) is deployed to develop a network TI function that relies only on the measurements available to DERMS. The propounded method is able to identify the network switching conﬁguration, as well as the status of protective devices. Following, to improve the TI resiliency against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. By deploying the propounded data recovery approach and Bayes’ theorem together, a benchmark is developed afterward to identify anomalous measurements. This benchmark can make the TI function resilient against cyberattacks. Having a low computational burden, this approach is fast-track and can be applied in real-time applications. Sensitivity analysis is performed to assess the contribution of different measurements and the impact of the system load type and loading level on the performance of the proposed approach."
2011.07981,data,51,2022-05-15,0,"[30] J. Hallinan, “Chapter 2 - data mining for microbiologists,” in Systems Biology of Bacteria, ser. Methods in Microbiology, C. Harwood and A. Wipat, Eds. Academic Press, 2012, vol. 39, pp. 27 – 79."
2011.07981,data,58,2022-05-15,0,"Fig. 9. The percentage of the data points in the test set placed in each speciﬁed range of α, with the normal values and manipulated values of the measurements from: a) DER1, b) DER2, c) DER3, d) DER4, e) DER5, f) DER6."
2011.07981,data,66,2022-05-15,0,"As discussed, the loss of multiple signals and malicious data are two phenomena that jeopardize the applicability of approaches relying on online measurements in practice. In this section, an approach is proposed to recover the original values of the measurements that are missing or suspected to contain anomalous values. From now on, they are referred to as the missing signals."
2011.07981,data,78,2022-05-15,0,"It should be emphasized that in this approach, we do not consider the objective of network reconﬁguration operations, but instead, to form the training and test data sets, all the possible conﬁgurations are considered and in each conﬁguration, all the probable variations of loads consumption and DERs generation are simulated. In this order, whatever the objective of network conﬁguration is, it will include a subset of this data set."
2011.07981,data,82,2022-05-15,0,"Afterward, a quadratic programming optimization approach, based on the recovery of the lost signals, was presented to make the proposed TI approach resilient against the interruption of communication channels. The speciﬁc optimization problem can be settled efﬁciently and a global extremum is guaranteed. Furthermore, by exploiting this data recovery approach, a benchmark was introduced to detect anomalous measurements. This benchmark can be employed to enhance the resiliency of the proposed TI against cyber-attacks."
2011.07981,data,85,2022-05-15,0,"To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,864,2022-05-15,0,"This approach requires a low processing time, which makes it suitable to be employed in real-time applications. This approach is able to consider weakly-meshed conﬁgurations, works with any type of mea (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)(cid:12)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:11)(cid:22)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:23)(cid:23)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:11)(cid:24)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:19)(cid:25)(cid:26)(cid:27)(cid:17)(cid:18)(cid:25)(cid:26)(cid:27)(cid:17)(cid:28)(cid:25)(cid:26)(cid:27)(cid:17)(cid:12)(cid:25)(cid:26)(cid:27)(cid:17)(cid:22)(cid:25)(cid:26)(cid:27)(cid:17)(cid:24)(cid:25)(cid:26)(cid:27)(cid:17)(cid:20)(cid:25)(cid:26)(cid:27)(cid:17)(cid:29)surements, can treat unbalanced networks, and can be applied to identify the switching conﬁguration and the operation of protective devices."
2011.07981,data,9,2022-05-15,0,of the proposed data recovery approach are presented.
2011.10563,code,23,2022-05-15,2,The code of HINDSIGHT++ is exclusively written in R and a proof of concept implementation is available in [8]. The
2011.10563,code,80,2022-05-15,0,"backbone is the CRAN interface to Keras, a high-level, userfriendly Application Programming Interface (API) which enables quick and dynamic experimentation with DL algorithms, while it further allows for easy neural network architectural prototyping. Keras is designed to run on top of multiple back-end environments, including Theano, CNTK, and Tensorflow (default). Furthermore, it offers the option for executing the code either on top of CPUs or GPUs."
2011.10563,data,108,2022-05-15,0,"[11] V. Raida, P. Svoboda, M. Kruschke, and M. Rupp, “Constant rate ultra short probing (crusp): Measurements in live lte networks,” in ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1–6, IEEE, 2019. [12] C. Maier, P. Dorﬁnger, J. L. Du, S. Gschweitl, and J. Lusak, “Reducing consumed data volume in bandwidth measurements via a machine learning approach,” in 2019 Network Trafﬁc Measurement and Analysis Conference (TMA), pp. 215–220, IEEE, 2019."
2011.10563,data,109,2022-05-15,0,"For the bandwidth prediction task under study, therefore, we ﬁnd that hyperparameter optimization is especially vital in 5G networks, since they introduce higher data rates and larger variations over time. Coupled with the mobility aspect, a suboptimal hyperparameter set will fail to capture the intrinsic network characteristics. Among the two available optimization solutions under study, we observe that BOA provides slightly better results, since it utilizes probability theory concepts for minimizing the error. We further observe that 4G networks offer signiﬁcantly lower data rates with a lower variation factor which signiﬁcantly diminishes the need for advanced hyperparameter optimization solutions."
2011.10563,data,160,2022-05-16,0,"• Random Search (RS). Instead of evaluating all possible hyperparameter combinations, RS iterates over a smaller sample [3]. As the number of iterations increase, the probability of converging to a better solution increases as well. The number of iterations required to reach to a good solution depends on several factors including the size of the hyperparameter space, search range, data complexity, and so forth. RS has a fairly simple implementation and does not require any scientiﬁc knowledge for the hyperparameters under study. In addition, it provides signiﬁcant gains in terms of computational complexity, since it only tests a limited number of combinations, which is a safe approach under the assumption that non all hyperparameters are equally important. As a rule of thumb, there is a 95% chance that RS will reach to a good solution with only 60 iterations."
2011.10563,data,18,2022-05-16,0,Multivariate data analysis often involves features with variable scales. Since LSTM weight allocation is prone to such
2011.10563,data,260,2022-05-16,0,"The topic of bandwidth forecasting in MBB networks has been in the spotlight for a long time. However, it has lately become more prominent due to the fast-expanding next generation network infrastructures, which in turn, yield to a larger exchange of network data, hence, increasing the need for higher and more accurate predictions. Furthermore, 5G introduces new frequency bands, such as the mmWave, which drives data rates to groundbreaking limits, thus, posing new challenges to the forecasting process. Research studies focus on integrating robust and effective algorithms, while maintaining the implementation and computational complexity in acceptable levels. Over the years, numerous solutions have been proposed ragning from traditional statistical methods, such as the Naive, Autoregressive Integrated Moving Average (ARIMA), and Vector Autoregression (VAR), to considerably higher complexity algorithms, namely dynamic linear models, TBATS [1] (i.e., based on exponential smoothing), and Prophet [2] (i.e., widely used by Facebook). Within the context of Artiﬁcial Intelligence (AI), both Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM) networks have been projected as high efﬁcient algorithms for time series modeling. Between the two, LSTM networks appears to be the most ﬁtting solution for the bandwidth forecasting study, considering the complex long term dependencies and the unforeseeable behavior that mobile traces display due to mobility."
2011.10563,data,270,2022-05-16,0,"A supervised Machine Learning (ML) solution for downlink throughput prediction in MBB networks is proposed in [9]. The authors argue that ML can be used as a tool for signiﬁcantly reducing the data volume consumption over the network, while maintaining the predictive error in acceptable levels. Furthermore, in [10], Raida et. al leverage constant rate probing packets to estimate available bandwidth in a controlled Long-Term Evolution (LTE) environment, while in [11], they extend their work by further testing and validating the developed framework in live LTE networks. In [12], Maier et al. introduce a novel AI model with feed-forward neural networks for both downlink and uplink bandwidth forecasting. To ﬁnd a ﬁtting speed test duration, the authors investigate two scenarios. First, they train a model with a duration ﬁxed to a value lower than the default, while second, they dynamically determine a duration by using the results of a pretrained neural network model. Experimental approaches studying the problem of bandwidth prediction have also been addressed in [13, 14, 15, 16, 17, 18, 19]. Beyond empirical-based studies, theoretical models have also been published. In [20], Gao et al. introduce a theoretical learning based throughput prediction system for reactive ﬂows, while authors in [21] propose a novel stochastic model for user throughput prediction in MBB networks that considers fast fading and user location."
2011.10563,data,3,2022-05-16,0,3.1.2 Data Import
2011.10563,data,3,2022-05-16,0,3.2 Data Preprocessing
2011.10563,data,30,2022-05-16,0,"The objective of this ﬁrst phase is two-fold. First, to provide support with all required software components, and second, to initiate the data import process."
2011.10563,data,32,2022-05-16,0,"data, it is critical that we consider a transformation function leveraging one of the following state-of-the-art normalization methods, known as min-max, z-score, and tanh [37]."
2011.10563,data,39,2022-05-16,0,"Fig. 2: A high-level representation of the HINDSIGHT++ control ﬂow diagram. The operations for each block are described in Section 3, highlighted with a matching title. Dashed lines delimit the data preprocessing stage."
2011.10563,data,42,2022-05-16,0,"[13] Y. Liu and J. Y. Lee, “An empirical study of throughput prediction in mobile data networks,” in 2015 IEEE Global Communications Conference (GLOBECOM), pp. 1– 6, IEEE, 2015."
2011.10563,data,44,2022-05-16,0,"Finally, data undergoes transformation and reshaping functions to comply with the LSTM input requirements. The outcome of this process are the 2-D and 3-D matrices summarized in Table 1. X and Y represent the LSTM input and output, respectively."
2011.10563,data,53,2022-05-16,0,"[34] R. Yu, Y. Li, C. Shahabi, U. Demiryurek, and Y. Liu, “Deep learning: A generic approach for extreme condition trafﬁc forecasting,” in Proceedings of the 2017 SIAM international Conference on Data Mining, pp. 777–785, SIAM, 2017."
2011.10563,data,54,2022-05-16,0,"[33] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, “Long short-term memory neural network for trafﬁc speed prediction using remote microwave sensor data,” Transportation Research Part C: Emerging Technologies, vol. 54, pp. 187–197, 2015."
2011.10563,data,55,2022-05-16,0,Experimental Design: All experiments are carried out on an x86-64 architecture with a 16GB RAM while an NVidia Titan X graphics card is used to accelerate data preprocessing. The operating system is based on a Linux Ubuntu 16.04.6 LTS distribution. All reported RT values could signiﬁcantly vary under different architectural designs.
2011.10563,data,60,2022-05-16,0,"[32] J. Wang, J. Tang, Z. Xu, Y. Wang, G. Xue, X. Zhang, and D. Yang, “Spatiotemporal modeling and prediction in cellular networks: A big data enabled deep learning approach,” in IEEE INFOCOM 2017-IEEE Conference on Computer Communications, pp. 1–9, IEEE, 2017."
2011.10563,data,61,2022-05-16,0,"4. In addition, HINDSIGHT++ supports one-hot-encoding for converting categorical variables to binary features and a number of options for dealing with missing data (e.g., interpolation, Moving Average (MA), Kalman ﬁltering, etc.). We do not cover these additional aspects in detail, since they are not part of our data."
2011.10563,data,77,2022-05-16,0,"The process of data preparation in LSTM networks is rather perplexed time consuming. The complexity factor signiﬁcantly increases considering the different data format combinations (e.g., multivariate, parallel) and parameters (e.g., lags, prediction steps). Figure 2 (dashed frame) encloses all steps followed for providing compatibility with the LSTM models. In line with the AutoML concept, data preprocessing is treated as a black box4."
2011.10563,data,80,2022-05-16,0,"Michael Alexander Riegler, Chief Research Scientist, SimulaMet, holds degrees in computer science from the University of Oslo (UiO) and Klagenfurt experienced in University. He is medical multimedia data analysis and understanding, image processing, image retrieval, parallel processing, crowdsourcing, social computing and user intent. He is a recognized expert in his ﬁeld, serving as a member of the Norwegian Council of Technology on Machine Learning for Healthcare."
2011.10563,data,93,2022-05-16,0,"Takeaways: The hyperparameter optimization paradigm is critical for improving several performance aspects in LSTM networks. However, its signiﬁcance is highly dependant on the underlying data attributes. For example, sequences that display large signs of variance over time are notably harder to predict, thus requiring hyperparameter conﬁgurations of high precision. On the contrary, sequences that either meet the stationarity test requirements or show consistent trends across time have limited gains, since most combinations of hyperaparameters within the predeﬁned search range will provide comparable performance."
2011.10563,data,96,2022-05-16,0,"Training of neural networks is an iterative process that involves identifying unique data properties and learning the model parameters. At each iteration, we track both training and validation error, which we use as bias and variance measures, or else, indicators of underﬁtting or overﬁtting. The validation set always reﬂects the last portion of the initial training set. We test the ﬁnal model on an never-seenbefore testing set and use the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as our error metrics."
2011.10563,"data available, code, open-source, code available, data, open-source code",70,2022-05-16,0,"3.1.1 Load Prerequisites HINDSIGHT++ relies on a number of open-source packages, or else, a collection of functions, compiled code, and data, available in CRAN. To guarantee software stability and robustness, it is critical that all dependencies are pre-installed in the system. Therefore, we enable a veriﬁcation process that automatically locates, installs, and loads any missing packages and libraries."
2011.10563,"data, code, open-source",186,2022-05-16,2,"In this work, we present HINDSIGHT++, a novel, lightweight, versatile, and open-source R-based framework for bandwidth forecasting experimentation in MBB networks with LSTM networks2. The main source of motivation behind the design of HINDSIGHT++ is two-fold. First, to limit the implementation complexity of LSTM networks, and second, to minimize the predictive error. Toward this goal, we adopt the concept of Automated Machine Learning (AutoML), an acronym used to describe the process of data pipeline automation. In particular, AutoML covers the complete learning routine, from raw data preprocessing to the ﬁnal model production. Moreover, HINDSIGHT++ is compliant with parallel and multivariate data and supports a wide forecasting horizon. Last, it offers different hyperparameter optimization options and LSTM variants. The source code of HINDSIGHT++ is structured in a dynamic fashion, so it can accommodate new features and algorithms. Users are encouraged to integrate more libraries and software components to satisfy their cause, but also to enrich the capabilities of the framework."
2011.10563,"data, dataset",112,2022-05-16,0,"Next is the data import process where users select a number of f ∈ [1, z] .csv ﬁles via an interactive window. Due to its universal design, HINDSIGHT++ operates under the assumption that the selected datasets are parallel3. In addition, equidimensional datasets are required, since LSTM models expect input arrays with equal sizes. The last prerequisite is that the dependent variable locates in the ﬁrst array position followed by any number of additional of regressors. We annotate the matrix dimensions for each dataset as df ∈ Rn×m, where n and m represent the number of samples and features, respectively."
2011.10563,"data, dataset",138,2022-05-16,0,"Furthermore, we observe that LSTM models operate on a low MAE region which is the result of the low-variance data rates experienced in 4G under mobility, thus restricting the room for vast performance improvement. Across the two dataset portions, we ﬁnd that, in average, validation error is lower than testing error, which is the common scenario, since model tuning takes place in the former set. Last, Figure 5 illustrates the bandwidth time series graph of three NYU-METS traces10. Color is used for mapping the forecasting lines ( (RSV L)) with the associated part of the trace, i.e., training, validation, and testing set. As evident, LSTM models follow the data trends along the traces with minimal error deviation."
2011.10563,"data, dataset",172,2022-05-16,0,"5Gophers: Exploring potential patterns across the 5G bandwidth measurements, we observe that in some scenarios the optimized LSTM models feature a third HL. Again, this is a rather expected result considering the higher data rates and variation in 5G in addition with the challenging bandwidth trends over time and under mobility. Likewise, the average number of neurons per HL across all layers equals to 197, 79, and 32 for HL-1, HL-2 and HL-3, respectively. We observe that the number of neurons per HL for both datasets follow a decreasing trend across the network, which also coincides with the neuron selection from authors in [30] (i.e., HL-1=256 and HL-2=128). Additionally, the average LR and BS across all traces equal to 0.004 and 28, respectively, while the number of epochs averages 76, which is an ≈ 17% increase comparing to the 4G counterpart result hinting a slower gradient descent convergence."
2011.10563,"data, dataset",62,2022-04-21,0,"At a glance, we observe that 5Gophers error show an ≈ 100-fold increase when compared to NUY-METS.This observation is in line with our expectations, considering the higher data rates that users experience in a 5G network environment (i.e., up to 1500Mbps, see Figure 6). Next, we follow up the results discussion per dataset."
2011.10563,"data, dataset",93,2022-05-16,0,"Takeaways: The designated results reveal that hyperparameter optimization is crucial for achieving superior performance in 5G networks, while trading computational resources. The above ﬁnding is further reinforced by the reported model hyperparameters, which signiﬁcantly vary across technology standards, mobility scenarios, and to some extent network operators. Across the two datasets, we ﬁnd that 5G data require LSTM conﬁgurations with an increased number of HLs and epochs, while featuring signiﬁcantly lower BS values, which further reveals the challenging network conditions in such scenarios."
2011.10563,"data, dataset, open-source",240,2022-05-16,3,"6 CONCLUSIONS AND FUTURE WORK In this paper, we studied the challenging task of bandwidth forecasting in next-generation MBB networks under mobility. To ease our goal, we designed HINDSIGHT++, an open-source R-based framework that allows for LSTM experimentation in time series data. We speciﬁcally focused on the hyperparameter optimization aspect which is critical for achieving state-of the performance. We analysed and performed a comparative analysis between two opensource datasets of bandwidth measurements in operational 4G and 5G networks, respectively, aiming to quantify the LSTM performance improvement under different conﬁguration setups. Results show that hyperparameter optimization provides signiﬁcant beneﬁts under 5G settings compared to 4G settings and the optimal parameters for 4G cannot be directly applied considering the substantially higher data rates and variation over time that users experience in 5G network conditions. As for the future, we plan on integrating feature selection algorithms, additional LSTM variants (e.g., Convolutional Neural Networks (CNN) LSTM and multiplicative LSTM [51]), and provide support for supervised classiﬁcation tasks. Furthermore, we will carry out a dedicated measurement campaign to collect additional network features (e.g., signal strength, latency, etc.). Using the new data, we aim to explore potential correlation or causation relationships and show whether or not they bring any signiﬁcant gains to the LSTM performance."
2011.10563,"data, dataset, open-source data, open-source",223,2022-05-16,1,"Abstract—Bandwidth forecasting in Mobile Broadband (MBB) networks is a challenging task, particularly when coupled with a degree of mobility. In this work, we introduce HINDSIGHT++, an open-source R-based framework for bandwidth forecasting experimentation in MBB networks with Long Short Term Memory (LSTM) networks. We instrument HINDSIGHT++ following an Automated Machine Learning (AutoML) paradigm to ﬁrst, alleviate the burden of data preprocessing, and second, enhance performance related aspects. We primarily focus on bandwidth forecasting for Fifth Generation (5G) networks. In particular, we leverage 5Gophers, the ﬁrst open-source attempt to measure network performance on operational 5G networks in the US. We further explore the LSTM performance boundaries on Fourth Generation (4G) commercial settings using NYU-METS, an open-source dataset comprising of hundreds of bandwidth traces spanning different mobility scenarios. Our study aims to investigate the impact of hyperparameter optimization on achieving state-of-the-art performance and beyond. Results highlight its signiﬁcance under 5G scenarios showing an average Mean Absolute Error (MAE) decrease of near 30% when compared to prior state-of-the-art values. Due to its universal design, we argue that HINDSIGHT++ can serve as a handy software tool for a multitude of applications in other scientiﬁc ﬁelds."
2011.10563,"data, open-source",218,2022-05-16,2,"Over the past years LSTM networks have been successfully applied for network performance related tasks. Cui et al. proposed a stacked bidirectional LSTM architecture for network-wide trafﬁc estimation [29], while authors in [30] studied the applicability of LSTM for a real-time bandwidth prediction problem. Furthermore, Zhao et. al. focused on short-term trafﬁc estimation by considering temporal–spatial correlation [31], whereas, authors in [32] proposed a hybrid framework that combines LSTM networks and an auto-encoder based model for network prediction from a spatio-temporal angle. Additional work on mobile trafﬁc forecasting has been further proposed in [33, 34, 35]. Distinct from the preceding studies, this paper stands out and distinguishes itself by putting focus on the hyperparameter optimization aspect applied on data from different countries, network operators, technology standards, and mobility scenarios. In addition, to the best of our knowledge, this is the ﬁrst study that attempts to perform a bandwidth forecasting comparative analysis between 4G and 5G networks under mobility with LSTM networks. Last, we make available to the community an open-source framework which follows an end-to-end AutoML paradigm and allows for ample experimentation with LSTM networks."
2011.10563,dataset,107,2022-05-16,1,"We study the topic of bandwidth forecasting in commercial 5G networks under mobility, while we revisit Fourth Generation (4G) scenarios and draw additional insights by performing a comparative analysis between the two technology standards. Thereupon, we leverage two opensource datasets, 5Gophers, which features network measurements from operational 5G networks in the US, and NYU Metropolitan Mobile Bandwidth Trace (NYU-METS), a counterpart dataset that comprises of hundreds of 4G bandwidth measurements in the wild. We adopt systematic investigation to showcase the signiﬁcance of hyperparameter optimization at each use case when compared to existing state-of-the-art approaches."
2011.10563,dataset,122,2022-05-16,0,"made after closely observing the fast saturation rate of the gradient descent error. However, we argue that future 5G applications may require a higher number of iterations to converge to a good solution, although, this would result to a substantial increase of the RT complexity. Second, the hyperparameter search range can be stretched to accom modate a higher pool of values, which could potentially add to the performance. The selected values were decided as a compromise to the performance versus RT complexity tradeoff. Last, both datasets are composed of traces with a limited number of samples which can have a negative effect on the LSTM performance since they are known to perform"
2011.10563,dataset,123,2022-05-16,0,"At present time, 4G networks are considered the norm of wireless telecommunication systems sustaining a big portion of society’s global network functions. However, the recent advances in technology indicate that in the upcoming decade 5G will become the next big thing with several network operators already moving toward its adoption. Therefore, it is dire that we identify the principal network characteristics and trends between the two technology standards and show to what extent they dictate the performance of LSTM networks. In addition, our experiments aim to identify the potential gains of hyperparameter optimization in delivering more efﬁcient and robust models. Next, we provide a description of the two datasets under study."
2011.10563,dataset,174,2022-05-16,0,"VALTE012301230123MAERMSE7TrainA7TrainBBus57Bus62ABus62BNTrainLIMSVLRSVLRSBDBOAVLVALTE050100150200050100150200050100150200MAERMSESprintASprintBVerizonAVerizonBWalkingMSVLRSVLRSBDBOAVL010002000300040005000051525t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)0500150025000246812t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)to the bandwidth forecasting problem. Likewise, we compare RSV L and RSBD to remove the hyperparameter optimization bias. On the one hand, we ﬁnd that Bidirectional LSTM networks outperform the Vanilla counterpart in two out of the four driving traces (i.e., SprintA and VerizonA) with an average testing MAE improvement of ≈ 15%. On the other hand, however, they seem to be performing worse in the other two driving traces (i.e., SprintB and VerizonB) by an average testing MAE percentage of ≈ 15%. This ﬁnding veriﬁes the fact that bidirectional LSTM networks are very dependant on the respective dataset underlying characteristics. Therefore, the question of whether or not they are ﬁtting for a given application can only be answered after systematic empirical analysis."
2011.10563,dataset,193,2022-05-16,0,"Tables 4 and 5 provide additional insights with respect to the RT complexity. In particular, each row reports the RT [min] per trace and across the available conﬁgurations. In average, we observe that RSBD present a higher RT of ≈ 54% when compared to the RSV L. The above result is not balanced across the two datasets due to the randomness bias during the hyperparameter selection process. For example, instances of the algorithm where higher values for HLs or neurons are selected will require more computational power to accomplish. On the other hand, the average increase in RT between BOAV L and RSV L reaches to a percentage of ≈ 154%, which can be attributed to the following. First, since BOA leverages the Bayes Theorem for minimizing the gradient descent error, it requires more time to reach to a good solution, opposed to RS which randomly scans the search space. And second, the R BOA implementation is not optimized to run on top of a GPU, which adds to the RT overhead."
2011.10563,dataset,215,2022-05-16,0,"Setting the Baseline: For the baseline experiments (M SV L), we adopt a stacked Vanilla architecture with two HLs and the model hyperparameters7 reported in [30] and illustrated in Table 2. Note that these hyperparameters was prior optimized (e.g., by means of sensitivity analysis or by using indicative values based on previous experience with datasets of similar nature) on the same dataset [30]. LSTM experimentation takes place in a per-trace fashion, i.e., the ﬁrst x% part of the sequence is used for training and validation, while the remaining y% is used for testing. In addition, we adopt a walk-forward validation, or else, rolling forecast approach, where each prediction value is used as new input to the model for forecasting the next step. Last, we overcome the random LSTM weights initialization bias by repeating each baseline experiment 25 times and reporting the median values for both of our error metrics8. Toward Performance Optimization: We repeat the experimental campaign by enabling hyperparameter optimization while testing the available LSTM variants. Our goal is to test whether the hypothesis stating that a set of ﬁxed model hyperparameters is sub-optimal for modelling"
2011.10563,dataset,39,2022-05-16,0,"To the best of our knowledge, little research has been conducted to provide guidelines on the LSTM hyperparameter selection, since most studies focus on particular architectures and datasets [28]. Therefore, we are moving"
2011.10563,dataset,46,2022-05-16,0,0204060801001200200400t (s)Bandwidth (Mbps)0204060801001200200400600t (s)Bandwidth (Mbps)02040608010012005001500t (s)Bandwidth (Mbps)02040608010012004008001200t (s)Bandwidth (Mbps)01002003004005006000400800t (s)Bandwidth (Mbps)1024102410240.011001283units1units2units3lrepochsbslayers7TrainA1024102410240.011001283units1units2units3lrepochsbslayersBus62A1024102410240.011001283units1units2units3lrepochsbslayersLIMSVLRSVLRSBDBOAVL1024102410240.011001283units1units2units3lrepochsbslayersSprintA1024102410240.011001283units1units2units3lrepochsbslayersSprintB1024102410240.011001283units1units2units3lrepochsbslayersVerizonA1024102410240.011001283units1units2units3lrepochsbslayersVerizonB1024102410240.011001283units1units2units3lrepochsbslayersWalkingMSVLRSVLRSBDBOAVLbest with larger datasets.
2011.10563,dataset,54,2022-05-16,0,"across the available traces and datasets. Figures 7 and 8 illustrate the selected hyperparameters along the selected NYU-METS traces and 5Gophers datasets, respectively. The edges of each radar plot map to the maximum search range values across the seven hyperparameters, while color is used to discriminate between each conﬁguration."
2011.10563,dataset,66,2022-05-16,0,"LSTM performance heavily relies on the selection of the model hyperparameters and the underlying dataset characteristics (i.e., periodic patterns and trends). Since these two aspects are tightly tied with each other, knowing how to efﬁciently tune an LSTM model becomes an important but rather challenging task, thus, urging for a hyperparameter optimization solution. Among the most popular approaches"
2011.10563,dataset,72,2022-05-16,0,"The next phase involves splitting each dataset into a training (80%) and a testing (20%) set. Both of these percentages can be conﬁgured as appropriate. We deﬁne the dimensions for f ∈ Rnte×m, where ntr and each set as dtr nte are the number of samples for the training and testing set, respectively, while m represents the number of features."
2011.10563,dataset,73,2022-05-16,0,"Time series visualization can be used to highlight certain phenomena at glance, including periodic patterns or trends, extreme outliers, odd observations, abrupt changes over time, and so forth. Moreover, it allows for visual inspection of the developed models to assess how well they ﬁt the datasets under study. A different color scheme is used to distinguish the training, validation, and testing set."
2011.10563,"dataset, open-source",24,2022-05-16,2,"• Last, we open-source HINDSIGHT++ to the community allowing for further experimentation with a variety of datasets and applications [8]."
2011.10563,"dataset, open-source data, open-source",188,2022-05-16,1,"5.1 Datasets We exploit 5Gophers5 [48, 49], the ﬁrst open-source dataset that attempts to study the performance of 5G in commercial settings, featuring three operational operators (two mmWave and one mid-band carrier) in the US. 5Gophers allows for a wide range of analyses, including impact of mobility, handoffs, network operator performance, and many more. In this study, we focus on a single walking, and two driving traces (i.e., medium mobility between 20 to 50Km), since we are primarily interested in the mobility aspect. The walking trace comprises from both 4G and 5G readings, followed by additional features, such as cell and handover identiﬁers6. The experimental setup under medium mobility features three SGS10 devices mounted on a vehicle’s front windshield and measuring the iPerf performance for three major operators in Atlanta, i.e., Sprint, T-Mobile, and Verizon. A bandwidth estimation measure is recorded every 120sec. All traces alongside the number of available samples are reported in Table 4."
2011.10563,"dataset, open-source data, open-source",96,2022-05-16,1,"We complement our study with NYU-METS [50], an open-source dataset that comprises of 4G LTE bandwidth measurements carried out in the New York University (NYU) metropolitan area. NYU-METS covers several transportation modes including bus, subway, and ferry. The experimental setup features an LTE-enabled mobile device and a remote server located at the NYU lab. Transmission Control Protocol (TCP) measurements are carried out using the iPerf cross-platform tool with a sampling rate of one second. Likewise, Table 5 lists the available bandwidth traces"
2011.10563,"dataset, used dataset",63,2022-05-16,0,"Overall, it is evident that the selected LSTM models adopt disparate strategies for minimizing the gradient descent error function, a result that veriﬁes the high degree of complexity in the neural network ecosystem. Next, we isolate a number of use cases per dataset in an effort to discover any hidden patterns or trends alongside the pool of hyperparameters."
2011.10563,github,37,2022-05-16,0,"[50] https://github.com/NYU-METS/Main. [51] B. Krause, L. Lu, I. Murray, and S. Renals, “Multiplicative lstm for sequence modelling,” arXiv preprint arXiv:1609.07959, 2016."
2011.10563,provide implementation,61,2022-05-16,0,"4 FRAMEWORK VALIDATION We organize the following content into three main parts. First, we provide an overview of HINDSIGHT++ technical and implementation details. Next, we discuss the training, validation, and testing process, while we outline the key LSTM parameters. Last, we present the available benchmarking options alongside a brief visualization description."
2011.10916,data,10,2022-05-16,0,"5. handling larger dimension-ed data efﬁciently across the modalities,"
2011.10916,data,122,2022-05-16,0,"Our observation is that aligning the entire data doesn’t improve the accuracy by a signiﬁcant margin, while we also agree that adding more depth to the DCCA in section 4.2 would certainly give us better results as the parameter count would go up. Generally, with multimodal fusion, we need a lot of parameters to capture all the nuances and complexity attached with cross-modal context and capturing these long-range minute dependencies might be improved as the parameter count go up. Thus, currently, the best way to improve accuracy seems to be adding more parameters on the upper sections of the model. We also suggest the following other ideas for the future work:"
2011.10916,data,137,2022-05-16,0,"Now we brieﬂy discussion the hyper-parameter selection. Most of the attention literature uses Adam optimizer and we found that for this model, it gave stable decrease in the loss as well. Due to the large parameter count, model time complexity and limited computational resources; we only tried handful of hyper-parameter setups. We split train:validation:test data into approxiamtely 18.5k:2.25k:2.25k sample ratio, which is a 80:10:10 percentage ratio. We used the validation set for hyper-parameter tuning, upon ﬁnding the satisfactory setup, we merged validation and train set into one and re-learned the model with it; testing the generalize-ability of the model only on the test set. The selected hyper-parameters are given in Table 1. The model is shown in Figure 2c."
2011.10916,data,147,2022-05-16,0,"We ﬁrst use our classiﬁcation head on all 3 unimodal delta self-attention module individually, to compare the gains of the proposed multimodal fusion technique later. Here we expect that multimodal fusion will add more context to the data and thus, the gains should be comparably higher. Third experiment is between aligned and unaligned data, with aligned data, we have aligned the sequence intervals of all 3 modalities according to the base text modality, parts of any modality not in the interval have been discard; with the unaligned data, we are not discarding any parts regardless of their inclusion in any interval, nor we have any “base” or reference modality. Our expectation is that aligned data of course gives us an indicator of which range to look into for a spoken word’s effect"
2011.10916,data,184,2022-05-16,0,"Here we evaluate the results in the table 2 quantitatively. We couldn’t compare out per class binary classiﬁcation results with the MulT[22] model as they hadn’t reported it, so it’s unfair to claim that this model can outperform MulT on per class basis. We can still observe a couple of things from the all class classiﬁcation accuracy Acc6 and the F1 score: 1) Our model is giving very competative results to the MulT model. We had only used 3 base attention modules in place of 6 in the MulT model. This would signiﬁcantly reduce the computation requirement. 2) We can see that this multi-modal fusion has been effective in capturing the context of the situation because of its superior performance over uni-modal models. 3) Given that the model is performing on-par for unaligned data, we can prove our hypothesis that attention would indeed naturally provide the alignment and thus we do not explicitly need to align the sequences as a pre-processing step."
2011.10916,data,4,2022-05-16,0,6.1 Pre-processing the Data
2011.10916,data,4,2022-05-16,0,Aligned Data Unaligned Data
2011.10916,data,51,2022-05-16,0,"If we ﬁnd out that we do not need to align the data, it will be a great time-saver for realtime inference. Our hypotheses is that due to the natural alignment provided by cross-attention, we wouldn’t need our data to be aligned for this architecture."
2011.10916,data,54,2022-05-16,0,"Table 2: Table showing results of experiments on the test date for unimodal performance of the delta self-attended modules, fused DCCA modules, and then multi-modal hierarchical attention modules for both aligned and unaligned data. The results are compared with MulT, G-MFN, SotA1, SotA2, and SotA3."
2011.10916,data,91,2022-05-16,0,"For all the approaches taken so far, the below mentioned missing gaps will be addressed in this project. In reccurent networks, looking at the entire sequence of data and outputting the last, single hidden state as its representation, often turns into the model forgetting information way past the current timestamp. They do not preserve the long-range dependencies [22] due to this bottleneck. This domino effect of updating a single hidden state each timestep can also result in exploding or vanishing gradients."
2011.10916,data,98,2022-05-16,0,"Second, data can be unaligned across modalities. A frowning face may relate to a pessimistic word spoken in the past. It’s not always between current word and current expression. And thus, third related point is, neutral expressions are quite idiosyncratic [8]. Some people may always look angry given their facial conﬁguration. This raises the need for delta attention [28], we need to take cross-modal context plus the temporal context within each individual modality to negate the effect of “monotonous-across-the-time” features."
2011.10916,"data, data available",30,2022-05-16,0,"[17] S. Nemati, R. Rohani, M. E. Basiri, M. Abdar, N. Y. Yen, and V. Makarenkov. A hybrid latent space data"
2011.10916,dataset,123,2022-05-16,0,"To have the same dimensionality for the fusion at later stage, we take word-level granularity and for the words spoken at each interval (note that there is interval information alongwith features for all 3 modalities in the dataset), we pad and stack the visual (Facet 4.2) and acoustic (COVAREP) sequences, making all three sequences for a certain time interval [t, t + (cid:15)] having the same length. To make sure that the attention modules don’t interpret padded values (“[PAD]” for words and 0s for the other two), we use attention masks which will mask away the padded values."
2011.10916,dataset,2,2022-05-16,0,5 Dataset
2011.10916,dataset,41,2022-05-16,0,"[13] P. P. Liang and R. Salakhutdinov. Computational modeling of human multimodal language: The mosei dataset and interpretable dyanamic fusion. In First Workshop and Grand Challenge on Computational Modeling of Human Multimodal Language, 2018."
2011.10916,dataset,54,2022-05-16,0,"Due to the novelty of the concept and components, we mainly focused the efforts on building the model architecture and trying out various layers more than trying the same architecture on various datasets. Thus, these experiments are only on the dataset described in section 5, CMU-MOSEI [2]."
2011.10916,dataset,60,2022-05-16,0,"The dataset comes with high-level features in form of glove embeddings having 300 dimensions. But for the purpose of using a transformer, we used raw text to get the advantage of dynamic context alignment. The CMU-MOSEI SDK provides the facility to align the visual and vocal computational sequences with the verbal as the base modality reference."
2011.10916,dataset,73,2022-05-16,0,"[2] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236–2246, Melbourne, Australia, July 2018. Association for Computational Linguistics."
2011.10916,dataset,75,2022-05-16,0,"We trained the uni-modal self-attention modules for all 3 modalities L, V, A separately ﬁrst, this can be done in parallel as these 3 modules are independent of each other after the initial interval alignment performed in the pre-processing step. The selected hypermeter is given in Table 1. The hypermeter selection method of splitting the dataset into train:valid:test sets is given in the section 4.4."
2011.10916,"dataset, used dataset",122,2022-05-16,1,"Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) 1 [2] is the largest and the latest dataset of sentence level sentiment analysis and emotion recognition. It contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics. It has around 23k samples, each with around 1000 text features, 300 visual frames at 15Hz sampling rate, and 150 acoustic features at 20Hz sampling frequency. Most of the transformer based papers have been benchmarked on this dataset, because of its size and the variety in “in-the-wild” emotions. Thus, this project will also use this dataset, to be able to compare the results fairly."
2011.10916,github,3,2022-05-16,0,1https://github.com/A2Zadeh/CMU-MultimodalSDK
2012.01288,data,13,2022-05-16,0,Research supported by BRD — Groupe Societe Generale Data Science Research Fellowships.
2012.01288,data,31,2022-05-16,0,"1 Faculty of Mathematics and Computer Science 2 Human Language Technologies Research Center 3 Data Science Center University of Bucharest ana.uban@gmail.com, alina.ciobanu@my.fmi.unibuc.ro, liviu.p.dinu@gmail.com"
2012.01288,data,50,2022-04-21,0,"17. Vulic, I., Moens, M.: Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014. (2014) 349–362"
2012.01288,dataset,42,2022-05-16,0,"6. Ciobanu, A.M., Dinu, L.P.: Building a Dataset of Multilingual Cognates for the Romanian Lexicon. In: Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014. (2014) 1038–1043"
2012.01288,dataset,57,2022-05-16,0,"The Romance Languages We compute the cosine similarity between cognates for each pair of modern languages, and between modern languages and Latin as well. We compute an overall score of similarity for a pair of languages as the average similarity for the entire dataset of cognates. The results are reported in Table 5."
2012.01288,"dataset, used dataset",48,2022-05-16,0,"The Romance Languages vs English Further, we introduce English into the mix as well. We run this experiment on a subset of the used dataset, comprising the words that have a cognate in English as well4. The subset has 305 complete cognate sets."
2012.01288,publicly available,152,2022-05-16,1,"1. Obtain word embeddings for each of the two languages. 2. Obtain a shared embedding space, common to the two languages. This is accomplished using an alignment algorithm, which consists of ﬁnding a linear transformation between the two spaces, that on average optimally transforms each vector in one embedding space into a vector in the second embedding space, minimizing the distance between a few seed word pairs (for which it is known that they have the same meaning), based on a small bilingual dictionary. For our purposes, we use the publicly available multilingual alignment matrices that were published in [12]. 3. Compute semantic distances for each pair of cognates words in the two languages, using a vectorial distance (we chose cosine distance) on their corresponding vectors in the shared embedding space."
2012.11723,data,131,2022-05-16,0,The paper is organized as follows. In section II we start by describing a representative network that we use to illustrate the methodology to calculate latency bounds. Section III explains the analytical results behind these calculations. Section IV presents the analysis of the representative network. Section V discusses a free-rider principle which states that a fast network designed to transport the signals from cameras and other high data rate sensors can transport the ﬂows from CAN buses and other slow sources for free. Section VI explains a simple method to derive bounds on the latency and storage of fast ﬂows. Section VIII summarizes the main points of the paper and gives hints on which future in vehicle network architectures may beneﬁt from this work.
2012.11723,data,166,2022-05-16,0,"From a more academic point of view, one could also assume links to be asymmetric in line rate. We examine the latency and memory utilization when links are designed to have a line rate equal to 125% of the peak bandwidth of data required, independent from the standardized Ethernet modes of operation. That is, we design the links so that their maximum utilization is 80%. We consider the case when the processor only sends slow trafﬁc to fast devices (e.g., cameras), as it it the situation that corresponds to the slowest links which might result in larger latency and memory occupancy. In particular, we assume that the processor can send at most a burst of 4 packets of 512 bits to a given camera. The delay of these four packets on the 10Mb/s link attached to the camera is then at least 4 × 512/(10Mb/s) = 200µs."
2012.11723,data,169,2022-05-16,0,"the aggregation of camera data requires high line rate links. Again the speciﬁc execution of this model could vary greatly between manufacturers and few have actually come to the market so far. It could be argued that a shift away from privately owned individual vehicles towards automated robotaxi ﬂeets may drive this transition from functional clusters to geometrical zones, as the aspects of functional variance between customer chosen options, generational carry over and nameplate spread become less important for automated robotaxi ﬂeets while integration of systems and sensors becomes more important to solve the task of perception and control for driver-less operation in such vehicles. A further building block to allow this data convergence, which can be observed in the telecommunications business for quite some years already, is the only recent availability of multi-Gb/s Ethernet physical layer links for application inside a vehicle. All these aspects together create the environment wherein the concepts of this paper can be successfully deployed."
2012.11723,data,204,2022-05-16,0,"Roy Myers received his Bachelor’s and Master of Science in Electrical Engineering from Georgia Institute of Technology in 1990 and 1991, respectively. He then joined National Semiconductor LAN Division designing mixed signal Ethernet IC products. In 1996, he joined Enable Semiconductor, as a founding member, developing Ethernet 100Base-TX transceivers. Enable Semiconductor was acquired by Lucent Technologies in March 1999. After leaving Lucent, he co-founded Terablaze Inc in 2000 where he was Director of Engineering developing highly scalable network fabrics and layer 2/3 Gigabit Ethernet switching solutions. TeraBlaze Inc was acquired by Agere Systems in 2004. Roy continued to lead the Gigabit Ethernet switch development at Agere and then at LSI after acquisition of Agere Systems in 2007. He left LSI in 2007 to join Aquantia Corp (AQ:NYSE) where he was Chief Architect developing a series of 10GBase-T and Multi-Gig MAC/PHY products to serve client, enterprise, and data center markets. Since April 2018, Roy is a co-founder and SVP of Engineering of Ethernovia Inc developing network solutions for the next generation of automobiles. Roy holds 14 granted patents in communication technology."
2012.11723,data,219,2022-05-16,0,"Due to the very different approaches of OEMs towards their product strategy and resulting network architectures, we need to classify them based on more abstract concepts in order to compare them. The most common starting point is and was a functional clustering. Here electronic control units (ECUs) which are generally related in their functionality and thereby often designed within the same organizational branch of the OEM [29] are connected together directly, offering only very limited interfaces to systems of other functional clusters. Such clusters may for example be the engine, the drive train, or the infotainment system. The theoretical extreme of this is often described as a domain-based system. How these different functional clusters or domains interact is very different for different OEMs. As the need to exchange data between domains has increased, e.g., to avoid duplication of expensive sensors in constrained packaging spaces, the so-called zonal model has gained much attention. In the zonal architecture the focus lies on integrating different functionalities onto a smaller set of ECUs, which share access to sensors and actuators. This also leads to shorter cables and more importantly for this paper, a reduced number of hops in the network where particularly"
2012.11723,data,219,2022-05-16,0,"Each source, such as a camera, a radar, an audio/video server, a 5G antenna, a CAN electronic control unit, a button, and so on, is attached to a bridge that polices and shapes the trafﬁc. The policing veriﬁes that the source is not misbehaving, such as a faulty switch that thinks it is being pushed every millisecond. The shaping separates the packets by the maximum gap consistent with the deadline to deliver a group of packets or with the rate of a stream or ﬁle transfer. For instance, say that a camera produces 2400 packets of 1500 Bytes every 16 milliseconds and that one wishes to deliver these 2400 packets in about 12 milliseconds. Then, the bridge transmits one packet every 12ms/2400 = 5µs. As another example, a bridge for a 5G antenna could send one 1500 Byte sized packet every 10µs to carry a data rate of 1.2Gb/s. Thus, every source of trafﬁc is shaped, including best effort sources. We assume that three of the four fast devices attached to a core switch send 12, 000b (or 1, 500 Bytes) packets every 5µs and one sends 12, 000b packets every 15µs."
2012.11723,data,42,2022-05-16,0,"For instance, a network designed to transport the data from cameras, radars, lidars, audio/video servers to processors and user interfaces can also transport the slow ﬂows between CAN buses, push-buttons, relays, and so on."
2012.11723,data,52,2022-05-16,0,"The network devices are classiﬁed as core, fast, or slow. Using Rene Cruz’s results, one shows that a network fast enough to transport signals from cameras and other fast sensors can also transport the data from slow sources and guarantee them their desired bounded latency."
2012.11723,"data, data available",85,2022-05-16,0,"required to control the camera. With the introduction of Energy Efﬁcient Ethernet (EEE)[30] this issue has partially been addressed. In EEE the power consumption of the lower bandwidth direction is reduced by turning off the transmitter while no data is available. Thus, the line rates at which the frames are transmitted remain symmetrical. As the Small Flow Approximation only depends on the line rate of a frame, it is completely untouched by EEE."
2012.11723,download,152,2022-05-16,0,"Thus, this architecture uses two central components of the IEEE time-sensitive networks standards: policing and shaping. It locates these components in the source bridges, instead of the network switches. The main beneﬁts of this approach are that the network uses a single priority class and that no further trafﬁc reshaping or scheduling is required in the switches. As we explain in the paper, control signals are delivered with guaranteed sub-millisecond latency even though they share queues with bursty best effort trafﬁc and audio/video streams. These latency guarantees are deterministic, not probabilistic: they are worst-case guarantees derived from analyzing the worst case behavior of the network, not by simulations. The architecture ensures that the web download of a new movie cannot interfere with the break signal from the self-driving control system or from the break pedal. Such guarantees are"
2101.03069,"data, data available",134,2022-04-21,0,"dominant Twitter accounts to be found in the data. One would expect to find highly active Twitter users in cases where the platform is used to amplify messaging. In the data, only two accounts (both from Spain) were found to have tweeted more than 100 times during the eight-month period. One of the accounts belongs to a paediatrician while the other to the Spanish Society for Paediatric Infectious Diseases. In the absence of an ideologically-motivated group, movement or collective, and some evidence that scientific rather than political activity is the driver of social media activity, Twitter is not in this case being used as a communication platform to amplify messaging about the risks or benefits of children attending school during the COVID-19 pandemic."
2101.03069,"data, data available",255,2022-05-16,0,"The scientific output results in mixed evidence of infection and transmission as they pertain to children. The limitations of the scientific studies and the consequent levels of uncertainty were conveyed when reporting findings. This is, however, not a unanimous approach. For example, a viewpoint in the Archives of Disease in Childhood (Munro and Faust, 2020), is entitled “Children are not COVID-19 super spreaders: time to go back to school”. The title appears to be inflated by the urgent need for policy decisions. The authors write “At the current time, children do not appear to be super spreaders. Serosurveillance data will not be available to confirm or refute these findings prior to the urgent policy decisions that need to be taken in the next few weeks such as how and when to reopen schools.” They continue “Governments worldwide should allow all children back to school regardless of comorbidities. Detailed surveillance will be needed to confirm the safety of this approach, despite recent analysis demonstrating the ineffectiveness of school closures in the recent past (Viner et al. 2020b). The media highlight of a possible rare new Kawasaki-like vasculitis that may or may not be due to SARS-CoV2 does not change the fact that severe COVID-19 is as rare as many other serious infection syndromes in children that do not cause schools to be closed”. The title suggests no uncertainty"
2101.03069,"data, database",149,2022-05-16,0,"The two databases (WHO and CORD-19) do not represent distinctive sets of publications, having quite  substantial overlap. In order to avoid duplicates, the two databases were merged and cleaned. For a reliable merging of the two databases, as well as for the further tracing of the (social) media reception of the publications, it was necessary to count with unique document identifiers (e.g., PubMed Identifiers, Digital Object Identifiers, etc.). Particularly Digital Object Identifiers (DOI) are commonly assigned to scientific publications to univocally identify scientific documents across databases and the web-at-large. The main inconvenience of using DOIs is that we can only identify and combine publication data for half of the papers included in the CORD-19 database and a third of those included in the WHO database (Figure 1)."
2101.03069,"data, dataset, data available",125,2022-05-16,0,"We also investigated the profiles of tweeters whose tweets were collected in our dataset. In particular, we looked into the share of tweeters who had tweeted about science (i.e., tweeted an academic publication) before the pandemic, that is, tweeters from our sample present in altmetric data from 2019. We found that 59.3% in Spain, 60.6% in the Netherlands and 65.1% in South Africa had mentioned other scientific articles in their tweets prior to the pandemic. From the total of 8,597 distinct tweeters identified in all three countries, 5,141 had already referenced scientific output before the COVID-19 pandemic. Moreover, we attempted to determine the professions of the 8,597 tweeters."
2101.03069,"data, dataset, database",131,2022-05-16,0,"Data collection The data collected for this study was extracted from a variety of sources: scientific publications, news outlets, and social media discussions and policy interventions. Since the outbreak of the pandemic, different community- and organization-led initiatives have been conducted to make scientific publications on COVID-19 openly accessible. In this study, we made use of the COVID-19 Open Research Dataset (CORD-19) and the World Health Organization (WHO) COVID-19 Global literature on coronavirus disease database. These two databases are of special interest due to the combination of sources they include, containing not only studies published in scientific journals but also preprints from the main global repositories (e.g., BioRxiv, MedRxiv, SSRN, etc.)."
2101.03069,"data, dataset, database",216,2022-05-16,0,"A final number of 5,713 publications along with their DOIs have been collected in our final dataset of scientific output. We proceeded to identify news outlets and social media discussions around the scientific publications in our dataset. News media items mentioning a DOI in our set were identified with data from Altmetric.com, retrieved in October 2020. From a total of 19,922 news items found globally for the set of DOIs in our database, 424 news articles could be identified as originating from the Netherlands, Spain, or South Africa. This was done by matching the URLs of the news outlet coming from Altmetric.com with the URLs of Dutch, Spanish, and South African national newspapers and broadcasting services, as extracted from Wikipedia and other websites listing news outlets. The final list of news outlets from each country was verified and curated manually. We identified 200 news items from Spain, which referenced 81 distinct DOIs. In South Africa, 79 news pieces referenced 72 distinct DOIs and in the Netherlands, 145 news items referenced 83 distinct DOIs. The titles and short abstracts of the news articles (where available in the data from Altmetric.com) were analyzed manually  for our study."
2101.03069,database,1,2022-05-16,0,database
2101.03069,database,145,2022-05-16,0,"The available scientific output about the role of children and schools in the COVID-19 pandemic has not been picked up in the social media in the three countries in our study to the same degree. We found that only 17.9% of the publications in our database have been tweeted about in the three countries; this is much less than the coverage of about 63.0% of all attention for CORD-19 publications as overall captured by Altmetric (Colavizza et al. 2020). A total of 932 DOIs (16.3% of the scientific output) has been mentioned in the Spanish tweets on the topic. In the Dutch tweets, only 4% of the scientific output (229 articles) has been mentioned, whereas in South Africa 289 articles (5%) have been mentioned."
2101.03069,database,222,2022-05-16,0,"Spain While in the Netherlands and South Africa schools reopened after around two months of closure, in Spain school reopening was delayed until after the summer holidays. Figure 3 depicts the announced and implemented measures, in chronological order, both at the national, as well as the regional levels. The policy measures registered no difference between primary and secondary schools. The figure also includes the timeline distribution of the news outlets and tweets in our database, which have been identified as originating from Spain. A total of 188 news articles and 15,603 tweets were identified between the beginning of February and the end of September 2020. News articles on the topic registered brief appearances before the school closure in March, as well as more consistent appearances around the reopening of schools in September. As for tweets, we can observe small peaks around the time of the announcements in March, as well as shortly before and after the schools reopening in September. Further activity has been registered during the school closure, with peaks around end of April, when the government announced a plan for easing lockdown restrictions, as well as in July and August, when no other policy intervention has been announced nor occurred."
2101.03069,database,67,2022-05-16,0,"Note: Number of total publications by database [Pubs], publications in 2020 [Pubs in 2020], share of publications with Document Object Identifier (DOI) [%DOI in 20202], number of publications related to children and schools [Pubs children] and share of publications with a DOI related to children and schools [%DOI children]"
2101.03069,dataset,90,2022-05-16,0,"An overview of the topics covered by tweets from Spain is depicted by a VOSviewer map in Figure 4. The nodes in the map present the co-occurrence of the most relevant keywords identified from the titles of the 5,713 articles in our dataset. The color coding of the map reflects the prevalence of mentions of those articles in tweets from Spain relative to the worldwide collected tweets: The darker the color, the more focus on the keywords relative to the worldwide tweets on the topic."
2101.03069,dataset,92,2022-05-16,0,"We identified 740 researchers, 741 health professionals, and 296 journalists based on terms found in the user descriptions of the tweeters. We note a possible overlap between the groups, as someone can be both a health professional and, e.g., hold a PhD (one of the indicators for being a researcher). Given the limited available information on Twitter and the limitation of our search algorithms, we expect that these results are underestimating the true presence of those professions in our dataset.  Discussion"
2101.03069,"dataset, database",88,2022-05-16,0,"We downloaded the two complete databases on October 15, 2020. Table 1 shows some descriptive values of the size of the database at the time. We searched within the title and abstract fields for documents containing the words ‘children’ and ‘schools’. After merging the ‘Pubs children’ documents of both datasets, a total of 5,713 publications were retrieved. This is our final set of scientific publications from which we trace their (social) media reception."
2101.10245,"data, dataset",149,2022-05-16,0,"We would also like to point out the problems with the Samsung S5 gesture sensing API. Samsung has deprecated support for the device and access to the sensor output is limited. Moreover, there is no way to access the raw sensor values without rooting the phone. This limits the impact of our current approach to pervade the current market, but doesn’t limit the research contribution. This deprecation did affect our user study. Because the sensor API was deprecated, many of the angle and velocity measures were flagged “unknown.” We removed those incomplete records from our dataset but the reliability of the sensor reading is called into question. As such, our results might represent a lower bound of performance and may be further increased with more reliable sensor readings or more expressive IR sensor data."
2101.10245,github,38,2022-05-16,0,"François Chollet et al. 2015. Keras. https://github.com/fchollet/keras. Bruno Dumas, Denis Lalanne, and Sharon Oviatt. 2009. Multimodal interfaces: A survey of principles, models and frameworks. Human"
2101.10245,open-source,41,2022-05-16,0,"To normalize and control dynamic range, we take the decibel magnitude of the STFT. The implementation of the STFT grid search and feature extraction techniques have been made open source and are available at [Mundada 2017]."
2101.10245,python,105,2022-05-16,0,"To create, train and validate machine learning algorithms we use a combination of packages in Python. Specifically, we use the “scikit-learn” library [Pedregosa et al. 2011] and Keras [Chollet et al. 2015] with the TensorFlow [Abadi et al. 2015] back-end. We chose to investigate several different machine learning baselines and also several different convolutional neural network architectures. It was unclear what neural network architecture and parameters of the architecture would be optimal, so we chose to train several variants and perform hyper parameter tuning for each architecture."
2101.10245,"python, github, open-source",70,2022-05-16,0,"Raunak Mundada. 2017. AirWare Open Source Repository. https://github.com/raunakm90/AirWare. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, Oct (2011), 2825–2830."
2102.0082,"data, dataset",112,2022-05-16,0,"An Adaptive Neuro Fuzzy Network with a TSK fuzzy type combined with an improved quantum subtractive clustering method to obtain appropriate number of fuzzy rules is proposed. The subtractive clustering, a density based algorithm, is used to determine number of cluster centers. Moreover a modified quantum clustering, an idea from quantum mechanics is applied to obtain cluster centers. Cluster centers represent a general model with essential characteristics of data which can be use as premise part of fuzzy rules. It caused impressive decrease in number of fuzzy rules and network accuracy. Finally we construct our model to predict fuel consumption in MPG dataset."
2102.0082,"data, dataset",181,2022-05-16,1,"Generally effective partitioning of input space can reduce number of fuzzy rules and increase learning speed of Anfis. In this paper we use a quantum subtractive clustering to determine fuzzy rules, and then a modified Anfis is applied to data set to predict the output. The experiments used the well-known automobile mile-per-gallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile. At first records with missing values has been eliminated, then we normalize data to [0,1] interval. Data set divided into two partition; train data and test data, according to even record and odd records from original dataset, respectively. Here we use training data set to construct and learn the model while test data is used to validate the model."
2102.0082,"data, dataset",203,2022-05-16,0,"We have compared our new Quantum Subtractive Clustering method with traditional QC applied in Anfis [1].  Fig. 2(a) shows comparison results between the desired and traditional QC model outputs for test data. Here horizontal axis shows number of test data and vertical axis determines desired and model fuel consumption. Also Fig .2(b) shows comparison diagrams between the desired and our model outputs for test data set.  i s  A s  d i f f e r e n c e  o b v i o u s  a n d  b e t w e e n  p r o p o s e d  m o d e l  o u t p u t  i s  l e s s  t h e  t h a n  p r e v i o u s  m e t h o d .  A s  h a s  r e s u l t ,  a n d  b e t t e r  g e n e r a l i z a t i o n  c a p a b i l i t y ."
2102.0082,dataset,138,2022-05-16,0,"Therefore we propose a new method to construct an adaptive neuro fuzzy network with a TSK fuzyy type. Also we use a modified QC to determine the premise part of fuzzy rules. Moreover subtractive clustering method is applied to determine the optimal number of clusters. We performed a learning method by a hybrid learning scheme using back propagation (BP) and a least-square estimator (LSE). The experiments used the well-known automobile mile-pergallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of a car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile."
2102.03237,data,30,2022-04-21,0,"Kim, J., & Owen-Smith, J. (In print). ORCID-linked labeled data for evaluating author name disambiguation at scale. Scientometrics. Doi: 10.1007/s11192-020-03826-6"
2102.03237,"data, database",259,2022-05-16,0,"Another group of studies has relied on third-party data sources that control the accuracy of researcher information. For example, Kawashima and Tomizawa (2015) evaluated the disambiguation performance of SCOPUS on a list of 75,405 Japanese author names in 573,338 papers. For this, they used the Database of Grants-in-Aid for Scientific Research (KAKEN) that maintains a unique ID number of a funded researcher in Japan with a list of her/his verified publications. An author name instance in a SCOPUSindexed paper was compared to each KAKEN researcher profile by comparing name strings, publication records, and affiliations. If a match was found, the KAKEN researcher ID was assigned to the author name instance. Such a record linking technique has been used in other studies to label name instances of Italian researchers (D'Angelo, Giuffrida, & Abramo, 2011) and Dutch researchers (Reijnhoudt, Costas, Noyons, Borner, & Scharnhorst, 2014) using each nation’s administrative scholarly databases. Other sources for labeling include NIH-funded researcher profiles2 (e.g., K. Kim, Sefid, Weinberg, & Giles, 2018; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009) and Highly Cited Researchers data3 (e.g., Liu et al., 2014; Torvik & Smalheiser, 2009). While these record linkage procedures produce large-scale, accurate labeling results, it also provides biased results (Lerchenmueller"
2102.03237,"data, database",40,2022-05-16,0,"Kim, J. (2017). The impact of author name disambiguation on knowledge discovery from large-scale scholarly data. (Ph.D.), University of Illinois at Urbana-Champaign, Retrieved from http://hdl.handle.net/2142/98269 IDEALS database."
2102.03237,"data, dataset",101,2022-05-16,0,"Furthermore, ORCID-linkage can help researchers label the name instances of authors who work in diverse research fields for which labeled data are scarce. Most existing labeled datasets for author name disambiguation were created to disambiguate author names in a few scientific domains, especially Computer Science and Biomedical Sciences (A. A. Ferreira, Gonçalves, & Laender, 2012; Müller et al., 2017). For those who need to mine ambiguous bibliographic data that represent diverse fields, ORCIDlinkage can be an effective way to generate labeled data for their ad-hoc disambiguation tasks."
2102.03237,"data, dataset",137,2022-05-16,0,"Second, ORCID-linked labeled data can complement other types of linkage-based labeled data. Our comparisons across three different types of linked labeled data showed that , ORCID-linked labeled data could captured the aspects of Author-ity2009’s performance that were also identified in the other two datasets. This means ORCID linkage can be used as an alternative to other labeling methods if they are unavailable. In addition, ORCID-linkage can be used to help researchers evaluate the labeling quality of other labeled data. Out of 312,951 instances in AUT-NIH, for example, a total of 32,131 instances were also linked to 3,578 ORCID ids. Among them, 99 name instances were assigned to different authors by the ORCID-linkage and the NIH-ExPORTER linkage used in Lerchenmueller and Sorenson (2016)."
2102.03237,"data, dataset",142,2022-05-16,0,"Different levels of name ambiguity might arise from the different sizes of labeled data in our study: AUTORC contains more than 3 million instances, while AUT-NIH consists of 312K instances. As name ambiguity in bibliographic data tends to increase with data size (Fegley & Torvik, 2013; J. Kim, 2017), AUT-ORC might be naturally more ambiguous than AUT-NIH. Other differences between these datasets may result from the data sources from which they were drawn. AUT-NIH relied on funded PI information. So, the name instances that could be labelled were restricted to those of researchers who have ever received funds from NIH, a group likely to be more prominent and more homogenous than science itself. In contrast, AUT-ORC utilized ORCID profile data for more than 5 million researchers"
2102.03237,"data, dataset",196,2022-05-16,0,"English name instances constitute the majority in AUT-NIH (diagonal-line bar, 54.53%), while other ethnicities are heavily underrepresented compared to their ratios in Author-ity2009. This might be because AUT-NIH is created based on information of PIs who have ever received funds from NIH in the U.S. Non-US investigators are generally ineligible to apply for NIH funds, so it makes sense that the name instance distribution in this dataset would skew toward English names. This English-skewed distribution is also confirmed in Lerchenmueller and Sorenson (2016) who found that 84% of all ethnicity-identified instances in the whole Author-ity2009 linked to NIH ExPORTER are ‘Caucasian’ (including many European names as well as English). Meanwhile, many instances in self-citation relation are also English (horizontal-line bar; 34.65%) but the ratio differences of other ethnicities against Authority2009 are smaller compared to those in AUT-ORC and AUT-NIH. As such, three labeled data are common in that English name instances are prevalent but none of them represents well the ethnicity distribution in Author-ity2009 because some ethnicities are over-represented while others underrepresented."
2102.03237,"data, dataset",21,2022-05-16,0,(1) How well do ORCID-linked labeled data represent the population of name instances in a large-scale bibliographic dataset?
2102.03237,"data, dataset",217,2022-05-16,0,"This study also creates a benchmark labeled dataset by linking Author-ty2009 with Principal Investigator (PI) information recorded in the National Institutes of Health (NIH) funded research data (ExPORTER). This NIH-linkage has been used in several studies to evaluate author name disambiguation for MEDLINE because ExPORTER provides the PMIDs of research papers in MEDLINE that result from NIH funds (e.g., K. Kim, Sefid, & Giles, 2017; K. Kim et al., 2018; Liu et al., 2014; Torvik & Smalheiser, 2009). After an Author-ity2009 paper’s PMID is found to be associated with a specific NIH grant, the author names in the paper are compared to the names of the PI who received the funding. If a PI’s name is found to match an author name, her/his unique NIH PI ID is assigned to the author name as a label. This study reuses the list of NIH PI IDs linked to the Author-ity2009 in Lerchenmueller and Sorenson (2016)13. To make this NIH-linked labeled data (AUT-NIH) comparable to AUT-ORC, each name instance in AUTNIH is assigned an ethnicity and a gender using Ethnea and Genni each."
2102.03237,"data, dataset",227,2022-05-16,0,"Block Size Distribution: Another way to discover how three labeled datasets represent Author-ity2009 is to compare the distributions of block sizes in each dataset. A common practice in author name disambiguation research is to collect author name instances into a block if they match on the full surname and first forename initial. Comparisons that support disambiguation are then performed within blocks (K. Kim et al., 2018). Many studies have used the block size distribution to characterize labeled data (e.g., J. Kim et al., 2019; Levin et al., 2012; Müller, Reitz, & Roy, 2017; Torvik & Smalheiser, 2009). Block sizes can become huge because labeled data contain a few hundreds of thousands (AUT-NIH) or millions (AUT-ORC) of name instances. So, block size distributions are plotted using a cumulative density function on log-log axes. Figure 5 visualizes the block size distributions in AUT-ORC and AUT-NIH. Note that AUT-SCT cannot produce a block size distribution because self-citation pairs only contain match information at the pair level and, thus, the matching status of name instance pairs that are not in self-citation relation but that may nevertheless fall within a block defined by surname and first initial are still unknown."
2102.03237,"data, dataset",253,2022-05-16,0,"Gender Distribution: To provide another indicator of how well each labelled dataset represents Authority2009, we turn to comparisons of the gender composition of author name instances. Figure 3 shows that the majority of name instances in all datasets are male (black bar; 57%) while female instances (22.32%) and NULL (i.e., gender unidentifiable) instances (20.28%) make up the rest with similar percentages. Such an imbalanced gender distribution is broadly characteristic of scientific authorship in general (Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013) as of biomedical science (Jagsi et al., 2006). The gender imbalance is also observed in AUT-ORC (gray bar) in which male names constitute 67.46 percent of all name instances while the percentage of female instances (22.70%) is quite similar to that in Autority2009. The higher ratio of male instances in AUT-ORC than in Author-ity2009 seems to be a trade-off with the reduced ratio of Null name instances. The same pattern is observed in AUT-NIH and AUT-SCT in which the dominance of male names are more prevalent (i.e., 73.95% and 65.44% each) than in Autority2009 and AUT-ORC but with lower ratios of Null names and similar ratios of female names. These observations indicate that despite the minute differences in gender ratios, three labeled data shared similar patterns of gender distribution."
2102.03237,"data, dataset",272,2022-05-16,0,"The answers to these questions can help disambiguation researchers to make informed choices of labeled data and to create evaluation and ground-truth datasets at scale. Several studies have attempted to answer similar questions by discussing how ORCID profiles represent the author population in Web of Science (Youtie, Carley, Porter, & Shapira, 2017), what issues need to be addressed before ORCID can be used as a gold standard for author disambiguation (Albusac, de Campos, Fernández-Luna, & Huete, 2018; Eichenlaub & Morgan, 2017), and how record-linkage-based labeling may or may not work in author disambiguation under certain conditions (Anderson A Ferreira, Gonçalves, & Laender, 2020; Reijnhoudt et al., 2014). This study contributes to that growing literature by demonstrating the use of ORCID-linked labeling against another large-scale disambiguated dataset constructed using different linkage-based labeling methods. Specifically, this study labels name instances in MEDLINE by linking them with ORCID researcher profiles. Then, the performances of Author-ity2009 which disambiguates MEDLINE author names, is evaluated using the labeled data. For comparison, two labeled datasets are created using two widely-used sources - NIH-funded researcher information and self-citation information. The three labeled datasets are compared for their representativeness of Author-ity2009 as well as to evaluate results of the Author-ity2009’s disambiguation performances. After that, a discussion follows about the implications and challenges of using ORCID for labeling. In the following section, labeling procedures via record-linkage for Author-ity2009 are described in detail."
2102.03237,"data, dataset",319,2022-05-16,0,"In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b. Both these distances may be due to the relatively larger European focus of ORCID and the US focus of NIH data. The largest name blocks in Author-ity2009 tend to be created by highly ambiguous Asian name instances. If the distributions of both labeled datasets were representative of Author-ity2009, we would expect to see their distributions track closely with those of the random subset of Author-ity2009 name instances."
2102.03237,"data, dataset",336,2022-05-16,0,"In Figure 5, the x-axis shows block sizes ranging from 1 (i.e., a single instance block) to 25,917 in Author-ity2009 (Figure 1a). They-axis represents the ratio of blocks with a specific size or larger (cumulative) over all blocks is shown. For example, in Author-ity2009, blocks with 2 or more name instances (blue circles) constitute 63.47% of all blocks, which in reverse means that 36.53% of blocks contain only one instance. In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b."
2102.03237,"data, dataset",98,2022-05-16,0,"Distribution of Publication Years: As shown in Table 3, the three labeled datasets – AUT-ORC, AUTNIH, and AUT-SCT –contain different numbers of labeled name instances (pairs). How do those instances differ and which is most representative of the overall Author-ity2009 dataset? To characterize the composition of labeled data, publication years of papers in which a labeled instance appears are counted. Figure 2 compares the publication year distributions of three labeled datasets. Note that for AUT-SCT, years associated with each self-citing name instance pair are counted."
2102.03237,"data, dataset provided",167,2022-05-16,0,"Second, the accuracy of ORCID records still needs to be verified. As acknowledged by ORCID, some records may contain errors due to “benign” (unintentional) mistakes by profile creators (e.g., claiming other researcher’s work as their own)18. Note that other labeled data may have the same verification problems. Human experts can produce inaccurate labels and often disagree on labeling decisions even given the same information (Shin et al., 2014; Song et al., 2015). Although NIH PI data are curated with special care by NIH, the linkage process for labeling may entail erroneous matching between PI names and author names in NIH-funded papers. As shown above regarding the labeling quality of AUT-NIH (see 3rd paragraph in Conclusion and Discussion), ORCID-linked data provided more accurate labeling results than the other method but still contained erroneous labels. To ensure that errors in ORCID records"
2102.03237,"data, dataset provided",299,2022-05-16,0,"Third, ORCID-linked labeled data can provide more enriched evaluation results. They can be used together with other labeled data for triangulating a disambiguation method’s performance. Unlike selfcitation-based labeled data, ORCID-linked labeled data can be used to measure both clustering and classification performances. Unlike NIH-linked labeled data, ORCID-linked labeled data contain a greater range of ambiguous names across ethnicities, which can enable a disambiguation method to be evaluated on name instances with different ambiguity levels. This in turn allows for more focused analysis to address difficult disambiguation tasks such as those presented by synonyms and homonyms. Moreover, ORCID-linkage can produce labeled instances that are challenging to disambiguate but are not easily collectable by other labeling methods. For example, FINI could not reach perfect recall in AUT-ORC (Figure 6a and Figure 7a). As detailed above (see Clustering Performance: AUT-ORC and AUT-NIH), 273,782 name instances of 12,646 authors (= unique ORCID ids) are recorded in a way that their ‘surname + first forename initial’ strings of the same author are different. This means ORCID-linkage could produce labeled name instances that refer to the same authors but do not belong to the same blocks. Such synonymous name variants existing across blocks have been insufficiently studied in disambiguation research (Backes, 2018; Gomide et al., 2017) because many studies have created labeled data by collecting (= blocking) ambiguous name instances sharing at least the full surname and first forename initial (J. Kim, 2018; Müller et al., 2017). Using ORCID-linked labeled data, scholars can develop disambiguation models that address synonyms as well as homonyms."
2102.03237,"data, dataset, data https",148,2022-05-16,1,"9 https://databank.illinois.edu/datasets/IDB-9087546 10 26 ethnicities include: African, Arab, Baltic, Caribbean, Chinese, Dutch, English, French, German, Greek, Hispanic, Hungarian, Indian, Indonesian, Israeli, Italian, Japanese, Korean, Mongolian, Nordic, Polynesian, Romanian, Slav, Thai, Turkish, and Vietnamese. Some name instances are assigned compound ethnicities (e.g., “Jane Kim” → Korean-English) if the surname and forename of an author name are associated frequently with different ethnicities.  11 Genni + Ethnea for the Author-ity 2009 dataset. (2018). Retrieved from: https://doi.org/10.13012/B2IDB9087546_V1 12 https://en.wikipedia.org/wiki/Andrea 13 https://dx.doi.org/10.6084/m9.figshare.3407461.v1. Instead of 355K instances in the original linked data, this study filters 313K instances recorded in papers published between 1991 and 2009."
2102.03237,"data, dataset, publicly available, data available",130,2022-05-16,0,"This study suggests several implications for researchers and practitioners of author name disambiguation. First, ORCID can be an effective source of authority for creating labeled data. This study illustrated that ORCID-linkage can generate millions of labeled name instances in a bibliographic data, which is not easily achievable by manual or other record-linkage-based labeling. In addition, ORCID-linkage can be repeated without much additional cost once technical procedures for record-linkage are implemented. Moreover, ORCID data continue to be expanded, publicly available, and released annually. This means labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated dataset and updated on a regular basis, enabling sustained evaluation of author name disambiguation in ever-growing digital libraries."
2102.03237,"data, publicly available",169,2022-05-16,1,"Author-ity2009 is chosen as an evaluation target for three reasons. First, Author-ity2009 conducts author name disambiguation on a digital library scale: 61.7M name instances in 18.6M papers published between 1966~2009 as indexed in MEDLINE. Evaluating disambiguation results for such a large bibliographic corpus can be a daunting challenge. So, Author-ity2009 can be a good use case to illustrate how ORCIDlinkage can contribute to the performance and evaluation of an important, large-scale disambiguation task. Second, the performance of Author-ity2009 have been evaluated on different types of labeled data in several studies (e.g., J. Kim, 2019b; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009), as summarized in Table 1. This provides a context for comparing ORCID with other labeling sources to better understand its strengths and weaknesses. Third, Author-ity2009 is publicly available for research, enabling scholars to replicate and validate this study."
2102.03237,"data, publicly available, dataset provided",132,2022-05-16,1,"In evaluating the clustering results of Author-ity2009, ORCID-linked labeled data effectively captured the ‘high precision over high recall’ strategy of Author-ity2009. Although comparative labeled data also produced the same evaluation results, ORCID-linked labeled data could provide more nuanced details about the Author-ity2009’s performance when name instances were evaluated across ethnic name groups. As such, ORCID-linkage can be used as a labeling method to produce large-scale truth data to evaluate the performance of a disambiguation method from various aspects. Three large-scale labeled data – AUTORC, AUT-NIH, and AUT-SCT – used in this study are publicly available17. The data sharing is expected to assist researchers to develop, compare, and validate disambiguation models using diverse, large-scale labeled data."
2102.03237,"data, publicly available, dataset provided, data available",221,2022-05-16,1,"How can we evaluate the performance of a disambiguation method implemented on big bibliographic data? This study suggests that the open researcher profile system, ORCID, can be used as an authority source to label name instances at scale. This study demonstrates the potential by evaluating the disambiguation performances of Author-ity2009 (which algorithmically disambiguates author names in MEDLINE) using 3 million name instances that are automatically labeled through linkage to 5 million ORCID researcher profiles. Results show that although ORCID-linked labeled data do not effectively represent the population of name instances in Author-ity2009, they do effectively capture the ‘high precision over high recall’ performances of Author-ity2009. In addition, ORCID-linked labeled data can provide nuanced details about the Author-ity2009’s performance when name instances are evaluated within and across ethnicity categories. As ORCID continues to be expanded to include more researchers, labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated data and updated on a regular basis. This can benefit author name disambiguation researchers and practitioners who need large-scale labeled data but lack resources for manual labeling or access to other authority sources for linkage-based labeling. The ORCID-linked labeled data for Authortiy2009 are publicly available for validation and reuse."
2102.03237,database,14,2022-05-16,0,"funding database in Japan. Scientometrics, 103(3), 1061-1071. doi:10.1007/s11192-015-1580-z"
2102.03237,database,140,2022-05-16,0,"This study shows the potential of ORCID-linkage-based labeling for evaluating author name disambiguation by assessing the disambiguation performance of Author-ity2009 (Torvik & Smalheiser, 2009; Torvik, Weeber, Swanson, & Smalheiser, 2005). Author-ity2009 is a bibliographic database that contains disambiguated author names in MEDLINE5, the world’s largest digital library of biomedical research, maintained by the U.S. National Library of Medicine (NLM). In Author-ity2009, author names are disambiguated in two steps. First, name pairs are compared for similarity over various features such as middle name initial, coauthor name, affiliation, and Medical Subject Headings. Next, the instance pairs are grouped into clusters by a maximum-likelihood-based, hierarchical agglomerative clustering algorithm using the pairwise similarity calculated in the first step."
2102.03237,database,27,2022-05-16,0,"author names in a large-scale bibliographic database. Paper presented at the Library of Congress International Symposium on Science of Science, Washington D.C. http://hdl.handle.net/2142/88927"
2102.03237,dataset,158,2022-05-16,0,"Other differences between labeled datasets also help illuminate particular strengths and weaknesses in Author-ity2019. AUT-ORC and AUT-NIH have different levels of name ambiguity. When the Authority2009’s performance was compared with two commonly-used baseline methods, it was less impressive on AUT-NIH where simpler the baseline methods accomplished equivalently high precision and recall to the more sophisticated Author-ity2009. In contrast, in AUT-ORC, the performance gaps between Authority2009 and baseline methods widened substantially. Considering that the baseline methods are deterministic (matching name instances on full surname and initialized forename), their strong performances mean that (1) while many name instances in AUT-ORC and AUT-NIH are not ambiguous, (2) AUT-ORC contains more ambiguous names than AUT-NIH. We observed the same patterns in comparisons of performance across groups of ethnic name instances known to vary in their ambiguity (Figure 7 and Figure 8)."
2102.03237,dataset,161,2022-05-16,0,"Clustering Measure: To assess the performances of Author-ity2009 on three labeled datasets, author name instances referring to the same author are grouped into a cluster. Specifically, a truth cluster is the collection of author name instances that share the same ORCID ID (AUT-ORC) or the same NIH PI ID (AUT-NIH). Meanwhile, a predicted cluster is the collection of author name instances that share the same Author-ity2009 ID (AUT-ORC and AUT-NIH). Then, the predicted cluster is compared to the truth cluster to quantify how well it contains only and all instances that belong to the truth cluster. This study uses B-Cubed (B3), one of most frequently used clustering metrics in author name disambiguation (J. Kim, 2019a). This measure is comprised of three metrics: B3 Recall, B3 Precision, and B3 F1, which are defined as follows:"
2102.03237,dataset,255,2022-05-16,0,"As reported above, three labeled datasets together highlight different aspects of Author-ity2009’s disambiguation performance. They all showed that Author-ity2009 is highly accurate in disambiguating author name instances. It demonstrated special strength in distinguishing author name instances that belong to different authors and in producing almost perfect clustering precision (AUT-ORC and AUTNIH). In addition, Author-ity2009 performed well in finding name instances of unique authors, producing very high clustering recall (> 0.96; AUT-ORC and AUT-NIH) and classification accuracy (= 98.06%; AUT-SCT) scores. Note that the Author-ty2009 is by design aimed to disambiguate with high precision because incorrectly matched name instances (merged author identities created by false positives) are more harmful than wrongly mismatched ones (split author identities created by false negatives) for bibliometric analyses (Fegley & Torvik, 2013; Liu et al., 2014; Torvik & Smalheiser, 2009). The evaluation results described so far strongly suggest that Author-ity2009 achieved its stated precision-over-recall goals. Using the name instances stratified into different ethnic groups, the three labeled datasets discussed here provide a deeper understanding of Author-ity2009’s disambiguation performance. Author-ity2009 achieved high precision regardless of ethnic name types (AUT-ORC and AUT-NIH). But its recall was relatively weak in disambiguating some ethnic names, when compared with baseline performances (AUTORC, AUT-NIH, and AUT-SCT), suggesting possibilities to improve the algorithm."
2102.03237,dataset,303,2022-05-16,0,"Another benchmark labeled dataset is a list of name instance pairs that represent self-citation relations. This self-citation information has been used in several studies to develop and test automatic labeling methods (e.g., J. Kim, 2018; J. Kim et al., 2019; Liu et al., 2014; Schulz, Mazloumian, Petersen, Penner, & Helbing, 2014; Torvik & Smalheiser, 2009). This labeling method is based on the assumption that if a paper cites another and they have the same author names, those names refer to the same author. To generate a list of citing references for a paper, reference lists of papers in MELDINE are connected to their cited papers via matching PMIDs. Then, author names in a cited paper are compared to those in citing papers. Following the common practice using this labeling method, if two name instances in cited and citing papers each match on the full surname and the first forename initial, we treat them as instances of the same author. More than 6.2M self-citation pairs are detected in Author-ity2009. To be comparable to AUT-ORC and AUT-NIH, each name instance in a self-citation pair is assigned an ethnicity and a gender, too. Table 3 characterizes the sources of record linkage and labeling methods of the three labeled datasets – AUT-ORC, AUT-NIH, and AUT-SCT – and presents the numbers of labeled instances and unique authors in each dataset. Note that the number of unique authors is unavailable for AUT-SCT because only name instances that have self-citation relationships can be labelled. It is thus impossible to know from this dataset alone whether name instances without self-citation refer to the same author.14."
2102.03237,dataset,327,2022-05-16,0,"To evaluate the disambiguation performances of Author-ity2009, author name instances disambiguated by Author-ity2009 need to be labeled. This study attempts to link ORCID ids to 40M author name instances that appear in about 9M papers published between 1991 and 2009 in Author-ity2009. Author-ity2009 disambiguates author name instances in MEDLINE but does not provide their raw name strings. So, this study proceeds from the whole MEDLINE corpus (2016 baseline version) retrieved from the National Library of Medicine repository7. We select MEDLINE records for papers published between 1991 and 2009 (MEDLINE2009) to align with the publication year range of Author-ity2009. Next, name instances in MEDLINE2009 are compared to the author profiles in ORICD. For this MEDLINE2009-ORCID linkage, a 2018 ORCID release version is used8. To find author name instances recorded in both MEDLINE2009 and ORCID, paper titles with five or more words in MEDLINE2009 are encoded into ASCII format, deprived of non-alphabetical characters, and lowercased. Any duplicate titles after the preprocessing are removed. Then, each title (which is associated with a unique PMID) is compared to the publication lists in ORCID researcher profiles. If a match is found between bibliographic records in MEDLINE2009 and ORCID, author name strings that appear in the matched MEDLINE2009 paper are compared with the name string of the ORCID researcher whose list of publications contains the matched title. If two name strings in MEDLINE2009 and ORCID are matched on the full surname plus the first forename initial, they are assumed to refer to the same author and the ORCID ID of the matched researcher profile is assigned to the name instance in MEDLINE2009. As shown in Figure 1, this matching process produces a labeled dataset, MED-ORC, in which an author name instance in a MEDLINE paper is associated with an ORCID ID."
2102.03237,dataset,39,2022-05-16,0,indicates that AUT-NIH most closely matches Author-ity2009 in terms of the publication year distribution of name instances. The other two labeled datasets over-represent recent years heavily (AUTORC) and slightly (AUT-SCT) relative to Author-ity2009..
2102.03237,dataset,8,2022-05-16,0,5 https://www.nlm.nih.gov/bsd/medline.html 6 https://databank.illinois.edu/datasets/IDB-4222651
2102.03237,dataset,88,2022-05-16,0,"Although three labeled datasets produced similar evaluation results, they had different characteristics. First, AUT-ORC and AUT-NIH were used to evaluate both the precision and the recall of Authority2009’s clustering of name instances that refer to the same unique authors. But AUT-SCT could be used only to evaluate how well Author-ity2009 decided that self-citing name instance pairs refer to the same authors (≈ recall). This means that AUT-SCT could only provide partial evaluation of Author-ity2009’s disambiguation performance."
2102.03237,dataset,9,2022-05-16,1,17 Datasets can be downloaded at https://doi.org/10.6084/m9.figshare.13404986.v1
2102.03237,"dataset, data https",23,2022-05-16,0,"Torvik, V. I., & Smalheiser, N. R. (2018). Author-ity 2009 - PubMed author name disambiguated dataset."
2102.03681,code,124,2022-05-16,0,"Sometimes, Stan is able to produce vectorized code such as in matrix multiplication. This is consistent with our benchmark results since Stan came closest to FastAD for this operation (see Section 5.3). It is also consistent with how it is implemented, since they allocate extra memory for double values for each matrix and the multiplication is carried out with these matrices of primitive types. However, this vectorization does come at a cost of at least 4 times extra memory allocation than what FastAD allocates. Moreover, the backward-evaluation requires heap-allocating a matrix on-the-ﬂy every time. FastAD incurs no such cost, only allocates what is needed, and never heap-allocates during AD evaluation."
2102.03681,code,145,2022-05-16,0,"N values. For a complicated model as such, there are many opportunities for FastAD to cache certain evaluations for constants as mentioned in Section 4.3 and 5.4. In particular, the exponential function eh reuses its forward-evaluated result, and many log-pdfs cache the log of its parameters such as log(σ) in the Normal log-pdfs and log(γ) in the Cauchy log-pdfs (σ, γ are the second parameters of their respective distributions, which are constant in this model). Note that this caching is automatically done in FastAD, which would be tedious to manually code for the baseline. Hence, this shows that due to automatic caching, FastAD forward and backward-evaluation combined can be faster than a manually-written forward evaluation only, which puts FastAD at an optimal performance."
2102.03681,code,167,2022-05-16,0,"One example is choosing the correct specialization of an operation depending on the shapes of the input. As seen in Section 4.1, all nodes are given a shape trait. Depending on the input shapes, one may need to invoke diﬀerent routines for the same node. For example, the normal log-pdf node behaves quite diﬀerently depending on whether the variance parameter is a scalar σ2 or a (covariance) matrix Σ. Namely, if the variance has a matrix shape, we must perform a matrix inverse to compute the log-pdf, which requires a diﬀerent code from the scalar case. Using a C++ design pattern called Substitution-Failure-Is-Not-An-Error (SFINAE), we can choose the correct routine at compile-time. The beneﬁt is that there is no time spent during run-time in choosing the routine anymore, whereas in libraries like CppAD, they choose the routines at run-time for every evaluation of the node [2]."
2102.03681,code,41,2022-05-16,0,"Fig. 4: Sum and product benchmarks of other libraries against FastAD plotted relative to FastAD average time. Fig. 4a,4c use built-in functions whenever available. Fig. 4b,4d use the naive iterative-based code for all libraries."
2102.03681,code,47,2022-05-16,0,"comments on the ﬁrst draft and for taking the time to optimize the benchmark code for their respective libraries. We also thank Art Owen and our colleagues Kevin Guo, Dean Deng, and John Cherian for useful feedback and corrections on the ﬁrst draft."
2102.03681,code,60,2022-05-16,0,"This example really highlights the beneﬁts of vectorization. As noted in Section 4.1, this was the one benchmark example where Stan was able to produce vectorized code, which is consistent with Figure 6 that Stan is the only library that has the same order of magnitude as FastAD. Other libraries did not produce vectorized code."
2102.03681,code,71,2022-05-16,0,"If we assume that the most expensive operation is the matrix multiplication, AD evaluation approximately takes two matrix multiplications between a matrix and a vector. We can then approximate a lower bound for the manually-written gradient computation time to be two times that of the baseline. The relative time of FastAD to this approximated time is 1.1, implying about 10% overhead from a manually-written code."
2102.03681,code,73,2022-05-16,0,"Hence, in total, one AD evaluation requires three matrix multiplications between two K ×K matrices. If we approximate a manually-written gradient computation to take three times as long as the baseline (one multiplication), FastAD time relative to this approximated time is 3.27 3 = 1.09. This shows then that FastAD only has about 9% overhead from a manually-written code, which is extremely optimal."
2102.03681,"data, code",208,2022-05-16,0,"Vectorization refers to the parallelization of operations on multiple data at the hardware level. On a modern Intel 64-bit processor supporting AVX, four doubleprecision ﬂoating point numbers can be processed simultaneously, roughly improving performance by a factor of four. While the compiler optimization is able to vectorize a user’s code sometimes, it is not guaranteed because vectorization requirements are quite stringent. For example, vectorization is not guaranteed if memory access is not done in a contiguous fashion and is impossible if there is any dependency between loop iterations. This makes it quite challenging to design an AD system that can always predict compiler optimization to create vectorized code. However, vectorization can make AD extremely fast, powerful, and practical even in complex problems. In practice, we come across many examples where operations can be vectorized during gradient computation. For example, matrix multiplication, any reduction from a multi-dimensional variable to a scalar such as summation or product of all elements, and any unary and binary function that is applied element-wise such as exponential, logarithm, power, sin, cos, tan, and the usual arithmetic operators."
2102.03681,github,7,2022-05-16,2,1 github page: https://github.com/JamesYang007/FastAD
2102.03681,github,7,2022-05-16,1,2 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,github,7,2022-05-16,1,3 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,package,18,2022-05-16,0,"2. Bell, B.: Cppad: a package for c++ algorithmic diﬀerentiation http://www.coin-or."
2102.03681,package,56,2022-05-16,0,"6. Griewank, A., Juedes, D., Utke, J.: Algorithm 755: Adol-c: A package for the automatic diﬀerentiation of algorithms written in c/c++. ACM Trans. Math. Softw. 22(2), 131–167 (Jun 1996). https://doi.org/10.1145/229473.229474, https: //doi.org/10.1145/229473.229474"
2103.02044,code,9,2022-05-16,0,Code availability (software application or custom code)
2103.02044,"data, data available",16,2022-05-16,0,Our future plan is to deploy a federated XNAT portal to collect preclinical imaging data from
2103.02044,"data, data available",17,2022-05-16,0,"Identifier is missing, preventing the data to be found by both humans and computers. In"
2103.02044,"data, database",18,2022-05-16,0,"images from several sources, to save data in a safe database, and to share data among"
2103.02044,"data, database",37,2022-05-16,0,"I. B. Ozyurt et al., “Federated web-accessible clinical data management within an extensible neuroimaging database,” Neuroinformatics, vol. 8, no. 4, pp. 231–249, Dec. 2010."
2103.02044,"data, dataset",11,2022-05-16,0,possibility to export proprietary raw image datasets to internationally recognized data
2103.02044,"data, dataset",17,2022-05-16,0,"process imaging datasets. It offers a platform to store and distribute the data, manage the"
2103.02044,"data, dataset provided",11,2022-05-16,0,data. Commercial softwares distributed by imaging device manufacturers provide the
2103.02044,"data, dataset provided",18,2022-05-16,0,"(Figure 3). Notably, XNAT-PIC users can adjust the custom variables accordingly to their data"
2103.02044,"data, open-source",17,2022-05-16,2,"data, all within an XNAT environment. All the developments are free and open-source. We"
2103.02044,"data, publicly available",15,2022-05-16,0,studies and the relative costs are prompting imaging scientists to share image data in public
2103.02044,"data, publicly available",38,2022-05-16,0,"J. Ellenberg et al., “A call for public archives for biological image data,” Nature Methods, vol. 15, no. 11. Nature Publishing Group, pp. 849–854, 01-Nov-2018."
2103.02044,"data, publicly available, data repository",14,2022-05-16,0,principles whose goal is to provide a public data repository compliant to open science
2103.02044,database,10,2022-05-16,0,eventually uploaded to the database as project level resources.
2103.02044,database,11,2022-05-16,0,"studies, the Medical Imaging Research Management and Associated Information Database"
2103.02044,database,112,2022-05-16,0,"Abbreviations ADNI AIM API CEST CIM COINS CT DICOM DICOM SEG DICOM Segmentation Digital Object Identifier DOI Decimal String DS Diffusion Weighted Imaging DWI European Open Science Cloud  EOSC European Population Imaging Infrastructure  EPI2 Findable, Accessible, Interoperable and Reusable  FAIR 18-fluorodeoxyglucose  FDG France Life Imaging FLI Human Imaging Database  HID Hypertext Transfer Protocol  HTTP Long String LO Longitudinal Online Research and Imaging System  LORIS Medical Database Multi-Modal Molecular Imaging Italian Node  Magnetic Resonance Imaging  Neuroimaging Informatics Technology Initiative Open Access Series of Imaging Studies  Optical Coherence Tomography  Open Health Imaging Foundation  Optical Imaging  Other Picture Archiving and Communication System  Photoacoustic Imaging  Positron Emission Tomography Representational State Transfer  Region of Interest"
2103.02044,database,13,2022-05-16,0,Human Imaging Database (HID) and the Collaborative Informatics and Neuroimaging Suite
2103.02044,database,13,2022-05-16,0,"based web application, exploiting the PostgreSQL database system. Users can personalize"
2103.02044,database,19,2022-05-16,0,the ROI and uploads the results back to the database as XNAT resources (Figure 8). A
2103.02044,database,20,2022-05-16,0,"facilitates scripting interactions with the XNAT database [41], [42], iii) Requests 2.23.0 that"
2103.02044,database,3,2022-04-21,0,"Science, Database"
2103.02044,dataset,12,2022-05-16,0,The processing of large volumes of biomedical image datasets requires dedicated platforms
2103.02044,dataset,12,2022-05-16,0,processing preclinical image datasets. MRI2DICOM is a MR image converter from
2103.02044,dataset,13,2022-05-16,0,"discover image datasets normally not accessible, promoting the free exchange and reuse"
2103.02044,dataset,13,2022-05-16,0,"neither tools for importing large, multimodal preclinical image datasets nor pipelines for"
2103.02044,dataset,13,2022-05-16,0,processing large image datasets. This workflow is based on the steps schematically
2103.02044,dataset,15,2022-05-16,0,"2. XNAT-PIC Uploader to upload large, multimodal image datasets in DICOM standard to"
2103.02044,dataset,15,2022-05-16,1,"The datasets analyzed in the current study are openly available in the CIM-XNAT repository,"
2103.02044,dataset,15,2022-05-16,0,complexity and the variety of preclinical trial datasets. The time needed to perform these
2103.02044,dataset,15,2022-05-16,0,dynamically added to the ‘standard’ DICOM dictionary to describe CEST-MRI datasets. The
2103.02044,dataset,15,2022-05-16,0,"heterogeneous datasets,” Front. Neuroinform., vol. 5, Dec. 2011."
2103.02044,dataset,15,2022-05-16,0,once the dataset is successfully imported to XNAT. The original raw images can be
2103.02044,dataset,16,2022-05-16,0,"Upon conversion to DICOM format, the image dataset can be uploaded to XNAT. XNAT-PIC"
2103.02044,dataset,16,2022-05-16,0,associate the dataset stored in XNAT with a Digital Object Identifier (DOI) or Persistent
2103.02044,dataset,16,2022-05-16,0,needs to process large scale image datasets. The urgency was therefore to scale up this
2103.02044,dataset,16,2022-05-16,0,"to our CIM-XNAT instance, upload their image datasets and add the pipelines to their own"
2103.02044,dataset,17,2022-05-16,0,"datasets need to be established. In this paper, we present an extension of XNAT for"
2103.02044,dataset,18,2022-05-16,0,"as imaging dataset, as well as the tools, workflows, and pipelines needed to process the"
2103.02044,dataset,19,2022-05-16,0,"image datasets of different modalities to XNAT such as MRI, PET, CT, and US, allowing"
2103.02044,dataset,26,2022-05-16,0,"several image repositories have emerged enabling the discovery of datasets from peer reviewed publications or research studies in the life science domain, from biological imaging"
2103.02044,dataset,30,2022-05-16,0,Figure 1: Schematic workflow of image archiving and processing. XNAT-PIC is a suite of tools aimed at facilitating the management and the analysis of preclinical image datasets.
2103.02044,"dataset, publicly available",17,2022-05-16,0,"[21] P. Kalendralis et al., “Multicenter CT phantoms public dataset for radiomics reproducibility"
2103.02044,download,14,2022-05-16,0,"in the resource folder, download the morphological image in DICOM standard and the"
2103.02044,download,14,2022-05-16,0,"parametric image in NIfTI format corresponding to the user selection, download the ROI"
2103.02044,download,16,2022-05-16,0,"2. For each subject, create a local folder and then download the corresponding DICOM"
2103.02044,download,16,2022-05-16,0,"contains XML instructions to take this user input, create the working directory, download the"
2103.02044,download,23,2022-05-16,0,projects (See: Adding Pipelines To Your Project: https://wiki.xnat.org/documentation/how to-use-xnat/adding-pipelines-to-your-project); ii) XNAT Admins can download XNAT-PIC
2103.02044,download,4,2022-05-16,0,available for download.
2103.02044,github,10,2022-05-16,0,"pipelines from https://github.com/szullino/XNAT-PIC-Pipelines, install and register the"
2103.02044,github,14,2022-05-16,0,https://github.com/szullino/XNAT-PIC [31]. The application uses the numpy 1.15.4 and
2103.02044,github,3,2022-05-16,0,https://github.com/szullino/XNAT-PIC-Pipelines
2103.02044,"github, download",25,2022-05-16,2,Table 2: Processing pipelines currently installed on our CIM-XNAT instance (http://cim-xnat.unito.it) and available for download at https://github.com/szullino/XNAT-PIC-Pipelines.
2103.02044,"github, download",51,2022-05-16,2,The latest releases of the source codes of XNAT-PIC are available to download from the and GitHub https://github.com/szullino/XNAT-PIC. XNAT-PIC is a free software and is distributed under the terms of the GNU General Public License v3 or any later version as stated by the Free Software Foundation.
2103.02044,open-source,13,2022-05-16,0,"available micro-services in SAS comprise Dicomifier, a generic and open-source Bruker to"
2103.02044,open-source,14,2022-05-16,1,"In this work we have developed XNAT-PIC, a free and open-source application consisting"
2103.02044,open-source,15,2022-05-16,0,"Preclinical Imaging Centers (XNAT-PIC). XNAT is a worldwide used, open-source platform"
2103.02044,open-source,15,2022-05-16,0,preclinical imaging available at http://cim-xnat.unito.it. XNAT is a free and open-source Java
2103.02044,open-source,22,2022-05-16,0,"[34] D. Mason, “SU‐E‐T‐33: Pydicom: An Open Source DICOM Library,” in Medical Physics,"
2103.02044,open-source,43,2022-05-16,0,"[54] T. Urban et al., “LesionTracker: Extensible open-source zero-footprint web viewer for cancer imaging research and clinical trials,” Cancer Res., vol. 77, no. 21, pp. e119–e122, Nov. 2017."
2103.02044,package,14,2022-05-16,0,"package to run MRI2DICOM and XNAT-PIC Uploader as a stand-alone executable, in both"
2103.02044,"publicly available, github",16,2022-05-16,0,GNU General Public License v3 or any later version and available on GitHub. Some work
2103.02044,python,14,2022-05-16,0,XNAT-PIC Uploader is built in Python 3.7.6. The communication with XNAT is possible
2103.02044,python,14,2022-05-16,0,descriptor invokes a bash script running a Python wrapper consisting of a sequence of
2103.02044,python,14,2022-05-16,0,script passes the resulted mask to a Python script that computes statistical calculations in
2103.02044,python,15,2022-05-16,0,to process multiple subjects within the same project. A Python 2.7 virtual environment has
2103.02044,python,15,2022-05-16,0,“Python Software Foundation.” [Online]. Available: https://www.python.org/.
2103.02044,python,16,2022-05-16,0,3.5 has been used to bundle the Python applications and all its dependencies into a single
2103.02044,python,16,2022-05-16,0,pipeline runs on Python 3.8.3 and uses the following libraries: numpy 1.18.5 [44]
2103.02044,python,18,2022-05-16,0,"for Python to run MATLAB scripts within a Python session [40], ii) pyxnat-1.2.1.0.post3 that"
2103.02044,python,19,2022-05-16,0,"acqp) into Python dictionaries [36]. Lastly, it saves all the relevant information into the"
2103.02044,python,19,2022-05-16,0,"mask [45], the image processing library opencv-python 4.4.0.40 [46], and nibabel 3.1.1 for"
2103.02044,python,20,2022-05-16,0,"Python library to encrypt the files containing the XNAT login credentials [37], [38]. PyInstaller"
2103.02044,python,21,2022-05-16,0,“opencv-python · PyPI.” [Online]. Available: https://pypi.org/project/opencv-python/4.4.0.40/. [Accessed: 18-Feb-2021].
2103.02044,python,30,2022-05-16,0,"[41] Y. Schwartz et al., “PyXNAT: XNAT in Python,” Front. Neuroinform., vol. 6, p. 12, May 2012."
2103.02044,python,30,2022-05-16,0,“Get Started with MATLAB Engine API for Python - MATLAB & Simulink.” [Online]. Available: https://www.mathworks.com/help/matlab/matlab_external/get-started-with-matlabengine-for-python.html. [Accessed: 18-Feb-2021].
2103.02044,python,6,2022-05-16,0,Python scripts in order to:
2103.02044,"python, code",28,2022-05-16,0,resource descriptor. The resource descriptor invokes a bash script passing both the T2 weighted DICOM image and the DICOM RT-STRUCT directories to a Python code to be
2103.02044,"python, github",24,2022-05-16,0,"[36] M. Caffini, “Project-Beat--Pyhton.” [Online]. Available: https://github.com/mcaffini/Project Beat---Python."
2103.02044,"python, open-source",13,2022-05-16,0,MRI2DICOM is a free and open-source tool built in Python 3.7.6 downloadable at
2103.02044,"python, open-source",15,2022-05-16,0,"through xnatpy 0.3.22, a new and open-source XNAT Python client, and pyAesCrypt 0.4.3"
2103.02044,"python, package",17,2022-05-16,0,converted into a NIfTI mask by the Python package dcmrtstruct2nii [55]. The same bash
2103.03806,code,154,2022-05-16,0,"In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to ﬁnd automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the ﬁeld of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers’ architecture to automatically detect malicious software. We propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection."
2103.03806,code,56,2022-05-16,0,"2. We model Android malware detection as a binary and a multi-label text classiﬁcation problem and propose a novel feature representation by considering the software applications’ source code as a set of features. We apply text preprocessing on these features to keep the important information like permissions, intents, and activities."
2103.03806,code,59,2022-05-16,0,"While in our work, we propose a noval approach, we used BERT to better detect malware, we reﬁned the pre-trained model to efﬁciently learn representations of source code language syntax and semantics. Our Context-Aware network learns contextual characteristics from a natural language sentences perspective thanks to the attention mechanism layers in the Transformer-based architecture."
2103.03806,code,89,2022-05-16,0,"The wildly used type is static analysis [3]. It is a known way of identifying malicious applications among benign applications, and this analysis focuses on the source code of software components that may be affected by malware. It is less expensive in terms of resources and time since no need to activate the malware by executing the code to capture the features, it can identify the maliciousness at the code level. For static analysis, there are mainly three practices to detect"
2103.03806,"data, dataset",108,2022-05-16,0,"Also, RoBERTa [13] an extension of BERT with some modiﬁcations in the pre-training procedure of the architecture, uses the same architecture. The modiﬁcations include training the model longer, with larger batches, and on a larger amount of data. Also, the predictive objective of the following sentence was deleted. And in order to on longer sequences, the authors applied dynamic modiﬁcation on the masking scheme [12] applied to the training data. They also collect a new dataset of comparable size to other privately used datasets to better control the effects of training set size."
2103.03806,"data, dataset",161,2022-05-16,0,"This paper studies the challenge of malware classiﬁcation using our novel approach for Transformer-based malware detection. We detailed the malware text classiﬁcation methodology and used it for feature representation. The BERT based model achieved high accuracy results for both binary and cross-category classiﬁcation, compared to the other baseline pre-trained language models. The results from the experiments show that the best binary accuracy is 0.9761 and for the multi-classiﬁcation, it is 0.9102. We can conclude that the proposed approach’s results of feature representation as text input for a Transformer-based model, are very good. So, the implementation of pre-trained linguistic models based on Transformer architectures in cybersecurity tasks can outperform standard RNN models like LSTM, when applied on a state-of-the-art dataset like Androzoo. Future work will therefore consist of testing other models, testing other types of data, and creating an API to detect malware in new applications."
2103.03806,"data, dataset",170,2022-05-16,0,"Once the data is created and annotated in the right format, we split the data into train and test. We conduct all our experiments using BERT, we ﬁne-tuned it on our train dataset. We ﬁxed the hyperparameters based on each classiﬁcation type. We train BERT to predict Malware/Benign (i.e., binary classiﬁcation) for each sample, then, to predict the categories of malware (i.e, multi-classiﬁcation). The Transformer architecture has speciﬁc input formatting steps including the creation of special tokens and ids. We use the Transformers implementation of the hugging face library [34] for the binary classiﬁcation of Android applications. Only the Transformer architecture, layers, and weights are implemented, while all data formatting must be done beforehand to be compatible with the Transformer. While most pre-trained Transformers have essentially the same steps. Here we test this approach with BERT. Figure 4 gives a detailed overview of our approach."
2103.03806,"data, dataset, code",145,2022-05-16,0,"In this paper, we propose a malware detection approach using a Transformer-based algorithm. We experiment with different Transformer model architectures on our data. The dataset includes 11 different malware categories namely adware, spyware, ransomware, clicker, dropper, downloader, riskware, SMS-sender, horse-trojan, backdoor, and banker [9]. Our methodology focuses on the static analysis level on the source code of Android applications, to identify different categories of malware. Indeed, we did not limit the features to permission-based only but considered the whole software code as an important set of feature representation for the analysis. We started with training the model with the features after preprocessing, then a binary classiﬁcation of the apps to malicious and benign, and ﬁnally a cross-category classiﬁcation at the malware level."
2103.03806,"data, dataset, code, download",270,2022-05-16,0,"Once the list of APKs is deﬁned, we write a script to download the ﬁles. Then, we decompiled the downloaded APKs using Jdax [32], which creates folders of the apps’ ﬁles [33]. We extracted the AndroidManifest.xml ﬁle from each sample. The manifest presents essential information about the application to the Android system, information the system must have before it can run any of the application’s code, including the list of permissions, the activities, services, broadcast receivers, content providers, the version, and the meta-data. These ﬁles are then treated as text ﬁles and passed through the preprocessing phase, in this step and to conserve the important information about the features, we apply speciﬁc cleaning of the not important, mostly repeated words, in the code. We manually analyzed different examples and created a list of words and expressions that do not provide additional info, so the cleaning included lexicon removal, punctuation removal and we conserved the digits and the cases of the characters. The purpose of the preprocessing is to reduce the size of the input. The ﬁnal dataset format has 4 columns, the ID column, represented by the APK hash name, the Text column representing the Manifest ﬁles after preprocessing, the Label column, a binary format equal to 1 if the app is malware and 0 if not, and ﬁnally the Category column representing the malware type name (exp: adware)."
2103.03806,"data, dataset, data https, download",310,2022-05-16,0,"Based on state-of-the-art taxonomies for Android malware categories [29] & [30] We selected 11 categories 3 namely; adware (displays advertising and entice a user to install it on their device), spyware (installs itself on the user device with the aim of collecting and transferring information without the user is aware of it), ransomware (takes personal data hostage), clicker (a type of trojan that performs a form of ad fraud. These “clickers” continuously make connections to websites, consequently awarding threat actors with revenue on pay-per-click bases), dropper (a syringe program or dropper virus, is a computer program created to install malicious software on a target system), downloader (a type of Trojan horse that downloads and installs malicious ﬁles), riskware (a software whose installation can represent a risk for the security of the computer, but however, not inevitably), SMS-sender (presents itself as a regular SMS messaging application and uses its basic permissions to send/receive short messages), horse-trojan (is designed to damage, disrupt, steal, or in general inﬂict some other harmful action on your data or network), backdoor (when introduced into the device, usually without the user’s knowledge, turns the software into a Trojan horse, and banker (is designed to steal data from users’ online bank accounts as well as data from online payment systems and plastic card systems). We select the list of APKs to download based on the recent creation and analysis date, then re-analyze this list with VirusTotal [31], to ﬁnally create our dataset list including 12,000 benign apps and 10,000 malware apps."
2103.03806,"data, dataset, publicly available",112,2022-05-16,1,"We collected the Android applications from the Androzoo public dataset. Androzoo, one of the stae of the art android malware dataset [27], is a growing collection of Android applications from several sources, including the ofﬁcial Google Play app market. It currently contains 13,320,014 different APKs, each of which has been analyzed by dozens of different antivirus products to ﬁnd out which applications are detected as malware. This public data is up to date with weekly analysis on the samples [28]. The data is labeled based on these analyses into malware and benign, and different malware categories and families."
2103.03806,dataset,145,2022-05-16,0,"We conducted the experiments on the preprocessed dataset. Fine-tuning the pre-trained models, clearly gave the highest accuracy results for this classiﬁcation task compared to the LSTM baseline. The best classiﬁcation model is BERT. The test metrics results of Table 1 show that each Transformer learns differently depending on each architecture. The results in Table 1 and Table 2 prove that BERT outperformed the other baseline models in both binary and multi-classiﬁcation malware detection. For BERT, the best learning rate shows that only two epochs are required before the loss starts to increase. Our ﬁne-tuning with the training set included changing the hyperparameters to boost the results. To evaluate the ﬁnal results, we used different evaluation metrics. The pretrained models achieved good results overall, but BERT obtained the best performance in both tasks."
2103.03806,dataset,22,2022-05-16,0,Table 1: Detection results using the feature representation approach across difference networks on the test dataset for both binary classiﬁcation.
2103.03806,dataset,22,2022-05-16,0,Table 2: Detection results using the feature representation approach across difference networks on the test dataset for cross-category malware classiﬁcation.
2103.03806,dataset,44,2022-05-16,0,"applies a CNN with an attention mechanism to images converted from binary datasets, by calculating an attention map to extract characteristic byte sequence. The distinction of regions in the attention map shows regions having higher importance for classiﬁcation in the image."
2103.03806,dataset,55,2022-05-16,0,2. MCC: The Matthews Correlation Coefﬁcient (MCC) is bast used for binary classiﬁcation with an unbalanced dataset. It has a range of -1 to +1. We chose MCC over F1-score for binary classiﬁcation as recommended in this study [36]. MCC equation is deﬁned as fellow :
2103.03806,dataset,55,2022-05-16,0,"To test the proposed approach, we evaluate it in terms of three main aspects: (1) the proﬁtability on large and recent categorical datasets, (2) the feature representation ability for information context extraction from android apps, and (3) the performance compared to the state-of-the-art approaches."
2103.03806,"dataset, code",118,2022-05-16,0,"This section explains the overall process of malware detection. The core idea of this work is to create, a malware detection framework using a Transformer-based approach. To reach this goal, we conducted a static analysis on the collected corpora from a natural language sentences perspective. So, we need a dataset including source code ﬁles and different categories of malware types. Figure 2 explains the logical ﬂow of our Android malware detection. This Process is mainly divided into 4 phases. First, the Android ﬁles collection, then the Decompilation phase of the APK ﬁles, Feature Mining, and ﬁnally Deep Learning (DL) models training experiments."
2103.03806,"dataset, code, package",277,2022-05-16,0,"Among these approaches this study [16] builds AMalNet, a DN framework to learn multiple integration representations and family assignment with Graph CNN (GCNs) to model high-level graph semantics and use an Independent RNN (IndRNN) to decode deep semantic information. SeqMobile [17], is a behavior-based sequence approach. it uses different recurrent neural networks (RNN). It extracts the semantic feature sequence, which can provide information of certain malicious behaviors, from binary ﬁles under a certain time constraint. This paper [18], presents a new approach based on OpCode-level FCG. The FCG is obtained through static analysis of Operation Code (OpCode) using a Long Short-Term Memory (LSTM). the authors conduct experiments on a dataset on 1,796 Android malware samples classiﬁed into two categories and 1,000 benign Android apps. The authors of [19] focused on step size as an important factor in relation to input size using RNN. They tested the model with three different feature vectors (hot-coding feature vector, random feature vector and Word2Vec feature vector) using hyper parameters. [20] transform the android package kit (APK) ﬁle into a lightweight RGB image using a predeﬁned dictionary and intelligent mapping, then apply a CNN on the obtained images for malware family classiﬁcation. Multiple other examples of DNN based approach [21] and [22], have been developed, with varying the feature extraction, selection, and representation methods in the aim of boosting the detection results."
2103.03806,"dataset, publicly available",32,2022-05-16,1,3. We conduct extensive experiments on our preprocessed Android dataset collected from public resources with different category-annotated labels. This preprocessed dataset will be released publicly for the research community.
2103.03806,github,68,2022-05-16,0,"[31] Virus Total. Virustotal-free online virus, malware and url scanner. Online: https://www. virustotal. com/en, 2012. [32] jadx. jadx - dex to java decompiler. Online: https://github.com/skylot/jadx, 2012. [33] Nicolas Harrand, César Soto-Valero, Martin Monperrus, and Benoit Baudry. Java decompiler diversity and its"
2103.03806,open-source,49,2022-05-16,0,"[28] Pei Liu, Li Li, Yanjie Zhao, Xiaoyu Sun, and John Grundy. Androzooopen: Collecting large-scale open source android apps for the research community. In Proceedings of the 17th International Conference on Mining Software Repositories, pages 548–552, 2020."
2103.03968,database,210,2022-05-17,1,"We ﬁrst applied our algorithm on scans simulated from a brain phantom, which we obtained from the BrainWeb database [3]. We simulated nθ projections from this phantom, for two values of nθ = 1440 and 960. For each nθ, we ﬁrst reconstructed the image of the phantom from the full set of nθ projections and from nθ/2 projections; we denote these images with xnθ and xnθ/2, respectively. We then applied the proposed algorithm and the dictionary-based interpolation algorithm to interpolate the subset of nθ/2 projections to generate nθ projections and reconstructed the image of the phantom from the interpolated projections. We will denote these images with xproposed nθ/2. We simulated two levels of noise in the projections with diﬀerent number of incident photons: N0 = 106 and N0 = 5 × 104. We will refer to these simulations as low-noise and high-noise, respectively. For both simulations, we assumed the detector electronic noise to be additive Gaussian with a standard deviation of 40. As the reference scan that we need for block matching for computation of Rs, we used the simulated scan of a diﬀerent brain phantom from the same database."
2103.03968,database,62,2022-05-17,0,"3. Cocosco, C.A., Kollokian, V., Kwan, R.K.S., Pike, G.B., Evans, A.C.: Brainweb: Online interface to a 3d mri simulated brain database. In: NeuroImage (1997) 4. Feldkamp, L.A., Davis, L.C., Kress, J.W.: Practical cone-beam algorithm. J. Opt."
2103.06238,"data, code",165,2022-05-17,0,"The study uses a Modified Iterative Development as a guide for the design and development of the VR 3D Model and its environment. Iterative development is a procedure on which there is a breaking down of the computer program advancement of a huge application into little portions. In the iterative development process, the highlighted code is planned, created, and tried in rehashed cycles. In each repetition, there is re-designing of additional features, developed and verified until it is ready to be deployed or installed. Since this project used Modified Iterative Development, there are some changes of the iteration on the stages to ensure that the program follows the desired outcomes set by the developers. Figure 1 illustrates the Modified Iterative Development to produce an increment project that is being developed. The process starts with the Planning, Data Gathering and Data Analysis, System Requirements, Designing, Testing, and Evaluation."
2103.12883,github,3,2022-05-17,2,1https://github.com/ricardoGrando/hydrone_deep_rl_icra
2103.12883,package,51,2022-05-17,0,"[31] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “Uuv simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in OCEANS MTS/IEEE Monterey. IEEE, 2016, pp. 1–8."
2103.12883,package,55,2022-05-17,0,"• We demonstrate that, with our approaches, the robot is capable to arrive at the desired target avoiding collisions. However, with a geometrically dependent tracking controller, the robot is unable to bypass the drilling risers. We also provide a completely built ROS package with a real-world described HUAUV."
2103.12883,python,63,2022-05-17,0,"The whole system was implemented using ROS and Gazebo frameworks. The Deep-RL approaches were implemented using Python programming language, while the vehicles’ related plugins were partially implemented in C++ and Python. The implementation of the neural networks was carried out with the PyTorch2 library. The performance of our approaches can also be observed in a complementary video3."
2103.12895,"code, package, python, data, dataset, dataset provided",151,2022-05-17,0,"the loading end), each named accordingly (front, middle, rear) in the dataset. Hence, there were 9 loggers in total for each instrumented shipment labeled with respect to the loggers’ location in the pallet and the pallets’ location in the container (front−top:FT, front−middle:FM, front−bottom:FB, ..., rear−bottom:RB). Figure 3 displays each sensor proﬁle separately for each of the 6 shipments. Please observe that these ﬁgures display real−world, noisy and complex multivariate time series as signature representatives of each shipment. Summary statistics for all variables from the shipments datasets are presented in Table 4. These statistics were obtained using the ‘Pandas’ Python package [8]. Python Code used for ﬁltering and analyzing these data will be provided based on request."
2103.12895,"data, data https",22,2022-05-17,0,Please visit the following website for more information about the sensors used in collecting the data: https://www. deltatrak.com/reusable-real-time-loggers
2103.12895,"data, data https, supplementary data, data available",16,2022-05-17,0,Supplementary data associated with this article can be found in the online version at Supplemental ﬁles
2103.12895,"data, dataset",7,2022-05-17,1,Direct URL to data: Dataset link
2103.12895,"data, dataset",78,2022-05-17,0,"• The analysis and processing of temperature time-series data for predictive tasks represent a signiﬁcant challenge especially when proﬁles may have variable lengths, high variability, and abnormalities as is common in many cold chain applications. The dataset will enable researchers from a wide array of ﬁelds and backgrounds to apply analytical tools such as machine learning and physical models in testing and comparing the performance of their predictive or diagnostic algorithms on the cold-chain."
2103.12895,"data, python",89,2022-05-17,0,Instruments: DeltaTrak’s Reusable Real−Time−Logger (RTL) Mini devices are used to log both temperature and location data in real time. The RTLs have a wide operational temperature range of −30◦C to 95.55◦C with a temperature accuracy of +/−1◦C. More information about the hardware used in this study can be found in the appendix. Data was extracted via the cloud application which can establish secure communications with the GSM loggers. Python [4] was employed to perform subsequent data analysis.
2103.12895,dataset,13,2022-05-17,0,A TIME-TEMPERATURE DATASET FOR THE STRAWBERRY COLD CHAIN ACROSS MULTIPLE SHIPMENTS AND LOCATIONS
2103.12895,dataset,196,2022-05-17,1,"Monitoring and controlling the refrigeration of food during the cold-chain (transportation, storage, and distribution of perishable food items) are critical to reducing the amount of food waste.However, the cost of installation of the monitoring devices such as wireless sensor networks (WSNs) and radio frequency identiﬁcation (RFID) systems limits monitoring resolution in commercial applications generally to one per container [[2], [3]]. Hence, with signiﬁcant collaboration between the stakeholders from growers to distributors to retailers to academics, six strawberry shipments across the continental United States datasets are shared to help in overcoming this limitation. The datasets now made available were collected aiming at understanding the holistic temperature behavior of the strawberry cold-chain and the development of prediction models to predict the future behavior of the strawberries during transportation from harvest to the DC. Nevertheless, due to the temporal heterogeneity, complexity, similarity, and discrepancy characteristics of the variables included in these datasets, their use goes beyond this future prediction problem to location-based-prediction, binary control criteria, classiﬁcation, clustering, etc."
2103.12895,python,22,2022-05-17,0,"[4] Guido Van Rossum and Fred L Drake Jr. Python tutorial. Centrum voor Wiskunde en Informatica Amsterdam, The"
2103.13219,"data, dataset",108,2022-05-17,0,"These music excerpts were derived from pieces by 84 different composers from Baroque to the Modern period. Their durations distribute between 0.3 and 2.3 seconds. To prepare ﬁxed-length data for training, excerpts that are shorter or longer than 2 seconds were repeated or trimmed to create a 2-second excerpt. Considering the large size of our dataset, we randomly took a thousand samples from the excerpts of each composer. In total, 62424 excerpts form a smaller dataset3. This also helps to compare convnet of different architectures in a more efﬁcient way, since the training time can be signiﬁcantly reduced."
2103.13219,"data, dataset",135,2022-05-17,0,"For the target task, the dataset consists of ten well known passages of Chopin’s piano music. A pianist was asked to perform the passages using a Yamaha baby grand piano situated in the MAT studios at Queen Mary University of London. The audio were recorded at 44.1 kHz and 24 bits using the spaced-pair stereo microphone technique with a pair of Earthworks QTC40 omnidirectional condenser microphones positioned about 50 cm above the strings. The positions were kept constant during the recording. Meanwhile, movement of the sustain pedal was recorded along with the audio with the help of the measurement system proposed in [4]. The audio data were annotated with frame-wise on or off labels as the ground truth, representing whether the sustain pedal"
2103.13219,"data, dataset",262,2022-05-17,0,"Abstract—Detecting piano pedalling techniques in polyphonic music remains a challenging task in music information retrieval. While other piano-related tasks, such as pitch estimation and onset detection, have seen improvement through applying deep learning methods, little work has been done to develop deep learning models to detect playing techniques. In this paper, we propose a transfer learning approach for the detection of sustainpedal techniques, which are commonly used by pianists to enrich the sound. In the source task, a convolutional neural network (CNN) is trained for learning spectral and temporal contexts when the sustain pedal is pressed using a large dataset generated by a physical modelling virtual instrument. The CNN is designed and experimented through exploiting the knowledge of piano acoustics and physics. This can achieve an accuracy score of 0.98 in the validation results. In the target task, the knowledge learned from the synthesised data can be transferred to detect the sustain pedal in acoustic piano recordings. A concatenated feature vector using the activations of the trained convolutional layers is extracted from the recordings and classiﬁed into frame-wise pedal press or release. We demonstrate the effectiveness of our method in acoustic piano recordings of Chopin’s music. From the crossvalidation results, the proposed transfer learning method achieves an average F-measure of 0.89 and an overall performance of 0.84 obtained using the micro-averaged F-measure. These results outperform applying the pre-trained CNN model directly or the model with a ﬁne-tuned last layer."
2103.13219,"data, dataset, publicly available",279,2022-05-17,1,"input.midpedal.wavno-pedal.wavexcerpts in pairsconvnetaudio recording.wavresultsconvnet featuresSVM classiﬁertransferlearningtrainsource tasktarget taskIII. DATASET For the source task, pedal and no-pedal versions of music excerpts are required to train a convnet, which is able to highlight the spectral or temporal characteristics that change with the sustain pedal instead of note events. For this reason, 1392 MIDI ﬁles publicly available from the Minnesota International Piano-e-Competition website1 were downloaded. They were recorded using a Yamaha Disklavier piano from the performance of skilled competitors. To render these MIDI ﬁles into high quality audio, the Pianoteq 6 PRO2 software was used. This physically modelled virtual instrument approved by Steinway & Sons can export audio using models of different instruments and recording conditions. We employed the Steinway Model D grand piano instrument and the closemiking recording mode. Audio with or without sustain-pedal effect was then generated with a sampling rate of 44.1 kHz and a resolution of 24 bits. These were rendered while preserving or removing the sustain-pedal message in the MIDI data. For each pedal-version audio, we can obtain the temporal regions when the sustain pedal is on or off by thresholding the MIDI message at 64 given its range of [0,127]. A pedalled segment is determined to start at a pedal onset (where the pedal state changes from off to on) and ﬁnish when the state returns to off. We can clip all the pedalled segments to form the pedal excerpts. The start and end times of the pedalled segments were also used to obtain no-pedal excerpts from the corresponding no-pedal-version of the audio."
2103.13219,database,64,2022-05-17,0,The rest of this paper is organised as follows. We ﬁrst introduce related works in Section II. The process of database construction is described in Section III. The methods of sustain-pedal detection including convnet design and transfer learning are discussed in Section IV. Experiments and results are presented in Section V. We ﬁnally conclude our work in Section VI.
2103.13219,dataset,43,2022-05-17,0,"2) A transfer learning method that allows the convnet trained from the source task to be adapted to the target task, where the recording instruments and room acoustics are different. This also allows effective learning with a smaller dataset."
2103.13219,"dataset, used dataset",321,2022-05-17,0,"In this paper, we focus on detecting the technique of the sustain pedal, which is the most frequently used one among the three standard piano pedals. All dampers are lifted off the strings when the sustain pedal is pressed. This mechanism helps to sustain the current sounding notes and allows strings associated to other notes to vibrate due to coupling via the bridge. A phenomenon known as sympathetic resonance [1] is thereby enhanced and embraced by pianists to create a “dreamy” sound effect. We can observe how the phenomenon reﬂects on the melspectrogram in Figure 1, where note F4 is played without (ﬁrst) and with (second) the sustain pedal in two bars respectively. Note that the symbol under the second bar of the music score in Figure 1 can be used to indicate the sustain-pedal techniques. Yet, even if pedal notations are provided, pedalling in the same piano passage can be executed in many different ways. Playing techniques are typically adjusted to the performer’s sense of tempo, dynamics, as well as the location where the performance takes place [2]. Given that detecting pedalling nuances from the audio signal alone is a rather challenging task [3], several measurement systems have been developed to capture the pedal movement. For instance, the Yamaha Disklavier piano can encode this movement into MIDI messages (0-127) along with note events. A dedicated system proposed in [4] enables synchronously recording the pedalling gestures and the piano sound. This can be deployed on common acoustic pianos, and it is used to provide the ground truth dataset introduced in Section III. Detection of pedalling techniques from audio recordings is necessary in the cases where installing sensors on the piano is"
2103.13219,python,46,2022-05-17,0,"[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel et al., “Scikit-learn: Machine learning in python,” Journal of machine learning research, vol. 12, no. Oct, pp. 2825–2830, 2011."
2104.00622,"data, dataset",106,2022-05-17,0,"and qualitative comparison on different training data respectively. We can see that training the model on both datasets can get best results. Ray Pooling. Table 12 shows that argmax performs consistently better than weighted sum on all types of testing data. Figure 15 also shows that argmax can better estimate missing depth of transparent objects on real images. Candidate points selection. Table 13 shows that directly learning offsets of candidate points is better than sampling points heuristically. Figure 16 further provides some examples on real images, showing that learning offset is more robust to strong background textures."
2104.00622,"data, dataset",226,2022-05-17,1,"We compare our approach to several state-of-the-art methods in Table 1. For fair comparison, we evaluate all related works using their released checkpoints (denoted by method name) as well as retraining on our data (denoted by method name with subscript ours. All baselines are trained on both datasets together which is the same setting as our proposed method). RGBD-FCNours is a strong baseline proposed by ourselves. It directly regresses depth maps using fully convolutional networks from RGB-D images. We use Resnet34-8s [51] as the network architecture and train the network on our data. NLSPN [38] is the state-ofthe-art method for depth completion on NYUV2 [48] and KITTI [49] dataset. Cleargrasp [46] is the state-of-the-art method for depth completion of transparent objects. For our approach, we use the best model: LIDF plus the depth reﬁnement model. Our method achieves the best result on all datasets even when baseline methods are trained on the same data. It also shows that training on Omniverse Object dataset can boost the performance of baseline methods. In Figure 6, we provide qualitative comparison by rendering point cloud in a novel view. Our approach can generate more meaningful depth than baseline methods."
2104.00622,"data, dataset",344,2022-05-17,1,"Function (LIDF) deﬁned on ray-voxel pairs consisting of camera rays and their intersecting voxels. The motivations for LIDF are: 1) The depth of a transparent object can be inferred from its color and the depth of its non-transparent In particular, color can provide useful vineighborhood. sual cues for the 3D shape and curvature while local depth helps to reason about the spatial arrangement and location of transparent objects. 2) Directly regressing the complete depth map using a deep network can easily overﬁt to the objects and scenes in the training data. By learning at the local scale (a voxel in our case) instead of the whole scene, LIDF can generalize to unseen objects because different objects may share similar local structures. 3) Voxel grids provide a natural partition of the 3D space. By deﬁning implicit function on ray-voxel pairs, we can signiﬁcantly reduce the inference time as the model only needs to consider occupied voxels intersected by the camera ray. Based on these motivations, we present a model to estimate the depth of a pixel by learning the relationship between the camera ray and its intersecting voxels given the color and local depth information. To further utilize the geometry of transparent object itself, we propose a depth reﬁnement model to update the prediction iteratively by combining the input RGB, input depth points and the predicted depth from LIDF. To train the whole pipeline, we create a large scale synthetic dataset, Omiverse Object dataset, using the NVIDIA Omniverse platform [2]. Our dataset provides over 60,000 images including both transparent and opaque objects in different scenes. The dataset is generated with diverse object models and poses, lighting conditions, camera viewpoints and background textures to close the sim-to-real gap. Experiments show that training on the Omniverse Object dataset can boost the performance for both our approach and competing methods in real-world testing cases."
2104.00622,"data, dataset",52,2022-05-17,1,"Training Data. We analyze the effects of training data in Table 5. We ﬁnd training our method purely on ClearGrasp or Omniverse leads to similar results, but training on both datasets can improve the performance a lot. This indicates that Omniverse dataset can be a good complementary to"
2104.00622,"data, dataset",80,2022-05-17,1,"more details about Omniverse Object dataset. The evaluation is done on the ClearGrasp dataset [46]. It has 4 types of different testing data: Synthetic images of 5 training objects (Syn-known); Synthetic images of 4 novel objects (Syn-novel); Real world images of 5 training objects (Realknown); Real world images of 5 novel objects (Real-novel), 3 of them are not present in synthetic data."
2104.00622,"data, dataset provided",10,2022-05-17,0,Training Data. Table 11 and Figure 14 provide quantitative
2104.00622,"data, dataset, code, data https",193,2022-05-17,3,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a selfcorrecting reﬁnement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs signiﬁcantly better than the current state-of-the-art In addimethods on both synthetic and real world data. tion, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp [46]. Code and dataset will be released at https: //research.nvidia.com/publication/202103_RGB-D-Local-Implicit."
2104.00622,"data, dataset, supplementary data",132,2022-05-17,0,"Datasets Our full pipeline is trained on the ClearGrasp dataset [46] and a new dataset we generated using the Omniverse Platform [2], which we call Omniverse Object dataset. The dataset provides various supervisions for transparent and opaque objects in cluterred scenes. Figure 5 visualizes some examples of the dataset. 3D object models in Omniverse Object dataset are collected from ClearGrasp and ShapeNet [7]. To get natural poses of objects, we use NVIDIA PhysX engine to simulate objects falling to the ground. Then we randomly select some objects and set their materials to the glass. We also augment the data by changing textures for the ground and opaque objects, lighting conditions and camera views. See supplementary for"
2104.00622,"data, dataset, supplementary data",57,2022-05-17,0,"In this section, we ﬁrst evaluate the effect of the depth reﬁnement model. After that, we compare several conﬁgurations for our ﬁrst stage networks. To focus on the generalization ability, we only report quantitative results on ClearGrasp Real-novel dataset. Please refer to the supplementary for results on other testing data."
2104.00622,dataset,10,2022-05-17,0,Figure 5. Examples from our Omniverse Object Dataset.
2104.00622,dataset,108,2022-05-17,0,"We have presented a novel framework for depth completion of transparent objects. Our method consists of a local implicit depth function deﬁned on ray-voxel pairs and an iterative depth reﬁnement model. We also introduce a large scale synthetic dataset for transparent objects learning, which can boost the performance for both our approach and other competing methods. Our pipeline is only trained on synthetic datasets but can generalize well to real world scenarios. We thoroughly evaluated our method compared to prior art and ablation baselines. Both quantitative and qualitative results demonstrate substantial improvements over the state-of-the-art in terms of accuracy and speed."
2104.00622,dataset,141,2022-05-17,0,"Transparent objects. Transparent objects have been studied in various computer vision tasks, including object pose estimation [26, 32, 31, 41, 30], 3D shape reconstruction [3, 20, 42, 28, 46] and segmentation [23]. However, most of these works assume known background patterns [20, 42], known object 3D models [26, 32, 41], or multi view/stereo input [30, 28]. Our approach does not require any priors and can estimate the depth of transparent objects from a single view RGB-D image. Sajjan et al. [46] is the closest work to ours. However, they pretrain their networks on out-ofdomain real datasets while our method is trained purely on"
2104.00622,dataset,309,2022-05-17,0,"Depth estimation. Depth estimation can be classiﬁed into three categories based on the input. Several methods have been proposed to directly regress the depth map from the color image using convolutional neural networks [14, 44, 27, 8, 16, 17, 19]. Most of them are trained on large scale datasets generated from RGB-D cameras, thus they can only reproduce the raw depth scan. Our method, on the contrary, focuses on the depth estimation for transparent objects where depth sensor typically fails. Another line of related work explores the task of depth completion given RGB images and sparse sets of depth measurements [33, 9, 43, 53, 11, 38]. These works improve the depth estimation over color-only methods, but they still produce low quality results because of limited information provided by sparse depth. Our method falls into the third category which tries to complete depth maps given noisy RGB-D images. Barron and Malik [4] propose a joint optimization for intrinsic images. Firman et al. [15] predict unobserved voxels from a single depth image using the voxlet representation. Matsuo and Aoki [34] reconstruct depths by ray-tracing to estimated local tangents. Recent works [56, 46] estimate surface normals and occlusion boundaries only from color images using deep networks and solve a global optimization based on those predictions as well as observed depths. The optimization is very slow and produces bad results if the network predictions are not accurate. We address these limitations by learning a implicit function using color and local depth jointly. Experiment shows that our method can achieve better results and 20× speedup compared to [46]."
2104.00622,dataset,47,2022-05-17,0,"[44] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset IEEE Transactions on Pattern Analysis and Matransfer. chine Intelligence (TPAMI), 2020. 2"
2104.00622,dataset,6,2022-05-17,0,D. Qualitative Results on NYUV2 Dataset
2104.00622,dataset,67,2022-05-17,0,"Figure 18. Qualitative results on NYUV2 dataset. For every example, ﬁrst row from left to right: input RGB, input depth, predicted depth, groundtruth depth; second row from left to right: input point cloud, predicted point cloud, groundtruth point cloud. Point clouds are rendered in a novel viewpoint. Please zoom in to see details."
2104.00622,dataset,73,2022-05-17,1,"We have done experiments on the NYUV2 dataset [48] to evaluate the performance of our method on general scenes and non-transparent objects. We corrupt the depth map by randomly creating some large holes. Our models are trained to predict the complete depth map given the corrupted depth map and RGB image. As shown in Figure 18, our method can predict reasonable missing depth in general scenes."
2104.00622,dataset,99,2022-05-17,0,"Our contributions are summarized as follows: 1) We pro pose LIDF, a novel implicit representation deﬁned on rayvoxel pairs, leading to fast inference speed and good generality. 2) We present a two-stage system, including networks to learn LIDF and a self-correcting reﬁnement model, for depth completion of transparent objects. 3) We build a large scale synthetic dataset proved to be useful to transparent objects learning. 4) Our full pipeline is evaluated qualitatively and quantitatively, and outperform the current state-of-theart in terms of accuracy and speed."
2104.00622,"dataset, dataset provided",115,2022-05-17,0,"In this section, we provide more details about our Omniverse Object Dataset. To generate the dataset, following categories from ShapeNet [7] are chosen: phone, bowl, camera, Following objects from ClearGrasp dataset [46] are chosen: cup-with-waves, ﬂower-bath-bomb, heart-bath-bomb, square-plastic-bottle, stemless-plastic-champagne-glass. Note that we only select training objects from ClearGrasp dataset to make sure testing objects are never seen during training. The background textures are randomly selected from the CC0 TEXTURES Dataset [1]. The textures for opaque objects are randomly selected from CC0 TEXTURES Dataset [1] and Describable Textures Dataset [13]."
2104.00622,"dataset, dataset provided",15,2022-05-17,0,Real-Known dataset. We also provide qualitative comparison of ablation studies on real images.
2104.00622,"dataset, dataset provided",4,2022-05-17,0,B. Omniverse Object Dataset
2104.00863,code,116,2022-05-17,0,"CrypTFlow [11] is a system that converts TensorFlow (TF) code automatically into secure multi-party computation protocol. The system has three parts: a compiler, from TF code into two and three-party secure computations, an optimized three-party computation protocol for secure interference, and a hardwarebased solution for computation integrity. The most salient characteristic of CrypTFlow is the ability to automatically translate the code into MPC protocol, where the speciﬁc protocol can be easily changed and added. The optimized three-party computational protocol is speciﬁcally targeted for NN computation and speeds up the computation. This approach is similar to the holistic approach of [1]."
2104.00863,data,9,2022-04-21,0,"encrypted data. CoRR, abs/1711.05189, 2017."
2104.00863,database,42,2022-05-17,1,"All tests were performed on the Fashion database of MNIST, which contains a training set of 60,000 and a testing set of 10,000 28x28 images of 10 fashion categories. The task is a multi-class classiﬁcation of a given image."
2104.00863,dataset,133,2022-05-17,0,"Abstract. The structure and weights of Deep Neural Networks (DNN) typically encode and contain very valuable information about the dataset that was used to train the network. One way to protect this information when DNN is published is to perform an interference of the network using secure multi-party computations (MPC). In this paper, we suggest a translation of deep neural networks to polynomials, which are easier to calculate eﬃciently with MPC techniques. We show a way to translate complete networks into a single polynomial and how to calculate the polynomial with an eﬃcient and information-secure MPC algorithm. The calculation is done without intermediate communication between the participating parties, which is beneﬁcial in several cases, as explained in the paper."
2104.11641,"data, data available",123,2022-05-17,0,"a) Graph Neural Networks: Graph Neural Networks (GNNs) [12, 13] have rapidly grown to become a popular research area, providing a highly competitive approach for tasks involving graph data. One line of research focuses on unsupervised models, e.g. VGAE [9] and Graphite [14]. These unsupervised variational models typically aim to use generative modelling of graphs for graph reconstruction, link prediction and clustering. Additionally, supervised models have attracted signiﬁcant attention, such as SCNN [15], ChebyNet [16], GAT [11] and GCN [10], which are widely used in tasks where labelled data is available."
2104.11641,"data, data available",168,2022-05-17,0,"Graph neural networks (GNNs) [1] have been shown to be effective in various graph machine learning tasks, such as link prediction and node classiﬁcation. The rapid growth of online social networks has led to the development of numerous methods for studying social behaviour online. However, many learning tasks on social networks have relied heavily on manual feature extraction. GNNs have provided an alternative to this with their ability to automatically learn representations end-to-end. One such task of interest, which has been shown to be enhanced using GNNs, is social inﬂuence prediction [2]. Data augmentation [3], which increases the amount of data available by creating informative variations of existing data, can improve the performance of machine learning models and has been widely used in many machine learning tasks [4, 5, 6]. In the ﬁelds of computer vision (CV) [7, 3]"
2104.11641,"data, dataset",110,2022-05-17,0,"• The Twitter dataset has been built by collecting Twitter data corresponding to tweets collected before, during and after the announcement of the discovery of the Higgs boson in 2012. The graph is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as whether a user retweets Higgs boson tweets. • The Weibo graph was built from 100 randomly selected users and their followers and followees. The social network is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as retweeting behaviors in the Weibo social network."
2104.11641,"data, dataset",118,2022-05-17,0,"Referring to the statistics of the datasets in Table I, we can see that the performance improvement of AugInf-GAT is particularly clear on the datasets with much fewer edges (Digg) but limited on datasets with more edges (Twitter and this is because the Twitter and Weibo). We believe that Weibo datasets contain enough edges to learn a sufﬁciently comprehensive representation, hence less beneﬁt is gained from the augmentation. We will further investigate the effect of removing edges from graphs as part of data augmentation in future work. Nonetheless, particularly for smaller graphs, we believe our proposed approach of train- and test-time augmentation can provide additional performance."
2104.11641,"data, dataset",151,2022-05-17,0,"b) Data Augmentation: Data augmentation has been shown to be an effective approach in machine learning which expands a dataset by producing transformed copies of data, thereby making the model invariant to these transformations. Data augmentation has been widely used to improve generalizability of machine learning models in natural language processing (NLP) and computer vision (CV). Most of the work on data augmentation has focused on improving augmentation at the training phase, e.g., batch augmentation [5] and UDA [6]. There are also studies that focus on augmentation during the testing phase [17]. However, data augmentation for graph neural networks has only been recently studied, such as SUBG-CON [18] and NodeAug [19]. Particularly, there is no research on test-time augmentation for GNNs. Inﬂuence:"
2104.11641,"data, dataset",165,2022-05-17,0,"Abstract—Data augmentation has been widely used in machine learning for natural language processing and computer vision tasks to improve model performance. However, little research has studied data augmentation on graph neural networks, particularly using augmentation at both train- and test-time. Inspired by the success of augmentation in other domains, we have designed a method for social inﬂuence prediction using graph neural networks with train- and test-time augmentation, which can effectively generate multiple augmented graphs for social networks by utilising a variational graph autoencoder in both scenarios. We have evaluated the performance of our method on predicting user inﬂuence on multiple social network datasets. Our experimental results show that our end-to-end approach, which jointly trains a graph autoencoder and social inﬂuence behaviour classiﬁcation network, can outperform stateof-the-art approaches, demonstrating the effectiveness of trainand test-time augmentation on graph neural networks for social inﬂuence prediction. We observe that this is particularly effective on smaller graphs."
2104.11641,"data, dataset",21,2022-05-17,1,1OAG dataset details: www.openacademic.ai/ 2Digg dataset details: www.isi.edu/ lerman/downloads/digg2009.html/ 3Twitter dataset details: snap.stanford.edu/data/higgs-twitter.html 4Weibo dataset details: www.aminer.cn/inﬂuencelocality
2104.11641,dataset,1,2022-05-17,0,Dataset
2104.11641,dataset,149,2022-05-17,0,"inﬂuence prediction method, AugInf, which incorporates train- and testtime augmentation with a jointly trained graph neural network approach. During training, this method takes into account the losses of both the graph representation learning and downstream social inﬂuence prediction task. We improve performance by applying numerous augmentations to the graphs using variational graph auto-encoders at both train- and testtime. Via an ablation study we show that the jointly trained model obtains more effective latent feature representations by using the joint loss along with both the train- and test-time augmentations. We compare our proposed end-to-end method with the state-of-the-art on several social network datasets. The experimental results show that our proposed method, AugInf-GAT, can improve the performance of predicting social inﬂuence on a number of social networks, and in particular, on the smallest of the social network graphs."
2104.11641,dataset,2,2022-05-17,0,A. Datasets
2104.11641,dataset,225,2022-05-17,0,"In our experiments we apply three augmentations to each graph with the augmentation hyperparameter threshold value set to 0.8 and train for 500 epochs. We will discuss the performance of varying these parameters in a later section. For the GAE component, each of the two hidden layers contain 64 hidden units for Digg and Twitter, and 32 for OAG and Weibo. They are trained with the Adagrad optimizer, using a 0.2 learning rate for OAG and Weibo, 0.05 for Digg and 0.1 for Twitter. Weight decay is set to 0.0005 all datasets except Digg, where it is 0.001. Additionally, we use dropout rate of 0.2. For the GNN prediction module, the ﬁrst and second layers each contain 128 hidden units and the third layer, as the output layer, has two hidden units. There are eight attention heads in each GAT layer, which means each head needs to process 16 hidden units for Digg and Twitter, with four attention heads for OAG and Weibo, which means each head needs to process 32 hidden units. The nonlinear activation function we use for both augmentation and prediction (σ in Eq. 1 and 5) is the exponential linear unit (ELU) [31]."
2104.11641,dataset,23,2022-05-17,0,"TABLE II THE PERFORMANCE OF TWO AUGINF MODELS ON DIFFERENT DATASETS, ALONG WITH THE PERFORMANCE OF THE BASELINES WITHOUT VERTEX FEATURES."
2104.11641,dataset,25,2022-05-17,0,"We evaluate using four datasets across different social network domains, namely OAG1 (Open Academic Graph), Digg2, Twitter3 and Weibo4."
2104.11641,dataset,26,2022-05-17,0,"1) Hyperparameter Analysis: We conduct an hyperparameter analysis on the Digg dataset with the same hyperparameters values mentioned previously, unless stated otherwise."
2104.11641,dataset,35,2022-05-17,0,TABLE I THE STATISTICS OF THE DATASETS. |V | AND |E| ARE THE TOTAL NUMBERS OF NODES AND EDGES OF THE ORIGINAL DATASET RESPECTIVELY AND N IS THE NUMBER OF SUBGRAPHS AFTER PREPROCESSING.
2104.11641,dataset,44,2022-05-17,0,"• The Digg dataset contains the timestamped voting behaviours of users on stories on a social news aggregation website. The edges of Digg graph are deﬁned as following relationships and the inﬂuence actions, which we are predicting, are voting behaviours."
2104.11641,dataset,66,2022-05-17,0,"We have further evaluated AugInf-GAT and AugInf-GCN on the individual components of our approach, to determine the contribution of each component to the overall performance. There are three main components in our approach: (1) train-time augmentation (2) test-time augmentation and (3) the jointly trained model. We have evaluated the following combinations on the Digg dataset:"
2104.11641,dataset,67,2022-05-17,1,"These datasets were used previously by Qiu et al. [2]. Qiu et al. [2] sampled the entire social network into sub-networks with 50 nodes in each sub-network by using a random walk with restart, extracted features for each node and provided a ground-truth for the dataset. The statistics of the three datasets are shown in Table I."
2104.11641,dataset,80,2022-05-17,0,"b) The Threshold for Augmentation: Another parameter we analyze is the threshold that determines which edges may be added. The results of this are shown in Figure 3. When the threshold is set to 0.8 for Digg dataset, our method achieves the highest performance, while on average the number of edges per dataset increases by 2.7%. As we increase the number of added edges, the performance of our method decreases."
2104.11907,"code, publicly available, code available",9,2022-05-17,2,4) The source code is publicly available.
2104.11907,"code, publicly available, code available, github, data, dataset",202,2022-05-17,3,"Abstract—As an essential procedure of data fusion, LiDARcamera calibration is critical for autonomous vehicles and robot navigation. Most calibration methods rely on hand-crafted features and require signiﬁcant amounts of extracted features or speciﬁc calibration targets. With the development of deep learning (DL) techniques, some attempts take advantage of convolutional neural networks (CNNs) to regress the 6 degrees of freedom (DOF) extrinsic parameters. Nevertheless, the performance of these DL-based methods is reported to be worse than the non-DL methods. This paper proposed an online LiDAR-camera extrinsic calibration algorithm that combines the DL and the geometry methods. We deﬁne a two-channel image named calibration ﬂow to illustrate the deviation from the initial projection to the ground truth. EPnP algorithm within the RANdom SAmple Consensus (RANSAC) scheme is applied to estimate the extrinsic parameters with 2D-3D correspondences constructed by the calibration ﬂow. Experiments on KITTI datasets demonstrate that our proposed method is superior to the state-of-the-art methods. Furthermore, we propose a semantic initialization algorithm with the introduction of instance centroids (ICs). The code will be publicly available at https://github.com/LvXudong-HIT/CFNet."
2104.11907,"data, dataset",231,2022-05-17,0,"utilized in the above experiments, se3 error [36] is a more direct evaluation metric. The off-range of the test dataset T3 is a litter smaller than that in dataset T2. The metric MSEE for β-RegNet and RGGNet is smaller in T3 than in T2. Nevertheless, the MSEE is the same for CFNet in these two test datasets, which proves that CFNet is more robust than βRegNet and RGGNet with different off-range settings. In test dataset T4, the performance of β-RegNet degrades heavily, and RGGNet needs to re-train on an additional dataset by adding a small number of data from 2009 10 03 sequence to achieve a good calibration result 0.010 (83.22%). CFNet does not need any additional training dataset and re-train process. The calibration results 0.001 (98.08%) demonstrates that CFNet has good generalization capability. Thus, FNet outperforms all of the learning-based calibration algorithms, RegNet, CalibNet, RGGNet, and even the motion-based calibration method. We can also see that, compared to the motion-based algorithm [30], our proposed method is more generalized, without the requirements of hand-crafted features or the extra IMU sensor. Furthermore, the semantic initialization process does not require motion information which is vital to the hand-eye calibration."
2104.11907,dataset,10,2022-05-17,0,TABLE IV THE COMPARISON RESULTS ON THE T1 TEST DATASET
2104.11907,dataset,101,2022-05-17,0,"We use the odometry recordings from the KITTI dataset, speciﬁcally the left color image and Velodyne point clouds recordings. We use the sequence 06 to 21 for training and validation (29416 frames), sequence 01 to 05 for evaluation/test (4854 frames). The initial calibration off-range ∆T is (±1.5m, ±20◦). To compare with other learning-based (CNNbased) methods, we design four different test datasets on the raw recordings of the KITTI dataset. Each test dataset is independent of the training dataset with the following test name conﬁgurations:"
2104.11907,dataset,111,2022-05-17,0,"In this paper, we presented a novel online LiDAR-camera extrinsic calibration algorithm. To represent the deviation from the initial projection of LiDAR point clouds to the ground truth, we deﬁne an image called calibration ﬂow. Inspired by the optical ﬂow network, we design a deep calibration ﬂow network CFNet. The initial projected points are rectiﬁed to construct accurate 2D-3D by the prediction of CFNet correspondences. EPnP algorithm within the RANSAC scheme is utilized to estimate the extrinsic parameters with iterative reﬁnement. Our experiments demonstrate the superiority of CFNet. The additional experiments on the KITTI360 datasets illustrate the generalization of our method."
2104.11907,dataset,111,2022-05-17,0,"The evaluation results on the KITTI odometry dataset are shown in Table I. It can be seen that in all of these test sequences, the mean translation error Et < 2cm and the mean rotation error ER < 0.13◦. Figure 5 shows two examples of CFNet predictions. We can see that the reference objects in the projected depth image and RGB image align accurately after re-calibration. In all of these test sequences, the calibration error of sequence 01 is the largest. The main reason is that this sequence is collected from a high-way scene, which is not included in the training dataset."
2104.11907,dataset,15,2022-05-17,0,"TABLE V THE COMPARISON RESULTS ON THE T2, T3, AND T4 TEST DATASET"
2104.11907,dataset,174,2022-05-17,1,"We evaluate our approach on the KITTI benchmark dataset including RGB images and Velodyne point clouds [42], recordings collected from different scenes. The timestamps of LiDAR and camera are synchronized, so the images and point clouds in each sequence correspond. To train our proposed calibration ﬂow prediction network CFNet, we need to ensure the input, output, and corresponding calibration ﬂow ground truth. We deﬁne the extrinsic parameters ground truth Tgt between LiDAR and camera as the transformation matrix from the camera coordinate to the LiDAR coordinate. By adding a random variable ∆T , we can obtain the initial calibration parameters Tinit = ∆T · Tgt. The LiDAR point cloud is projected onto the image plane with initial extrinsic parameters Tinit and the camera intrinsic matrix K to generate the LiDAR-image Dinit. The network takes an RGB image I, and the corresponding projected LiDAR-image Dinit as input. The calibration ﬂow ground truth can be provided by Eq. 7."
2104.11907,dataset,21,2022-05-17,0,Fig. 8. Examples of the reconstructed 3D color map on the KITTI360 datasets with the prediciton of CFNet.
2104.11907,dataset,3,2022-05-17,0,A. Dataset Preparation
2104.11907,dataset,34,2022-05-17,0,"The comparison results on the test datasets T2, T3, and T4 shown in Table V also illustrate the superior of the CFNet. Compared to the translation errors and the rotation errors"
2104.11907,dataset,49,2022-05-17,0,Fig. 7. Examples of CFNet predictions with semantic initialization on the KITTI360 datasets.(First Row) 2D-IC. (Second Row) 3D-IC. (Third Row) Semantic Initialization. (Forth Row) CFNet Prediction. (Fifth Row) Ground Truth.
2104.11907,dataset,62,2022-05-17,0,"KITTI360 datasets are utilized as an additional dataset to test our proposed LiDAR-Camera calibration algorithm CFNet. The models trained on KITTI odometry training datasets are regarded as the pre-trained models. We only use sequences 0000, 0002, and 0003 for training and validation during training to ﬁne-tune the CFNet models. Other sequences are selected as test datasets."
2104.11907,dataset,75,2022-05-17,1,"We also test the performance of CFNet on the KITTI360 benchmark dataset. The results are shown in Table VI and Figure 7. Despite re-training on a tiny sub dataset with one epoch, excellent results are obtained in the test sequences. Therefore, when the sensor parameters change, such as the camera focal length or the LiDAR-Camera extrinsic parameters, an excellent prediction model can be obtained with simple re-training."
2104.11907,dataset,9,2022-05-17,0,TABLE VI THE CALIBRATION RESULTS ON KITTI360 TEST DATASET
2104.14114,data,18,2022-04-21,0,"Following data processing, we obtained the time series of the number of publications by each author,"
2104.14114,dataset,10,2022-05-17,0,"in our model, as are the test datasets."
2104.14114,dataset,13,2022-05-17,0,time series and a target of any author in the training dataset.
2104.14114,dataset,16,2022-05-17,0,We extracted parts from the dataset dblp to construct training and test datasets for the experiments
2104.14114,dataset,16,2022-05-17,0,"dataset is comprised of 315,677 publications produced by 441,501 authors, which have been published in"
2104.14114,dataset,16,2022-05-17,0,"their annual number of publications in years 1951–2013, involving 105,806 publications. The test dataset"
2104.14114,dataset,17,2022-05-17,0,"involving 83,302 publications. The test dataset consists of those the same as the training dataset and"
2104.14114,dataset,17,2022-05-17,0,the power-law distribution. The training datasets used in these three experiments are the same as those
2104.14114,dataset,17,2022-05-17,1,"this method was validated by applying it to a high-quality dblp dataset, demonstrating that the proposed"
2104.14114,dataset,18,2022-05-17,0,"Here, we applied the piecewise Poisson model to the dataset. The training dataset consists of the"
2104.14114,dataset,18,2022-05-17,0,"in Section 6. The training dataset consists of 5,741 authors who produced publications at year 2000 and"
2104.14114,dataset,18,2022-05-17,0,"the piecewise Poisson model, the eﬀectiveness of which has been veriﬁed on the dblp dataset for the"
2104.14114,dataset,18,2022-05-17,0,"the piecewise Poisson model. Herein, this model was applied to the dblp dataset and exhibited good"
2104.14114,dataset,18,2022-05-17,0,"with a small training dataset; hence, integrating their advantages to design a mixed architecture can be"
2104.14114,dataset,19,2022-05-17,0,function. RMSprop is chosen to optimize the network. The training dataset used here is the same as
2104.14114,dataset,19,2022-05-17,0,the predicted number of publications. We applied this model to the datasets that are the same as those
2104.14114,dataset,20,2022-05-17,0,"1951–2000, which count for about 94% authors in the dblp dataset. Notably, the hyperparameters in the"
2104.14114,dataset,20,2022-05-17,0,of publications for each author. The dblp dataset is used to test the practicability of our model. The
2104.14114,dataset,20,2022-05-17,0,"on the given dataset. This indicates a direction for improving our model. Second, in the distributions of"
2104.14114,dataset,21,2022-05-17,0,"author s in years 1951–t. The proposed model is trained on every author s in the training dataset, where"
2104.14114,dataset,22,2022-05-17,0,"the training dataset is randomly divided into 4 packets: one of the packets is used as the test set, while"
2104.14114,dataset,23,2022-05-17,0,"The training dataset is the same as that in Section 6, but the test dataset is diﬀerent. It still consists of"
2104.14114,dataset,23,2022-05-17,0,"[4, 5]. Statistical factor analysis seems to be useful for predicting this index, as it helps address datasets"
2104.14114,dataset,26,2022-05-17,0,"In this model, the training dataset is divided into 4 parts, namely, Part I, Part II, Part III, and Part"
2104.14114,dataset,26,2022-05-17,0,"hs(tX−1)} and the target hs(tX ) of any author s in the training dataset. For fourfold cross validation,"
2104.14114,dataset,32,2022-05-17,0,"The dblp computer science bibliographic dataset was applied in this study, which provides open biblio graphic information on most of the journals and conference proceedings in computer science. The quality"
2104.14114,dataset,37,2022-05-17,0,"of this dataset is guaranteed by a range of measures, such as applying several methods of name disam biguation, linking 60,000 manually conﬁrmed external IDs to dblp author bibliographies, and so on. The"
2104.14114,dataset,53,2022-05-17,0,"We used the deep architecture of our model directly to predict the number of publications. The deep archi tecture, the LSTM, was applied to the test dataset, with the time series {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,56,2022-05-17,0,"The training and test datasets used here are the same as those in Section 5. Figs. 15-17 show the com parisons between the GRU and our model on the test dataset, where the input is {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,6,2022-05-17,0,requiring a large training dataset.
2105.00129,code,113,2022-05-17,0,"Problem Statement – Given W , the objective is to produce the code for a workﬂow generator that generates realistic synthetic workﬂow instances. This workﬂow generator takes as input an integer, n ≥ minw∈W (|w|). It outputs a workﬂow w(cid:48) with n(cid:48) ≥ n vertices that is as realistic as possible. n(cid:48) may not be equal to n, because real worfklows for most scientiﬁc applications cannot be feasibly instantiated for arbitray numbers of tasks. Our approach guarantees that n(cid:48) is the smallest feasible number of tasks that is greater than n."
2105.00129,code,122,2022-05-17,0,"generator for any given workﬂow application. WfChef takes as input a set of real workﬂow instances from an application, and outputs the code of a synthetic workﬂow generator for that application. WfChef analyzes the real workﬂow graphs in order to identify subgraphs that represent fundamental task dependency patterns. Based on the identiﬁed subgraphs and on measured task type frequencies in the real workﬂows, WfChef outputs a generator that can generate realistic synthetic workﬂow instances with an arbitrary numbers of tasks. In this work, we evaluate the realism of the synthetic workﬂows generated by our approach, both in terms of workﬂow structure and execution behavior. Speciﬁcally, this work makes the following contributions:"
2105.00129,code,153,2022-05-17,0,"The pseudo-code for REPLICATEPOS is shown in Algorithm 3. It takes as input a desired number of vertices (n), a base workﬂow (base), the list of POs in the base workﬂow (bP Os), and the list of POs in the workﬂow whose number of vertices is the closest to n (cP Os). The intent is to replicate POs in the base workﬂow, picking which pattern to replicate based on the frequency of POs for that pattern in the closest workﬂow. At Line 2, the algorithm ﬁrst sets the generated workﬂow to be the base workﬂow. Lines 4-9 are devoted to computing a probability distribution. More speciﬁcally, for each PO in bP Os, the algorithm computes the probability with which this PO should be replicated. Given a PO in bP Os, nc"
2105.00129,code,22,2022-05-17,0,The pseudo-code for WFCHEFGENERATE is shown in Algorithm 2. It takes as input a recipe (rcp) and a desired
2105.00129,code,267,2022-05-17,0,"The pseudo-code for WFCHEFRECIPE is shown in Algorithm 1. Lines 2 to 16 are devoted to detecting all POs in W . For each w in W , the algorithm visits w’s vertices (Lines 515). An arbitrary unvisited vertex v is visited, and another arbitrary unvisited vertex v(cid:48) is found, if it exists, that has the same type-hash as v (Lines 6-7). If no such v(cid:48) exists then the algorithm visits another vertex v (Line 8). Otherwise, it marks v(cid:48) as visited (Line 9) and computes the set of closest common ancestor and successor vertices for v and v(cid:48) (Lines 10-11). The pseudo-code of the CLOSESTCOMMONANCESTORS and CLOSESTCOMMONDESCENDANTS functions is not shown as they are simple DAG traversals. If v and v(cid:48) do not have at least one common ancestor and one common descendant, then the algorithm visits another vertex v (Line 12). Otherwise, two POs have been found, which are constructed and appended to the list of POs that occur in w at Lines 13 and 14. The pseudocode for function SUBDAG is not shown. It takes as input a vertex in a DAG, a set of ancestors of that vertex, and a set of descendants of that vertex. It returns a DAG that contains all paths from all ancestors to all descendants to traverse v, but"
2105.00129,code,334,2022-05-17,0,"Addressing these challenges requires a solid experimental methodology for evaluating and benchmarking workﬂow algorithms and systems. A fundamental component of this methodology is the availability of sets of representative workﬂow instances. One approach is to infer workﬂow structures from real-world execution logs. We have ourselves followed this approach in previous work [4], [5], resulting in a repository that provides ∼20 workﬂow instances for each of a handful of scientiﬁc applications. These instances have been used by researchers, often for driving simulation experiments designed to evaluate scheduling and resource management algorithms. Real workﬂow instances are by deﬁnition representative of real applications, but they cover only a limited number of scenarios. To overcome this limitation, in previous work we have developed tools for generating synthetic workﬂows by extrapolating the patterns seen in real workﬂow instances. The work in [4] presented a synthetic workﬂow generator for four workﬂow applications, which has been used extensively by researchers3. The method for generating the synthetic workﬂows was ad-hoc and based on expert knowledge and manual inspection of real workﬂow instances. Our more recent generator in [5] improves on the previous generator by using information derived from statistical analysis of execution logs. It was shown to generate more realistic workﬂows than the earlier generator, and in particular to preserve key workﬂow features when generating workﬂows at different scales [5]. The main drawback of these two generators is that implementing the workﬂow generation procedure is labor-intensive. Generators are manually crafted for each application, which not only requires signiﬁcant development effort (several hundreds of lines of code) but also, and more importantly, expert knowledge about the scientiﬁc application semantics that deﬁne workﬂow structures. As a result, this approach is not scalable if synthetic workﬂow instances are to be generated for a large number of scientiﬁc applications."
2105.00129,code,38,2022-05-17,0,"The pseudo-code in this section is designed for clarity. Our actual implementation, described in the next section, is more efﬁcient and avoids all unnecessary re-computations (e.g., the probabilities computed in WFCHEFGENERATE)."
2105.00129,"data, data available",53,2022-05-17,0,"work, for the purpose of evaluating WfChef and of comparing it to previously proposed approaches, we use as training data all available real workﬂow instances with fewer than the desired number of workﬂow tasks. But it may be that using fewer such instances would still lead to good results."
2105.00129,"data, data https, data available",205,2022-05-17,0,"[17] M. Rynge, G. Juve, J. Kinney, J. Good, G. B. Berriman, A. Merrihew, and E. Deelman, “Producing an infrared multiwavelength galactic plane atlas using montage, pegasus and amazon web services,” in 23rd Annual Astronomical Data Analysis Software and Systems (ADASS) Conference, 2013. [Online]. Available: http://pegasus.isi.edu/ publications/2013/rynge-montage-pegasus-amazon-adass2013.pdf [18] E. Deelman, K. Vahi, G. Juve, M. Rynge, S. Callaghan, P. J. Maechling, R. Mayani, W. Chen, R. Ferreira da Silva, M. Livny, and K. Wenger, “Pegasus, a workﬂow management system for science automation,” Future Generation Computer Systems, vol. 46, no. 0, pp. 17–35, 2015. [19] K. Keahey, J. Anderson, Z. Zhen, P. Riteau, P. Ruth, D. Stanzione, M. Cevik, J. Colleran, H. S. Gunawi, C. Hammock et al., “Lessons learned from the chameleon testbed,” in 2020 USENIX Annual Technical Conference (USENIX ATC 20), 2020, pp. 219–233."
2105.00129,"data, publicly available",242,2022-05-17,0,"Our ground truth consists of real Montage and Epigenomics workﬂow instances. These instances are publicly available on the WorkﬂowHub repository [5]. They were obtained based on logs of application executions with the Pegasus Workﬂow Management System [18] on the Chameleon academic cloud testbed [19]. Speciﬁcally, we consider 14 Montage workﬂow instances with between 105 and 9807 tasks, and 25 Epigenomics workﬂow instances with between 75 and 1697 tasks. We generate synthetic workﬂow instances with the same number of tasks as real workﬂow instances, so as to compare synthetic instances to real instances. Both WorkﬂowGenerator and WorkﬂowHub encode application-speciﬁc knowledge to produce synthetic workﬂow instances for any desired number of tasks, n. Instead, WfChef generators rely on training data, i.e., real workﬂow instances. We use a simple “training and testing” approach. That is, for generating a synthetic workﬂow instance with n tasks, we invoke WFCHEFRECIPE with all real workﬂow instances with < n tasks. For instance, say we want to use WfChef to generate an Epigenomics workﬂow with 127 tasks. We have real Epigenomics instances for 75, 121, and 127 tasks. We invoke WFCHEFRECIPE with the 75and 121-tasks instances to generate the recipe. We then invoke WFCHEFGENERATE, passing to it this receipt and asking it to generate a 127-tasks instance."
2105.00129,database,12,2022-05-17,0,1The IEEE Xplore digital database includes 118 articles with both the words
2105.00129,github,12,2022-05-17,0,"[20] “WRENCH Pegasus Simulator,” https://github.com/wrench-project/"
2105.00129,github,16,2022-05-17,0,"[6] “DAGGEN: a synthetic task graph generator,” https://github.com/"
2105.00129,github,6,2022-05-17,2,4https://github.com/wfcommons/workﬂow-schema 5https://github.com/tainagdcoleman/wfchef
2105.00129,"python, github, package",85,2022-05-17,2,"We have implemented our approach in a Python package called wfchef. Speciﬁcally, this package deﬁnes a Recipe class. The constructor for that class takes as input a list of workﬂow instances and implements algorithm WFCHEFRECIPE. The workﬂow instances are provided as ﬁles in the WfCommons JSON format 4. The class has a public method duplicate that implements the WFCHEFGENERATE algorithm, and a private method duplicate_nodes that implements the REPLICATEPOS algorithm. This Python package is available on GitHub5."
2105.00129,"python, package",182,2022-05-17,0,"Approximate Edit Distance (AED) – Given a real workﬂow instance w and a synthetic workﬂow instance w(cid:48), the AED metric is computed as the approximate number of edits (vertex removal, vertex addition, edge removal, and edge addition) necessary so that w = w(cid:48), divided by |w|. Lower values include a higher similarity between w and w(cid:48). We compute this metric via the optimize_graph_edit_distance method from the Python’s NetworkX package. Note that NetworkX also provides a method to compute an exact edit distance, but its complexity is prohibitive for the size of the workﬂow instances we consider. Even though the AED metric can be computed much faster, because it is approximate, we were able to compute it only for workﬂow instances with 865 or fewer tasks for Epigenomics and 750 or fewer tasks for Montage. This is because or RAM footprint issues (despite using a dedicated host with 192 GiB of RAM)."
2105.00775,"code, github, code available",50,2022-04-21,2,"Copyright © 2021 O. Mesnard and L.A. Barba, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Lorena A. Barba (labarba@gwu.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/barbagroup/petibm-rollingpitching.."
2105.00775,"code, open-source, code available, python, github, data",323,2022-05-17,3,"We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results. Admittedly, not everyone has access to an HPC cluster with GPU nodes and with Singularity installed. Lacking those resources, it becomes difficult to fully reproduce our workflow. However, we made the effort to deposit on Zenodo7 the primary data (directly output from our CFD solver) and post-processing scripts needed to reproduce the figures of the present manuscript. Once the Zenodo repository is downloaded, readers should be able to spin up a Docker container and run a Bash script to compute the secondary data and generate the figures, or generate different figures to explore the data in new ways. The Docker images produced and used for this study are stored on DockerHub8, under a basic free subscription. In the event Docker adopts a policy to automatically purge inactive images (those who have not been recently downloaded) from the Hub, the Dockerfiles are version-controlled on the GitHub repository and can be used to rebuild the images. We spent time engineering a transparent and reproducible workflow to produce the artifacts of this replication study. Surely, we cannot assert our steps will be fully reproducible in years from now; the software stack could very well become obsolete with new hardware generations. While the likelihood of the study being reproducible may decrease with the years, the transparency of the steps we took to generate the data shall remain constant."
2105.00775,"code, open-source, code available, python, github, data",342,2022-05-17,3,"production, wake topology, and propulsive performance of a pitching and rolling wing. Although our numerical values do not fully match those from the original study Li and Dong5, we obtain the same trends and thus consider this replication attempt to be successful. A CFD solver typically outputs the solution of primary variables. For example, PetIBM outputs the pressure and velocity fields, as well as the body forces. We often use multiple post-processing scripts to generate the final data and figures reported and analyzed in the manuscript; it involves computing secondary data, such as the vorticity field, the aerodynamic power and forces. If the code is not made available, readers cannot inspect what has been done to produce these data; bugs introduced in these post-processing steps would go undetected. If no code is available, we cannot explain discrepancies observed between our replication and the original study. As Donoho and coworkers14 once said: “The only way we’d ever get to the bottom of such a discrepancy is if we both worked reproducibly and studied detailed differences between code and data.” We made our best efforts to ensure that our replication study is reproducible. Our computational application makes use of fully open-source tools, and we created a GitHub repository6 for this study. The repository contains the source code of the PetIBM application, as well as all input files of the simulations reported here, and pre- and postprocessing Python scripts. We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results."
2105.00775,"code, open-source, github, data, open-source code",327,2022-05-17,2,"Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way. We have reimplemented the three-dimensional rolling and pitching kinematics in an open-source code shared on GitHub1 and prepared extensive reproducibility packages for all results."
2105.00775,"code, open-source, github, data, open-source code",333,2022-05-17,2,"of this field are the Navier-Stokes equations, which are notoriously difficult to solve numerically, with computational experiments often taking a long time, even on parallel compute clusters. Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way."
2105.00775,"code, publicly available, open-source, github, data, data repository",335,2022-05-17,2,"The final product of the original study is a published manuscript in the journal Physics of Fluids. Although the manuscript is well detailed, the code and input data used to produce the computational results were not made publicly available by the authors. In that regard, we consider the study to not be reproducible. Thus, we aim to replicate the scientific findings claimed in the original study with our own research software stack and deliver reproducible results. PetIBM10 is developed in the open under the permissive (non-copyleft) 3-Clause BSD license, version-controlled with Git, and hosted on a public GitHub repository.2 Each major release of the software is archived on the data repository Zenodo. Our implementation of the three-dimensional rolling and pitching wing, which relies on PetIBM, is also open source and available on GitHub3 under the same license. The repository contains all input data and processing scripts that were used to produce the computational results reported in the next section. This allows anyone to inspect the code, to verify the steps that were taken to produce computational results, and to modify and re-use it for other applications. The repository also includes README files to guide readers that may be interested in re-running the analysis. Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others."
2105.00775,"code, publicly available, open-source, github, data, data repository",337,2022-05-17,2,"Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others. We have already used Docker containers in the past to create a reproducible workflow for scientific applications on the public cloud provider Microsoft Azure.13 Here, we aim to adopt a similar workflow on our local HPC cluster. Early in this replication study, we hit a snag: Docker is not available to users on Pegasus. Indeed, Docker is not available at most HPC centers for security reasons. Submitting container-based jobs with Docker implies running a Docker daemon (a background process) that requires root privileges that users do not and should not have on shared production clusters. Thus, we decided to leverage the Singularity container technology to conduct the replication study on Pegasus. Singularity is more recent than Docker, was designed from the ground up to prevent escalation of user privileges, and is compatible with Docker images. Our reproducible workflow starts with creating a Docker image that installs PetIBM and its applications, as well as all their dependencies. We then push the image to a public registry on DockerHub.4 Anyone interested in using the application code can now pull the image from the registry and spin up a Docker container to get a faithfully reproduced computational environment. Next, we use the cloud service Singularity Hub to build a"
2105.00775,"data, code",185,2022-05-17,0,"Abstract This article reports on a full replication study in computational ﬂuid dynamics, using an immersed boundary method to obtain the ﬂow around a pitching and rolling elliptical wing. As in the original study, the computational experiments investigate the wake topology and aerodynamic forces, looking at the effect of: Reynolds number (100–400), Strouhal number (0.4–1.2), aspect ratio, and rolling/pitching phase difference. We also include a grid-independence study (from 5 to 72 million grid cells). The trends in aerodynamic performance and the characteristics of the wake topology were replicated, despite some differences in results. We declare the replication successful, and make fully available all the digital artifacts and workﬂow deﬁnitions, including software build recipes and container images, as well as secondary data and post-processing code. Run times for each computational experiment were between 8.1 and 13.8 hours to complete 5 ﬂapping cycles, using two compute nodes with dual 20-core 3.7GHz Intel Xeon Gold 6148 CPUs and two NVIDIA V100 GPU devices each."
2105.00775,"data, code",24,2022-05-17,0,"Reproducibility is obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis."
2105.00775,"data, code",333,2022-05-17,0,"θ(t) = −Aθ cos(2πf t + ψ) where Aθ is the pitching amplitude and ψ is the phase-difference angle between the pitching and rolling motions. For the present replication study, we use the same wing kinematics and numerically solve the three-dimensional Navier-Stokes equations (velocity/pressure formulation) for , where an incompressible viscous flow. The Reynolds number is defined as Re = U∞c U∞ is the incoming freestream speed and ν is the kinematic viscosity. The convective and diffusion terms of the partial differential equations are time-integrated using second-order accurate Adams-Bashforth and Crank-Nicolson methods, respectively. We enforce a Dirichlet condition (streamwise velocity set to the freestream speed U∞) on all boundaries, except at the outlet where we use a convective boundary condition (to carry vortical structures outside the computational domain). Our code base, PetIBM, solves the incompressible Navier-Stokes equations using a projection method, seen as an approximate block-LU decomposition of the fully discretized equations.6 To compute the flow around a moving object (e.g., a pitching-rolling wing), we use an immersed boundary technique. The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved)."
2105.00775,"data, code",337,2022-05-17,0,"The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved). Our code base includes several implementations of the immersed-boundary projection method;8 we use the formulation of Li et al.9 for all computations of the present study. These methods fall into the category of diffuse-interface methods, as the discrete delta function smears the solution over a few grid cells around the boundary. Each time step, we successively solve three linear systems for an intermediate velocity field, the Lagrangian forces, and the pressure field. The system for the velocity is solved using a stabilized bi-conjugate gradient method (from the PETSc library) with a Jacobi preconditioner and a convergence criterion based on the absolute L2-norm of the residual set to atol = 10−6. We solve the system for the Lagrangian forces with a direct solver (SuperLU_dist library). The pressure Poisson system is solved with a conjugate-gradient method using a classical algebraic multigrid technique (via the NVIDIA AmgX library); here, too, convergence is reached when the absolute L2-norm of the residual is 10−6. To quantify aerodynamic performance of the wing, we report the thrust, lift, and spanwise force coefficients, defined as"
2105.00775,"data, code, data available, code available",122,2022-05-17,0,"The minimum requirement for computational research to be reproducible is to make code and data available to others. Peng2 introduced the concept of a reproducibility spectrum, in which reproducible research is a “minimum standard for judging scientific claims when full independent replication of a study is not possible” or not available. The two extremes on the reproducibility spectrum are “not reproducible” (when a published manuscript is the sole deliverable from a study) and “fully replicated” (the gold standard for a study). This paper addresses reproducibility and replicability in computational fluid dynamics, a mature field and one of the oldest branches of computational science. At the center"
2105.00775,github,10,2022-04-21,2,2PetIBM: github.com/barbagroup/petibm 3PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 4DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,10,2022-04-21,2,6PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 7Repro-packs: doi.org/10.5281/zenodo.4732946 8DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,3,2022-05-17,0,1PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching
2105.00775,open-source,216,2022-05-17,0,"pitching-rolling plates.” In: Physics of Fluids 28.7 (2016), p. 071901. J. B. Perot. “An analysis of the fractional step method.” In: Journal of Computational Physics 108.1 (1993), pp. 51–58. R. Mittal, H. Dong, M. Bozkurttas, F. Najjar, A. Vargas, and A. Von Loebbecke. “A versatile sharp interface immersed boundary method for incompressible ﬂows with complex boundaries.” In: Journal of computational physics 227.10 (2008), pp. 4825–4852. K. Taira and T. Colonius. “The immersed boundary method: a projection approach.” In: Journal of Computational Physics 225.2 (2007), pp. 2118–2137. R.-Y. Li, C.-M. Xie, W.-X. Huang, and C.-X. Xu. “An efﬁcient immersed boundary projection method for ﬂow over complex/moving boundaries.” In: Computers & Fluids 140 (2016), pp. 122–135. P.-Y. Chuang, O. Mesnard, A. Krishnan, and L. A. Barba. “PetIBM: toolbox and applications of the immersedboundary method on distributed-memory architectures.” In: The Journal of Open Source Software 3.25 (May 2018), p. 558."
2105.00775,"python, github",316,2022-05-17,2,"In the original study, the authors reported the results of a grid-independence study to justify the spatial and temporal grid resolutions used for the parametric study. They compared force coefficients, profiles of the velocity components, profiles of the fluctuating kinetic energy, and distances between vortical structures in the near wake, obtained with different grid resolutions. Here, we also report the results of our grid-independence study before moving on to the results of the parametric study. We use the same domain size as in the original study: 30c × 25c × 25c (where c is the chord length of the wing). The root of the wing (around which the plate undergoes the rolling/pitching motion) is located at the center of the computational domain. We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository)."
2105.00775,"python, github",331,2022-05-17,2,"We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository). In the present study, we model the wing with a flat elliptical surface, discretized with Lagrangian markers uniformly distributed on its surface (with a similar resolution as the grid-spacing size of the background Eulerian grid). As in the original study, we consider the case of a circular wing (AR = 1.27) with Reynolds number Re = 200, Strouhal number St = 0.6, and phase-difference angle ψ = 90o, to assess independence in the numerical results. We investigated the effect of the grid-spacing size, the time-step size, and the convergence criterion of the iterative solvers, on the numerical solution. To assess the effect of the grid-spacing size ∆x in the vicinity of the wing on the solution, we computed five flapping cycles on three grids: coarse (∆x = 0.03c), nominal (∆x ="
2105.09146,"data, dataset",113,2022-05-17,0,"The noisy observation data was used to train two models for each noise level: a HNN using the architecture from Section 3 and a baseline SINDy model. After training, each HNN model was used to generate 5000 coordinate predictions. These predictions were then used to ﬁt a SINDy+HNN model for each noise level. In total three models were returned for each noise level: a SINDy Baseline, a HNN, and a SINDy+HNN model. The SINDy+HNN approach is illustrated in Figure 8. For this example, the proposed SINDy+HNN approach to noise regulation is data-efﬁcient requiring only a small data set of 5k observations."
2105.09146,"data, dataset",117,2022-05-17,0,"Neural network performance is highly dependent on the quantity and quality of the available training data. Experimental datasets are unavoidably noisy and scarce in quantity. This can limit a network’s ability to learn features and generalize. For neural networks modeling physical behaviors, this can be particularly problematic as it can lead to physically inconsistent predictions that violate governing laws. The black-box nature of the learned features and relations makes it challenging to assess if the network is accurately learning the underlying physics, constraints, and parameters from data. These issues of network’s interpretability and generalization ability limit their utility to model and simulate physical systems."
2105.09146,"data, dataset",174,2022-05-17,0,"Neural networks often require large amounts of data to generalize and can be ill-suited for modeling small and noisy experimental datasets. Standard network architectures trained on scarce and noisy data will return predictions that violate the underlying physics. In this paper, we present methods for embedding even–odd symmetries and conservation laws in neural networks and propose novel extensions and use cases for physical constraint embedded neural networks. We design an even–odd decomposition architecture for disentangling a neural network parameterized function into its even and odd components and demonstrate that it can accurately infer symmetries without prior knowledge. We highlight the noise resilient properties of physical constraint embedded neural networks and demonstrate their utility as physics-informed noise regulators. Here we employed a conservation of energy constraint embedded network as a physics-informed noise regulator for a symbolic regression task. We showed that our approach returns a symbolic representation of the neural network parameterized function that aligns well with the underlying physics while outperforming a baseline symbolic regression approach."
2105.09146,dataset,163,2022-05-17,0,"Implementation. A standard MLP and an MLP with an even–odd hub layer were implemented in PyTorch following the architecture described in [15]. The standard MLP model consisted of two fully connected layers of 5 neurons each, with sigmoid activations and one fully connected output layer. The MLP with the even–odd hub layer replaced the last hidden layer with an even–odd hub neuron. As in [15] we applied the models to a dataset generated from even (cosine) or odd (sine) functions with normally distributed noise N ∼ (µ = 0, σ = 0.2). The models were trained to minimize the mean squared error (MSE) of x(t) and ˆx(t). This loss function was then modiﬁed to include the symmetry metric term and an additional standard MLP model was trained. Figure 1 compares results for even symmetry function."
2105.09146,"open-source, package, python, data open-source , data",117,2022-05-17,0,"[31] Rick Chartrand. Numerical differentiation of noisy, nonsmooth data. ISRN Applied Mathematics, 2011, 2011. [32] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445–22451, 2019. [33] Brian de Silva, Kathleen Champion, Markus Quade, Jean-Christophe Loiseau, J. Kutz, and Steven Brunton. Pysindy: A python package for the sparse identiﬁcation of nonlinear dynamical systems from data. Journal of Open Source Software, 5(49):2104, 2020."
2105.09146,python,142,2022-05-17,0,"[24] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020."
2105.09146,python,149,2022-05-17,0,"All SINDy models in this work were implemented using the PySINDy python library [33]. For each SINDy implementation, the same feature libraries were provided: a polynomial library containing terms up to the 2nd degree and a Fourier library limited to 1 frequency and containing both sine and cosine terms. The only argument that varied between the SINDy models was the threshold parameter provided to the STLSQ optimizer. Varying this parameter was necessary to ensure model performance and prevent the complete dropout of the terms in one model. For each model, the threshold parameter was selected as the value that dropped out the largest number of terms while maintaining the quality of the ﬁt. Each ﬁt was then integrated with an ODE solver to return the coordinate predictions. The same solver setup as in section 3 was used."
2105.09146,python,220,2022-05-17,0,"Implementation. As in [11] we implemented two networks in PyTorch [22] a Baseline MLP Figure 4(a) and a HNN Figure 4(b). The Baseline MLP directly outputs the time derivatives and served as a point of comparison for the constraint embedded HNN network. The Baseline MLP consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 2 corresponding to ( dq dt ). The HNN consisted of two equation parameterizations, Equation 8 and 9. The former parameterized the Hamiltonian and consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 1 corresponding to Hθ. The latter parameterized the time evolutions and used the python Autograd library [23] to return the partials of Hθ with respect to q and p, ( ∂H ∂q ). After training an ODE solver was used to integrate both the Baseline and HNN models to generate coordinate predictions. The SciPy [24] solve_ivp function was used to integrate the network with a tolerance of 1e-12 and a 4th order Runge-Kutta method was used for the solver [25]."
2105.09869,"code, github, code available",11,2022-04-21,2,1The MATLAB code is available at https://github.com/amasoumi60/Robust Dynamic-Mode-Decomposition.
2105.09869,"data, data available",141,2022-05-17,0,"A wealth of data science methods have been developed by researchers and made available to practitioners. Dynamic mode decomposition (DMD) stands out because of its connection with the Koopman operator theory [3], which reconciles data analysis and the mathematical knowledge of dynamical systems; the reader is referred to [4], [5] for more details. Since the publication of the paper authored by Schmid and Sesterhenn [6], [7], DMD has become the mainstream method for data-driven modeling of dynamical systems, mainly applied to ﬂuid mechanics [3], electric power grids [8], neuroscience [9], ﬁnance [10], climate science [11], and transportation [12], to name a few."
2105.09869,"data, data available",49,2022-05-17,0,"of human activity is a crucial driver for the research and development of data science methods [1], [2]. This fact especially applies to complex dynamical systems for which ﬁrst-principles models are challenging to obtain while a large amount of data are available."
2105.09869,"data, dataset",131,2022-05-17,0,"Abstract—This paper develops a robust dynamic mode decomposition (RDMD) method endowed with statistical and numerical robustness. Statistical robustness ensures estimation efﬁciency at the Gaussian and non-Gaussian probability distributions, including heavy-tailed distributions. The proposed RDMD is statistically robust because the outliers in the data set are ﬂagged via projection statistics and suppressed using a Schweppe-type Huber generalized maximum-likelihood estimator that minimizes a convex Huber cost function. The latter is solved using the iteratively reweighted least-squares algorithm that is known to exhibit a better convergence property and numerical stability than the Newton algorithms. Several numerical simulations using canonical models of dynamical systems demonstrate the excellent performance of the proposed RDMD method. The results reveal it outperforms several other methods proposed in the that literature."
2105.09869,"data, dataset",192,2022-05-17,0,"It turns out that solving the sensitivity of DMD to deviations from the assumptions made about the data set is a challenging task [20] due to the vulnerability of the least-squares estimator to non-Gaussian noise and outliers, which is a great concern to practitioners. This fact motivated several independent investigations to assess the accuracy of the DMD in capturing the underlying system dynamics directly from the data set [21]–[24]. For instance, Dawson et al. [25] and Hemati et al. [26] address, respectively, the bias introduced by Gaussian noise and the bias resultant from asymmetrically processing snapshots. In Section IV of this paper, numerical experiments conﬁrm that the DMD variant proposed by Hemati et al. [26] has excellent performance in the presence of Gaussian noise and has good performance in the presence of non-Gaussian but symmetrically distributed noise. This is achieved thanks to a reformulation of the DMD using a total least-squares estimator [27]; however, this estimator is still vulnerable to outliers."
2105.09869,"data, dataset",224,2022-05-17,0,"The vulnerability of the least-squares estimator to outliers is not directly solvable without data preprocessing; therefore, Askham et al. [28] reformulate DMD as an optimization problem and make use of a least trimmed squares (LTS) estimator, speciﬁcally the trimmed M-estimator introduced by Rousseeuw [29]. To the best of the authors’ knowledge, [28] is the only formulation of DMD that makes use of a robust estimator. In particular, the LTS estimator has a high breakdown point [29]—that is, this estimator is very robust from a statistical standpoint; however, the formulation in [28] lacks a mechanism to identify outliers. Indeed, identifying outliers without access to a system model is challenging but necessary in DMD. The formulation in [28] circumvents this challenge by making a blanket assumption that the time-series data can be represented “by the outer product of a matrix of exponentials, representing Fourier-like time dynamics, and a matrix of coefﬁcients, representing spatial structures.” Consequently, nonexponential dynamics in the data set are, therefore, classiﬁed as outliers. This fact precludes the application of the method proposed in [28] to dynamical systems that present nonexponential dynamics."
2105.09869,"data, dataset",267,2022-05-17,0,"In the classic literature in robust statistics [16], [17], one deﬁnes robustness as insensitivity to deviations from the assumptions. In this sense, the least-squares estimator is not robust. Two cases of deviations from the assumptions are of particular concern. The ﬁrst case arises when the probability distribution of the observations is not Gaussian. The leastsquares estimator quickly loses its statistical efﬁciency (that is, accuracy) when the tails of the probability distribution of the observations become slightly thicker than the Gaussian distribution or when the probability distribution of the observations becomes slightly asymmetric. The second case arises when the probability distribution of the majority of the observations is Gaussian except for a few observations, which may take arbitrary values. In this respect, one deﬁnes an outlier as a data point that violates the underlying assumptions—in other words, it is a data point that is distant from the majority of the point cloud [18]. The least-squares estimator produces strongly biased results in the presence of a single outlier in the data set [19]. Both cases of deviations from the assumptions often occur in practice—for example, when the probability distribution of the observations is not known while being assumed to be Gaussian or when outliers arise because of instrumentation and communications errors or a poor experimental setup. This fact precludes the DMD from being applied to practical settings, especially for control purposes where pre-cleaning the data set is not an option."
2105.09869,"data, dataset",59,2022-05-17,0,"Deﬁnition 2 (Median absolute deviation from median in the case of a univariate data set). Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A very robust estimator of scale [38] is the median absolute deviation from the median, which is deﬁned as"
2105.09869,"data, dataset",86,2022-05-17,0,"Deﬁnition 3 (Median absolute deviation from the median in the case of a multivariate data set). Let a multivariate data set, P ⊆ Rm, be {p1, ..., pN }. The median absolute deviation from the median is deﬁned as (cid:0) (cid:12) (cid:12)pT for k, j = {1, 2, ..., N }, where v is the direction to which the data points are projected."
2105.09869,"data, dataset",87,2022-05-17,0,"Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A measure of the distance between a data point, pk ∈ P, and the center of the data cloud is given by pk−(cid:98)(cid:96) , where (cid:98)(cid:96) denotes an estimator of location, and (cid:98)s denotes an estimator of scale. A classic measure of distance in the univariate case is provided by"
2105.09869,"data, dataset, data available",116,2022-05-17,0,"and the results are depicted in Fig. 7. Note that for the data collected from nonlinearly evolving signals, the approximated DMD modes are reﬂecting the behavior of the most dominant Koopman modes. As shown in Fig. 7, when there is no outlier contamination within the data set, all considered DMD methods calculate approximately the same dominant eigenvalues, and a good response of x2 is reconstructed; however, when there are some outliers among the data set, only N-RDMD can capture the same eigenvalues as found for the outlier-free data. In other words, the process of ﬁnding eigenvalues has been made robust against outliers."
2105.09869,dataset,56,2022-05-17,0,"[26] M. S. Hemati, C. W. Rowley, E. A. Deem, and L. N. Cattafesta, “De-biasing the dynamic mode decomposition for applied Koopman spectral analysis of noisy datasets,” Theoretical and Computational Fluid Dynamics, vol. 31, no. 4, pp. 349–368, 2017."
2105.10037,code,2,2022-05-17,0,A. Pseudo-code
2105.10037,code,68,2022-05-17,0,"∈P Next, the learnt inverse model is used to augment ˆ with agent speciﬁc actions. Finally, these action augmented trajectories are used to learn the ﬁnal policy πTA via behavioral cloning. Note that our correspondence learning framework is agnostic to the imitation from observation algorithm used for learning the agent policy. The pseudo-code for training our framework is presented in Appendix A."
2105.10037,"code, github, code available",73,2022-04-21,2,"In this section, we analyze the efﬁcacy of our proposed method on the xDIO task. We adopt MuJoCo (Todorov et al., 2012) as the experimental test-bed and evaluate on several cross-domain tasks, along with a thorough ablation study of different modules in our overall framework. Implementation details are presented in Appendix B. Code and videos are available at: https://driptarc.github. io/xdio.html."
2105.10037,"data, dataset provided",131,2022-05-17,0,"In this paper, we present a novel framework to tackle the xDIO task by learning a state-map across domains using both local and global alignment. Local alignment is performed via transition distribution matching and cycleconsistency in both the state and latent space, while global alignment is enforced via the idea of temporal position preservation. While previous approaches rely on paired data and expert actions, we provide a general framework that can learn the mapping from unpaired, unaligned demonstrations without expert actions. We demonstrate the efﬁcacy of our approach on multiple cross-domain tasks encompassing dynamics, viewpoint and morphological mismatch. Our future work will concentrate on extending our method for learning correspondence using random trajectories, thus mitigating the need for proxy tasks."
2105.10037,dataset,115,2022-05-17,0,"The objective of xDIO is to learn an optimal policy πTA in in the agent domain, given state-only demonstrations the expert domain. In this paper, we propose to ﬁrst learn a transformation ψ : SE → SA between the domains and then leverage ψ to imitate from the expert demonstrations. Following prior work (Gupta et al., 2017; Liu et al., 2018; Kim et al., 2020), we assume access to a dataset consisting of expert-agent trajectories for M different proxy tasks: M j=1. Proxy tasks encompass simple ) D } primitive skills in both domains and are different from the inference task"
2105.10037,dataset,147,2022-05-17,0,"We use the learned ψ to map the states in the inference task expert demonstrations to the agent domain. Given DM the set of transferred state-only demonstrations ˆ , we can use any imitation from observation algorithm to learn the ﬁnal policy. In this work, we follow the Behavioral Cloning from Observation (BCO) approach proposed in (Torabi et al., 2018). BCO entails learning an inverse dySA × SA → AA to infer missing acnamics model tion information. First, we collect a dataset of state-action A, st+1 triplets by random exploration. The A ) } inverse model is subsequently estimated by Maximum Like. lihood Estimation (MLE) of the observed transitions in P Assuming a Gaussian distribution over actions, this reduces to minimizing an (cid:96)2 loss as follows,"
2105.10037,dataset,63,2022-05-17,0,"Given the alignment dataset D containing trajectories from the M proxy tasks, we ﬁrst pre-train the temporal position M j=1 using Equation 6. This is folestimators lowed by adversarial training of the state maps ψ, φ, where we use separate discriminators on the state space and latent space for each proxy task. The full objective is then:"
2105.10702,"data, data available",114,2022-05-17,0,"Many of these machine learning algorithms are supervised learning approaches that require large amounts of annotated image data for training. Gathering suitable data is especially challenging in the medical domain as it is incredibly time consuming for radiologists to generate ground-truths of the standard and volume required for training a predictive model. An alternative approach is to use past clinical images and corresponding radiological reports available through a hospital’s picture archiving and communication system (PACS); the advantage being that, although this data is largely unstructured (free text), it is available to us in high volumes and removes the need for manual annotation."
2105.10702,database,14,2022-05-17,0,"hierarchical image database. In CVPR, pages 248–255. IEEE, 2009."
2105.10702,database,55,2022-05-17,0,"[13] Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, and Ronald M Summers. Interleaved Text / Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation. Journal of Machine Learning Research (JMLR), 17:1–31, 2016. doi: 10.1109/CVPR.2015.7298712."
2105.10702,dataset,189,2022-05-17,0,"The knee X-ray dataset has been extracted from the PAC system of St Thomas Hospital (part of Guys and St Thomas NHS Foundation Trust) and has been fully anonymised to remove sensitive patient information. It comprises a total of 330 knee X-ray exams collected over the years 2015 and 2016. Each exam consists of a textual report and one or more X-ray images (left/right knees or both, taken from different views: anteroposterior (AP), lateral (L) and skyline (S), and different positions: weight-bearing (WB) and non-weight-bearing(nonWB)). The most common exam consists of both AP and L views of left and right knees separately, making up 42% of total exams. The reports vary in length between 2 and 145 words, with an average of 30 and standard deviation of 18.7; and between 1 and 16 sentences with an average of 2.7 per report. The X-ray images vary in sizes between 420 × 650 × 3 and 3056 × 3056 × 3."
2105.10702,dataset,2,2022-05-17,0,3 Dataset
2105.10702,dataset,93,2022-05-17,0,"We adopt the GoogLeNet [18] CNN architecture, pre-trained on the ImageNet dataset [19], and extract image features of each X-ray image view (V1 - VK) from the last spatial average pooling layer (R1024). The maximum value of each feature is aggregated across the exam images to create a ﬁxed-size input to the RNN of dimension R1024, which is then passed through a fully connected layer in order to reduce the dimension to R256, equal to the RNN input size."
2105.12306,"data, dataset",33,2022-05-17,0,Table 2: Statistics of the used datasets. All the training data are merged to train the REALISE model. The test sets are used separately to evaluate the model performance.
2105.12306,"data, dataset",69,2022-05-17,1,"in 2013, 2014 and 2015 (denoted as SIGHAN13, SIGHAN14 and SIGHAN15). Table 2 shows the data statistics. Originally, the SIGHAN datasets are in the Traditional Chinese. Following previous works (Wang et al., 2019; Cheng et al., 2020; Zhang et al., 2020), we convert them to the Simpliﬁed Chinese using the OpenCC tool2."
2105.12306,dataset,1,2022-05-17,0,Dataset
2105.12306,dataset,14,2022-05-17,0,Table 6: Ablation results of the REALISE model on each SIGHAN dataset.
2105.12306,dataset,28,2022-05-17,0,"Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018. Being negative but constructively: Lessons learnt from creIn ating better visual question answering datasets."
2105.12306,dataset,284,2022-05-17,0,"""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!! ""!#OITAnd the Yankees’ acepitcherThe number ofbabiesbornto womencontinues todecline(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:18415)(cid:12787)(cid:12713)!!!""!#OIT(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:16855)(cid:20043)(cid:7529)(cid:15919)!!! ""!#OITDataset"
2105.12306,dataset,383,2022-05-17,0,"(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10449)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)1.001.001.001.001.000.971.000.991.001.001.001.001.000.200.200.140.340.180.130.370.250.190.310.250.230.170.090.040.130.120.080.060.520.090.040.040.060.080.05(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10450)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)!!! ""!#OIIplan to watchamoviewith mygirlfriendT(cid:10356)(cid:24485)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)0.991.001.001.000.991.000.991.000.991.000.990.260.230.240.170.370.340.210.210.210.300.210.220.620.240.070.180.230.090.060.040.050.05(cid:10356)(cid:8148)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)The affair  also  happened  from  this  point!!!""!#OIT(cid:18619)(cid:7870)(cid:11870)(cid:12065)1.001.001.001.000.600.340.370.520.100.060.090.06(cid:8711)(cid:7870)(cid:11870)(cid:8402)!!! ""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!!"
2105.12306,github,10,2022-05-17,2,1Code and model are available at https://github.
2105.12306,github,3,2022-05-17,0,2https://github.com/BYVoid/OpenCC
2105.12309,"code, github, code available",55,2022-04-21,2,the EKF Algorithm for RexROV becomes as shown in Algorithm (2). The algorithm has been applied to several test courses from [23] in the next section and their results are discussed. The source code for implementation of the EKF algorithm shown below is available in https:// gitlab.engr.illinois.edu/auvsl/submarine
2105.12309,open-source,108,2022-05-17,0,"4x4 state matrix thereby reducing computational cost.The 4 DOF model is proposed in Ref. [19] and was successfully used by Ref. [20] in control development. Since motion predictions are being augmented with sensor readings, it is expected from this approach to work effectively with the problem proposed in this paper. A Robotic Operating System (ROS) Gazebo-based open-source marine vehicle simulator was used in this research [14]. The simulator incorporates the dynamic model by Fossen [14] with a vehicle model based on parameters derived from the works by Berg [17]."
2105.12309,open-source,238,2022-05-17,0,"Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) are used for a wide variety of missions related to exploration and scientiﬁc research. Successful navigation by these systems requires a good localization system. Kalman ﬁlter based localization techniques have been prevalent since the early 1960s and extensive research has been carried out using them, both It has been found that the in development and in design. use of a dynamic model (instead of a kinematic model) in the Kalman ﬁlter can lead to more accurate predictions, as the dynamic model takes the forces acting on the AUV into account. Presented in this paper is a motion-predictive extended Kalman ﬁlter (EKF) for AUVs using a simpliﬁed dynamic model. The dynamic model is derived ﬁrst and then it was simpliﬁed for a RexROV, a type of submarine vehicle used in simple underwater exploration, inspection of subsea structures, pipelines and shipwrecks. The ﬁlter was implemented with a simulated vehicle in an open-source marine vehicle simulator called UUV Simulator and the results were compared with the ground truth. The results show good prediction accuracy for the dynamic ﬁlter, though improvements are needed before the EKF can be used on real time. Some perspective and discussion on practical implementation is presented to show the next steps needed for this concept."
2105.12309,open-source,30,2022-05-17,0,"Experiments were conducted on open source simulation software Gazebo 7. For underwater scenario and sensors simulation, UUVSIM [14] is used in parallel with ROS Kinetic."
2105.12309,package,62,2022-05-17,0,"[14] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “UUV simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in Proceedings of OCEANS 2016 MTS/IEEE Monterey, 19-23 September, 2016, Monterey, CA, USA., IEEE, sep 2016."
2105.15074,"data, data available",212,2022-05-17,0,"This research aimed to evaluate the performance of a classiﬁcation algorithm based on ANN of children with FASD and without the syndrome, using the result of psychometric, DTI, and saccade tests and comparing the accuracy of the model with the SVMR developed by Zhang et al. et al. (2019). Our results suggest that ANN can make a preliminary diagnosis of pathologies reasonably. When numerical data are available. Most of the studies carried out to classify FASD patients using machines learning, study brain images or natural language Fang et al. (2006); Wozniak and Muetzel (2011); Suttie et al. (2018). Only the study conducted by Zhang et al. et al. (2019) uses numerical data from psychometric and eye movement studies and is freely available. Then, the results obtained with the use of ANN were compared with the results obtained by these authors in their study (Fig. 2). The purpose of this study was to evaluate, whether ANN can be used to classify patients with FASD from data obtained non-invasively and that do not require many studies for a preliminary diagnosis."
2105.15074,"data, data available",28,2022-05-17,0,"In this section, we analyzed the data available and evaluated the performance of the ANN method and the conﬁguration of the neural network with reasonable accuracy."
2105.15074,"data, dataset",110,2022-05-17,0,"The data used for testing included psychometric tests. Associated with analyzing social behavior, memory activities, language delay and all altered behavioral factors in FASD children. Also, the data set included other developmental diseases, making the classiﬁcation process more difﬁcult. This paper used two models of dense networks using this type of data. The ﬁrst model archived an accuracy of 75.55%. However, the model did not show signiﬁcant improvements, despite the variation in the layers and neurons in the layers. In the second model, a feature layer was added, achieving an accuracy of 88.46%."
2105.15074,"data, dataset",129,2022-05-17,0,"Once the network conﬁguration was chosen, training and testing or validation behavior are shown in the model precision and loss functions. The loss function with a high result indicates that the neural network has a poor performance and a low result, that it is doing a good job. Fig. 3a shows the accuracy for each of the data set. In terms of the number of that it was aspects found and related to the number of individuals evaluated. We attempted to ﬁnd a model with no signiﬁcant difference, between the labeled data and the prediction. Nonetheless, with the implemented conﬁguration, the training data has increased accuracy. In addition, the validation data are also increasing accuracy."
2105.15074,"data, dataset",147,2022-05-17,0,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model."
2105.15074,"data, dataset",159,2022-05-17,0,"Prenatal alcohol exposure causes brain damage. And the neuropsychological consequences are deep. These deﬁcits in cognitive functions include: difﬁculties in planning, organization, and attention, consequential learning failures, and memory deﬁciencies. Some have speech and/or language difﬁculties, visuospatial functions and spatial memory, that are increased by exposure to prenatal alcohol (Glass et al., 2017; Green, 2007; Mohammad et al., 2020). Those characteristics could be seen in the task and the evaluation. It is a vital diagnosis based on the abnormalities founded and not only in the traditional facial characteristic (Wozniak et al., 2009). The novelty of the Zhang et al. (2019)’s data set is that it includes numeric and image data. This selection observes a deep learning operation in numerical data because most of the studies are based on images."
2105.15074,"data, dataset",253,2022-05-17,0,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model. For the Antisaccade movement, Prosaccade used a dense network with two hidden layers of 128 neurons with ""ReLu"" activation. For Memory-guide saccade, Diffusion tensor imaging (DTI), and Psychometric data, we used four interleaved hidden layers of 64 and 128 neurons with sigmoid, ReLu activation, and Leaky ReLu just for DTI. The models trained for a total of 50 epochs. We used Leaky ReLu just in one case (DTI), which increasing the perfection. In other cases, the results not improved and we discarded to use the optimization function."
2105.15074,"data, dataset, data available",83,2022-05-17,1,"The data used in this research is based on an open-access dataset collected by (Zhang et al., 2019). This research will use psychometric, saccadic eye movement and DTI data. From an open data set collected and analyzed by (Zhang et al., 2019) to address the research question. This data set contains children’s information from 3 to 18 years old. Including subject that are clinically diagnosed with or without FASD."
2105.15074,"data, python, dataset provided",181,2022-05-17,0,"The input data are the features of each test. And the output is binary to classify FASD or control. The amount of data must be large enough to provide training examples. From which a large set of parameters can be drawn. Only a large number of parameters give rise to the wealth of class functions that model implicit knowledge Faust et al. (2018). Unfortunately, there are little data on children with FASD, making the classiﬁcation difﬁcult. The Keras library of TensorFlow was used in Python to implement the algorithm (Ketkar, 2017). We designed various models with dense layer connections, changing the number of neurons in each input layer, hidden layer, and output layer. To compile the model, we used “backpropagation with optimization Adam” and loss “sparse categorical cross-entropy” in the ﬁrst model. And “Binary Crossentropy” in another model, which are ideal conﬁgurations for classiﬁcation of categories in the FASD and non-FASD cases."
2105.15074,github,14,2022-04-21,2,1The algorithms and functions used are available in the GitHub repository https://github.com/vjduarte/ANN_FASD
2105.15074,python,18,2022-05-17,0,"Ketkar, N. (2017). Deep Learning with Python: A Hands-on Introduction. Apress."
2106.03907,code,113,2022-05-17,2,"In this section, we report the empirical performance of the DFPV method. First, we present the results of estimating structural functions; we design two experimental settings for low-dimensional treatments and high-dimensional treatments, respectively. Then, we show the result of applying PCL methods to the bandit oﬀ-policy evaluation problem with confounding. We include the results for problems considered in prior work in Appendix E. The experiments are implemented using PyTorch [27]. The code is included in the supplemental material. All experiments can be run in a few minutes on Intel(R) Xeon(R) CPU E5-2698 v4 2.20GHz."
2106.03907,"data, data available",245,2022-04-21,0,"One common assumption to cope with confounding bias is to assume no unobserved confounders exist [8], or more generally, the ignorable treatment assignment assumption [28], which states that the treatment assignment is independent of the potential outcomes caused by the treatment, given the background data available. Although a number of methods are proposed based on this assumption [7, 9, 36], it can be too restrictive, since it is often diﬃcult to determine how the confounder aﬀects treatment assignments and outcomes. A less restrictive assumption is that we have access to proxy variables, which contain relevant side information on the confounder. In the ﬂight tickets example, we can use the number of views of the ticket reservation page as a proxy variable, which reﬂects peoples’ desire for ﬂights. Note that if we can completely recover the confounder from proxy variables, the ignorable treatment assignment assumption can be satisﬁed. Motivated by this, Lee et al. [14] and Louizos et al. [17] aim to recover the distribution of confounders from proxy variables using modern machine learning techniques such as generative adversarial networks [5] or variational auto-encoders (VAE) [11]. Although these methods exhibit powerful empirical performance, there is little theory that guarantees the correct recovery of the causal eﬀects."
2106.03907,"data, data available",74,2022-05-17,0,"where each element of the matrix B ∈ R10×4096 was generated from Unif(0.0, 1.0) and ﬁxed throughout the experiment. We ﬁxed the shape parameter to heart and used other parameters as the treatment-inducing proxy Z. We sampled another image that shared the same posY as treatment A, which is used as output-inducing proxy W . Details of data generation process can be found in Appendix F.2."
2106.03907,"data, dataset",105,2022-05-17,0,"Here, we describe the data generation process for the dSprites dataset experiment. This is an image dataset parametrized via ﬁve latent variables (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. In this experiment, we ﬁxed the shape parameter to heart, i.e. we only used the heart-shaped images. The other latent parameters take values of scale ∈ [0.5, 1], rotation ∈ [0, 2π], posX ∈ [0, 1], posY ∈ [0, 1]."
2106.03907,"data, dataset",109,2022-05-17,0,"Table 1 shows the result for this dataset. In this setting, the performance of DFPV matches KPV and PMMR. As in the experiment described in the revious section, the setting is low-dimensional (one-dim treatment variable, three-dim treatment-inducing proxy, four-dim outcome-inducing proxy) and the generative model is smooth (the ""ground truth"" being a generalized additive model and a Gaussian mixture model). For these reasons, we might again expect this data to favor kernel methods, such as KPV and PMMR; nonetheless, our method matches them. DFPV again outperforms CEVAE in this setting."
2106.03907,"data, dataset",145,2022-05-17,1,"To test the performance of DFPV in a more realistic setting, we conducted the experiment on the Grade Retention dataset introduced by Deaner [3]. This aims to estimate the eﬀect of grade retention based on the score of math and reading on the long-term cognitive outcomes, in which we use scores in elementary school as a treatment-inducing proxy (Z) and cognitive test scores from Kindergarten as the an outcome-inducing proxy (W). Following Mastouri et al. [18], we generate a synthetic ""ground truth"" by ﬁtting a generalized additive model to learn a structured causal model (SCM), and a Gaussian mixture model to learn unmeasured confounder based on the learned SCM. Note, this is needed since for real-world data there is no measured ground truth."
2106.03907,"data, dataset",274,2022-05-17,0,"We compare the DFPV method to three competing methods, namely KPV [18], PMMR [18], and an autoencoder approach derived from the CEVAE method [17]. In KPV, the bridge function is estimated through the twostage regression as described in Section 2, where feature functions are ﬁxed via their kernel functions. PMMR also models the bridge function using kernel functions, but parameters are learned by moment matching. CEVAE is not a PCL method, however, it represents a state-of-theart approach in correcting for hidden confounders using observed proxies. The causal graph for CEVAE is shown in Figure 2, and CEVAE uses a VAE [11] to recover the distribution of confounder U from the “proxy” Q. We make two modiﬁcations to CEVAE to apply it in our setting. First, we include both the treatment-inducing proxy Z and output-inducing proxy W as Q in CEVAE (we emphasize that this does not follow the causal graph in Figure 2, since there exist arrows from Q to A, Y ). Second, CEVAE is originally used in the setting where Q is conditioned on a particular value, whereas we marginalize Q. See Appendix F.4 for the choice of the network structure and hyper-parameters. We tuned the regularizers λ1, λ2 as discussed in Appendix A, with the data evenly split for Stage 1 and Stage 2. We varied the dataset size and ran 20 simulations for each setting. Results are summarized in Figure 3."
2106.03907,"data, dataset, dataset provided, data available",147,2022-05-17,1,"Experiments for Structural Function We present two structural function estimation experiments. One is a demand design experiment based on a synthetic dataset introduced by Hartford et al. [6], which is a standard benchmark for the instrumental variable regression. Here, we modify the data generating process to provide a benchmark for PCL methods. We consider the problem of predicting sales Y from ticket price P , where these are confounded by a potential demand D ∈ [0, 10]. To correct this confounding bias, we use the fuel price (C1, C2) as the treatment-inducing proxy, which has an impact on price P , and the number of views of the ticket reservation page V as the outcome-inducing proxy. Details of the data generation process can be found in Appendix F.1."
2106.03907,dataset,108,2022-05-17,1,"treatment variables. We test this using the dSprite dataset [19], which is an image dataset described by ﬁve latent parameters (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. Based on this, Xu et al. [35] introduced the causal experiment, where the treatment is each ﬁgure, and the confounder is posY. Inspired by this, we consider the PCL setting that learns the same structural functions with nonlinear confounding, which is not possible to handle in the instrumental variable setting. Speciﬁcally, the"
2106.03907,dataset,14,2022-05-17,0,"From this dataset, we generate the treatment variable A and outcome Y as"
2106.03907,dataset,31,2022-05-17,0,5: 6: until convergence 7: Compute ˆu(θ(t)) from (20) 8: Compute mean feature for W using stage 1 dataset
2106.03907,dataset,39,2022-05-17,0,5: 6: until convergence 7: Compute ˆu(θ(t)) from (7) 8: Compute mean feature for W using stage 1 dataset: µθW ← 1 n (cid:18)
2106.03907,dataset,59,2022-05-17,0,"If ˆRS1(H1) → 0 and Corollary 3. Let Assumption 7 hold and κ1, κ2 = 0. ˆRS2 (H2) → 0 in probability as the dataset size increases, ˆh converges to h∗ in probability with respect to (cid:107) · (cid:107)P(A,W )."
2106.03907,dataset,6,2022-05-17,0,E.2 Experiments using Grade Retention dataset
2106.03907,dataset,62,2022-05-17,0,"Proposition 6. [Theorem 3.3 24, with slight modiﬁcation] Let S be a measurable space and H be a family of functions mapping from S to [0, M ]. Given ﬁxed dataset S = (s1, s2, . . . , sn) ∈ S n, the empirical Rademacher complexity is given by"
2106.03907,dataset,8,2022-05-17,0,Table 1: Results of grade retension dataset
2106.03907,"dataset, github, data https",31,2022-05-17,0,"[19] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. URL https://github.com/deepmind/ dsprites-dataset/."
2106.06064,"code, code available",10,2022-05-17,2,Code to reproduce our experiments is available at https:
2106.06064,data,114,2022-04-21,0,"In this work, we model multivariate time-series as random realizations from a nonlinear state-space model, and target Bayesian inference of the hidden states for probabilistic forecasting. The general framework we propose can be applied to univariate or multivariate forecasting problems, can incorporate additional covariates, can process an observed graph, and can be combined with data-adaptive graph learning procedures. For the concrete example algorithm deployed in experiments, we build the dynamics of the state-space model using graph convolutional recurrent architectures. We develop an inference procedure that employs particle ﬂow, an alternative to particle ﬁlters, that can conduct more effective inference for high-dimensional states."
2106.06064,"data, data https",1,2022-05-17,0,tlc-trip-record-data.page
2106.06064,"data, dataset",102,2022-05-17,0,"Point forecasting results on non-graph datasets : We evaluate our proposed ﬂow-based RNN on the Electricity and Trafﬁc datasets, following the setting described in Appendix C.4 in (Oreshkin et al., 2020). We augment the results table in (Oreshkin et al., 2020) with the results from an FC-GRU (a fully connected GRU encoder-decoder) and GRU+ﬂow. We use a 2 layer GRU with 64 RNN units in both cases. We follow the preprocessing steps in (Oreshkin et al., 2020). In the literature, four different data splits have"
2106.06064,"data, dataset",157,2022-05-17,0,"Spatio-temporal forecasting has numerous applications in analyzing wireless, trafﬁc, and ﬁnancial networks. Many classical statistical models often fall short in handling the complexity and high non-linearity present in time-series data. Recent advances in deep learning allow for better modelling of spatial and temporal dependencies. While most of these models focus on obtaining accurate point forecasts, they do not characterize the prediction uncertainty. In this work, we consider the time-series data as a random realization from a nonlinear state-space model and target Bayesian inference of the hidden states for probabilistic forecasting. We use particle ﬂow as the tool for approximating the posterior distribution of the states, as it is shown to be highly effective in complex, high-dimensional settings. Thorough experimentation on several real world time-series datasets demonstrates that our approach provides better characterization of uncertainty while maintaining comparable accuracy to the state-of-theart point forecasting methods."
2106.06064,"data, dataset",310,2022-05-17,1,"Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets. Some qualitative visualization of the conﬁdence intervals for 15-minute ahead predictions for the PeMSD3, PeMSD4, PeMSD7, and PeMSD8 datasets are shown in Figures 7, 8, 9, and 10 respectively. We observe that the conﬁdence intervals from the proposed algorithm are considerably tighter compared to its competitors in most cases, whereas the coverage of the ground truth is still ensured."
2106.06064,"data, dataset",344,2022-05-17,1,"In Table 1 of the main paper, we report the average MAE of the top 10 algorithms. The detailed comparisons in terms of MAE, MAPE, and RMSE with all the baseline algorithms on the four PeMS datasets are provided in Tables 10, 11, 12, and 13. We observe that statistical models such as HA, ARIMA, and VAR and basic machine learning models such as SVR, FNN, and FC-LSTM show poor predictive performance as they cannot model the complex spatio-temporal patterns present in the real world trafﬁc data well. Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets."
2106.06064,"data, dataset",47,2022-05-17,0,"both for the ground-truth test data, and samples of forecasts, and then computing the (normalized) CRPS on the summed data. The results are summarized in Table 7. We observe that the proposed GRU+ﬂow achieves the lowest CRPSsum for all datasets."
2106.06064,"data, dataset",71,2022-05-17,0,"test split is set at 70/10/20% chronologically and standard normalization of the data is used as in (Li et al., 2018). We use one hour of historical data (P = 12) to predict the trafﬁc for the next hour (Q = 12). Graphs associated with the datasets are constructed using the procedure in (Huang et al., 2020)."
2106.06064,"data, dataset",90,2022-05-17,0,"We address the task of discrete-time multivariate time-series prediction, with the goal of forecasting multiple time-steps ahead. We assume that there is access to a historical dataset for training, but after training the model must perform prediction based on a limited window of historical data. Let yt ∈ RN ×1 be an observed multivariate signal at time t and Zt ∈ RN ×dz be an associated set of covariates. The i-th element of yt is the observation associated with time-series i at time-step t."
2106.06064,"data, dataset, data available",113,2022-05-17,0,"5.2. Inference We assume that a dataset Dtrn is available for training. Although this data may be derived from a single time-series, because our task is to predict yt0+P +1:t0+P +Q using a limited historical window yt0+1:t0+P , we splice the time-series and thus construct multiple training examples, denoted by (y(m) P +1:P +Q). In the training set, all of these observations are available; in the test set yP +1:P +Q are not. In addition, the associated covariates z1:P +Q are known for both training and test sets."
2106.06064,"data, dataset, data available",168,2022-05-17,0,"In Table 6, we observe that the ﬂow based approach performs comparably or better than the state-of-the-art NBEATS algorithm for the Electricity dataset, even with a simple GRU as the state transition function. The better performance of the univariate N-BEATS compared to the multivariate models suggests that most time-series in these datasets do not provide valuable additional information for predicting other datasets. This is in contrast to the graphbased datasets, where the performance of N-BEATS was considerably worse than the multivariate algorithms. The proposed ﬂow-based algorithm achieves prediction performance on the Trafﬁc dataset that is comparable to N-BEATS except for one split with limited training data. Across all datasets and split settings, our ﬂow-based approach signiﬁ cantly outperforms the FC-GRU. The proposed algorithm outperforms TRMF, DeepAR, DeepState and DeepGLO. It outperforms DeepFactors for the Electricity dataset, but is worse for the Trafﬁc dataset (for the same split with limited available training data)."
2106.06064,"data, dataset, publicly available",125,2022-05-17,0,"6.1. Datasets We evaluate our proposed algorithm on four publicly available trafﬁc datasets, namely PeMSD3, PeMSD4, PeMSD7 and PeMSD8. These are obtained from the Caltrans Performance Measurement System (PeMS) (Chen et al., 2000) and have been used in multiple previous works (Yu et al., 2018; Guo et al., 2019; Song et al., 2020; Bai et al., 2020; Huang et al., 2020). Each of these datasets consists of the trafﬁc speed records, collected from loop detectors, and aggregated over 5 minute intervals, resulting in 288 data points per detector per day. In non-graph setting, we use Electricity (Dua"
2106.06064,dataset,1,2022-05-17,0,Dataset
2106.06064,dataset,1,2022-05-17,0,gluon-ts/tree/mv_release/datasets
2106.06064,dataset,10,2022-05-17,0,Table 9. Summary statistics of the multivariate non-graph datasets
2106.06064,dataset,12,2022-05-17,0,Dataset Algorithm 11.41/13.11/14.62/16.27 DeepAR 14.16/15.87/17.59/18.99 DeepFactors GRU+ﬂow 11.23/12.70/13.98/15.25 DCGRU+ﬂow 11.21/12.14/12.87/13.64 AGCGRU+ﬂow 10.53/11.39/12.03/12.47
2106.06064,dataset,127,2022-05-17,0,"Probabilistic forecasting results on non-graph datasets : For comparison with state-of-the-art deep learning based probabilistic forecasting methods on standard non-graph time-series datasets, we evaluate the proposed GRU+ﬂow algorithm following the setting in (Rasul et al., 2021). The results reported in Table 1 of (Rasul et al., 2021) are augmented with the results of the GRU+ﬂow algorithm. We use a 2 layer GRU with 64 RNN units in each case. We follow the preprocessing steps as in (Salinas et al., 2019; Rasul et al., 2021). The evaluation metric is (normalized) CRPSsum (deﬁned in the supplementary material), which is obtained by ﬁrst summing across the different time-series,"
2106.06064,dataset,141,2022-05-17,0,"In this experiment, we compare the proposed state-space model with different learnable noise variance at each node (parameterized by the softplus function in eq. (9) in the main paper with ﬁxed and uniform noise standard deviation γ = 0.01/0.05/0.10 at all nodes. Other hyper-parameters and the training setup remain unchanged. The results in Table 18 demonstrate that the learnable noise variance approach is not particularly beneﬁcial in comparison to a uniform, ﬁxed variance approach in most cases. However, we note that the probabilistic metrics reported in Table 19 are the lowest for the learnable noise variance model in all cases. This suggests that different time-series in these road trafﬁc datasets have different degrees of uncertainty which cannot be effectively modelled by the uniform, ﬁxed noise variance approach."
2106.06064,dataset,15,2022-05-17,0,Table 8. Summary statistics of the PeMS road trafﬁc datasets PeMSD3 PeMSD4 PeMSD7 PeMSD8
2106.06064,dataset,17,2022-05-17,1,We perform experiments on four graph-based and four nongraph based public datasets to evaluate proposed methods.
2106.06064,dataset,177,2022-05-17,0,"posed AGCGRU+ﬂow algorithm achieves on par or better performance with the best-performing spatio-temporal models, such as GWN, GMAN and AGCRN. We present a comparison of the average rankings across datasets in Figure 3. Our proposed method achieves the best average ranking and signiﬁcantly outperforms the baseline methods. Table 3 summarizes the results for probabilistic forecasting. We observe that in most cases, the proposed ﬂow based algorithms outperform the competitors. MQRNN also shows impressive performance in predicting the forecast quantiles, as it is explicitly trained to minimise the quantile losses. In particular, comparison of GRU+ﬂow with the DeepAR model reveals that even without a sophisticated RNN architecture, the particle ﬂow based approach shows better characterization of prediction uncertainty in most cases. Figure 4 provides a qualitative comparison of the uncertainty characterization, showing example conﬁdence intervals for 15-minute ahead prediction for the PeMSD7 dataset. We see that the proposed algorithm provides considerably tighter intervals, while still achieving coverage of the observed values."
2106.06064,dataset,2,2022-05-17,0,Dataset Algorithm
2106.06064,dataset,2,2022-05-17,0,PeMS datasets
2106.06064,dataset,27,2022-05-17,0,"6.2. Preprocessing For the PeMS datasets, missing values are ﬁlled by the last known value in the same series. The training, validation and"
2106.06064,dataset,28,2022-05-17,0,"For the experiments on the PeMS road trafﬁc datasets, we compare the proposed AGCGRU+ﬂow algorithm with four different classes of forecasting techniques, listed as follows:"
2106.06064,dataset,283,2022-05-17,0,"6.4. Hyperparameters and training setup For our model, we use an L = 2 layer AGCGRU (Bai et al., 2020) as the state-transition function. The dimension of the learnable node embedding is de = 10, and the number of RNN units is dx = 64. We treat ρ and σ as ﬁxed hyperparameters and set ρ = 1 and σ = 0 (no process noise). We train for 100 epochs using the Adam optimizer, with a batch size of 64. The initial learning rate is set to 0.01 and we follow a decaying schedule as in (Li et al., 2018). Hyperparameters associated with scheduled sampling (Bengio et al., 2015), gradient clipping, and early stoppng are borrowed from (Li et al., 2018). We set the number of particles Np = 1 during training and Np = 10 for validation and testing. The number of exponentially spaced discrete steps (Li & Coates, 2017) for integrating the ﬂow is Nλ = 29. For each dataset, we conduct two separate experiments minimizing the training MAE (results are used to report MAE, MAPE, RMSE, and P50QL) and the training negative log posterior probability (results are used to report CRPS, P10QL, and P90QL). We also experiment with alternative state transition functions, including the DCGRU (Li et al., 2018) and GRU (Chung et al., 2014). For these, the hyperparameters are ﬁxed to the same values as presented above."
2106.06064,dataset,3,2022-05-17,0,THE PEMS DATASETS
2106.06064,dataset,3,2022-05-17,1,https://archive.ics.uci.edu/ml/datasets/
2106.06064,dataset,32,2022-05-17,0,"Table 25. Execution time, memory consumption (during training) and model size for AGCRN-ensemble, GMAN-ensemble and AGCGRU+ﬂow for the four PeMS datasets. Lower numbers are better."
2106.06064,dataset,36,2022-05-17,0,Table 6. Normalized Deviation on Electricity and Trafﬁc datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better.
2106.06064,dataset,38,2022-05-17,0,Figure 3. Boxplot of ranks of the top 10 algorithms across the four trafﬁc datasets. The means of the ranks are shown by the black triangles; whiskers extend to the minimum and maximum ranks.
2106.06064,dataset,40,2022-05-17,0,Figure 6. Scatter-plots of average MAE at each node for AGCGRU+ﬂow v.s. that of AGCRN on PeMS datasets. The AGCGRU+ﬂow has lower average MAE compared to AGCRN at most of the nodes for all four datasets.
2106.06064,dataset,40,2022-05-17,0,"Table 7. Average CRPSsum for Electricity, Trafﬁc, Taxi, and Wikipedia datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better"
2106.06064,dataset,42,2022-04-21,0,"3) we show that the proposed method provides a superior characterization of the prediction uncertainty compared to existing probabilistic multivariate time-series forecasting methods, both for datasets where a graph is available and for settings where no graph is available."
2106.06064,dataset,42,2022-05-17,0,"Table 10. Average MAE, MAPE and RMSE for PeMSD3 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 11. Average MAE, MAPE and RMSE for PeMSD4 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 12. Average MAE, MAPE and RMSE for PeMSD7 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,2022-05-17,0,"Table 13. Average MAE, MAPE and RMSE for PeMSD8 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,49,2022-05-17,0,"Figure 4. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at node 4 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 10. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 1, 17, 95, and 164 of PeMSD8 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 7. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 37, 54, 100, and 187 of PeMSD3 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 8. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 2, 44, 57, and 213 of PeMSD4 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,2022-05-17,0,"Figure 9. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 43, 108, 163, and 201 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,64,2022-05-17,0,"For spatio-temporal predictions using the graph-based recurrent architectures, this can be done if the graph can be partitioned meaningfully. For non-graph datasets, we can use the cross-correlation among different time-series to group them into several lower-dimensional problems. Alternatively, we can train a univariate model based on all the time-series as in (Rangapuram et al., 2018)."
2106.06064,dataset,7,2022-05-17,0,10. Description and statistics of datasets
2106.06064,dataset,78,2022-05-17,0,"& Graff, 2017) (hourly time-series of the electricity consumption), Trafﬁc (Dua & Graff, 2017) (hourly occupancy rate, of different car lanes in San Francisco), Taxi (Salinas et al., 2019), and Wikipedia (Salinas et al., 2019) (count of clicks to different web links) datasets. The detailed statistics of these datasets are summarized in the supplementary material."
2106.06064,dataset,82,2022-05-17,0,The statistics of the PeMS datasets and the non-graph datasets used in our experiments are summarized in Tables 8 and 9 respectively. The description of the PeMS datasets are provided in Section 6.1 of the main paper. The Electricity dataset contains electricity consumption for 370 clients. The Trafﬁc dataset is composed of 963 time-series of lane occupancy rates. The Taxi dataset contains counts of taxis on different roads and the Wikipedia dataset speciﬁes clicks to web links.
2106.06064,dataset,9,2022-05-17,0,Dataset No. nodes No. time steps Interval
2106.06064,dataset,92,2022-05-17,0,"If our focus is on obtaining a point estimate, then we can perform optimization on the training set with respect to a loss function derived from Mean Absolute Error (MAE) or Mean Square Error (MSE). The point forecast ˆy(m) P +1:P +Q is obtained based on a statistic such as the mean or median of the samples {yj,(m) j=1. The MAE loss function on a dataset indexed by D can then be expressed as:"
2106.06064,dataset,93,2022-05-17,0,"Table 25 summarizes the run time, GPU usage during training, and the size of the learned model for AGCRN-ensemble, GMAN-ensemble, and the proposed AGCGRU+ﬂow for the four PeMS datasets. We observe that if we choose the ensemble size so that the algorithms have an approximately equal execution time, then the model-size of the ensemble algorithms are comparable to our approach as well. However, our method requires more GPU memory compared to the ensembles during training because of the particle ﬂow in the forward pass."
2106.06064,dataset,93,2022-05-17,0,"The novel contributions in this paper are as follows: 1) we propose a graph-aware stochastic recurrent network architecture and inference procedure that combine graph convolutional learning, a probabilistic state-space model, and particle ﬂow; 2) we demonstrate via experiments on graph-based trafﬁc datasets that a speciﬁc instantiation of the proposed framework can provide point forecasts that are as accurate as the state-of-the-art deep learning based spatio-temporal models. The prediction error is also comparable to the existing deep learning based techniques for benchmark non-graph multivariate time-series datasets;"
2106.06064,"dataset, code, publicly available",152,2022-05-17,0,"6.5. Results and Discussion Comparison with baselines : Results for the point forecasting task are summarized in Table 1. We observe that most of the spatio-temporal models perform better than graph agnostic baselines in most cases. Moreover, the pro Some of the recent spatio-temporal models such as (Chen et al., 2020; Zhang et al., 2020; Park et al., 2020) do not have publicly available code. Although the codes for (Wu et al., 2020; Song et al., 2020; Pan et al., 2019) are available, these works use different datasets for evaluation. We could not obtain sensible results from these models for our datasets, even with considerable hyperparameter tuning. The code for (Kurle et al., 2020; de B´ezenac et al., 2020) is not publicly available."
2106.06064,"dataset, used dataset",29,2022-05-17,0,"been used for the Electricity dataset, and three different splits have been used for the Trafﬁc dataset. The evaluation metric is P50QL (Normalized Deviation)."
2106.06064,github,1,2022-05-17,0,//github.com/networkslab/rnn_flow
2106.06064,github,3,2022-05-17,0,https://github.com/mbohlkeschneider/
2106.14178,"data, code, publicly available, code available",122,2022-05-17,2,"In this work, we proposed a novel residual moment loss function to extract location information in medical image segmentation. Motivated by image moments, we explicitly encoded the coordinate information of pixels (or voxels) to the RM loss, which is simple but can capture the target location eﬀectively. In addition, our method is also easy to optimize with high computational eﬃciency. The experimental results demonstrated that our method could be adapted to various data types and network architectures. The method can also be easily embedded into other network training strategies and used in diﬀerent practical problems, which will be investigated in our future research. Source code will be publicly available."
2106.14178,dataset,128,2022-05-17,1,"The LA dataset contains 100 training cases and 54 testing cases. Following the experimental setting in [10], we randomly selected 16 cases for training and 20 cases for testing for a fair comparison with [10]. We cropped all cases centering at the heart region to alleviate the problem of unbalanced categories. All cases are normalized by subtracting the mean and dividing by the standard deviation. We use 3D V-Net [12] as the baseline which is optimized by SGD with the 0.01 learning rate. To prevent overﬁtting, we use dropout during training but turn it oﬀ in the inference stage. Besides, the RM loss weight is α = 0.01 in Eq. 6."
2106.14178,dataset,19,2022-05-17,0,Table 2. Left atrial segmentation accuracy with mean (standard deviation) on the LA MRI dataset.
2106.14178,dataset,2,2022-05-17,0,Dataset Method
2106.14178,dataset,20,2022-05-17,0,Table 1. Optical cup and disk segmentation accuracy with mean (standard deviation) on the GS dataset.
2106.14178,dataset,39,2022-05-17,0,"Fig. 3. Visualization of the left atrial segmentation results on the LA MRI dataset. The ﬁrst column is ground-truth. The second and third columns are the results of baseline and our method, respectively."
2106.14178,dataset,43,2022-05-17,0,"To verify the eﬀectiveness and generalization of our method, we apply the residual moment loss to 2D and 3D neural networks with various public datasets. All our experiments are implemented with the PyTorch (1.3.0) framework [14]."
2106.14178,dataset,45,2022-05-17,1,"We evaluate our method on two datasets, which are the 2D optic cup and disk segmentation dataset: Drishti-GS (GS) [16] and the left atrial (LA) 3D gadolinium-enhanced magnetic resonance imaging (MRI) [19]."
2106.14178,dataset,55,2022-05-17,0,"16. Sivaswamy, J., Krishnadas, S., Joshi, G.D., Jain, M., Tabish, A.U.S.: Drishti-GS: Retinal image dataset for optic nerve head (ONH) segmentation. In: IEEE 11th International Symposium on Biomedical Imaging. pp. 53–56. IEEE (2014)"
2106.14178,dataset,68,2022-05-17,0,"Fig. 2. The optic cup and disk results of the GS dataset. The green and blue lines represent the contours of ground-truth and the segmentation results, respectively. The ﬁrst row is the results of the baseline and the second row is our method, while the ﬁrst two columns are the cup results and the last two columns are the disk results."
2106.14178,dataset,77,2022-05-17,0,"For the GS dataset, we use 50 images for training and 50 for testing. These images are resized to 512 × 512 for computational eﬃciency. We apply the 2D U-Net [15] as the baseline and set the weight of RM loss in Eq. 6 as α = 1. The model is trained by the stochastic gradient descent (SGD) with the learning rate of 0.001 for 2000 iterations."
2106.14178,dataset,87,2022-05-17,0,"Our method can be easily extended to 3D. Table 2 and Fig. 3 show the quantitative and qualitative results of the LA dataset. Here, we compare with some implicit position embedding methods, which are boundary loss [8], Hausdorﬀ distance loss [7] and signed distance function loss [20]. The experimental results of these three comparison methods are directly taken from Ma et al. [10] and our experimental settings followed their work."
2106.14178,"dataset, publicly available",228,2022-05-17,1,"Abstract. Location information is proven to beneﬁt the deep learning models on capturing the manifold structure of target objects, and accordingly boosts the accuracy of medical image segmentation. However, most existing methods encode the location information in an implicit way, e.g., the distance transform maps, which describe the relative distance from each pixel to the contour boundary, for the network to learn. These implicit approaches do not fully exploit the position information (i.e., absolute location) of targets. In this paper, we propose a novel loss function, namely residual moment (RM) loss, to explicitly embed the location information of segmentation targets during the training of deep learning networks. Particularly, motivated by image moments, the segmentation prediction map and ground-truth map are weighted by coordinate information. Then our RM loss encourages the networks to maintain the consistency between the two weighted maps, which promotes the segmentation networks to easily locate the targets and extract manifold-structure-related features. We validate the proposed RM loss by conducting extensive experiments on two publicly available datasets, i.e., 2D optic cup and disk segmentation and 3D left atrial segmentation. The experimental results demonstrate the eﬀectiveness of our RM loss, which signiﬁcantly boosts the accuracy of segmentation networks."
2107.05429,"data, dataset",113,2022-05-17,1,"In order to test the performance under various unknown noise, we also used the test set from WSJ-0 [29] as the test speech. It contains 651 utterances from 8 speakers. There are two noise datasets used for test; one is the music data from MUSAN [30], the other is babble, factory1 and f16 from NOISEX92 [31]. The SNR range of the test noisy speech is the same as the training set. We also evaluated the model on the development test set and blind test set provided by DNS challenge. All the audio used is sampled at 16kHz."
2107.05429,database,51,2022-05-17,0,"[31] A. Varga and H. Steeneken, “Assessment for automatic speech Ii. noisex-92: A database and an experiment to recognition: study the effect of additive noise on speech recognition systems,” Speech Communication, vol. 12, pp. 247–251, 1993."
2107.05429,database,52,2022-05-17,0,"[27] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,” The Journal of the Acoustical Society of America, vol. 133, p. 3591, 2013."
2107.05429,dataset,112,2022-05-17,1,"We trained the DPCRN on the Interspeech 2021 DNS challenge dataset. 60000 clips of reverberant speech (about 500 h) were generated, with 55000 clips for training and 5000 for validation. The noise clips were mainly generated from Audioset [26], DEMAND [27] and Freesound1. In training stage, we randomly split the waves into 5-second segments and convolved them with room impulse responses (RIRs) randomly-selected from openSLR26 and openSLR28 [28]. Then the noisy speech was generated by mixing reverberant speech and noise. The SNR range of the mixture is set between -5 and 5 dB."
2107.05429,dataset,115,2022-05-17,1,"The widespread noise and reverberation may seriously degrade the performance of automatic speech recognition (ASR) systems and decrease speech intelligibility in communication. Speech enhancement aims at separating clean speech from background interference for higher speech intelligibility and perceptual quality. Despite the rapid progress of DNN-based speech enhancement recently, its performance in real applications still faces the challenges such as low signal-to-noise ratio (SNR), high reverberation and far-ﬁeld pickup. The Interspeech 2021 deep noise suppression (DNS) challenge [1] is organized to foster more competitive speech enhancement system in adverse environments, and training datasets and evaluation metrics are provided for such purpose."
2107.05429,dataset,119,2022-05-17,1,"convolutional layers. The CRM is output from the last transposed convolutional layer. We evaluate the DPCRN on the Interspeech 2021 DNS challenge dataset. Experimental results show that the DPCRN outperforms the baseline models, including NSNet2 [17], DTLN [18] and DCCRN [13]. On simulated test datasets, our model achieves competitive results as baseline models and show better performance in the case of low SNR. With only 0.8M parameters, our model achieves an overall MOS of 3.57 according to the ITU-T P.835 [19] subjective evaluation on DNS challenge blind test set, and reaches the third place in the wide band scenario track."
2107.05429,dataset,131,2022-05-17,0,"The performance on simulated WSJ0-MUSAN test set is presented in Table 1. It can be seen that when the SNR is greater than or equal to 0 dB, the performance of DPCRN-1 is slightly weaker than DCCRN, but better than DTLN. It should be noted that DPCRN-1 performs better than DCCRN at lower SNR. Table 2 shows the results on WSJ0-NOISEX92 test set. Under more disruptive noise from NOISEX92, the DPCRN-1 exceeds the baseline models in terms of all three metrics, demonstrating the beneﬁt of the DPRNN module for spectrogram modeling. On both datasets, DPCRN-2 has better performance than DPCRN-1 in terms of PESQ and STOI but its SDR is slightly worse, indicating that including the time-frequency MSE in the"
2107.05429,dataset,3,2022-05-17,0,3.1. Datasets
2107.05429,dataset,68,2022-05-17,0,"[26] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 776–780."
2107.05429,dataset,95,2022-05-17,0,"Inspired by the successful application of DPRNN and CRN, we propose a deep learning-based speech enhancement model in the time-frequency domain, named as DPCRN. It combines the local pattern modeling capability of CNN and the long-term modeling capability of DPRNN. Compared with CRN, DPCRN demonstrates the beneﬁt of RNN for spectrum modeling. With only 0.8M parameters, our model achieves competitive results on various unknown noise datasets. In the future, we will try to reduce the computational complexity of the model for wider band spectrum processing."
2107.09477,"data, dataset, open-source, used dataset",94,2022-05-17,1,"All recognizers were trained with the LibriSpeech dataset [24]. For the multispeaker TTS dataset, we used the “clean” subsets LibriTTS dataset [25], except in the text-based system for task 2, where we merged open-source single-speaker TTS datasets in Finnish [26], German [27], and Mandarin [28], as in [4]. For each of the two tasks, a separate neural vocoder was trained with the training data of the source and target speakers."
2107.09477,"data, dataset, used dataset",115,2022-05-17,1,"We used the VCC2020 dataset [3], which contained two tasks in our evaluation. The data conditions are summarized in Table 1. Both tasks share the same two source English male and female speakers whose data were not used. There were two target male and female speakers of English in task 1 whereas in task 2 there were one male and one female speaker each of Finnish, German, and Mandarin. During conversion, the source speaker’s voice in the source language was converted as if it was uttered by the target speaker while keeping the linguistic contents unchanged. For each target speaker, 70"
2107.09477,"data, open-source",179,2022-05-17,0,"For each system, ﬁve random utterances were chosen for each conversion pair. In the naturalness test, recordings of the target speakers were also included and served as the upper bound. In the similarity test for task 2, following [3], we selected three English recordings and two L2 language recordings as the natural reference for the ﬁve converted utterances. All subjective evaluations were performed using the open-source toolkit [44] that implements the ITU-T Recommendation P.808 [45] for subjective speech quality assessment in a crowd using the Amazon Mechanical Turk (Mturk) and screens the obtained data for unreliable ratings. We recruited more than 100 listeners from the United States and had each sample rated by ﬁve different participants on average. Note that to reduce cost, we eliminated SPT systems that freeze GST in the listening tests, since they yield inferior performance compared with systems that do not freeze GST, as shown in Section 5.1. Audio samples are available online9."
2107.09477,dataset,13,2022-05-17,0,"Speech Datasets for 10 Languages,” 2019, pp. 1566–1570."
2107.09477,dataset,216,2022-05-17,0,"that SPT can be a sub-optimal strategy for prosody modeling in ASR+TTS-based VC. First, the target-speaker-dependent TTS training causes a mismatch between training and conversion, because the speech of the target speaker is used as input to the reference encoder during training but that of the source is used during conversion. A speaker adversarial classiﬁer can alleviate this issue [5, 13] but requires careful hyperparameter tuning. Second, there are scenarios where SPT is not desired, such as emotion VC or accent conversion. In this work, we examine two prosody modeling methods for ASR+TTS-based VC. In addition to SPT, we propose a novel technique, which we refer to as target text prediction (TTP). We borrow the idea from [14] and train a text prediction (TP) module to generate the prosody embedding from the text derived from the source speech. Figure 1b illustrates this process. The TP module is ﬁrst pretrained with a GST-TTS on a multispeaker dataset, and further ﬁne-tuned in a target-speaker-dependent manner. As a result, TTP does not suffer from a mismatch between training and conversion unlike SPT. Our contributions in this work are as follows."
2107.09477,dataset,23,2022-05-17,0,"[27] Munich Artiﬁcial Intelligence Laboratories GmbH, “The MAILABS speech dataset,” 2019, accessed 30 November 2019."
2107.09477,dataset,53,2022-05-17,0,"The ASR and TTS models adopt sequence-to-sequence (seq2seq) structures, which were shown to improve conversion similarity by modeling the long-term dependencies in speech. Note that the two models can be separately trained and thus beneﬁt from advanced techniques and a wide variety of datasets in their own ﬁelds."
2107.09477,dataset,82,2022-05-17,0,"ASR: A multispeaker dataset DASR ensures the speaker independence of the recognizer. TTS: Synthesizer training involves a pretraining and a ﬁne-tuning stage. Pretraining is performed on a multispeaker TTS dataset DTTS, which is followed by ﬁne-tuning on the limited target speaker dataset Dtrg. This is a common practice in building modern neural TTS models, as pretraining ensures stable quality and ﬁne-tuning retains high speaker similarity [21]. Such a training strategy allows for"
2107.09477,github,3,2022-05-17,0,5https://github.com/pytorch/fairseq/tree/master/
2107.09477,github,3,2022-05-17,0,8https://github.com/kan-bayashi/ParallelWaveGAN
2107.09477,github,3,2022-05-17,0,9https://unilight.github.io/Publication-Demos/
2107.09477,github,6,2022-05-17,0,6https://kaldi-asr.org/models/m8 7https://github.com/espnet/espnet/tree/master/
2107.09477,github,8,2022-05-17,0,implementation on ESPnet: https://github.com/espnet/espnet/tree/master/ egs/vcc20
2107.09477,open-source,135,2022-05-17,0,"All synthesizers map their respective inputs to 80-dimensional mel ﬁlterbanks with 1024 FFT points and a 256-point frame shift (16 ms). The x-vector [37] was used as the speaker embedding, and we used the pretrained model provided by Kaldi 6. The average of all x-vectors of the training utterances of each speaker was used during inference. Synthesizers with discrete input including text and VQW2V had a Transformer-TTS architecture [38] with detailed settings [4, 17]. For the BNF-based synthesizer, we adopted the Voice Transformer Network (VTN) [39, 40] and followed the ofﬁcial implementation7. For the neural vocoder, we adopted the Parallel WaveGAN (PWG) [41] and followed the open-source implementation8."
2107.09477,open-source,180,2022-05-17,0,"For task 2, all systems showed performance degradation brought about by SPT, and TTP signiﬁcantly outperformed SPT for all representations. For systems based on frame-level features, TTP could even outperform the baseline and was also comparable to the textbased system. To investigate this gap, we plotted the breakdown per target language in the bottom graphs of Figure 3. We suspect that the relative performance change was again correlated with the text preprocessing, as stated in Section 5.1.2. As in [4], thanks to the open-source community, we utilized G2P tools to convert both English and Mandarin text into phonemes, resulting in a better acoustic model and a larger improvement brought about by TTP. On the other hand, because of the lack of linguistic knowledge, characters were used for Finnish and German, resulting in degradation when combined with TTP. We thus conclude that TTP is an effective method for improving naturalness in task 2, if the input representation is properly processed."
2107.09477,open-source,33,2022-05-17,0,"[44] B. Naderi and R. Cutler, “An Open Source Implementation of ITU-T Recommendation P.808 with Validation,” in Proc. Interspeech, 2020, pp. 2862–2866."
2107.09477,open-source,58,2022-05-17,0,"[30] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, “Espnet-TTS: Uniﬁed, Reproducible, and Integratable Open Source End-toin Proc. ICASSP, 2020, pp. End Text-to-Speech Toolkit,” 7654–7658."
2107.09477,publicly available,108,2022-05-17,0,"The system was implemented using ESPnet, a well-developed opensource end-to-end (E2E) speech processing toolkit [29, 30]. Following [4], the ASR model for the text-based system was based on the Transformer [31–33] with joint CTC/attention loss [34], and a RNNbased language model for decoding4. The ASR model for BNF extraction was based on TDNNF-HMM [35], where we concatenated 40-dimensional MFCCs and 400-dimensional i-vectors as input. For VQW2V, we used the publicly available pretrained model provided by fairseq [36]5, as in [17]."
2108.05075,"code, github",15,2022-05-17,0,"[52] “Code for synthesizing adversarial patch attack,” https://github.com/A-LinCui/"
2108.05075,"code, github",21,2022-05-17,0,"[41] “Code for lgs defense,” https://github.com/metallurk/local_gradients_smoothing. [42] https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial."
2108.05075,"code, github",61,2022-05-17,0,"[44] “Code for patchguard defense,” https://github.com/inspire-group/PatchGuard. [45] X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang, “Interpretable deep learning under fire,” in 29th {USENIX} Security Symposium ({USENIX} Security 20), 2020."
2108.05075,"code, publicly available, github, code available",9,2022-05-17,2,Our code is publicly available at https://github.com/DependableSystemsLab/
2108.05075,"data available, dataset",133,2022-05-17,0,"45%. We do not consider patches of larger size because a 7% patch is already able to achieve very high attack success rates (average 99%) and a larger patch will make the adversarial samples become visually more suspicious. For each patch, we train it for 30 epochs on a training set with 2000 images and evaluate the attack success rate on a separate test set and choose the one with the highest success rate. For the attack evaluation on ImageNette and CelebA, we use the entire test set in each dataset; for ImageNet and Place365, we use 10000 images from the validation set for each. Examples of adversarial samples for each dataset can be found in Fig. 6."
2108.05075,database,43,2022-05-17,0,"[35] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places: A 10 million image database for scene recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017."
2108.05075,database,44,2022-05-17,0,"[32] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE conference on computer vision and pattern recognition."
2108.05075,dataset,1,2022-05-17,0,Dataset
2108.05075,dataset,101,2022-05-17,0,"2.2 Threat Model This work assumes a white-box attacker, who has full knowledge of the victim DNN such as its structure, parameters. We assume however that the attacker has no knowledge of the exact inputs to the DNN, but instead has access to a surrogate dataset, which follows the same distribution as the legitimate inputs. This is similar to the assumptions in universal attack studies, and it is shown that the knowledge of the input distribution often suffices for the attacker to generate universal adversarial perturbations [2, 3, 5]."
2108.05075,dataset,103,2022-05-17,0,"Our evaluation shows that AT’s performance degrades as the number of target classes increases, on both robust accuracy and FP. This is because with more target classes, the learning objective for AT becomes increasingly difficult - this is similar to how common DNNs would yield lower accuracy on a 1000-class dataset than on a simple 10-class dataset. On the other hand, we see that Jujutsu achieves consistently high performance in terms of both robust accuracy and FP across attacks targeting different classes. Further, Jujutsu yields significantly better performance than AT in all cases."
2108.05075,dataset,107,2022-05-17,1,"CelebA. CelebA [34] is a large-scale face dataset with more than 200k celebrity images. We follow the implementation in [36] to first create a 307-class subset from the original dataset, where each class represents a celebrity identity and contains at least 15 images. The resulting dataset contains 4263 training images and 1215 validation images. We use a pre-trained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 30 epoches, and we use the SGD optimizer with an initial learning rate of 0.01 and momentum of 0.9."
2108.05075,dataset,11,2022-05-17,0,ImageNet. ImageNet [32] is 1000-class datasets with high-resolution
2108.05075,dataset,111,2022-05-17,0,"4.1.3 Attack Setup. The attacker’s goal is to synthesize adversarial patches that achieve both high attack success rate and remain stealthy. Therefore, for each dataset, we generate patches of different sizes, occupying 5%, 6% and 7% of the image pixels. We use 𝑥% patch to refer to a patch that occupies 𝑥% of the pixels of the image. We do not consider patches of smaller size because we find that they are unable to universally cause misclassification, e.g., use of a 4% patch on CelebA degraded the attack success rate by more than"
2108.05075,dataset,12,2022-05-17,1,"• Evaluate Jujutsu on 4 datasets (ImageNet, ImageNette, CelebA"
2108.05075,dataset,134,2022-05-17,0,"Benign inputs are the same as the adversarial inputs, except that they do not have the adversarial patch. We consider an image adversarial if and only if the predicted label for it is identical to that of both the hold-out images implanted with the suspicious features. Table 1 shows Jujutsu’s detection performance on all 4 datasets. Detection success recall. Jujutsu is able to consistently detect adversarial samples, with a detection success recall rate of over 93% across patch sizes (in most cases). The detection success recall increases with the size of the patch as a larger patch has a higher attack success rate. On average, Jujutsu can detect around 96% of the adversarial samples on all the datasets."
2108.05075,dataset,139,2022-05-17,1,"4.1.2 Datasets. We evaluate Jujutsu on ImageNet [32], ImageNette [33], CelebA [34] and Place365 [35]. ImageNet is a 1000-class dataset and we use the pre-trained ResNet-50 from the torchvision library. ImageNette is a 10-class subset of ImageNet and we train a ResNet18 on this dataset. CelebA is a facial dataset with diverse celebrity faces. We created a 307-classes subset from the original set and train a ResNet-18 model following [36] to perform identity classification. Place365 is a 365-class dataset containing common natural sceneries (e.g., patio, restaurant) and we use the pre-trained ResNet-50 from [37]. All images are resized to 224*224. We provide more details of the datasets in Appendix A.1."
2108.05075,dataset,160,2022-05-17,0,"Mitigation success recall: While masking alone is able to achieve higher detection recall compared to masking and inpainting when the masking percentage is small, the difference becomes negligible when the masking percentage increases. This is because when the masking percentage is low, the masked images are more likely to have a label different from those of the original images; while the inpainted images are more likely to have the same label as the original image - this is similar to the reason why robust accuracy from masking alone is higher than that from masking and inpainting for 25% masking. However, when the masking percentage increases, both the masked and inpainted images are likely to have labels that are different from that of the original image - thus the difference becomes negligible between both approaches. We see that Jujutsu is highly effective in detecting adversarial samples on all the datasets."
2108.05075,dataset,163,2022-05-17,0,"derivative. The parametric softplus function can be expressed as: 𝑓 (𝑥) = 1 𝛼 log(1 + exp(𝛼𝑥)), where 𝑥 is the original input to the ReLu function, and 𝛼 is the hyper-parameter to control the shape of the curve. We follow Xie et al. [47] to empirically set 𝛼 as 10 in our experiment. Finally, we only use the parametric softplus for backward propagation, and use ReLU for the normal forward pass. To be conservative, we consider the 7% patch, which allows the attacker to inject larger perturbations to evade Jujutsu. We choose 200 samples for training the adversarial patch, 500 steps per sample and 20 epochs in total. For each dataset, we choose 𝛽 ∈ [0.1, 0.5, 1, 5] and choose the one yielding the highest attack success rate."
2108.05075,dataset,207,2022-05-17,0,"(2) Mitigation FPR is the (reduced) FPR from the two-staged combination of detection and mitigation (explained in Section 3.3.3). (3) Mitigation success recall is the detection recall from the combination of detection and mitigation (explained in Section 3.3.3) - we distinguish this from the detection success recall, which is the detection recall from the detection technique alone. Mitigation success recall gives the final amount of adversarial samples detected by Jujutsu. Result. Table 2 shows Jujutsu’s mitigation performance on all 4 datasets. The results are averaged across patches of different sizes (5% to 7%). The detection performance is higher on larger patches as these patches have higher attack success rate (difference between the largest and smallest patch is about 9%) and the mitigation performance is consistent across different patch sizes (differences on robust accuracy and FP are both less than 2%). We consider two mitigation techniques, (1) masking alone, and (2) masking with inpainting (our mitigation technique). We discuss the results in terms of the 3 aforementioned metrics."
2108.05075,dataset,22,2022-05-17,0,Jujutsu is able to detect an average of 95.93% of the adversarial samples with 3.33% FPR on 4 datasets.
2108.05075,dataset,235,2022-05-17,0,"Equation 5 requires several forward and backward passes for calculating the saliency map ˆ𝑀∗ 𝑗 (𝑥), which is much more timeconsuming than the original optimization (Equation 3). Therefore, we reduce the sampling size 𝑛 in Equation 4 from 50 to 5 for faster training. We experimentally verified that the smaller sampling size 𝑛 does not significantly affect the resulting saliency map, and that we can still find all the salient features. Under this setting, it took around 18 days to generate an adversarial patch on Place365 dataset, compared to about 540 days if we had followed our previous setup. Result. We compare the attack success rate of the patches generated from the undefended models and the ones guarded by Jujutsu in Fig. 9. With the protection of Jujutsu, the adaptive attacker who attempts to evade Jujutsu’s detection suffers a significant drop in attack success rate, from 99% to just 4.9% (on average). This is because in Equation 5, the first term aims to increase the influence on the final prediction to manipulate the output label; while the second term reduces the influence on the output. This equation constrains the adaptive attacker, who cannot evade detection without also significantly degrading the attack’s effectiveness."
2108.05075,dataset,30,2022-05-17,0,This section shows Jujutsu’s performance when using different number of hold-out images for attack detection on the ImageNet dataset. The results are presented in Table 9.
2108.05075,dataset,33,2022-05-17,0,"accuracy on ImageNette, as it is a 10-class dataset, and performing correct image classification on this dataset is easier than on the other complicated datasets such as the 1000-class ImageNet."
2108.05075,dataset,36,2022-05-17,0,"When 75% of the perturbations are masked, it is almost infeasible for the attacker to generate a successful adversarial patch, and hence the success rate is near 0% on all datasets."
2108.05075,dataset,45,2022-05-17,0,"We train the rectangular patches on each dataset using a 7% patch (36*96). We change the square bounding box to a rectangular one, which occupies around 20% of pixels as before. Note that we do not"
2108.05075,dataset,48,2022-05-17,1,Place365. Place365 [35] is a large-scale dataset containing 365 unique scene categories. We use the pre-trained ResNet-50 model from [37]. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments.
2108.05075,dataset,49,2022-05-17,0,Detection FPR. Jujutsu yields an average FPR of 3.3% on the 4 datasets. We find that the FPRs on the two object-recognition datasets (ImageNet and ImageNette) are higher than that on the facial and scenery datasets. This is because the salient features in
2108.05075,dataset,56,2022-05-17,0,know the exact width/height ratio of the rectangular shape created by the attacker. Our detection bounding box has a width/height ratio of 6:4. Table 7 shows the results. We find that Jujutsu is able to achieve high detection and mitigation performance on rectangular patches with a very low FPR across different datasets.
2108.05075,dataset,74,2022-05-17,1,"We evaluate Jujutsu on four diverse datasets (ImageNet, ImageNette, CelebA and Place365), and show that Jujutsu achieves superior performance and significantly outperforms existing techniques. We find that Jujutsu can further defend against different variants of the basic attack, including 1) physical-world attack; 2) attacks that target diverse classes; 3) attacks that construct patches in different shapes and 4) adaptive attacks."
2108.05075,dataset,74,2022-05-17,0,"object-recognition datasets might contain the entire object (e.g., a small bird), which can cause the model to continue to assign the same label to the transplanted image. However, for the facial and scenery datasets, the salient features only contain a fraction of the image pixels (e.g., a partial face), which is unlikely to result in the same label on the transplanted image."
2108.05075,dataset,76,2022-05-17,0,"Under the VGGFace2 dataset, Jujutsu achieves a robust accuracy of 37.26%2, which is significantly higher than that of 0.2%3 by Februus. This is because Februus relies on the pre-defined threshold to identify the regions associated with adversarial patch. This method would fail to locate the adversarial patch if the patch’s influence to the prediction is lower than the threshold, and our experiment validates this."
2108.05075,dataset,77,2022-05-17,0,"4.6 RQ5 - Attacks Targeting Different Labels This section evaluates Jujutsu against attacks that target different class labels. For each target label, we need to perform training to generate the universal adversarial patches. Unfortunately, training is highly time-consuming, and hence, for each dataset, we train five 7% patches targeting different labels. Note that training is only needed for creating the adversarial patches, and not for Jujutsu."
2108.05075,dataset,77,2022-05-17,1,"We evaluate Jujutsu on 4 datasets and show that Jujutsu achieves superior detection and mitigation performance, and significantly outperforms existing techniques. We also demonstrate Jujutsu’s effectiveness in defending against different variants of the patch attack, including: 1) physical-world attacks; 2) attacks that use patches in various shapes; 3) attacks that target diverse classes. Finally, Jujutsu can thwart adaptive attacks in fooling the DNNs."
2108.05075,dataset,78,2022-05-17,0,"We also notice that the robust accuracy by Jujutsu on CelebA is lower than that on the other datasets, which is because the inpainting technique needs to synthesize the correct facial features belonging to a particular celebrity’s face to enable correct identity prediction. This is a much more challenging task for image inpainting than for the other three datasets, and hence Jujutsu yields a lower robust accuracy. Jujutsu achieves the highest robust"
2108.05075,dataset,81,2022-05-17,0,"Mitigation FPR: Masking with inpainting achieves low FPR, because the inpainted inputs are more similar to the original benign inputs than the masked inputs (in the latter case many features are simply masked). Therefore predictions on the original and inpainted inputs are more likely to be the same, which is not the case for inputs that are merely masked. We also see that Jujutsu consistently achieves very low FPRs on all the datasets."
2108.05075,dataset,9,2022-05-17,0,Figure 6: Adversarial samples for each dataset.
2108.05075,"dataset, code",151,2022-05-17,0,"Jujutsu has a lower robust accuracy on VGGFace2 than those on the other datasets due to the insufficient performance yielded by the inpainting technique (PICNet [29]). This is because we need to train the PICNet from scratch on VGGFace2, which is very time-consuming as VGGFace2 is a very large dataset. We trained the PICNet on a small subset of the dataset for a week and used it in our evaluation due to time constraint. The performance of Jujutsu can be further improved with more resources to train the PICNet (e.g., increase the size of training set and number of epoches). 3To ensure the code was implemented correctly, we verified that the code was able to reproduce the results reported in the original paper for trojan attack. We then used the code to evaluate against patch attacks."
2108.05075,"dataset, code, github",79,2022-05-17,0,"[48] “Code for strip defense,” https://github.com/garrisongys/STRIP. [49] “Code for februus defense,” https://github.com/AdelaideAuto-IDLab/Februus.git. [50] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “Vggface2: A dataset for recognising faces across pose and age,” in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)."
2108.05075,"dataset, code, github, data https",39,2022-05-17,0,"[37] “Place365 dataset,” https://github.com/CSAILVision/places365. [38] “Code for smoothgrad,” https://github.com/hs2k/pytorch-smoothgrad. [39] “Code for image inpainting technique,” https://github.com/lyndonzheng/"
2108.05075,"dataset, dataset provided",30,2022-05-17,0,A APPENDIX A.1 Dataset Details We provide the details for each of the 4 datasets in our evaluation below. All the images are resized to 224*224.
2108.05075,"dataset, github, data https",12,2022-05-17,0,"[51] “Imagenette dataset,” https://github.com/fastai/imagenette."
2108.05075,"dataset, github, data https",13,2022-05-17,0,"[36] “Celeba dataset,” https://github.com/ndb796/CelebA-HQ-Face-Identity-and Attributes-Recognition-PyTorch."
2108.05075,"dataset, github, data https",53,2022-05-17,0,"[33] “Imagenette dataset,” https://github.com/fastai/imagenette. [34] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of International Conference on Computer Vision (ICCV), December 2015."
2108.05075,"dataset, used dataset",116,2022-05-17,1,"images. We use the pre-trained ResNet-50 model from the torvision library. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments. ImageNette. ImageNette [33] is a 10-class subset of ImageNet dataset, with 9469 training images and 3925 validation images. We used this dataset to compare with adversarial training. We use a pretrained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 20 epoches, and we use the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.0013 and momentum of 0.9."
2108.05075,"python, dataset",192,2022-05-17,0,"4.1.4 Defense Setup. For saliency map generation, we use the implementation from [38]. For image inpainting, we use the implementation from [39] and their pre-trained models for each dataset. The original implementation does not support pixel-wise random masking, and we added this feature. We use cv2.blur from the Python cv2 library with a filter size of 51 to pre-process the saliency map to make it robust to noise. We then identify the point with the highest value in the saliency map as the center of the detection box. The length of the detection box is set to 102, which is around 20% of the pixels in the images. We sample a total of 1000 random images from the test set as the hold-out dataset. For each image, we empirically choose 2 random images from the the hold-out dataset for feature transfer, as we find that this setup balances the detection success recall and FPR. We show in Appendix A.2 Jujutsu’s performance when we use different numbers of random images."
2108.09408,dataset,1,2022-05-17,0,Datasets
2108.09408,dataset,111,2022-05-17,1,"Datasets and implementation details. The model is trained on the DUTSTR with 10553 images. In detail, we trained the model using the SGD optimizer with initial learning rate 3e-5, 0.9 momentum, 5e-4 weight decay, and batch size 16. Because the ResNet-50 parameters are pre-trained on ImageNet, the learning rate of this part is a tenth of the randomly initialized parts which is set as 3e-5. Then, the trained model is tested on ﬁve datasets, including DUTS-TE with 5019 images, DUT-OMROM with 5168 images, HKU-IS with 4447 images, ECSSD with 1000 images and PASCAL-S with 850 images."
2108.09408,dataset,133,2022-05-17,1,"Abstract. Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at diﬀerent scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edgebased U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods."
2108.09408,dataset,334,2022-05-17,0,Datasets mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em Metrics .745 .049 .862 .860 .868 .044 .911 .914 .692 .064 .809 .837 .871 .038 .907 .937 BMPM[30] .751 .059 .839 .861 .889 .059 .893 .914 .713 .062 .814 .846 .874 .045 .888 .931 RAS[3] .785 .057 .834 .867 .914 .040 .910 .929 .747 .063 .815 .850 .893 .036 .895 .939 R3Net[5] PiCANet[13] .749 .051 .867 .852 .885 .044 .917 .910 .710 .065 .835 .834 .870 .039 .908 .934 MLMSNet[26] .799 .045 .856 .882 .914 .038 .911 .925 .735 .056 .817 .846 .892 .034 .901 .945 .777 .051 .854 .869 .906 .042 .912 .920 .736 .066 .824 .853 .882 .037 .903 .940 PAGE[23] .805 .043 .869 .886 .917 .037 .918 .925 .747 .056 .825 .866 .891 .034 .905 .944 CPD[28] .756 .048 .866 .884 .880 .037 .916 .921 .756 .056 .836 .869 .895 .032 .909 .946 BASNet[19] .840 .035 .888 .902 .925 .033 .924 .927 .766 .053 .838 .870 .840 .062 .855 .859 F3Net[24] .799 .040 .879 .881 .910 .042 .917 .921 .739 .055 .832 .858 .885 .032 PoolNet[12] .941 .767 .048 .865 .879 .880 .040 .918 .922 .739 .059 .837 .854 .878 .038 .907 .942 TDBU[22] .815 .039 .875 .891 .920 .041 .918 .927 .755 .052 .818 .867 .898 .031 .918 .948 EGNet[31] .792 .044 .861 .886 .892 .033 .928 .924 .761 .054 .847 .871 .896 .031 .916 .948 U2Net[18] .828 .037 .884 .917 .924 .033 .925 .953 .756 .055 .833 .873 .908 .028 .920 .961 MINet[16] .855 .034 .892 .910 .930 .034 .924 .925 .773 .051 .838 .873 .914 .027 .919 .954 LDF[25] .870 .031 .904 .917 .936 .028 .934 .929 .790 .052 .851 .881 .917 .026 .925 .956 Ours
2108.09408,dataset,35,2022-05-17,0,Table 3. Quantitative comparison with state-of-the-art methods on ﬁve datasets. The best results are highlighted in bold. The best and the second best results are highlighted in red and green respectively.
2108.09408,dataset,46,2022-05-17,0,"• We build an eﬃcient framework to fully combine and fuse edge information, detailed information and semantic clues. Many experiments are conducted to illustrate the validity of our algorithm and this model could surpass most models on four large-scale salient object detection datasets."
2108.09408,dataset,93,2022-05-17,0,"Quantitative comparison. Table. 3 shows the quantitative evaluation results of the SOTA methods mentioned above and our model in terms of mF , M AE, Sm, and Em. The proposed method consistently performs better than all the competitors across four metrics on four datasets. In terms of Em, our method achieves the second best overall performance, which is slightly inferior to MINet. It is worth noting that MEUNet achieves the best performance in terms of the mean F-measure and structure quality evaluation Sm."
2109.03162,python,119,2022-05-17,0,"PYGLAF [2] is implemented in Python and uses CIRCUMSCRIPTINO (http://alviano.com/software/circumscriptino/), a circumscription solver extending the SAT solver GLUCOSE [3]. Linear reductions are used for all semantics [4]. For the ideal extension, the reduction requires the union of all admissible extensions of the input graph; such a set is computed by means of iterative calls to CIRCUMSCRIPTINO. The communication between PYGLAF and CIRCUMSCRIPTINO is handled in the simplest possible way, that is, via stream processing. This design choice is principally motivated by the fact that the communication is often minimal, limited to a single invocation of the circumscription solver."
2109.03162,python,48,2022-05-17,0,"[4] Mario Alviano. Ingredients of the argumentation reasoner pyglaf: Python, circumscription, and glucose to taste. In Marco Maratea and Ivan Serina, editors, RCRA 2017, volume 2011 of CEUR Workshop Proceedings, pages 1–16. CEUR-WS.org, 2017."
2109.03162,python,92,2022-05-17,0,"The PYGLAF reasoner takes advantage of circumscription to solve computational problems of abstract argumentation frameworks. In fact, many of these problems are reduced to circumscription by means of linear encodings, and a few others are solved by means of a sequence of calls to an oracle for circumscription. Within PYGLAF, Python is used to build the encodings and to control the execution of the external circumscription solver, which extends the SAT solver GLUCOSE and implements algorithms taking advantage of unsatisﬁable core analysis and incremental computation."
2109.04359,"data, data available",74,2022-05-17,0,"Operational Modes and Physical Characteristics Each different operational mode causes different types of gears behavior based on wind intensity, torque and vibration frequencies (Table 6). The data available for this analysis based on 10 minutes intervals cannot be used for analysis of dynamic forces for each operational mode; however, can be used for measuring gear ratios, as an indication of gear tooth wear over time."
2109.04359,"data, data https",39,2022-05-17,0,"Yang, W., Court, R., & Jiang, J. (2013). Wind turbine condition monitoring by the approach of SCADA data analysis. Renewable Energy, 53, 365–376. https://doi.org/10.1016/j.renene.2012.11.030"
2109.04359,"data, data https",42,2022-05-17,0,"Liu, X., Lu, S., Ren, Y., & Wu, Z. (2020). Wind Turbine Anomaly Detection Based on SCADA Data Mining. Electronics, 9(5), 751. https://doi.org/10.3390/electronics9050751"
2109.04359,"data, data https",45,2022-05-17,0,"Zaher, A., McArthur, S., Infield, D., & Patel, Y. (2009). Online wind turbine fault detection through automated SCADA data analysis. Wind Energy, 12(6), 574–593. https://doi.org/10.1002/we.319"
2109.04359,"data, data https",47,2022-05-17,0,"Wilkinson, M., Darnell, B., Delft, T., & Harman, K. (2014). Comparison of methods for wind turbine condition monitoring with SCADA data. IET Renewable Power Generation, 8(4), 390–397. https://doi.org/10.1049/iet-rpg.2013.0318"
2109.04359,"data, data https",49,2022-05-17,0,"Du, M., Yi, J., Mazidi, P., Cheng, L., & Guo, J. (2017). A Parameter Selection Method for Wind Turbine Health Management through SCADA Data. Energies, 10(2), 253. https://doi.org/10.3390/en10020253"
2109.04359,"data, data https",51,2022-05-17,0,"Qiu, Y., Chen, L., Feng, Y., & Xu, Y. (2017). An Approach of Quantifying Gear Fatigue Life for Wind Turbine Gearboxes Using Supervisory Control and Data Acquisition Data. Energies, 10(8), 1084. https://doi.org/10.3390/en10081084"
2109.04359,"data, dataset",66,2022-05-17,0,"3. Operational Wind Farm Data The data used in this analysis consist of 5 turbines from EDP Renewables Spain, open data center dedicated to wind energy research. The dataset contains 521, thousand data points of five wind turbines, 2MW production capacity each. The data recordings are for the period of beginning 2016 to end of 2017, 10-minute intervals."
2109.04359,"data, dataset",80,2022-05-17,0,"Clustering Operational Modes Clustering is an unsupervised machine learning technique, used for grouping of data points, which have similar characteristics. For wind turbine analytics the similarities are based on the four identified parameters, which are also closely related to the operational modes of the turbines. The algorithm used in this analysis is Normal Mixture, which is an effective classification algorithm for large datasets with overlapping clusters (Fraley & Raftery, 2007)."
2109.04359,dataset,16,2022-05-17,0,Grid Integration Nacelle Tower Pitch Rotor Meteorological Table 1. Wind turbine components within the dataset
2109.04359,dataset,39,2022-05-17,0,"Using the parameters defined in Figure 1, wind turbine operating modes can be identified using statistical clustering methods. For clustering the wind turbines dataset four parameters illustrated in Figure 1, were selected as the following:"
2109.04359,dataset,55,2022-05-17,1,"9. Acknowledgements Author would like to express gratitude to EDP Renewables Spain, for providing datasets such as wind turbines and solar panels, free of charge available to research community. Also great gratitude to CFREF (Canadian First research Excellent Fund) for providing the opportunity working on green energy technologies."
2109.04359,dataset,76,2022-05-17,0,"As nature of wind is turbulent with rapid fluctuations, a wind turbine may operate in variety of modes within relatively short period of time. Monitoring rotational speed ratio over time, requires consistent operational conditions such as wind speed and torques within the gearbox. This paper also introduces the concept of clustering such as Normal Mixture algorithm for dividing operating datasets into consistent subgroups, which are used for long term monitoring."
2109.04359,dataset,86,2022-05-17,0,"There have been extensive research into gear tooth wear, fatigue, pitting and microcracks conducted with reliability engineers for decades. In order to analyses different types of gear tooth failures, more detailed dataset with higher sampling frequency in addition to oil analysis is required (Zeng et al., 2020). Given the available dataset, analysis of gear ratio change over time is feasible, which is an indication of gear tooth wear (Jiang et al., 1999)."
2109.04359,dataset,94,2022-05-17,0,"Power Curve Analysis Power curves (active power generated versus wind speed) have been used within the wind energy industry for performance measuring and production prediction under different wind regimes (Sohoni et al., 2016). Table 2, illustrates the power curves of the wind turbines within the analysis dataset. Although power curves are similar in shape, they also exhibit differences which analysis of the detailed cause, are out of scope for this paper. As illustrated in Table 2, Turbine T06 exhibits the most clearly"
2109.05366,code,98,2022-05-17,0,"In this work we show that these limitations are not inherent in GPUfs design, and demonstrate how to achieve high performance that is singificantly faster than the original design, and even exceeds the throughput of a commonly-used traditional CUDA-based [8] baseline where files are accessed from the CPU code. The key to our optimization is a GPU-side I/O readahead prefetcher and page cache replacement mechanism. Together they alleviate the GPUfs performance limitations, by adjusting the GPU I/O layer to match the characteristics of the operating system I/O mechanisms and PCIe."
2109.05366,"data, code",154,2022-05-17,0,"I/O prefechers. [15] introduces a new algorithm that makes the minimum number of I/O instructions to service the I/O requests. [11] proposes an I/O signature-based prefetcher which mainly targets to detect the I/O pattern of an application and issues the requests as early as possible. [21] raises the issue of performing more aggressive I/O prefetching in terms of speculation and data size. Our work follows this direction but also considers other factors as well, like the PCIe data transfers, because of the complexity of a heterogeneous CPU-discrete GPU system. [13] pre-executes a fragment of code, via a pre-execution thread that runs at the same time as the main thread, in order to prefetch I/O requests. This solution cannot be efficiently applied for GPUs because it would require code divergence and would raise performance issues."
2109.05366,"data, data available",43,2022-05-17,0,"(6) If the data is not found in the private buffer, then the threadblock issues a read request to the CPU with size equal to GPUfs PAGE_SIZE + PREFETCH_SIZE (PREFETCH_SIZE is static and is defined before execution)."
2109.05366,"data, data available",58,2022-05-17,0,"(4) If there is a page cache miss, the threadblock allocates a new page and searches if the data is available in its private buffer. (5) If the data is in its private buffer, it updates the page cache, returns the data to the user-level buffer and continues execution."
2109.05366,"data, dataset",114,2022-05-17,0,"GPUfs [26] is the recently introduced system infrastructure which allows GPU threads to access files directly from GPU kernels. GPUfs provides standard POSIX-like APIs (e.g., (read()/write()) for GPU threads to perform file I/O, thus reducing the programming complexity, it implements a local page cache in the GPU memory, allows access to very large datasets and enables applications to perform data-driven, e.g., indexed-based, data accesses efficiently. GPUfs passes the I/O requests to the CPU, yet these remain invisible to the programmer, who only uses a standard and convenient I/O abstraction from GPU kernels."
2109.05366,"data, dataset provided",70,2022-05-17,0,We run a simple experiment to evaluate the sequential I/O performance of GPUfs by using it to move 960MB file into GPU memory. The goal is to compare the effective I/O bandwidth that GPUfs can achieve with the bandwidth of reading the same file from the CPU (no data transfer to GPU). We expect that GPUfs would provide full pipe-lining and therefore achieve similar bandwidth.
2109.05366,"data, dataset, data available",71,2022-05-17,0,"Sequential pattern is among the most common. For example, it is employed in deep neural networks [9] inference and training when reading the input dataset [5]. Such sequential access to data is very popular in high performance computing. In addition, I/O benchmark suites for manycore/multi-node architectures [1] consider sequential access as one of the most popular in data-intensive applications."
2109.05366,"data, dataset, used dataset",203,2022-05-17,0,"GPUs are broadly used in I/O-intensive big data applications. Prior works demonstrate the benefits of using GPU-side file system layer, GPUfs, to improve the GPU performance and programmability in such workloads. However, GPUfs fails to provide high performance for a common I/O pattern where a GPU is used to process a whole data set sequentially. In this work, we propose a number of system-level optimizations to improve the performance of GPUfs for such workloads. We perform an in-depth analysis of the interplay between the GPU I/O access pattern, CPU-GPU PCIe transfers and SSD storage, and identify the main bottlenecks. We propose a new GPU I/O readahead prefetcher and a GPU page cache replacement mechanism to resolve them. The GPU I/O readahead prefetcher achieves more than 2× (geometric mean) higher bandwidth in a series of microbenchmarks compared to the original GPUfs. Furthermore, we evaluate the system on 14 applications derived from the RODINIA, PARBOIL and POLYBENCH benchmark suites. Our prefetching mechanism improves their execution time by up to 50% and their I/O bandwidth by 82% compared to the traditional CPU-only data transfer techniques."
2109.05366,database,77,2022-05-17,0,"To evaluate the performance impact of the page size in such applications, we run the Mosaic benchmark [23] which creates an image collage from multiple tiny images fetched at input-dependent location from a large database (19GB). Each tiny image is 4KB. We run the application with GPUfs configured to use 4KB and 64KB pages. We observe that smaller pages result in 45% higher performance compared to 64KB."
2109.1263,"data, data open-source , open-source",328,2022-05-17,0,"includes a huge number of people, responses are commonly very quick. This platform facilitates humans’ fundamental social instincts. Through sharing on Twitter, users can easily express their opinions on everything at any time. Connected friends or followers (on Twitter) immediately obtain the information on the current situations in the lives of people. This, in turn, contributes to another emotion of humans—i.e., the innate need for knowing the current life situations of people. The user interface (UI) of Twitter is not only real-time but also easy to use. It is understood instinctively and naturally -i.e., the Twitter UI has a very intuitive nature. If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes."
2109.1263,"data, data open-source , open-source",346,2022-05-17,0,"If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes. These brands are chosen since they are all located in the USA, and they have a good, appropriate, and active twitter account that can be dug to obtain information, and had many campaigns over the years. Regarding celebrities, almost the same reason was the case, and their reputation and connection to the companies were a major concern. The present study utilized R studio, R language, and Twitter developer API for data mining as it is an open-source instrument, has many useful packages, and has result visualization ability. Appropriate data were gathered by searching hashtags and keywords relating to companies and celebrities. Then, after they were cleansed and made them meaningful, the analysis part was carried out through the two above-mentioned methods."
2109.1263,"data, dataset provided",148,2022-05-17,0,"The current data age is known for the rapid development in the measure of data and information that are electrically collected, stored, provided. A considerable portion of business information is stored in basically-unstructured text documents. According to Merrill Lynch and Gartner, 85% of a corporate datum is captured and stored in an unstructured format. The equivalent study also suggested that the size of such non-structurally-stored information is increasing every 18 months. Since knowledge provides power in the current business world and is derived from information and data, organizations which make eﬀective and eﬃcient use of their text information sources enjoy the crucial knowledge to make better choices, inducing a competitive advantage over the organizations that lag behind. Thus, the need for text analytics and mining becomes part of the all-inclusive view of the current organizations."
2109.1263,"data, dataset provided",162,2022-05-17,0,"Today, as [12] mentioned, ""Big Data” opportunities are a lot and in fact they provide manual ways for illogically dealing with sentiment analysis and add to the need for making robotized devices for the analysis of textually-expressed consumer sentiment. The present study selected Twitter as the case-study social media since it can be seen as an internet-based short message service (SMS) extension. IAs Jack Dorsey, the co-founder and co-creator of Twitter, said: ""...We came across the word ’twitter’, and it was just perfect. The deﬁnition was ’a short burst of inconsequential information,’ and ’chirps from birds’. And that’s exactly what the product was."" Twitter functions as a utility via which individuals can send SMSs throughout the world. It allows for continuously becoming heard and receiving answers. As the audience of Twitter"
2109.1263,"data, package",194,2022-05-17,0,"11- Install Rstem and sentiment packages 12- Make use of the classify_emotion function that returns a class data frame object, consisting of seven columns (including disgust anger, joy, fear, surprise, sadness, and best_ﬁt) and a row for each of the documents 13- Replace such NA values 14- Employ another function, i.e., classify_polarity(), that is provided by the sentiment package, for the classiﬁcation of tweets into two groups, namely neg (negative sentiment) and pos (i.e., positive sentiment) (It should be noted that the overall tweet sentiment is considered as neutral when this ratio is found to be 1) 15- Create consolidated results in a data frame from these two functions 16- Sort and rearrange the data within the frame 17- Generate a single function so that it is used by the tweets of each business and the sentiment can be plotted for each of the businesses 18- Likewise, plot the polarity distribution of the tweets 19- Try to obtain a sense of the tweets’ overall content through the word clouds."
2109.1263,download,108,2022-05-17,0,"8- Download both negative and positive English opinion/sentiment words (nearly 16000 words) 9- Add some industry-speciﬁc and/or particularly emphatic terms, depending on the requirements 10- Generate a function in R that calculates the raw sentiment by the simple matching algorithm by considering the following: # Remove digits, punctuations, and control characters # Convert them all into the lower sentence case # Divide each of the sentences by using space-delimiter words # Obtain the Boolean matches of the words with the negative and positive opinion-lexicon # Obtain the score in the form of the total positive sentiment dedicated from the total negative sentiment"
2110.0282,code,10,2022-05-17,0,E.1. Randomized Powering algorithm. The pseudo-code for estimating
2110.0282,code,72,2022-05-17,0,"E.2. Adaptive rank selection algorithm. The pseudocode for adaptive rank selection by a-priori error estimation is given in Algorithm E.2. The code is structured to reuse use the previously computed Ω and Y , resulting in signiﬁcant computational is estimated from q iterations of the randomized power method E savings. The error (cid:107) (cid:107) U ˆΛU T . on the error matrix A −"
2110.0282,"data, dataset",137,2022-05-17,1,"where H(ˆθ) Rd×d is the Hessian of the loss function at the solution ˆθ, for each datapoint aj. The main computational challenge is computing the inverse Hessian vector product H −1(ˆθ) θl(ˆθ, aj). When n is very large, we can also subsample the data and average (6.1) over the subsample to estimate ALOOCV. Since ALOOCV solves the same problem with several right-hand sides, blocked PCG methods (here, Nystr¨om blocked PCG) are the tool of choice to eﬃciently solve for multiple righthand sides at once. To demonstrate the idea, we perform numerical experiments on ALOOCV for logistic regression. The datasets we use are all from LIBSVM [6]; see Table 5."
2110.0282,"data, dataset",179,2022-05-17,0,"C.3. Kernel Ridge Regression. We converted the binary classiﬁcation problem to a regression problem by constructing the target vector as follows: We assign +1 to the ﬁrst class and -1 to the second class. For multi-class problems, we do one-vs-all classiﬁcation; this formulation leads to multiple right hand sides, so we use block PCG for both methods. We did no data pre-processing except for EMNIST, MiniBooNE, MNIST, and Santander. For EMNIST and MNIST the data matrix was scaled by 255 so that its entries lie in [0, 1], while for MinBooNE and Santander the features were normalized by their z-score. The number of random features, mrf from 103 for j = 1, . . . , 9. For adaptive Nystr¨om PCG we capped the linear grid mrf = j the maximum rank for the preconditioner at (cid:96)max = and used a tolerance of 40 for the ratio ˆλ(cid:96)/nµ on all datasets."
2110.0282,"data, dataset",63,2022-05-17,0,"6.4. Large scale ALOOCV experiments. Table 7 summarizes results for block Nystr¨om PCG and block CG on the larger datasets. When µ = 10−4, block Nystr¨om PCG oﬀers little or no beneﬁt over block CG because the data matrices are very sparse (see Table 5) and the rcv1 problem is well-conditioned (see Table 13)."
2110.0282,"data, dataset",83,2022-05-17,0,"C.1. Ridge regression experiments. Most of the datasets used in our ridge regression experiments are classiﬁcation datasets. We converted them to regression problems by using a one-hot vector encoding. The target vector b was constructed by setting bi = 1 if example i has the ﬁrst label and 0 otherwise. We did no data pre-processing except on CIFAR-10, where we scaled the matrix by 255 so that all entries lie in [0, 1]."
2110.0282,"data, dataset",83,2022-05-17,0,"Table 13: For µ = 10−4 the Hessian is well-conditioned for both datasets, so there is little value to preconditioning. For µ = 10−8, the ill-conditioning of the Hessian increases signiﬁcantly, making preconditioning more valuable. Furthermore, as ALOOCV uses Block PCG on at least several batches of data points, the cost of constructing the preconditioner is negligible compared to the cost of solving the linear systems (see Table 7 in subsection 6.3)."
2110.0282,"data, dataset",87,2022-05-17,0,"6. Applications and experiments. In this section, we study the performance of Nystr¨om PCG on real world data from three diﬀerent applications: ridge regression, kernel ridge regression, and approximate cross-validation. The experiments demonstrate the eﬀectiveness of the preconditioner and our strategies for choosing the rank (cid:96) compared to other algorithms in the literature: on large datasets, we ﬁnd that our method outperforms competitors by a factor of 5–10 (Table 3 and Table 10)."
2110.0282,dataset,1,2022-05-17,0,Dataset
2110.0282,dataset,105,2022-05-17,0,"6.3.2. Experimental overview. We perform two sets of experiments in this section. The ﬁrst set of experiments uses Gisette and SVHN to test the eﬃcacy of Nystr¨om sketch-and-solve. These datasets are small enough that we can factor H(θ) using a direct method. We also compare to block CG and block PCG with the computed Nystr¨om approximation as a preconditioner. To assess the error due to θl(ˆθ, aj). For any putative an inexact solve for datapoint aj, let x(cid:63)(aj) = H −1(θ)"
2110.0282,dataset,11,2022-05-17,0,Table 8: Kernel ridge regression datasets and experimental parameters.
2110.0282,dataset,123,2022-05-17,0,"6.5.3. Experimental results. Tables 9 to 11 summarize the results for the KRR experiments. Table 9 shows that both versions of Nystr¨om PCG deliver better performance than random features preconditioning on all the datasets considered. Nystr¨om PCG also uses less storage. Table 10 shows that Nystro¨om PCG yields better performance than random features PCG on the larger scale datasets when both are restricted to ranks of 1, 000. Table 11 shows the adaptive strategy proposed in subsection 5.4.2 to select (cid:96) works very well. In contrast, it is diﬃcult to choose mrf for random features preconditioning: the authors of [2] provide no guidance except for the polynomial kernel."
2110.0282,dataset,141,2022-05-17,0,"version uses the oracle best value of (cid:96) found by grid search (from the same grid used to select mrf) to minimize the total runtime, and the second is the adaptive algorithm described in subsection 5.4.2. The grid for (cid:96) and mrf is restriced to less than 10, 000 to keep the preconditioners cheap to apply and store. The adaptive algorithm for each dataset was initialized at (cid:96) = 2, 000, which is smaller than 0.05n for all datasets. For 105, we restricted both (cid:96) and mrf to 1, 000, which corresponds the datasets with n to less than 0.01n. We then run both algorithms till they reach the desired tolerance or the maximum number of iterations are reached."
2110.0282,dataset,172,2022-05-17,0,"We now give the details of the random features experiments. For Shuttle-rf we used random features corresponding to a Gaussian kernel with bandwidth parameter σ = 0.75, we set µ = 10−8/n. For smallNORB-rf we used ReLU random features 10−4. For Higgs we normalized the features by their z-score and we with µ = 6 used random features for a Gaussian Kernel with σ = 5 and regularization µ = 10−4. Similarly for YearMSD, we normalized the matrix by their z-score and used random features for a Gaussian kernel with σ = 8 and µ = 10−5. The sketch size for R&T was d, 2d selected from , to prevent the cost of the forming and applying preconditioner } { from becoming prohibitive. We selected the AdaIHS parameter ρ from the same grid used for the regularization path experiments. We also capped the sketch size for AdaIHS for each dataset by the sketch sized used for R&T."
2110.0282,dataset,19,2022-05-17,0,Table 7: ALOOCV: Large datasets. Block Nystr¨om PCG outperforms block CG as µ becomes small.
2110.0282,dataset,31,2022-05-17,0,We run two sets of experiments. For the datasets with n < 105 we run oracle random features PCG against two versions of the Nystr¨om PCG algorithm. The ﬁrst
2110.0282,dataset,32,2022-05-17,0,"Table 6: ALOOCV: Small datasets. The error for a given value of µ is the maximum relative error on 100 randomly sampled datapoints, averaged over 20 trials."
2110.0282,dataset,39,2022-05-17,0,"6.5.1. Background. We brieﬂy review KRR [32]. Given a dataset of inputs R for i = 1, . . . , n and a kernel function R in the associated reproducing kernel Hilbert"
2110.0282,dataset,43,2022-05-17,0,"Table 3: Ridge regression: Nystr¨om PCG versus AdaIHS and R&T PCG. Nystr¨om PCG outperforms AdaIHS and R&T PCG in iteration and runtime complexity for both datasets. Additionally, Nystr¨om PCG requires much less storage."
2110.0282,dataset,54,2022-05-17,0,"For the large scale problems the adaptive algorithm for Nystr¨om PCG was initialized at (cid:96)0 = 500 and is capped at (cid:96)max = 4000. We set the solve tolerances for both algorithms to 10−10. As before, we sample 100 points randomly from each dataset."
2110.0282,dataset,56,2022-05-17,0,"C.2. ALOOCV. The datasets were chosen so that n and d are both large, the challenging regime for ALOOCV. The ﬁrst three datasets are binary classiﬁcation problems, while SVHN has multiple classes. For SVHN we created a binary classiﬁcation problem by looking at the ﬁrst class vs. remaining classes."
2110.0282,dataset,62,2022-05-17,0,"6.2.3. Random features regression. Tables 3 and 4 compare the performance of Nystr¨om PCG, AdaIHS, and R&T PCG for random features regression. Table 3 shows that Nystr¨om PCG performs best on all datasets for all metrics. The most striking feature is the diﬀerence between sketch sizes: AdaIHS and R&T require much"
2110.0282,dataset,62,2022-05-17,0,"The second set of experiments uses the larger datasets real-sim and rcv1.binary and small values of µ, the most challenging setting for ALOOCV. We restrict our comparison to block Nystr¨om PCG versus the block CG algorithm, as Nystr¨om sketchand-solve is so inaccurate in this regime. We employ Algorithm E.2 to construct the preconditioner for block Nystr¨om PCG."
2110.0282,dataset,69,2022-05-17,0,"consider the shuttle-rf dataset (subsection 6.2). The matrix G has dimension 43, 300 × 10, 000, while the preconditioner is based on a Nystr¨om approximation with rank (cid:96) = 800. Figure 1 shows the progress of the residual as a function of the iteration count. Nystr¨om PCG converges to machine precision in 13 iterations, while CG stalls."
2110.0282,dataset,7,2022-05-17,0,Dataset CIFAR-10 Guillermo smallNorb-rf shuttle-rf Higgs-rf YearMSD-rf
2110.0282,dataset,7,2022-05-17,0,Table 2: Ridge regression datasets.
2110.0282,dataset,73,2022-05-17,0,"All datasets either come with speciﬁed test sets, or we create one from a random 80-20 split. The PCG tolerance, σ, and µ were all chosen to achieve good performance on the test sets (see Table 11 below). Both methods were allowed to run for a maximum of 500 iterations. The statistics for each dataset and the experimental parameters are given in Table 8."
2110.0282,dataset,8,2022-05-17,0,Dataset ijcnn1 MNIST Sensorless SensIT MiniBooNE EMNIST-Balanced Santander
2110.0282,dataset,82,2022-05-17,0,"6.5.2. Experimental overview. We use Nystr¨om PCG to solve several KRR problems derived from classiﬁcation problems on real world datasets from [6, 38]. For 2/(2σ2)). We all experiments, we use the Gaussian kernel (cid:107) compare our method to random features PCG, proposed in [2]. We do not compare to vanilla CG as it is much slower than Nystr¨om PCG and random features PCG."
2110.0282,dataset,86,2022-05-17,0,"6.1. Preliminaries. We implemented all experiments in MATLAB R2019a and MATLAB R2021a on a server with 128 Intel Xeon E7-4850 v4 2.10GHz CPU cores 105), every numerical and 1056 GB. Except for the very large scale datasets (n experiment in this section was repeated twenty times; tables report the mean over the twenty runs, and the standard deviation (in parentheses) when it is non-zero. We highlight the best-performing method in a table in bold."
2110.0282,dataset,9,2022-05-17,0,Table 5: ALOOCV datasets and experimental parameters.
2110.05239,"data, data available",87,,0,"Clinicians will typically base diagnosis on several information sources either implicitly or explicitly. Demographic factors such as age can inﬂuence the likelihood of disease prevalence. In this work we investigate the combination of imaging data with related metadata to enhance classiﬁcation performance evaluated by several metrics. We utilise transfer learning due to the limited volumes of data available, comparing the performance with and without metadata. Additionally we repeat the experiments with and without data augmentation during the training of the model."
2110.05239,"data, dataset",155,,0,"Abstract— In this work, we compare the performance of six state-of-the-art deep neural networks in classiﬁcation tasks when using only image features, to when these are combined with patient metadata. We utilise transfer learning from networks pretrained on ImageNet to extract image features from the ISIC HAM10000 dataset prior to classiﬁcation. Using several classiﬁcation performance metrics, we evaluate the effects of including metadata with the image features. Furthermore, we repeat our experiments with data augmentation. Our results show an overall enhancement in performance of each network as assessed by all metrics, only noting degradation in a vgg16 architecture. Our results indicate that this performance enhancement may be a general property of deep networks and should be explored in other areas. Moreover, these improvements come at a negligible additional cost in computation time, and therefore are a practical method for other applications."
2110.05239,"data, dataset provided",58,,0,Extraction is the time to obtain the dK dimensional features from the network processing over 48 CPU cores. Training times refer to time to train the softmax classiﬁer based on either input features from F or H. Times for the unprocessed (left) and augmented (right) data are provided respectively for each case.
2110.05239,dataset,119,,0,"We compare several state-of-the-art deep convolutional neural network architectures for obtaining F. All the networks used here have been pretrained using the ImageNet [22] dataset, and the network weights transferred to the ISIC image dataset. In this conﬁguration, we are using the networks as feature extractors. Speciﬁcally we evaluate alexnet [23], densenet201 [24], resnet50 [25], inceptionresnetv2 [26], vgg16 [27] and googlenet [28] each with and without augmentation added to the input images. To account for the difference in input size to each network, all images are resized to the required dimensions using bi-linear interpolation."
2110.05239,dataset,73,,0,"[11] H. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, and R. M. Summers, “Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35, no. 5, pp. 1285–1298, May 2016."
2110.05239,"dataset, database",117,,0,"A large collection of digital skin images from the International Skin Imaging Collaboration (ISIC) Melanoma Project [20] have been collated, processed and classiﬁed by expert dermatologists. The HAM1000 dataset from the ISIC database contains 10,015 digital images of skin lesions, each belonging to one of eight classes of skin conditions. Additionally the images have associated metadata containing clinical and acquisition information. The clinical ﬁelds contain a small amount of patient information including diagnosis of the images, an example is shown in Table I. Speciﬁcally, these are, age (numerical), sex (categorical) and anatomical site of the lesion (text)."
2110.10422,"data, data available",129,,0,"number of points, the closer is the estimated mean to the ground truth curve. Areas without any data available in their proximity, show higher uncertainty than areas informed by closely located data points. Eﬀective sample size (ESS) is an important measure of the eﬃciency of MCMC sampling (Martino et al., 2017). For example, we have run inference with 1000 warm-up and 1000 posterior MCMC samples for diﬀerent number of data points. Average ESS for the posterior of the function evaluated at the observed points increased together with the number of points, while inference time remained constant. The original GP model displayed the reverse trend: average ESS remained constant, while computation time increased."
2110.10422,"data, data available",130,,0,"Spatially referenced data come in a variety of forms, including exact geographical coordinates such as a latitude and longitude or predeﬁned geographical areal units such as a village, administrative unit or pixel of a raster image. The latter are known as areal unit data, and are found in ﬁelds such as epidemiology, environmental and political science; a variety of relevant methods come under the banner of small-area statistics (Rao and Molina, 2015). There are many motivations for modelling such data, from surveillance program evaluation to identifying environmental risk factors for disease. Small-area statistics are particularly relevant to informing policy decisions, which are often made at the areal unit level (Clements et al., 2006)."
2110.10422,"data, dataset",144,,1,"Scottish lip cancer dataset. The Scottish lip cancer dataset, originally presented by Kemp et al. (1985), has become a benchmark dataset for areal models. It has been used to demonstrate performance and implementations of CAR, ICAR, BYM and its variations (Duncan et al., 2017; Morris et al., 2019). The dataset consists of the observed and expected numbers of cases (y and E, respectively) at 56 counties in Scotland, as well as a covariate measuring the proportion of the population engaged in agriculture, ﬁshing, or forestry (aﬀ ). The covariate is related to exposure to sunlight, which is a risk factor for lip cancer. We model the count data y as following a Poisson distribution with the log-Normal rate λ"
2111.13023,data,43,2022-04-21,0,"[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017. 2"
2111.14671,"code, github, code available",16,2022-04-21,2,"∗Equal contribution 2Download instructions, baselines, and code are available at: https://github.com/RolnickLab/"
2111.14671,"data available, publicly available, open-source, data open-source , dataset",82,2022-05-25,1,"To date, different datasets, setups, and evaluation procedures have made results in this space hard to compare. This can be attributed to the fact that no comprehensive public dataset exists, and the creation of it requires access to, and knowledge of, the relevant climate model. To address these issues and catalyze further work, we introduce a new and comprehensive dataset for the RT problem and open-source it under the Creative Commons license."
2111.14671,"data, dataset",224,2022-05-25,0,"The computational burden of the RT physics motivated early pioneering work to seek out its emulation with shallow multi-layer perceptron (MLP) networks [8, 9, 15, 16], including decadal climate model simulations [17]. More recent work still focuses on using MLPs to emulate (a part of) the RT physics [19, 20, 22, 27, 28]. 2D CNNs have been also used in [19], which however treat the different input variables within the second spatial dimension instead of in the channel dimension. Prior work on such ML emulators, however, employed datasets that simplify Earth (e.g., with Aqua-planet conditions [6, 25, 30]), use a limited subset of climate model variables as predictors [15, 19, 25, 28], use manually perturbed test sets [19, 28], and generally fail to accurately probe the generalization power of ML models [19, 20, 22, 28]. The latter is particularly important, randomly-split test sets [20, 22, 28], and/or test data coming from at most two different years [19, 20, 22] can overestimate the actual skill"
2111.14671,"data, dataset",93,2022-05-25,0,"The present work is closer to [19] that used a small fraction of ERA-Interim data from 1979-85 and 2015-16. Their concern was however, limited, to longwave radiative ﬂux proﬁles for simpliﬁed clear-sky atmospheric conditions (without greenhouse gases like methane). Similarly to [28], they employ a test set that includes manually perturbed atmospheric states. We believe that our dataset that includes data of pre-industrial and future climate drawn from an actual climate model, is more realistic and the better choice."
2111.14671,"data, dataset, code",165,2022-05-25,0,"Our dataset focuses on pristine-sky (no aerosols and no clouds) as well as clear-sky (no clouds) conditions, i.e. it leaves out the most general all-sky condition that includes clouds. These input conditions, which consist of surface properties and proﬁles of pressure, temperature, humidity, and trace gases, were simulated by setting input variables corresponding to clouds (and aerosols for pristine-sky) to zero. These input snapshots, for the respective atmospheric conditions, were then forwarded through CanESM5’s RT physics code. for each atmospheric condition, the outputs are proﬁles of up- and down-welling ﬂuxes for both, shortwave (solar) and longwave (thermal) radiation, plus their respective heating rates. These raw inputs and outputs are stored in separate NetCDF4 ﬁles for each snapshot. All together (for the main dataset of 1979-2014), they amount to over 1.5Tb of data."
2111.14671,"data, dataset, dataset provided",58,2022-05-25,0,We introduce a novel dataset ClimART which aims to provide a comprehensive dataset for parameterization of radiative transfer using ML models. We conduct a series of experiments to demonstrate which models are able to perform well under the inherent structure of atmospheric data in Experiments. Future work for improving upon the current baselines could include:
2111.14671,"data, dataset, publicly available",47,2022-05-25,1,"• ClimART: Climate Atmospheric Radiative Transfer, is the most comprehensive publicly available dataset for ML emulation of weather and climate model parameterizations. It comes with more than 10 million samples, including three subsets of data for evaluating out-of-distribution (OOD) generalization."
2111.14671,"data, python",55,2022-05-25,0,"Using Python, ClimART’s input and target arrays can be accessed as follows (for the example year 2007, and assuming that the user wants to predict longwave heating rates under pristine-sky conditions): # Assume that h5py and numpy are installed and we are in the root data directory."
2111.14671,dataset,111,2022-05-25,0,"surface properties was then passed through CanESM5’s RT physics model in order to collect the corresponding RT output: Shortwave and longwave (up- and down-welling) ﬂux and heating rate proﬁles for pristine- and clear-sky conditions. The resulting NetCDF4 datasets were then processed to NumPy arrays stored in Hdf5 format (one per year), with three distinct input arrays as described in the following subsection, and one output array per potential target variable. We proceeded analogously for the pre-industrial and future climate years, 1850-52 and 2097-99 respectively (see section 4.3.1). More details are available in the Appendix B.1."
2111.14671,dataset,29,2022-04-21,0,"In our experiments we focus on pristine shortwave radiation. Our dataset, however, allows the user to choose the desired target variables based on their needs."
2111.14915,data,15,2022-04-21,0,is trained on data from 2000-2010 to predict gentrifying neighborhoods from 2010-2020. The present
2201.02733,"dataset, github",89,2022-04-21,2,"3.2 Training HDFS Once the train sets have been generated, we deploy HDSF available in HDSF’s authors’ Github repository [8]. We trained HDSF with each of the five training sets (one original and four noisy dataset) over 20 steps, and generated models, on which we performed our tests using a separate test set (Figure 2 (b)). Thereafter we collected loss and accuracy scores for each model obtained for the same test set."
2201.02733,github,15,2022-04-21,0,[8] Hamid Karimi. 2020. HDSF Source Repository. https://github.com/hamidkarimi/
2201.08452,github,15,2022-04-21,0,-- repo_link_a nd _S HA https :// github . com / streamich / memfs
2201.08452,github,21,2022-04-21,0,"5.2 Basic usage This tool can either take JavaScript packages speciﬁed as GitHub repository links, or as npm packages."
2201.08452,github,25,2022-04-21,0,""" repo_link "" : "" https :// github . com / streamich / memfs "" , "" repo_commit_S HA "" : REDACTED FOR LENGTH"
2201.08452,"github, repo",11,2022-04-21,0,More examples are included in the npm-ﬁlter GitHub repo Readme.
2201.08452,"github, repo",12,2022-04-21,0,"To run npm-ﬁlter over GitHub repo links, use the following:"
2201.08452,"github, repo",24,2022-04-21,0,• repo_link: a link to a single GitHub repo to be analyzed • repo_link_and_SHA: link to a GitHub repo followed by a
2201.08452,"github, repo",42,2022-04-21,0,"• repo_list_file: a ﬁle containing a list of GitHub repo links to be analyzed. Each line of the input ﬁle must specify one repo link, with an optional whitespace delimited commit SHA to check the repo out at."
1607.0085,code,33,,0,"MPI, and that as long as number crunching is performed using vectorized expressions, a code may run on thousands of processors at speeds closing in on the optimal low-level codes."
1607.0085,code,40,,0,"For short of space the implementation for the pencil decomposition is not shown here, but it requires about twice the amount of code since the mesh needs to be transformed and distributed twice (along two indices)."
1607.0085,code,50,,0,"M. Verma, A. Chatterjee, K. Reddy, R. Yadav, S. PAUL, M. Chandra and R. Samtaney Benchmarking and scaling studies of pseudo-spectral code Tarang for turbulence simulations, Pramana Journal of Physics, (81), (4) p. 617-629, (2013)"
1607.0085,code,62,,0,"(shown in the cross code listing) uses ufuncs as well and runs approximately as fast as the code shown. Moving this routine to Numba or Cython we can hardcode the loop over the mesh just once and speed-up is approximately a factor of 5. A Numba implementation is shown below from numba import jit, float64 as float"
1607.0085,"code open-source, code, open-source",177,,0,"To further elaborate on the performance of the code, we note that the open source pseudo-spectral C++ solver [Tarang] has been benchmarked on exactly the same computer (Shaheen). Furthermore, Tarang is using the same dealiasing technique and the same 4th order Runge-Kutta integrator as we are, which should open up for direct comparison of computational efﬁciency. In Figure 2 of [Tarang] it is shown that a computational box of size 10243 is running with 512 CPUs at approximately 50 seconds per time step. In the lower panel of Figure 2, we see that the current optimized Cython solver is running the same box (10243) with twice as many CPUs (1024) at approximately 20 seconds per time step. Assuming perfect strong scaling (which may be unfair considering Figure 2 of [Tarang]) this would correspond to 40 seconds per time step using half the number of CPUs, which is actually 20 % faster than Tarang."
1607.0085,"code, publicly available, python",103,,0,"Abstract—Direct Numerical Simulations (DNS) of the Navier Stokes equations is a valuable research tool in ﬂuid dynamics, but there are very few publicly available codes and, due to heavy number crunching, codes are usually written in low-level languages. In this work a ~100 line standard scientiﬁc Python DNS code is described that nearly matches the performance of pure C for thousands of processors and billions of unknowns. With optimization of a few routines in Cython, it is found to match the performance of a more or less identical solver implemented from scratch in C++."
1607.0085,"code, python",13,,0,MASSIVELY PARALLEL IMPLEMENTATION IN PYTHON OF A PSEUDO-SPECTRAL DNS CODE FOR TURBULENT FLOWS
1607.0085,"code, python",13,,0,Massively parallel implementation in Python of a pseudo-spectral DNS code for turbulent ﬂows
1607.0085,"code, python",65,,0,"The Numba code works out of the box and is compiled on the ﬂy by a just-in-time compiler. A Cython version looks very similar, but requires compilation into a module that is subsequently imported back into python. The Cython code below uses fused types to generate code for single and double precision simultaneously. cimport numpy as np ctypedef fused T:"
1607.0085,"data https, data, python",50,,0,"The annual International Workshop on High-Order CFD Methods https://www.grc.nasa.gov/hiocfd/. Reference data: https://www.grc.nasa.gov/wp-content/uploads/sites/22/C3.3_ dataﬁles.zip J. Enkovaara, N. A. Romero, Sameer Shende and J. J. Mortensen, GPAW - massively parallel electronic structure calculations with Python-based software, Procedia Computer Science, 2011."
1607.0085,"data, code, python",189,,0,"Besides the FFTs, the major computational cost of the pseudospectral solver lies in element-wise multiplications, divisions, subtractions and additions that are required to assemble the right hand side of Eq (10). For efﬁciency it is imperative that the NumPy code is vectorized, thus avoiding for-loops that are very expensive in Python. When properly vectorized the element-wise operations are carried out by NumPy universal functions (so called ufuncs), calling compiled C-code on loops over the entire (or parts of) the data structures. When properly set up many arithmetic operations may be performed at near optimal speed, but, unfortunately, complex expressions are known to be rather slow compared to lowlevel implementations due to multiple calls to the same loop and the creation of temporary arrays. The [numexpr] module has actually been created with the speciﬁc goal of speeding up such element-wise complex expressions. Besides numexpr, the most common ways of speeding up scientiﬁc Python code is through [Cython], [Numba] or [weave]."
1607.0085,"data, python",109,,0,"The dynamic loading of Python on supercomputers can be very slow due to bottlenecks in the ﬁlesystem when thousands of processors attempt to open the same ﬁles. A solution to this problem has been provided by the scalable Python version developed by J. [Enkovaara] and used by [GPAW], where CPython is modiﬁed slightly such that during import operations only a single process performs the actual I/O, and MPI is used for broadcasting the data to other MPI ranks. With scalable Python the dynamic loading times are kept at approximately 30 seconds for a full rack (4096 cores)."
1607.0085,"data, python",170,,0,"The regular Python modules numpy.fft, scipy.fftpack and [pyfftw] all provide routines to do FFTs on regular (nondistributed) structured meshes along any given axis. Any one of these modules may be used, and the only challenge is that the FFTs need to be performed in parallel with MPI. None of the regular Python modules have routines to do FFT in parallel, and the main reason for this is that the FFTs need to be performed on a distributed mesh, where the mesh is distributed before the FFT routines are called. In this work we present 3D FFT routines with MPI for both the slab and the pencil decomposition. The FFTs themselves are performed on data local to one single processor, and hence the serial FFT of any provider may be used. All other operations required to perform the 3D FFT are implemented in Python. This includes both transpose operations and an MPI call to the Alltoall"
1607.0085,"github, python",24,,0,https://gitorious.org/scalable-python https://bitbucket.org/mpi4py/ https://github.com/hgomersall/pyFFTW http://www.fftw.org/ https://github.com/pydata/numexpr http://cython.org/ http://numba.pydata.org/ https://github.com/scipy/weave
1607.0085,"github, python",129,,2,"The purpose of this work is to describe a ~100 line pseudospectral DNS solver developed from scratch in Python, using nothing more than NumPy and MPI for Python (mpi4py), possibly optimized with pyFFTW and Cython. It is important to stress that the entire solver is written in Python, this is not simply a wrapper of a low-level number cruncher. The mesh is created and decomposed in Python and MPI communications are implemented using mpi4py. Two popular strategies, slab and pencil, for MPI communications of the three-dimensional Fast Fourier Transform (FFT), required by the pseudo-spectral method, will be described. The entire solver is available online (https://github.com/mikaem/spectralDNS) under the GPL license."
1607.0085,"package, code, python",99,,0,"The [mpi4py] Python package contains wrappers for almost the entire MPI and it has been shown to be able to distribute NumPy arrays at the speed of regular C arrays. The MPI for Python module allows us to write Python code with MPI just like regular low-level languages, but with a much simpler and user-friendly syntax. Since coding is performed like in C, the Python implementation may, as such, be used as an easy to follow, working prototype for a complete low-level implementation in Fortran, C or C++."
1607.0085,python,7,,0,3.1 MPI/MPI for Python (mpi4py)
1607.0085,python,7,,0,3.5 Dynamic loading of Python on supercomputers
1607.0085,python,14,,0,"Index Terms—computational ﬂuid dynamics, direct numerical simulations, pseudo-spectral, python, FFT"
1607.0085,python,17,,0,PROC. OF THE 8th EUR. CONF. ON PYTHON IN SCIENCE (EUROSCIPY 2015)
1607.0085,python,25,,0,"float[:,:,:,:], float[:,:,:,:]), nopython=True)"
1607.0085,python,31,,0,"function. The entire Python implementation of the 3D FFT with MPI for a slab mesh is shown below from pyfftw import fft, ifft, rfft2, irfft2, empty"
1607.0085,python,51,,0,The major challenges one has to deal with when implementing a high performance solver for Eq. (10) in Python are the following • MPI • Mesh decomposition • Three dimensional Fourier transforms with MPI • Vectorization (NumPy ufuncs) • Dynamic loading of Python on a supercomputer
1607.0085,python,53,,0,"Fig. 3: Strong scaling of various versions of the DNS solver. The C++ solver uses slab decomposition and MPI communication is performed through the FFTW library. The top ﬁgure is for a standard scientiﬁc Python solver, whereas the lower ﬁgure has some key routines optimized by Cython."
1607.0085,python,60,,0,"Keys to the efﬁciency of the solver are the mesh decomposition and three dimensional FFT routines, implemented directly in Python using MPI, wrapped through MPI for Python, and a serial FFT module (both numpy.fft or pyFFTW may be used). Two popular decomposition strategies, slab and pencil, have been implemented and tested."
1607.0085,python,61,,0,Two bottlenecks appear in the standard scientiﬁc Python implementation of the pseudo spectral solver. The ﬁrst is the for loops seen in the fftn_mpi/ifftn_mpi functions previously described. The second is the cross product that needs to be computed in Eq. (10). A straight forward vectorized implementation and usage of the cross product is import numpy
1607.0085,python,73,,0,"Fig. 2: Weak scaling of various versions of the DNS solver. The slab decomposition uses 4 · 643 nodes per core, whereas the pencil decomposition uses 2 · 643. The C++ solver uses slab decomposition and MPI communication is performed through the FFTW library. The top ﬁgure is for a standard scientiﬁc Python solver, whereas the lower ﬁgure has some key routines optimized by Cython."
1607.0085,python,76,,0,"In this short paper we will ﬁrst describe the Fourier transformed Navier Stokes equations that are solved for a triply periodic domain. We will then give a brief description of the implementation and show the results of performance tests conducted on a BlueGene/P supercomputer at the KAUST supercomputing laboratory. The performance of the scientiﬁc Python solver, as well as a version optimized with Cython, is compared to a pure C++ implementation."
1607.0085,python,87,,0,"Strong scaling is tested for a computational box of size 5123, for a various number of processors larger than 64. For slab decomposition the maximum number of CPUs is now 512, whereas for pencil 5122 CPUs can be used. The top panel of Figure 3 shows the performance of the scientiﬁc Python solvers. Evidently, the performance is degrading when the number of mesh nodes per CPU becomes lower and the number of processors increases. The main reason for this poor"
1607.0085,python,95,,0,"4 PARALLEL SCALING ON BLUE GENE/P In this section we compare the performance of the solver with a pure C++ implementation on Shaheen, a Blue Gene/P supercomputer at the KAUST supercomputing Laboratory. The C++ solver we are comparing with has been implemented using the Python solver as prototype and the only real difference is that the C++ solver is using the 3D FFT routines from [FFTW] with MPI included. For optimization we are only considering the Cython implementation, because we were not able to install Numba on Shaheen."
1607.0085,python,106,,0,"All known DNS codes (at least to the knowledge of the author) running on supercomputers are implemented in low-level languages like Fortran or C/C++. These languages are known for excellent performance in heavy duty number crunching algorithms, which goes a long way to explain the popularity. Python, on the other hand, is a scripting language known for being very convenient to work with, but as a research tool more aimed at post-processing, visualization or fast prototyping than high performance computing. However, a lesser known fact is that Python is very convenient to program also with"
1607.0085,python,150,,0,"5 CONCLUSIONS In this paper we show that it is possible to write a very good solver for direct numerical simulations of turbulent ﬂows directly in Python, with nothing more than standard modules like NumPy, SciPy and MPI for Python (mpi4py). We also show that it is possible to get a fully competitive solver, that runs with the speed of C on thousands of processors with billions of unknowns, but then it is necessary to move a few computationally heavy routines from NumPy’s ufuncs to Cython or Numba. The current paper discusses only the triply periodic domain, suitable for studying isotropic turbulence. However, the use of Python/Cython for studying turbulence is not limited to only this conﬁguration and work is currently in progress to develop efﬁcient Python/Cython solvers for ﬂows with one or two inhomogeneous directions."
1607.0085,python,225,,0,"of mesh nodes per CPU constant. Since the FFT is known to scale with problem size as N log2 N, and assuming further that FFT is the major cost, the ideal weak scaling computing to log2 N. The upper time should then scale proportional panel of Figure 2, shows the scaling of the scientiﬁc Python solver, both with slab and pencil decomposition, compared also with the C++ solver. The slab solver uses mesh sizes of N = (2, 16, 128, 1024), whereas the pencil solver uses mesh sizes of N = (4, 32, 256, 2048). The scientiﬁc Python solver is evidently 30-40 % slower, but scaling is good - indicating that the MPI communications are performing at the level of C++. The lower panel of Figure 2 shows the performance of the solver when certain routines, most notably the cross product and the for-loop in the routines fftn_mpi/ifftn_mpi, have been computed with Cython. The results show that the Python solver now operates very close to the speed of pure C++, and the scaling is equally good. Note that the largest simulations in Figure 2 are using a computational box of size 20483 - approximately 8 billion mesh nodes."
1704.07709,benchmark,25,,0,"CIFAR-10 benchmark (Krizhevsky & Hinton, 2009) consisting of 32 × 32 color images representing 10 classes. It is split into 50,000"
1704.07709,database,18,,0,"digit recognition tested on mnist database. Vision Computing, 22(12):971–981, 2004."
1704.07709,dataset,1,,0,DATASET
1704.07709,dataset,19,,0,Figure 10. Testing accuracy of proposed IRCNN model against EIN and EIRN on augmented dataset of CIFAR-100.
1704.07709,dataset,151,,0,"improvement with respect to RCNN (Liang & Hu, 2015). In addition, this architecture accelerates the training procedure, which is a concerning issue right now for training large scale deep learning approaches. Furthermore, we empirically investigated our model and determined that it outperforms against both the equivalent model of the Inception Networks and the Inception-Residual Networks. In the future, we would like to improve this model and experiment with large scale implementation using the teacher-student paradigm (Net2Net) on the ImageNet dataset (Chen et al., 2015b). In addition, the propose IRCNN will be tested with advanced activation functions such as ELU (Clevert et al., 2015).Furthermore, from our observation, this new architecture would be able to model context in input videos, which is another future direction for this work."
1704.07709,dataset,286,,0,"SVHN (Netzer et al. 2011) is one of the most challenging datasets for street view house number recognition (Netzer et al., 2011). This dataset contains color images representing house numbers from Google Street View. In this experiment, we have considered the second version, which consists with 32 × 32 color examples. There are 73,257 samples are in the training set and 26,032 samples in testing set. In addition, this dataset has 531,131 extra samples that are used for training purposes. As single input samples of this dataset contain multiple digits, the main goal is to classify the central digit. Due to the huge variation of color and brightness, this dataset is much for difﬁcult to classify compared to the MNIST dataset. In this case, we have experimented with the same model as is used in CIFAR-10 and CIFAR-100. We have used the same preprocessing steps applied in the experiments of RCNN (Liang & Hu, 2015). The experimental results show better recognition accuracy, as shown in Table 1. We have obtained around 1.89% testing errors with IRCNN+SGD and 1.74% errors with IRCNN+LSUV+EVE respectively. It is noted that Local Contract Normalization (LCN) is applied during experiments of MaxOut (Goodfellow et al., 2013), NiN (Lin et al., 2013), DSN (Lee et al., 2015), and Drop Connect (Wan et al., 2013). The drop connection results in (Wan et al., 2013) are based on the average performance of ﬁve networks."
1704.07709,"dataset, benchmark",28,,1,"• Experimental evaluation of the proposed learning models performance against different DCNN architec tures on different benchmark datasets such as MNIST, CIFAR-10, CIFAR-100 and SVHN."
1704.07709,"dataset, benchmark",74,,0,"demonstrates less loss with better recognition accuracy. We have also empirically evaluated the rate of convergence of our proposed IRCNN algorithm compared with traditional EIN and EIRN models. The proposed model converged earlier with much lower model loss compared to EIN and EIRN. The computational cost (in seconds) per epoch of this IRCNN,EIN, and EIRN models for different benchmark datasets are shown in Table 2."
1704.07709,"dataset, benchmark",103,,1,"We have evaluated the proposed IRCNN method (as well as several others for comparison) with a set of experiments on different benchmark datasets: MNIST (Kussul & Baidyk, 2004), Cifar-10 (Krizhevsky & Hinton, 2009), Cifar-100 (Krizhevsky & Hinton, 2009), and SVHN (Netzer et al., 2011). The entire experiment has been conducted on a Linux environment with Keras (Chollet, 2016) and Theano (Bastien et al., 2012) in the Backend running on a single GPU machine with an NVIDIA GTX-980."
1704.07709,"dataset, benchmark",116,,1,"In this paper, we have proposed a new architecture: Inception Recurrent Convolutional Neural Network (IRCNN) for object recognition where we have utilized the power of recurrent techniques for context modulation with the architecture of Inception networks. The experimental results show the promising recognition accuracy compared with different state-of-the-art Deep Convolutional Neural Networks (DCNN) models on different benchmark datasets such as MNIST, CIFAR-10, CIFAR-100, and SVHN. However, when the proposed IRCNN architecture is initialized with LSUV initialization technique, and optimization function of EVE, it achieved an object recognition accuracy of 71.76% on the CIFAR-100 dataset. This is about a 3.5%"
1704.07709,"dataset, benchmark",162,,1,"Deep convolutional neural networks (DCNNs) are an inﬂuential tool for solving various problems in the machine learning and computer vision ﬁelds. In this paper, we introduce a new deep learning model called an InceptionRecurrent Convolutional Neural Network (IRCNN), which utilizes the power of an inception network combined with recurrent layers in DCNN architecture. We have empirically evaluated the recognition performance of the proposed IRCNN model using different benchmark datasets such as MNIST, CIFAR-10, CIFAR100, and SVHN. Experimental results show similar or higher recognition accuracy when compared to most of the popular DCNNs including the RCNN. Furthermore, we have investigated IRCNN performance against equivalent Inception Networks and Inception-Residual Networks using the CIFAR-100 dataset. We report about 3.5%, 3.47% and 2.54% improvement in classiﬁcation accuracy when compared to the RCNN, equivalent Inception Networks, and InceptionResidual Networks on the augmented CIFAR100 dataset respectively."
1704.07709,"dataset, benchmark",325,,0,"The deep learning revolution began in 1998 with (LeCun et al., 1998). From then on, several different architectures have been proposed that have shown massive success using many different benchmark datasets including MNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet, and many more. Of the DCNN architectures, AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), NiN (Lin et al., 2013), the All Convolutional Network (Springenberg et al., 2014), GoogleNet (Szegedy et al., 2015), Inception-v4 (He et al., 2016a; Szegedy et al., 2016a), and Residual Networks(He et al., 2016b) can be considered the most popular architectures due to their improved performance on different benchmarks for object classiﬁcation. In 2012, Alex Krizhevesky et al. proposed an improved version of a CNN model compared to LeNet (LeCun et al., 1998), and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. This was a signiﬁcant breakthrough in the ﬁeld of machine learning and computer vision, as this was the ﬁrst time a deep network outperformed the alternative approaches for visual recognition tasks. GoogleNet, or Inception-v1 (Szegedy et al., 2015), and the Residual Network (He et al., 2016b) won ILSVRC in 2014 and 2015 respectively. Inception architecture has become very popular in the deep learning and computer vision community, and has been reﬁned in several ways. The improved versions of Inception networks with batch normalization (Ioffe & Szegedy, 2015) (Inception-v2) were proposed by Ioffe et al. Later, an Inception network (Inception-v3) was proposed with factorization ideas in (Szegedy et al., 2016b)."
1704.07709,"dataset, benchmark",330,,0,"However, the concept of recurrence in the visual cortex is only included in few DCNN models such as the Recurrent Convolutional Neural Network (RCNN) (Liang & Hu, 2015), and a CNN with LSTM for visual description (Donahue et al., 2015). Additionally, Inception-V4 (Szegedy et al., 2016a), and Residual (He et al., 2016b) architectures are popular among the computer vision community. The intension of most recently developed DCNNs is to use Inception and Residual networks to implement larger deep networks. As the model becomes larger and deeper, the computational parameters of the architecture are increased dramatically. As a result, training the model becomes increasingly complex and thus, more computationally expensive. It is very challenging to include a recurrent property within popular Inception architectures, but recurrence is essential for improving the overall training and testing accuracy with fewer computational parameters. Others are trying to implement bigger and deeper DCNN architectures like GoogleNet (Szegedy et al., 2015), or a residual network with 1001 layers (He et al., 2016a) that achieves high recognition accuracy on different benchmark datasets. However, we are presenting an improved version of the DCNN model inspired by the information processing mechanisms of the human visual cortex, and recently developed some promising DCNN architectures like Inception-v4 (Szegedy et al., 2016a), and RCNN (Liang & Hu, 2015). Therefore, we call this model the Inception Recurrent Convolutional Neural Network (IRCNN). This model not only ensures better recognition accuracy with fewer computational parameters against the state-of-the-art DCNN architectures, but also helps to improve the overall training process of the deep learning approach. This proposed architecture generalizes both Inception networks and RCNN models. The contributions of this work are as follows:"
1704.07709,"dataset, benchmark",345,,0,"In 2012, Alex Krizhevesky et al. proposed an improved version of a CNN model compared to LeNet (LeCun et al., 1998), and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. This was a signiﬁcant breakthrough in the ﬁeld of machine learning and computer vision, as this was the ﬁrst time a deep network outperformed the alternative approaches for visual recognition tasks. GoogleNet, or Inception-v1 (Szegedy et al., 2015), and the Residual Network (He et al., 2016b) won ILSVRC in 2014 and 2015 respectively. Inception architecture has become very popular in the deep learning and computer vision community, and has been reﬁned in several ways. The improved versions of Inception networks with batch normalization (Ioffe & Szegedy, 2015) (Inception-v2) were proposed by Ioffe et al. Later, an Inception network (Inception-v3) was proposed with factorization ideas in (Szegedy et al., 2016b). In most cases, the improvement of deep learning approaches has been due to the development of the following components: Initialization techniques of DCNNs (Mishkin & Matas, 2015), new deep network architectures (Chen et al., 2015a; Shankar et al., 2016), optimized network structures (depending upon computational parameters) (Szegedy et al., 2016b; Iandola et al., 2016), deeper and wider deep networks (Urban et al., 2016; Zagoruyko & Komodakis, 2016), activation functions (Clevert et al., 2015), and optimization methods for training DCNNs (Koushik & Hayashi, 2016; Ngiam et al., 2011). Some researchers have been focusing on design alternatives that produce the same level of recognition accuracy as state-of-the-art architectures (like InceptionV4 with Residual Net (Szegedy et al., 2016a)) with fewer computational parameters (Iandola et al., 2016)."
1704.07709,"dataset, data",123,,0,"In this implementation, we have augmented data applying only random horizontal ﬂipping techniques whereas other models published results with more data augmentation with transaction, central crop, and ZCA. This proposed model will provide better recognition accuracy when using datasets with additional augmentation.Due to hardware constrains, we were not able to experiment on massive scale implementations of IRCNN. This architecture will probably provide even better classiﬁcation accuracy with large networks on the same datasets. The large scale implementation of proposed IRCNN model with advanced components such as LSUV, EVE, and Exponential Linear Unit (ELU) (Clevert et al., 2015) will likely provide further improved recognition on CIFAR-10 and CIFAR-100 datasets."
1704.07709,"dataset, data",124,,0,"This is another benchmarks for object classiﬁcation from the same group (K and Hinton, 2009) (Krizhevsky & Hinton, 2009). The dataset contains 60,000 (50,000 for training and for 10,000 testing) color 32 × 32 images, and it has 100 classes. We have used SGD and LSUV (Mishkin & Matas, 2015) as the initialization approach with the EVE optimization technique (Koushik & Hayashi, 2016) in this experiment. The experimental results are shown in Table 1. In both cases, the proposed technique shows state-of-the-art accuracy compared with different DCNN models. IRCNN+SGD shows about 34.13% testing errors without data augmentation and 31.22% classiﬁcation errors"
1704.07709,"dataset, data",145,,0,"with data augmentation. In addition, this models achieved around 30.87% and only 28.24% errors with SGD and LSUV+EVE on augmented dataset. This is the highest accuracy achieved in any of the deep learning models summarized in Table 1. For augmented datasets, we have achieved 71.76 % recognition accuracy with LSUV+EVE, which is about a 3.51% improvement compared to RCNN(Liang & Hu, 2015). Figure 6 shows the training and validation loss of IRCNN for both experiments on CIFAR-100 with data augmentation(with initialization and optimization). It is clearly shown that the proposed model has lower error in the both experiments, showing the effectiveness of the proposed IRCNN learning model. The training and testing accuracy of the IRCNN with LSUV and EVE are shown in Figure 7."
1704.07709,"dataset, data",166,,0,"(Mishkin & Matas, 2015). On the other hand, we have only applied random horizontal ﬂipping for data augmentation in this implementation and achieved about 1.72% better recognition accuracy against FitNet4 (Mishkin & Matas, 2015). For an impartial comparison with the EIN and EIRN models, we have implemented the Inception network with the same number of layers and parameters as in the transaction and Inception-block. Instead of using recurrent connectivity in the convolutional layers, we used sequential convolutional layers for the same time-step with the same kernels. During the implementation of EIRN, we only added residual connection in the Inception-Residual block, where the inputs of the Inception-Residual block are accumulated with the outputs of that particular block. In this case, all of the experiment have been conducted on the augmented CIFAR-100 dataset (Krizhevsky & Hinton, 2009). The model loss and accuracy for both training and"
1704.07709,"dataset, data",205,,1,"MNIST is one of the most popular datasets for handwritten digits from 0-9 [36], the dataset contains 28 × 28 pixel grayscale images with 60,000 training examples and 10,000 testing examples. For this experiment, we trained the proposed model with two IRCNN convolution blocks (IRCNN-block 1 and IRCNN-block 2) and used the ReLU activation function. The model was trained with 60,000 samples and 10,000 samples were used for validation of the model. Eventually the trained network was tested with 10,000 testing examples. We obtained a test error of 0.32% with the IRCNN and the SGD, and achieved about 0.29% error for the IRCNN when initializing with LSUV (Mishkin & Matas, 2015) and the EVE (Koushik & Hayashi, 2016) optimization function. This provided the best accuracy compared to the RCNN, as well as the other state-of-the-art networks. The summary of the classiﬁcation accuracies is given in Table I. No data augmentation techniques have been applied in this experiment on MNIST. On the contrary, global contract normalization and ZCA whitening were applied in the experiments using most of the mentioned models."
1704.07709,github,11,,0,"Chollet, F. Keras. https://github.com/fchollet/keras, 2016."
1705.01464,"data, database",83,,0,"Nowadays the academic community is using three major multi-disciplinary scientific databases (Web of Science Thomson Reuters (WoS), Scopus, and Google Scholar) in an extensive way. It is admitted that there is no ideal database [33]. Each has strengths and weaknesses. For the current research paper, WoS has been chosen as the data source. A synopsis of information with regards to our current research framework is presented in table 1."
1705.01464,dataset,215,,0,"How much is the h-index of an editor of a well ranked journal improved due to citations which occur after his/her appointment? Scientific recognition within academia is widely measured nowadays by the number of citations or h-index. Our dataset is based on a sample of four editors from a well ranked journal (impact factor – IF – greater than 2). The target group consists of two editors who seem to benefit by their position through an increased citation number (and subsequently h-index) within journal. The total amount of citations for the target group is bigger than 600. The control group is formed by another set of two editors from the same journal whose relations between their positions and their citation records remain neutral. The total amount of citations for the control group is more than 1200. The timespan for which citations’ pattern has been studied is 1975-2015. Previous coercive citations for a journal benefit (increase its IF) has been signaled. To the best of our knowledge, this is a pioneering work on coercive citations for personal (editors’) benefit. Editorial teams should be aware about this type of potentially unethical behavior and act accordingly."
1705.01464,retrieve,17,,0,"42. Journal Citation Reports Notice. Retrieved on November 28th, 2015. Available online:"
1705.01464,retrieve,24,,0,"34. Journal Citation Reports, Retrieved on multiple occasions in November-December 2015 and September 2016. Available online: https://jcr.incites.thomsonreuters.com/JCR JournalHomeAction.action"
1705.01464,retrieve,31,,0,"26. Brooks, J.J. Promotion at the Johns Hopkins School of Medicine. Retrieved on September 1st, 2016, Available online: http://www.hopkinsmedicine.org/gim/useful_links/ppc%20 promotion.pdf"
1705.01464,retrieve,94,,0,"study. Science and engineering ethics, 2015, 21(4), 829-835 13. Davis, P. Citation cartel or editor gone rogue? Scholarly Kitchen, 9th March, 2017 14. Rasmussen, M.; Savenije, H.; Thybo, H. ; Bamber, J. EGU & Copernicus report about citation stacking in the EGU journals SE and SOIL, European Geosciences Union, 27th February, 2017 15. Chaos, Solitons & Fractals, Volume 38, Issue 5. Retrieved on multiple occasions in November"
1705.05541,benchmark,56,,0,"We focus on a speciﬁc MC benchmark, XSBench. This benchmark models the calculation of macroscopic neutron “cross sections” [42] within a nuclear reactor, which is the most computationally intensive part of a typical MC transport algorithm [43]. Listing 9 shows the major computation of XSBench."
1705.05541,benchmark,78,,0,"[41] D. H. Bailey, L. Dagum, E. Barszcz, and H. D. Simon, “Nas parallel benchmark results,” in ACM/IEEE conference on Supercomputing, 1992. [42] J. R. Tramm, A. R. Siegel, T. Islam, and M. Schulz, “XSBench the Development and Veriﬁcation of a Performance Abstraction for Monte Carlo Reactor Analysis,” in International Conference on Physics of Reactors, 2014."
1705.05541,benchmark,84,,0,"We perform the above process for each iteration of the XSBench loop, and introduce ﬁve counters to count how many times each interaction type is chosen for all iterations. Given the sufﬁcient number of lookups (i.e., iterations of the main computation loop), the number of times an interaction type is chosen is roughly the same for all interaction types. This method gives us a deterministic and meaningful way to quantify the validness of the benchmark result."
1705.05541,benchmark,185,,0,"XSBench is only a benchmark for performance study, hence its result (particularly macro xs vector) does not have sufﬁcient physical meaning. From one run to another, the result can be different due to the random nature of the benchmark. It is difﬁcult to know if the benchmark result remains correct for our crash consistence evaluation. Based on the domain knowledge, we slightly extend the benchmark such that the benchmark result has physical meaning. In particular, at the end of each iteration, we apply a cumulative distribution function (CDF) to the ﬁve elements of macro xs vector, and then normalize the CDF result by the largest element. Then we generate a uniformly distributed random number x ( 0 < x < 1). This random number represents a computation result in a full-featured simulation of the nuclear reactor. Based on the random number, we ﬁnd which interaction type (i.e., which element of macro xs vector) should be chosen based on the normalized CDF result."
1705.05541,benchmark,212,,0,"To verify the above basic idea, we run XSBench with an input problem of 34 fuel nuclides in a Hoogenboom-Martin reactor model. With such input problem, the energy grid and nuclide grid take about 246MB memory. There are 1.5 × 107 lookups in the main computation loop. We use our crash simulator to run the benchmark and trigger a crash when the benchmark is in the 1.5 × 106th lookup (10% of all lookups). Figure 10 shows how many times each interaction type is counted for two tests. In one test, we do not have crash (labeled as “No crash”); in the other test, we have the crash but immediately restart based on the above basic idea (labeled as “Crash and restart based on the basic idea”). The numbers of times counted for the ﬁve interaction types are normalized by the total number of lookups and shown as percentage in the y axis. These two tests use the same randomly sampled inputs (Line 2 in Figure 9) for each lookup, such that we enable a fair comparison of the XSBench results of the two tests."
1705.05541,benchmark,275,,0,"To measure recomputation cost, we use the crash emulator to trigger a crash at a speciﬁc program execution point, particularly Line 10 (Figure 2) in the 15th iteration of the main loop in NPB CG (one benchmark in NAS parallel benchmark suite [41]). Figure 3 shows the performance on the heterogeneous NVM/DRAM system with different input problems of CG. The recomputation time in the ﬁgure is broken down, and it includes the time to detect from which iteration CG is resumable (labeled as “Detecting where to restart”) and the time to resume from the resumable iteration to the crashed iteration (labeled as “Resuming computation time” in the ﬁgure). The recomputation time is normalized by the average execution time of individual iterations of the main loop in CG. The number of iterations on the top of each column is the number of iteration we lose because of the crash. the recomputation cost becomes smaller when we use a larger input problem size. When the input problem size is small (Classes S and W), the recompaution time is relatively large. We lose all of the iterations (15 iterations) when the crash happens. However, when the input problem size is large (Classes B and C), we lose only 1 iteration and the recomputation time is very small. This result is aligned with our performance characterization: in particular, a larger input problem tends to lose smaller computation when a crash happens."
1705.05541,code,9,,0,Fig. 9. Pseudo code for XSBench.
1705.05541,code,49,,0,"1 2 3 4 5 6 7 8 9 10 11 12 Fig. 1. Pseudo-code for CG. Capital letters such as A represent matrices; lowercase letters such as x, y, z represent vectors; Greek letters α, ρ represent scalar numbers."
1705.05541,"code available, code",172,,0,"The above algorithm extension also increases the working set size which could cause extra cache misses and lose performance. However, we do not see big performance loss (no bigger than 8.2%) in our evaluation. The reason is as follows. Given a large matrix size n × n, the matrix multiplication based on the submatrix multiplication in the original code causes similar cache misses as the new algorithm, because both of the original code and the new algorithm fetch different submatrices for multiplication and save the results in either C f or Cs. Those submatrices multiplications, which dominates the computation time, have “streaming-like” memory access patterns — we need to fetch submatrices one by one for multiplication. Such memory access patterns cause similar cache misses in the original code and the new algorithm. Furthermore, the regular memory access patterns in matrix multiplication allows prefetching to take effect and further alleviate the effects of cache misses."
1705.05541,"data, code",160,,0,"Crash consistence in NVM. Leveraging persistent extensions from ISA (e.g., CLFLUSH), some work introduces certain program constructs to enable crash consistence in NVM. Mnemosyne [14], Intel NVM library [15], [44], NVheaps [13], and REWIND [45] provide transaction systems optimized for NVM. NVL-C [46] introduces ﬂexible directives and runtime checks that guard against failures that corrupt data consistence. SCMFS [47] provides a PM-optimized ﬁle system based on the persistent extensions from ISA. Atlas [48] uses those extensions for lock-based code. To use the existing efforts for HPC applications, we may have to make extensive changes to applications or operating systems. The application can suffer from large runtime overhead because of frequent runtime checking or data logging. Our evaluation with the Intel NVM library shows such large overhead."
1705.05541,"data, database",85,,0,"the application. In fact, our preliminary work with CG and dense matrix multiplication based on a undo-log [15] has 4.3× and 5.5× performance loss, respectively. While such large overhead is tolerable in speciﬁc domains (e.g., database) with data persistence prioritized over performance, this overhead is not acceptable in HPC. To leverage NVM as persistent memory and build a consistent and correct state, we must introduce a lightweight mechanism with minimum runtime overhead."
1705.05541,"data, github, benchmark",101,,0,"[43] J. Tramm, “XSBench: The Monte Carlo Macroscopic Cross Section Lookup Benchmark,” 2014, https://github.com/jtramm/XSBench. [44] A. Rudoff, “Programming Models for Emerging Non-Volatile Memory Technologies,” ;login: The USENIX Magazine, vol. 38, no. 3, 2013. [45] A. Chatzistergiou, M. Cintra, and S. D. Viglas, “REWIND: Recovery Write-ahead System for In-Memory Non-Volatile Data Structures,” Proceedings of the VLDB Endowment, vol. 8, no. 5, 2015."
1705.05541,database,45,,0,"[26] K. Suzuki and S. Swanson, “The Non-Volatile Memory Technology Database (NVMDB),” Department of Computer Science & Engineering, University of California, San Diego, Tech. Rep. CS2015-1011, 2015, http://nvmdb.ucsd.edu."
1706.06497,code,62,,0,"The rest of the paper is organized as follows. Section 2 formally introduces additional constructs of PEG for specifying code layout, deﬁnes their semantics and studies their semantic properties. In Sect. 3, a semantics-preserving process of eliminating the alignment construct from grammars is described. Section 4 refers to related work and Sect. 5 concludes."
1706.06497,package,77,,0,"Our speciﬁcation of PEG>diﬀers from the deﬁnition used by Adams and A˘gacan [3] by three essential aspects listed below. The last two discrepancies can be understood as bugs in the original description that have been corrected in the Haskell indentation package by Adams [1]. This package also provides means for locally changing the token mode. All in all, our modiﬁcations fully agree with the indentation package."
1706.06497,package,82,,0,"Adams and A˘gacan [3] extend PEGs with the indentation and alignment constructs. We propose a slightly diﬀerent extension with three rather than two extra constructs. Our approach agrees with that implemented by Adams in his indentation package for Haskell [1], whence calling the grammars in our approach Adams’ grammars is justiﬁed. All diﬀerences between the deﬁnitions in this paper and in [3] are listed and discussed in Subsect. 2.4."
1706.06497,package,106,,0,Adams [2] and Adams and A˘gacan [3] provide an excellent overview of previous approaches to describing indentation-sensitive languages and attempts of building indentation features into parser libraries. Our work is a theoretical study of the approach proposed in [3] while some details of the semantics used in our paper were “corrected” in the lines of Adams’ indentation package for Haskell [1]. This package enables specifying indentation sensitivity within the Parsec and Trifecta parser combinator libraries. A process of alignment operator elimination is previously described for CFGs by Adams [2].
1706.06497,package,329,,0,"References Michael D. Adams. The indentation package. URL: http://hackage.haskell.org/ package/indentation. Michael D. Adams. Principled parsing for indentation-sensitive languages: Revisiting In Roberto Giacobazzi and Radhia Cousot, editors, The 40th Landin’s oﬀside rule. Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’13, Rome, Italy - January 23 - 25, 2013, pages 511–522. ACM, 2013. URL: http://doi.acm.org/10.1145/2429069.2429129, doi:10.1145/2429069.2429129. Michael D. Adams and Ömer S. A˘gacan. In Indentation-sensitive parsing for Parsec. Wouter Swierstra, editor, Proceedings of the 2014 ACM SIGPLAN symposium on Haskell, Gothenburg, Sweden, September 4-5, 2014, pages 121–132. ACM, 2014. URL: http://doi. acm.org/10.1145/2633357.2633369, doi:10.1145/2633357.2633369. Alfred V. Aho and Jeﬀrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1972. Alexander Birman and Jeﬀrey D. Ullman. Parsing algorithms with backtrack. Information and Control, 23(1):1–34, 1973. URL: http://dx.doi.org/10.1016/S0019-9958(73) 90851-6, doi:10.1016/S0019-9958(73)90851-6. Bryan Ford. Parsing expression grammars: A recognition-based syntactic foundation. In Neil D. Jones and Xavier Leroy, editors, Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2004, Venice, Italy, January 14-16, 2004, pages 111–122. ACM, 2004. URL: http://doi.acm.org/10.1145/964001. 964011, doi:10.1145/964001.964011. Tetsuro Matsumura and Kimio Kuramitsu. A declarative extension of parsing expression grammars for recognizing most programming languages. JIP, 24(2):256–264, 2016. URL: http://dx.doi.org/10.2197/ipsjjip.24.256, doi:10.2197/ipsjjip.24.256. Sérgio Medeiros, Fabio Mascarenhas, and Roberto Ierusalimschy. Left recursion in parsing expression grammars."
1706.06497,package,343,,0,"Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’13, Rome, Italy - January 23 - 25, 2013, pages 511–522. ACM, 2013. URL: http://doi.acm.org/10.1145/2429069.2429129, doi:10.1145/2429069.2429129. Michael D. Adams and Ömer S. A˘gacan. In Indentation-sensitive parsing for Parsec. Wouter Swierstra, editor, Proceedings of the 2014 ACM SIGPLAN symposium on Haskell, Gothenburg, Sweden, September 4-5, 2014, pages 121–132. ACM, 2014. URL: http://doi. acm.org/10.1145/2633357.2633369, doi:10.1145/2633357.2633369. Alfred V. Aho and Jeﬀrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1972. Alexander Birman and Jeﬀrey D. Ullman. Parsing algorithms with backtrack. Information and Control, 23(1):1–34, 1973. URL: http://dx.doi.org/10.1016/S0019-9958(73) 90851-6, doi:10.1016/S0019-9958(73)90851-6. Bryan Ford. Parsing expression grammars: A recognition-based syntactic foundation. In Neil D. Jones and Xavier Leroy, editors, Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2004, Venice, Italy, January 14-16, 2004, pages 111–122. ACM, 2004. URL: http://doi.acm.org/10.1145/964001. 964011, doi:10.1145/964001.964011. Tetsuro Matsumura and Kimio Kuramitsu. A declarative extension of parsing expression grammars for recognizing most programming languages. JIP, 24(2):256–264, 2016. URL: http://dx.doi.org/10.2197/ipsjjip.24.256, doi:10.2197/ipsjjip.24.256. Sérgio Medeiros, Fabio Mascarenhas, and Roberto Ierusalimschy. Left recursion in parsing expression grammars. In Francisco Heron de Carvalho Junior and Luís Soares Barbosa, editors, Programming Languages - 16th Brazilian Symposium, SBLP 2012, Natal, Brazil, September 23-28, 2012. Proceedings, volume 7554 of Lecture Notes in Computer Science, pages 27–41. Springer, 2012. URL: http://dx.doi.org/10.1007/978-3-642-33182-4_4, doi:10.1007/978-3-642-33182-4_4."
1706.06497,"package, data",279,,0,"Parsing may succeed, fail, or diverge. If parsing succeeds, it returns as a result a new triple containing the rest of the input w0, a new set I 0 of baseline candidates updated according to the information gathered during parsing, and a new alignment ﬂag b0. This result is denoted by >(w0, I 0, b0). If parsing fails, there is no result in a triple form; failure is denoted by ⊥. Triples of the form (w, I, b) ∈ T ∗ × ℘(N) × B are behaving as operation states of parsing, as each parsing step may use these data and update them. We will write State = T ∗ × ℘(N) × B (as we never deal with diﬀerent terminal sets, dependence on T is not explicitly marked), and denote by State + 1 the set of possible results of parsing, i.e., {>(s) : s ∈ State} ∪ {⊥}. The assertion that parsing expression e in grammar G with input string w in the context of I and b assuming token mode τ results in o ∈ State + 1 is denoted by e, τ ‘G (w, I, b) → o. The formal deﬁnition below must be interpreted inductively, i.e., an assertion of the form G, τ ‘e s → o is valid iﬀ it has a ﬁnite derivation by the following ten rules:"
1706.06497,"package, data",283,,0,"Parsing of an expression pρ means parsing of p while assuming that the part of the input string corresponding to p forms a new indentation block whose baseline is in relation ρ to the baseline of the surrounding block. (Baselines are identiﬁed with column numbers.) The , missing in [3], determines how tokens of the input can be situated w.r.t. position construct pσ the current indentation baseline. Finally, parsing an expression ¦p¦ means parsing of p while assuming the ﬁrst token of the input being positioned on the current indentation baseline (unlike the position operator, this construct does not aﬀect processing the subsequent tokens). Inspired by the indentation package [1], we call the relations that determine token positioning w.r.t. the indentation baseline token modes. In the token mode > for example, tokens may appear only to the right of the indentation baseline. Applying the position operator with relation > to parts of Haskell grammar to be parsed in the indentation mode avoids indenting every single terminal in the example in Sect. 1. Also, indenting terminals with > is inadequate for do expressions occurring inside a block of relaxed mode but the position construct can be easily used to change the token mode for such blocks (e.g., to ≥). We call a PEG extended with these three constructs a PEG>. Recall from Sect. 1 that N and T denote the set of non-terminal and terminal symbols of the grammar, respectively, and δ : N → EG is the production function."
1706.06497,"package, data",327,,0,"Inspired by the indentation package [1], we call the relations that determine token positioning w.r.t. the indentation baseline token modes. In the token mode > for example, tokens may appear only to the right of the indentation baseline. Applying the position operator with relation > to parts of Haskell grammar to be parsed in the indentation mode avoids indenting every single terminal in the example in Sect. 1. Also, indenting terminals with > is inadequate for do expressions occurring inside a block of relaxed mode but the position construct can be easily used to change the token mode for such blocks (e.g., to ≥). We call a PEG extended with these three constructs a PEG>. Recall from Sect. 1 that N and T denote the set of non-terminal and terminal symbols of the grammar, respectively, and δ : N → EG is the production function. Concerning the semantics of PEG>, each expression parses an input string of terminals (w ∈ T ∗) in the context of a current set of indentation baseline candidates (I ∈ ℘(N)) and a current alignment ﬂag indicating whether the next terminal should be aligned or not (b ∈ B), assuming a certain token mode (τ ∈ <(N)). Parsing may succeed, fail, or diverge. If parsing succeeds, it returns as a result a new triple containing the rest of the input w0, a new set I 0 of baseline candidates updated according to the information gathered during parsing, and a new alignment ﬂag b0. This result is denoted by >(w0, I 0, b0). If parsing fails, there is no result in a triple form; failure is denoted by ⊥."
1706.06497,python,134,,0,"Languages like Python and Haskell allow the syntactic structure of programs to be shown by indentation and alignment, instead of the more conventional braces and semicolons. Handling indentation and alignment in Python has been speciﬁed in terms of extra tokens INDENT and DEDENT that mark increasing and decreasing of indentation and must be generated by the lexer. In Haskell, rules for handling indentation and alignment are more sophisticated. Both these languages enable to locally use a diﬀerent layout mode where indentation does not matter, which additionally complicates the task of formal syntax speciﬁcation. Adams and A˘gacan [3] proposed an extension of PEG notation for specifying indentation sensitivity and argued that it considerably simpliﬁes this task for Python, Haskell and many other indentation-sensitive languages."
1807.04616,code,84,,0,"To gauge the practicability of seamlessly oﬄoading a typical HPC workload to a cloud system, the same application binaries were run on both Stampede2 and Jetstream. The applications themselves are built as multi-architecture binaries that allow for code branching depending on what level of vectorization instruction is supported on the underlying chip. AVX2 instructions serve as the baseline to support the Haswell architecture on Jetstream while the newer Skylake and KNL architectures of Stampede2 can take advantage of AVX-512 instructions."
1807.04616,"data available, data",107,,0,"3 RESULTS Several applications were chosen to run on the Jetstream cloud extension system that are regularly executed on Stampede2 and may be ﬂexibly scaled by the number of processors used in computation. Table 2 provides the applications chosen along with version information and a brief description of each. GROMACS [1], NAMD [14], OpenSeesSP [11], and WRF [21] were chosen because input data and historical information were readily available. All applications were launched via the Slurm sbatch command on both Stampede2 and Jetstream. NAMD and OpenSeesSP were additionally launched with Agave."
1807.04616,"data, code",122,,0,"4.1 Future Work The results reported in this paper establish the eﬃcacy of on-premises virtualization of HPC systems. Yet, this drive toward automatic cloud bursting raises a number of interesting questions. For instance, more experimentation is needed to determine what policies should be in place to eﬀectively ascertain which applications should even be considered for cloud bursting. Or another, how well would massively parallel computations involving hundreds or even thousands of processors run in this cloud environment? What about oﬄoading jobs involving I/O heavy workloads? Moreover, is there a way to statically qualify or disqualify an application from being considered for cloud execution by inspecting its code, data, and dependencies?"
1807.04616,"data, code, data repository",37,,0,System used for computation where application binaries can be run Data repository that can be accessed through Agave for I/O Executable code invoked by Agave on a speciﬁc execution system Runtime instance of an application with parameters
1807.04616,"data, data https",19,,0,"David Micklos, and Parke Antin. 2018. through Data-Driven Discovery. (2018). http://www.cyverse.org"
1807.04616,"data, dataset provided",78,,0,"With this setup, migrating applications from Stampede2 to Jetstream requires much less work for the user than migrating to an oﬀ-site, public cloud. Input, output, application, and library directories are already mounted and ready for use. This also means that VMs are comparatively light weight in their instantiation, minimizing initialization times. File system mounting and network locality further help to avoid potential expensive application and data transfer costs."
1807.04616,database,95,,0,"2.3 Virtual Cluster Creation To begin, a persistent master node VM was created to serve as the orchestration point for the rest of the virtual cluster. Other basic conﬁgurations included setting up a virtual private network and adding a pair of shared SSH keys automatically upon creation of any new VMs. A non-production node on Stampede2 was designated for serving out the ﬁle systems via NFS while the site ﬁrewall was conﬁgured to allow traﬃc for this as well as for the Slurm controller to interact with the Stampede2 Slurm database."
1807.04616,database,126,,0,"The other key set of RPMs that were presented from the TACC repository included the basic components of the Slurm workload manager used on Stampede2 for batch scheduling of users’ compute jobs [24]. The three node classes discussed in Section 2.1 were conﬁgured via Ansible to support the Slurm controller host, Slurm worker hosts and the job submission host on the Jetstream master node VM, compute node VMs, and login node VM, respectively. The Jetstream Slurm controller was conﬁgured to tap into a common Slurm database housed within the Stampede2 system. This allowed for inquiries and submission requests to pass from one system to another without the need for any other intermediary service for communication."
1807.04616,database,180,,0,"Another consideration involves automation of the cloud bursting process. This initial implementation uses a common Slurm database and command-line ﬂags to transfer jobs from the Stampede2 controller to the Jetstream controller. In the future, it is possible to enable Slurm’s federation process that will submit a job to all federated clusters simultaneously only to remove pending duplicates once one of the systems is able to schedule the job. Another possibility would be to construct a job submission ﬁlter either via Slurm or Agave to realize a more sophisticated predictive model. One example might be as described by Guo et al. [7] and would be able to dynamically route jobs to the cloud as HPC backlogs grow. Future work will include dynamically scaling the number of compute node VMs available based on the HPC system’s congestion and the cloud system’s current idle resource availability. Finally, adaptations and policy decisions to integrate accounting mechanisms for the two distinct systems will need to be investigated."
1807.04616,github,3,,0,https://cobbler.github.io
1807.04616,github,7,,0,managing HPC clusters. https://github.com/hpcsi/losf
1807.04616,open-source,18,,0,[5] Rion Dooley. 2018. Agave Platform: SaaS platform for open source community.
1807.04616,retrieve,6,,0,"Retrieved March 20, 2018 from"
1807.04616,retrieve,8,,0,"2018. Retrieved March 20, 2018 from"
1807.04616,retrieve,13,,0,"(2018). Retrieved March 20, 2018 from https://agaveapi.co"
1807.04616,retrieve,13,,0,"(2018). Retrieved March 20, 2018 from https://www.openstack.org"
1807.04616,retrieve,14,,0,"LosF: A Linux operating system Framework for Retrieved March 20, 2018 from"
1807.04616,retrieve,28,,0,"[13] Nirav Merchant, Eric Lyons, Stephen Goﬀ, Matthew Vaughn, Doreen Ware, Cyverse:Transforming Science Retrieved March 20, 2018 from"
1807.04616,retrieve,34,,0,"[24] Andy B Yoo, Morris A Jette, and Mark Grondona. 2018. Slurm Workload Manager. (2018). Retrieved March 20, 2018 from https://slurm.schedmd.com"
1807.04616,retrieve,63,,0,"[15] Ellen M Rathje, Clint Dawson, Jamie E Padgett, Jean-Paul Pinelli, Dan Stanzione, Ashley Adair, Pedro Arduino, Scott J Brandenberg, Tim Cockerill, Charlie Dey, et al. 2018. Designsafe-CI: A natural hazards engineering research infrastructure. (2018). Retrieved March 20, 2018 from https://www.designsafe-ci.org"
1810.09868,code,41,,0,"5The HLO IR speciﬁcation does contain provisions for calling arbitrary opaque functions via the ‘CustomCall‘ operation, but that facility is not available on TPUs (as there is no non-XLA method to generate code for the TPU)"
1810.09868,code,43,,0,"In this paper, we discussed how to compile Julia code to XLA IR, thus enabling ofﬂoad to TPU devices. The described implementation re-uses signiﬁcant parts of the existing Julia compiler and is thus less than 1000 lines of code,"
1810.09868,code,46,,0,"Lattner, C. and Adve, V. Llvm: A compilation framework for lifelong program analysis & transformation. In Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization, pp. 75. IEEE Computer Society, 2004."
1810.09868,code,53,,0,"6These operations are inserted by the ﬁnal compiler step, translating the XLA embedding into proper XLA IR. Had they been inserted earlier julia-level destructuring passes would have optimized away such a construct. Similar passes exist on the XLA level and will optimize these operations away before ﬁnal code generation."
1810.09868,code,55,,0,"To obtain the backwards pass, we make use of the Zygote.jl compiler-based AD framework (Innes, 2018). Zygote operates on Julia code and its output is again a Julia function (suitable for reintroduction into Zygote to obtain higher order derivatives, but also suitable for compilation to TPUs)."
1810.09868,code,64,,0,"Listing 5: The ﬁnal XLA IR, ready for XLA’s high level optimization and eventual generation of TPU machine code. Notice that the + being broadcasted turned into a separate computation computing scalar addition (We saw the corresponding scalar + deﬁnition in section 5. The listing was edited to introduce line breaks and shorten autogenerated variable names."
1810.09868,code,74,,0,"Julia code, it is also compatible with the Zygote.jl (Innes, 2018) automatic differentiation tool, which performs automatic differentiation as a high-level compiler pass. Putting these together, we are able to compile full machine learning models written using the Flux machine learning framework, fusing the forward and backwards model passes as well as the training loop into a single executable that is ofﬂoaded to the TPU."
1810.09868,code,89,,0,"Assuming an implementation of the most speciﬁc and compatible predicates (which are highly non-trivial to implement - see (Nardelli et al., 2018) - particularly in a performance-oriented manner, but are generally intuitively understandable by users), we can obtain functioning - if slow - implementations of these semantics with no more than a hundred lines of code. The relative minimality of the dynamic semantics reduces the effort required for the compiler implementation and allows work to be shared between different backends."
1810.09868,code,110,,0,"In this section, we present preliminary performance results of the code generated via the method in this paper. Note that because we were able to fully ofﬂoad the function of interest, we expect future performance improvements to be the result of improvements to XLA’s high level and backend optimizations, as opposed to modiﬁcations to the method in this paper. We note that the XLA developers have not yet had a chance to implement any improvements as a result of our work and we thus expect all XLA performance results to improve in the future. Our results are shown in ﬁgure 2."
1810.09868,code,113,,0,"We begin by considering the dynamic semantics of the Julia programming language. These are the semantics that form the mental model of the programming language execution for the user. Additionally, these semantics are what an interpreter written for the language would implement. Such an interpreted implementation need not be fast, merely possible. Simpler semantics are generally better as they reduce the cognitive load on the user of the programming language. All code generated by the compiler should then operate as if following the dynamic semantics, but ideally much faster. In Julia, the local dynamic semantics of the base language are as follows:"
1810.09868,code,115,,0,"In the example in listing 2, we have spliced the HLO operands (including the static operands) right into the AST. This yields a mapping to XLA that is extremely straightforward (go through every statement, obtain the static operands from the spliced instruction speciﬁcation, and the dynamic shapes from type inference and generate the corresponding XLA code). Of course, we don’t generally splice these instructions in manually, however the manual example illustrates why separating the static operands is useful and illustrates the conditions for a successful ofﬂoad to XLA. IR is completely ofﬂoadable if, after all relevant Julia level optimizations:"
1810.09868,code,121,,0,"but is nevertheless able to compile both the forward and the backward pass (and the fusion thereof, including the training loop) of models of practical interest such as VGG19 into a single XLA kernel. We have also demonstrated how Julia’s multiple dispatch semantics aid in the speciﬁcation of this transformation. This work suggests that it is possible to not only compile a number of ML models written in Julia to TPUs, but also more general non-ML Julia code (as long as such code is also dominated by linear algebra operations). We hope that this facility may hasten the exploration of non-ML problem areas for which TPUs may be useful."
1810.09868,code,147,,0,"Our implementation provides introspection macros inspired by those provided by base Julia itself. In particular, in base Julia, @code lowered provides the state of the code after parsing, macro expansion and lowering, @code typed provides the state of the code after type inference and mid-level optimizations, and @code llvm provides the generated llvm code. Analogously, We provide @code typed xla for showing the typed IR after our enhanced optimization passes (described in section 6) as well as @code xla for printing the resulting XLA IR in its native textual representation. For the dense example, we show steps along the full pipeline, in particular after inference (listing 3), inlining and optimizations (listing 4) as well as the ﬁnal XLA IR (listing 5),"
1810.09868,code,150,,0,"In order to understand how to compile Julia code to XLA code, it is instructive to consider how the regular Julia compiler works. Julia is semantically a very dynamic language. However, in standard conﬁguration, Julia’s ultimate backend compiler is LLVM (Lattner & Adve, 2004) which is a static compiler backend. The Julia compiler needs to bridge the semantic gap between the dynamic semantics of the language to the static semantics of the LLVM representation. To understand this process, we will look at four aspects of the Julia system: The dynamic semantics, the embedding of the static compiler intrinsics, interprocedural type inference and the extraction of static sub graphs. In addition we will look at the interaction of these features with macros and generated functions which will be relevant to the XLA compiler."
1810.09868,code,207,,0,"Our work heavily leverages Julia type inference capabilities, which were recently signiﬁcantly enhanced by Jarrett Revels, Jameson Nash and Jeff Bezanson in support of the Cassette.jl dynamic compiler framework (Revels & Contributors, 2018). We are indebted to Mike Innes for his work on Flux.jl and Zygote.jl, without which we would not have been able to show the applicability of our method to a model of real world interest. Matt Bauman kindly provided guidance on properly implementing Julia’s broadcast semantics against the XLA backend. More generally, we thank the Julia community for their attention to detail and the clarity of Julia’s array abstraction that allowed us to achieve our results without requiring signiﬁcant amounts of code. We gratefully acknowledge Zak Stone, Michael Isard, Mark Heffernan, James Bradbury, Roy Frostig, Eli Bendersky and Chris Leary of Google’s TPU and XLA teams for their openness and willingness to answer our questions about TPUs, answer our bug reports and provide assistance in our quest to make this project a reality. We thank Christopher Rackauckas and Alan Edelman for helpful comments on earlier drafts of this paper."
1810.09868,code,213,,0,"Performing the analysis required by this technique puts signiﬁcant burden on Julia’s type inference to infer code very precisely. In order to be ofﬂoadable to XLA, type inference needs to be able to ﬁgure out every static operand to every HLO instruction as well as the shapes of every dynamic operand. In addition, it needs to be able to constant fold or otherwise eliminate all utility computations that are not expressed in terms of HLO operations. Julia’s type inference includes a number of heuristics that are supposed to prevent excessive time spent in inference when there would be little runtime beneﬁt. However, for ofﬂoading to XLA, these heuristics are mistuned. In addition, parts of Julia’s type inference are not designed to handle such a large number of constants as are required to generate a sufﬁciently high quality XLA embedding. For example, inference results are cached based on the precise, concrete signatures of the operands. In our case, the shapes of operands are part of the signature, thus forcing inference to re-infer every function for every new signature. This could be signiﬁcantly improved if instead of caching concrete signatures, type"
1810.09868,code,219,,0,"ABSTRACT Google’s Cloud TPUs are a promising new hardware architecture for machine learning workloads. They have powered many of Google’s milestone machine learning achievements in recent years. Google has now made TPUs available for general use on their cloud platform and as of very recently has opened them up further to allow use by non-TensorFlow frontends. We describe a method and implementation for ofﬂoading suitable sections of Julia programs to TPUs via this new API and the Google XLA compiler. Our method is able to completely fuse the forward pass of a VGG19 model expressed as a Julia program into a single TPU executable to be ofﬂoaded to the device. Our method composes well with existing compiler-based automatic differentiation techniques on Julia code, and we are thus able to also automatically obtain the VGG19 backwards pass and similarly ofﬂoad it to the TPU. Targeting TPUs using our compiler, we are able to evaluate the VGG19 forward pass on a batch of 100 images in 0.23s which compares favorably to the 52.4s required for the original model on the CPU. Our implementation is less than 1000 lines of Julia, with no TPU speciﬁc changes made to the core Julia compiler or any other Julia packages."
1810.09868,code,246,,0,"i.e. the derivative with respect to the model at the current value of the model and a particular training example (or a batch of training examples). We use sum as a simple stand in for the loss function. Fortuitously, but not entirely coincidentally, the type inference modiﬁcations we describe in section 6 also improve the precision of type inference to be able to infer through all of the VGG19 backwards pass. As for the forward pass, the total optimized and unoptimized instruction counts are shown in ﬁgure 1. The backwards pass generates signiﬁcantly more XLA instructions than the forward pass. One of the biggest contributors to the instruction bloat is Zygote’s mixed mode broadcast fusion, which computes both the forward pass and the backwards pass in one map kernel. Because XLA currently does not support multiple outputs from one map instruction, the function body gets duplicated across multiple map instructions, which XLA’s DCE then needs to clean up. In general, our compilation process stresses XLA’s handling of the map instruction, because of the prevalance of calls to julia’s map and broadcast functions in generic code. We are in the process of improving XLA’s handling of map to inline mapped computations providing XLA backends with a form of IR more similar to that generated by other frontends."
1810.09868,code,257,,0,"Having implemented type inference, we now have a performant way to implement the semantics described in the ﬁrst two sections. Whenever the interpreter performs a function call, it uses the dynamic type information to perform type inference to discover and infer the largest possible static sub-region of code reachable from this entry point (recall that determining call targets requires type information). We can then hand this sub-region to the compiler and have it generate an efﬁcient version of the entirety of the statically reachable sub-region 2. Whenever the compiler encounters a call for which type inference was unable to determine the call target, it emits a call back into the runtime system (making sure to obey the dynamic semantics at the boundary) to begin the process anew. As such, execution generally proceeds as a chain of relatively large static sub regions, chained together by the runtime system. It is only on the boundaries between these regions that the dynamic semantics are materialized (i.e. values moved to the heap, dynamic method selection performed etc.). The performance of this scheme depends heavily on the size of these static sub-regions, which is highly sensitive to both the quality of the type inference implementation and the semantics of the language. In Julia, these static subregions can easily encompass thousands of functions covering tens of thousands of source lines, without once re-entering the runtime system for method selection."
1810.09868,"code, github",21,,0,"Revels, J. and Contributors. Cassette.jl: Overdub your julia code, 2018. URL https://github.com/ jrevels/Cassette.jl."
1810.09868,"code, provide implementation",55,,2,"In addition to these simple operations, we also provide implementations of the higher level array abstractions, in particular, mapreduce and broadcast. The implementation of broadcast in terms of HLO operations is about 20 lines of code and omitted for space, but the implementation of ‘mapreduce‘ is simply:"
1810.09868,"code, python",193,,0,"In this paper, we present initial work to compile general Julia code to TPU using this interface. This approach is in contrast to the approach taken by TensorFlow (Abadi et al., 2016), which does not compile Python code proper, but rather uses Python to build a computational graph, which is then compiled. It is aesthetically similar to JAX (Frostig et al., 2018), which does aim to ofﬂoad computations written in Python proper by tracing and ofﬂoading high-level array operations. Crucially, however, we do not rely on tracing, instead we leverage Julia’s static analysis and compilation capabilities to compile the full program, including any control ﬂow to the device. In particular, our approach allows users to take advantage of the full expressiveness of the Julia programming language in writing their models. This includes higher-level features such as multiple dispatch, higher order functions and existing libraries such as those for differential equation solvers (Rackauckas & Nie, 2017) and generic linear algebra routines. Since it operates on pure"
1810.09868,"data, code",153,,0,"The obtained results are very promising and show the feasibility and generality of this approach to mapping Julia code to XLA and thus compiling to TPUs. However, signiﬁcant challenges remain. For one, the current compilation model is very “all or nothing”. The largest subregions considered for ofﬂoading are those that consist (after optimizations), entirely of XLA operations. As shown above, this is not a bad situation and is sufﬁcient for applications of real world interest. However, we can do better. Right now, we terminate such static regions, even if intervening instructions do not have a data dependency on the intervening region. A better approach would be to use the compiler to automatically separate functions into ofﬂoadable and non-ofﬂoable parts and insert infeed/outfeed operations for any data dependencies. This will also require extending the dynamic"
1810.09868,"data, code",162,,0,"XLA (“Accelerated Linear Algebra”) is a partially open source compiler project by Google. It features a rich input IR for specifying multilinear algebra computations and provides backend code generation capabilities for CPUs, GPUs and TPUs. XLA’s Input IR (dubbed the HLO High-Level Optimization IR) operates on arbitrary dimensional arrays of basic data types (integers and ﬂoats of various bit widths, bﬂoat16s and complex numbers) or tuples thereof (but no arrays of tuples). HLO operations include basic arithmetic operations, special functions, generalized linear algebra operations, high level array operations, as well as primitives for distributed computation. XLA can perform semantic simpliﬁcations of input programs, as well as performing whole-program memory scheduling for efﬁcient use and re-use of available memory (a very important consideration for large machine learning models). Each HLO operation has two kinds of operands:"
1810.09868,"data, code",221,,0,"fuse the entire forward pass of VGG19. After Julia-level optimizations, the ﬁnal IR for the top level function contains 181 instructions (each an HloOp with properly inferred constant static parameters and properly shape inferred dynamic paramters. The total number HLO operands in the entry level computation is 183 (two extra for the parameter instructions which are implicit in the embedding) and 361 total over 29 computations7. The count of instructions is summarized 3 (a more detailed breakdown can be found in the appendix). Since we are able to ofﬂoad the entire forward pass computation, the Julia is not involved at any step of the evaluation and can thus simultaneously perform other tasks (e.g. data preparation for the next batch). Additionally, the performance of the resulting code is limited only by the quality of the code generated by XLA, not by frontend considerations (we perform a performance evaluation in section 7.4). We validated correctness of the generated XLA code by evaluating the VGG19 model on images from the ImageNet validation set and validating that the obtained results match the results obtained from vanilla Metalhead (up to minor ﬂoating point rounding differences that generally don’t affect the prediction)."
1810.09868,github,20,,0,"Flux.jl: The ml library that doesn’t make you tensor, 2017. URL https: //github.com/FluxML/Flux.jl."
1810.09868,github,20,,0,"Structsofarrays.jl: that behave like arrays of Structures of arrays structures, 2018. URL https://github.com/ JuliaArrays/StructsOfArrays.jl."
1810.09868,github,22,,0,"Ferris, A. and Contributors. Staticarrays.jl: Statically sized arrays for julia, 2018. URL https://github.com/ JuliaArrays/StaticArrays.jl."
1810.09868,github,25,,0,"Mike Innes, A. P. and Contributors. Metalhead.jl: Computer vision models for ﬂux, 2018. URL https://github. com/FluxML/Metalhead.jl."
1810.09868,package,94,,2,"The results in this paper were obtained with a custom version of Julia that disabled several limiting heuristics and ﬁxed a number of bugs leading to suboptimal inference. We are in the process of contributing these improvements back to Julia (for the limiting heuristics as options to the compiler interface to request they be disabled for a particular invocation). Other than that, no TPU or XLA speciﬁc changes had to be made to Julia and all functionality described in this paper lives entirely within a self-contained Julia package."
1810.09868,package,128,,0,"Our ﬁrst, more complex example is the full VGG19 forward pass. We use the implementation of VGG19 as found in the Metalhead package (Mike Innes & Contributors, 2018), which leverages the Flux (Innes & Contributors, 2017) framework to translate the familiar machine learning layers (convolutional layer, dense layer) into linear algebra operations. However, importantly each layer in the Flux framework is just a regular function that in turn calls regular linear algebra operations. As such, machine learning models expressed in Flux, including VGG19, are just simply regular Julia functions and thus amenable to the methods described in this paper. Our compiler is able to fully infer, ofﬂoad and"
1810.09868,"package, code",71,,0,"Lastly, the element types of XRTArrays are restricted to those supported by XLA. However, part of the appeal of Julia is that most code is generic over datatypes. XLA’s limitation restricting element types can be to some extent overcome by performing AoS → SoA transformations (which is possible in Julia by using the StructsOfArrays package (Kornblith & Contributors, 2018))."
1810.09868,"package, code",103,,0,"abstractions. Julia’s standard library arrays are mutable and parameterized over type and dimension. Additionally, the StaticArrays.jl (Ferris & Contributors, 2018) package provides immutable arrays parameterized on element type and shape. As a result, the notion of shaped, N-dimensional immutable tensors is not foreign to Julia code and most existing, generic code is able to handle it without problem. We thus embed XLA values by deﬁning a runtime structure corresponding to immutable, shaped, N-dimensional tensors backed by handles to remote, XRT-managed, memory (ﬁgure 1)."
1812.02536,dataset,30,,1,4) Answer Prediction: this evaluation shows how well the proposed models perform on predicting the correct triple and how they compare to other systems on SimpleQuestions dataset.
1812.02536,dataset,47,,1,"We deﬁne a KG as a set of triples of the form (si, pi, oi) that appear in the Freebase-2M dataset. Given a subject si we deﬁne the set P red(si) of all the properties that si has as"
1812.02536,dataset,54,,1,"V. CONCLUSION In this paper, we analyze four different model architectures that are evaluated on the SimpleQuestions dataset using the same Named Entity Recognition and Linking system to facilitate the comparison. The results show how well the building components of a QA system perform in isolation and together in a pipeline."
1812.02536,dataset,67,,0,Bordes et al. [2] have presented the ﬁrst results on the SimpleQuestions dataset. Their approach is based on Memory Networks [16]. It generates candidate entities using n-grams from the question text that match some Freebase entity. The approach corrupts the dataset to generate negative samples by assigning random questions from the dataset to Freebase entity and predicate pairs.
1812.02536,dataset,78,,1,"The task of question answering on the SimpleQuestions dataset requires a system to output a single triple consisting of a subject and a predicate. We evaluated the four proposed models on prediction of a triple consisting of a subject and a predicate. The predicated pairs are ranked using Equation 1. Moreover, we compared our results with other published systems that evaluated using the same dataset. All results are shown in Table III."
1812.02536,dataset,117,,1,"1) Architecture: Similar to the NER model, the question text is encoded on the word and character level. Characterlevel word embeddings are computed by applying a CNN layer with Max-Pooling on the characters of each token. This process is the same as explained above in Figure 3. Word and character embeddings are concatenated and passed through a BiLSTM layer. The ﬁnal states of the BiLSTM are concatenated and fed into a feed-forward layer with softmax activation function, which calculates a probability distribution over a set of predicates. We identiﬁed 1629 predicates in the training split of the SimpleQuestions dataset. The architecture is shown in Figure 4."
1812.02536,"dataset, dataset provided",65,,0,"As the SimpleQuestions dataset does not explicitly provide the subjects, we rely on weak supervision to infer the subject during training process. We infer the position of the subject by querying the inverted index for each n-gram in the question. We assume that the correct subject span is the one that matches the expected subject URI when queried on an index."
1812.02536,"dataset, provide implementation",54,,0,"We trained a Named Entity Recognizer (NER) system similar to the one proposed by Chiu and Nichols [4] using weak supervision, for which Raj [13] provided the implementation. Since the dataset requires a single subject we adapted the NER to identify a single entity span."
1812.02536,github,3,,0,1https://github.com/facebookresearch/fastText
1812.02536,github,6,,0,"https://github.com/kamalkraj/Named-Entity-Recognition-withBidirectional-LSTM-CNNs, 2018."
1812.02536,retrieve,22,,0,from the surface form index. We queried the mention m on an index and retrieved subjects with corresponding frequency values.
1812.02536,retrieve,56,,0,2) Named Entity Linking: the evaluation shows in how many cases the subject can be retrieved by index lookup using the detected entity mention from the NER step. 3) Predicate Prediction: this evaluation shows how well the four models perform in predicting the correct predicate for the given question text.
1812.02536,retrieve,57,,0,"where P (pi|q; θ) is the probability of predicate pi as computed by our four predicate models described below. P (si|q : θ) is the probability of a subject si computed by normalizing the frequency scores retrieved for the mention m. In the following sections, we describe our proposed models"
1812.02536,"used dataset, dataset",106,,0,"The task of Question Answering (QA) has received increasing attention in the last few years. Most research has concentrated on the task of answering factoid questions such as Who wrote Mildred Pierced?, yielding the answer Stuart Kaminsky. Typically, such answers are extracted from a knowledge base (KB). A frequently used dataset in this context is the SimpleQuestions [2] dataset, which consists of simple questions that can be answered with a single fact from the Freebase KB. For instance, the question above can be answered using the following triple from Freebase:"
1902.04574,code,16,,0,"Applied to Handwritten Zip Code Recognition. Neural Comput. 1989, 1, 541–551."
1902.04574,"data available, data, open-source data, code, dataset, code available",239,,2,"We ﬁltered all utterances that redirect the user to another communication channel, e.g., direct messages, which are not informative for the model and only bring noise. Moreover, since answers evolve over time, we divided our dataset into a training and a testing part, keeping earlier posts for training and the latest ones for testing. We further excluded from the training set all conversations that are older then sixty days. For evaluation, we used dialogs from the last ﬁve days in the dataset, to simulate a real-world scenario for customer support. We ended up with a dataset of 49,626 question–answer pairs divided into 45,582 for training and 4,044 for testing. Finally, we open-sourced our code for pre-processing and ﬁltering the data, making it available to the research community [37]. Table 1 shows some statistics about our dataset. On the top of the table, we can see that the average number of turns per dialog is under three, which means that most of the dialogues ﬁnish after one answer from the customer support. The bottom of the table shows the distribution of the words in the user questions vs. the customer support answers. We can see that answers tend to be slightly longer, which is natural as replies by customer support must be extensive and helpful."
1902.04574,database,81,,0,"Answer combination has been recognized as an important research direction in the domain of customer support chatbots. For example, Qiu et al. [26] used an attentive seq2seq re-ranker to choose dynamically between the outputs of a retrieval-based and a seq2seq model. Similarly, Cui et al. [27] combined a fact database, FAQs, opinion-oriented answers, and a neural-based chit-chat generator, by training a meta-engine that chooses between them."
1902.04574,dataset,45,,0,"Table 1. Statistics about our dataset. (Reprinted by permission from Springer Nature: Springer Lecture Notes in Computer Science (Hardalov, M.; Koychev, I.; Nakov, P. Towards Automated Customer Support [20]), 2018)"
1902.04574,dataset,106,,0,"Our goal is to distinguish “good” vs. “bad” answers, but the original dataset only contains valid, i.e., “good” question–answer pairs. Thus, we use negative sampling [29], where we replace the original answer to the target question with a random answer from the training dataset. We further compare the word-based cosine similarity between the original and the sampled answer, and, in some rare cases, we turn a “bad” answer into “good” one if it is too similar to the original “good” answer."
1902.04574,dataset,113,,0,"The remainder of this paper is organized as follows: Section 2 presents some related work in the domain of conversational agents and answer combination. Section 3 describes our framework and the general workﬂow for answer re-ranking. Section 4 introduces the original dataset and explains how we used in to build a new, task-speciﬁc one with negative sampling; it also offers insights about the dialogs and the pre-processing. Section 5 describes our experiments, and gives details about the training parameters. Section 6 presents the performance of each model and discusses the results. Finally, Section 7 concludes and suggests possible directions for future work."
1902.04574,dataset,117,,1,"In this work, we focus on ﬁnding the most suitable answer for a question, where each candidate can be produced by a different system, e.g., knowledge-based, rule-based, deep neural network, retrieval, etc. In particular, we propose a re-ranking framework based on machine reading comprehension [1–3] for question–answer pairs. Moreover, instead of selecting the top candidate from the re-ranker’s output, we use probabilistic sampling that aims to diversify the agent’s language and to up-vote popular answers from different input models. We train our model using negative sampling based on question–answer pairs from the Twitter Customer Support Dataset."
1902.04574,dataset,118,,0,"Next, we combine the top-K answers from different models: IR and seq2seq. We did not include the Transformer in the mix as its output is generative and similar to that of the seq2seq model; moreover, as we have seen in Table 3 above, it performs worse than seq2seq on our dataset. We set K = 2 for the baseline, Random Top Answer, which selects a random answer from the union of the top K answers by the models involved in the re-ranking. For the remaining re-ranking experiments, we use K = 5. We found these values using cross-validation on the training dataset, trying 1–5."
1902.04574,dataset,127,,0,"Lowe, R.; Pow, N.; Serban, I.; Pineau, J. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Prague, Czech Republic, 2–4 September 2015; pp. 285–294. Lison, P.; Tiedemann, J. OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation, Portorož, Slovenia, 23–28 May 2016. Reddy, S.; Chen, D.; Manning, C.D. CoQA: A conversational question answering challenge. arXiv 2018, arXiv:1808.07042."
1902.04574,dataset,186,,0,"Table 2 shows the results for the auxiliary task of question–answer goodness classiﬁcation. The ﬁrst column is the name of the model. It is followed by three columns showing the type of embedding used, the size of the hidden layer, and the number of heads (see Section 3.2). The last column reports the accuracy. Since our dataset is balanced (we generate about 50% positive, and about 50% negative examples), accuracy is a suitable evaluation measure for this task. The top row of the table shows the performance for a majority class baseline. The following lines show the results for our full QANet-based model when using different kinds of embeddings. We can see that contextualized sentence-level embeddings are preferable to using simple word embeddings as in GloVe or token-level ELMo embeddings. Moreover, while token-level ELMo outperforms GloVe when the size of the network is small, there is no much difference when the number of parameters grows (dmodel = 128, #Heads = 8)."
1902.04574,dataset,254,,0,"We can see in Table 3 that the seq2seq model outperforms IR by a margin on all ﬁve evaluation measures, which is consistent with previous results in the literature. What is surprising, however, is the relatively poor performance for the Transformer, which trails behind the seq2seq model on all evaluation measures. We hypothesize that this is due to the Transformer having to learn more parameters as it operates with higher-dimensional word embeddings. Overall, the Transformer is arguably slightly better than the IR model, outperforming it on three of the ﬁve evaluation measures. The last row of Table 3 is not an individual model; it is our re-ranker applied to the top answers returned by the IR model. In particular, we use QANet with Sentence level ELMo (dmodel = 128, #Heads = 8). We took the top-5 answer candidates (the value of 5 was found using cross-validation on the training dataset) from the IR model, and we selected the best answer based on our re-ranker’s scores. We can see that re-ranking yields improvements for all evaluation measures: +1.18 on BLEU@2, +0.93 on ROUGE_L, +1.12 on Embedding Average, +0.67 on Greedy Matching, and +1.64 in Vector Extrema. These results show that we can get sizable performance gains when re-ranking the top-K predictions of a single model; below we will combine multiple models."
1902.04574,"dataset, data",193,,1,"Abstract: Recent advances in deep neural networks, language modeling and language generation have introduced new ideas to the ﬁeld of conversational agents. As a result, deep neural models such as sequence-to-sequence, memory networks, and the Transformer have become key ingredients of state-of-the-art dialog systems. While those models are able to generate meaningful responses even in unseen situations, they need a lot of training data to build a reliable model. Thus, most real-world systems have used traditional approaches based on information retrieval (IR) and even hand-crafted rules, due to their robustness and effectiveness, especially for narrow-focused conversations. Here, we present a method that adapts a deep neural architecture from the domain of machine reading comprehension to re-rank the suggested answers from different models using the question as a context. We train our model using negative sampling based on question–answer pairs from the Twitter Customer Support Dataset. The experimental results show that our re-ranking framework can improve the performance in terms of word overlap and semantics both for individual models as well as for model combinations."
1902.04574,"dataset, open-source data, data available",87,,1,"In early 2018, this situation changed as a new open dataset for Customer Support on Twitter [36] was made available on Kaggle. It contains 3M tweets and replies for twenty big companies such as Amazon, Apple, Uber, Delta, and Spotify, among others. As customer support topics from different organizations are generally unrelated to each other, we focus only on tweets related to Apple support, which represents the largest number of tweets in the corpus."
1902.04574,"dataset, publicly available, data",86,,0,"The data and the resources that could be used to train customer support conversational agents are generally very scarce, as companies keep conversations locked on their own proprietary support systems. This is due to customer privacy concerns and to companies not wanting to make public their know-how and the common issues about their products and services. An extensive 2015 survey on available dialog corpora by Serban et al. [35] found no good publicly available dataset for real-world customer support."
1902.04574,github,15,,0,37. Codebase of Towards Automated Customer Support. Available online: https://github.com/mhardalov/
1902.04574,package,251,,0,"Yu, A.W.; Dohan, D.; Luong, M.T.; Zhao, R.; Chen, K.; Norouzi, M.; Le, Q.V. QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension. In Proceedings of the 2018 International Conference on Learning Representations; Vancouver, BC, Canada, 30 April–3 May 2018. Seo, M.; Kembhavi, A.; Farhadi, A.; Hajishirzi, H. Bi-directional attention ﬂow for machine comprehension. In Proceedings of the 2017 International Conference on Learning Representations; Toulon, France, 24–26 April 2017. Chen, D.; Fisch, A.; Weston, J.; Bordes, A. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, BC, Canada, 30 July–4 August 2017; pp. 1870–1879. Papineni, K.; Roukos, S.; Ward, T.; Zhu, W.J. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA, 7–12 July 2002; pp. 311–318. Lin, C.Y. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of the ACL Workshop on Text Summarization Branches Out, Barcelona, Spain, 25–26 July, 2004; pp. 74–81."
1902.07688,benchmark,70,,0,"The preliminary evaluation is conducted in terms of the validation of clustering result and justified query benchmark generation. We used eight ontologies from Bio2RDF release 3 to evaluate our system. Detailed is given in Table 1. In addition, we eliminated some RDF built-in predicates and types for getting the best clustering result. The total number of predicate for HFCM reduced from 1099 to 1064."
1902.07688,"case study data, data, benchmark",195,,0,"We    categorize research in this area into two parts: query generation and query processing. For query generation, SP2Bench [29] proposed a query design system focusing on generating queries with combination of different operations. But this query generation was not designed from a semantic perspective for cross domain. LUBM [30] and BSBM [31] generated benchmarks on university and ecommerce respectively, but neither benchmark was based on more than one domain. FedBench [32] provided a benchmark suite for federated queries on semantic data which can cover semantic multiple domain data use cases. However, query benchmarks were manually generated by authors. Our approach provides a way to automatically help people find the semantic relationship without acquiring knowledge explicitly. MedTQ provided a dynamic topic query generation approach over medical ontologies [33]. BmQGen [34, 35] supported query generation for a specific biomedical detecting task on postsurgical complications [36]. Zhu et al. investigated the usage of the PharmGKB knowledge base for searching and inferring repositioning breast cancer drugs [37]."
1902.07688,"case study data, data, benchmark",342,,0,"Mizan: a system for dynamic load balancing in large-scale graph processing. Proceedings of the 8th ACM European Conference on Computer Systems: ACM; 2013. p. 169-82. [25] Malewicz G, et al. Pregel: a system for large-scale graph processing. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data: ACM; 2010. p. 135-46. [26] Gerbessiotis AV, et al. Direct bulk-synchronous parallel algorithms. Journal of parallel and distributed computing. 1994;22:251-67. [27] Simmhan Y, et al. Goffish: A sub-graph centric framework for large-scale graph analytics. European Conference on Parallel Processing: Springer; 2014. p. 451-62. [28] Lang J, et al. Similarity-driven semantic role induction via graph partitioning. Computational Linguistics. 2014;40:633-69. [29] Schmidt M, et al. SP^ 2Bench: a SPARQL performance benchmark. 2009 IEEE 25th International Conference on Data Engineering: IEEE; 2009. p. 222-33. [30] Guo Y, et al. LUBM: A benchmark for OWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 2005;3:158-82. [31] Bizer C, et al. The berlin sparql benchmark. International Journal on Semantic Web and Information Systems (IJSWIS). 2009;5:1-24. [32] Schmidt M, et al. Fedbench: A benchmark suite for federated semantic data query processing. International Semantic Web Conference: Springer; 2011. p. 585600. [33] Shen F, et al. MedTQ: Dynamic Topic Discovery and Query Generation for Medical Ontologies. arXiv preprint arXiv:180203855. 2018. [34] Shen F, et al. BmQGen: Biomedical query generator for knowledge discovery. 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): IEEE; 2015. p. 1092-7."
1902.07688,"case study data, data, benchmark",343,,0,"[20] Shen F, et al. Using semantic web technologies for quality measure phenotyping algorithm representation and automatic execution on EHR data. IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI): IEEE; 2014. p. 531-4. [21] Tao C, et al. Phenotyping on EHR data using OWL and semantic web technologies. International Conference on Smart Health: Springer; 2013. p. 31-2. [22] Peterson KJ, et al. Mining Hierarchies and Similarity Clusters from Value Set Repositories. AMIA Annual Symposium Proceedings: American Medical Informatics Association; 2017. p. 1372. [23] Yang S, et al. Towards effective partition management for large graphs. Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data: ACM; 2012. p. 517-28. [24] Khayyat Z, et al. Mizan: a system for dynamic load balancing in large-scale graph processing. Proceedings of the 8th ACM European Conference on Computer Systems: ACM; 2013. p. 169-82. [25] Malewicz G, et al. Pregel: a system for large-scale graph processing. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data: ACM; 2010. p. 135-46. [26] Gerbessiotis AV, et al. Direct bulk-synchronous parallel algorithms. Journal of parallel and distributed computing. 1994;22:251-67. [27] Simmhan Y, et al. Goffish: A sub-graph centric framework for large-scale graph analytics. European Conference on Parallel Processing: Springer; 2014. p. 451-62. [28] Lang J, et al. Similarity-driven semantic role induction via graph partitioning. Computational Linguistics. 2014;40:633-69. [29] Schmidt M, et al. SP^ 2Bench: a SPARQL performance benchmark. 2009 IEEE 25th International Conference on Data Engineering: IEEE; 2009. p. 222-33. [30] Guo Y, et al."
1902.07688,"case study data, data, benchmark",350,,0,"[27] Simmhan Y, et al. Goffish: A sub-graph centric framework for large-scale graph analytics. European Conference on Parallel Processing: Springer; 2014. p. 451-62. [28] Lang J, et al. Similarity-driven semantic role induction via graph partitioning. Computational Linguistics. 2014;40:633-69. [29] Schmidt M, et al. SP^ 2Bench: a SPARQL performance benchmark. 2009 IEEE 25th International Conference on Data Engineering: IEEE; 2009. p. 222-33. [30] Guo Y, et al. LUBM: A benchmark for OWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 2005;3:158-82. [31] Bizer C, et al. The berlin sparql benchmark. International Journal on Semantic Web and Information Systems (IJSWIS). 2009;5:1-24. [32] Schmidt M, et al. Fedbench: A benchmark suite for federated semantic data query processing. International Semantic Web Conference: Springer; 2011. p. 585600. [33] Shen F, et al. MedTQ: Dynamic Topic Discovery and Query Generation for Medical Ontologies. arXiv preprint arXiv:180203855. 2018. [34] Shen F, et al. BmQGen: Biomedical query generator for knowledge discovery. 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): IEEE; 2015. p. 1092-7. [35] Shen F, et al. Predicate oriented pattern analysis for biomedical knowledge discovery. Intelligent information management. 2016;8:66. [36] Shen F, et al. Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes. Journal of Healthcare Informatics Research. 2018:1-16. [37] Zhu Q, et al. Exploring the pharmacogenomics knowledge base (PharmGKB) for repositioning breast cancer drugs by leveraging Web ontology language (OWL) and cheminformatics approaches. Biocomputing 2014: World Scientific; 2014. p. 172-82."
1902.07688,"data available, data",8,,0,amounts of data in a standalone machine.
1902.07688,"data, benchmark",76,,1,Validation The proposed approach will be validated by comparing with more existing algorithm and framework     Scalability Indexing approach will be applied to facilitate the management of big data. All datasts from Bio2RDF will be used to test the scalability of the proposed framework    Benchmark The existing Bio2RDF query sets will be analyzed to compare with our benchmark generation scheme in order to improve the coverage and semantic for each cluster association. 9. Reflections
1902.07688,"data, database",339,,0,"[38] Papailiou N, et al. H 2 RDF+: High-performance distributed joins over largescale RDF graphs. 2013 IEEE International Conference on Big Data: IEEE; 2013. p. 255-63. [39] Zeng K, et al. A distributed graph engine for web scale RDF data. Proceedings of the VLDB Endowment: VLDB Endowment; 2013. p. 265-76. [40] Neumann T, et al. RDF-3X: a RISC-style engine for RDF. Proceedings of the VLDB Endowment. 2008;1:647-59. [41] Zou L, et al. gStore: answering SPARQL queries via subgraph matching. Proceedings of the VLDB Endowment. 2011;4:482-93. [42] Noy NF, et al. BioPortal: ontologies and integrated data resources at the click of a mouse. Nucleic acids research. 2009;37:W170-W3. [43] Povey S, et al. The HUGO gene nomenclature committee (HGNC). Human genetics. 2001;109:678-80. [44] Bult CJ, et al. The Mouse Genome Database (MGD): mouse biology and model systems. Nucleic acids research. 2008;36:D724-D8. [45] Shannon P, et al. Cytoscape: a software environment for integrated models of biomolecular interaction networks. Genome research. 2003;13:2498-504. [46] Shen F, et al. Biobroker: Knowledge discovery framework for heterogeneous biomedical ontologies and data. Journal of Intelligent Learning Systems and Applications. 2018;10:1. [47] Shen F, et al. Knowledge discovery from biomedical ontologies in cross domains. PloS one. 2016;11:e0160005. [48] Shen F. A pervasive framework for real-time activity patterns of mobile users. 2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops): IEEE; 2015. p. 248-50."
1902.07688,"data, database",347,,0,"[10] Hewett M, et al. PharmGKB: the pharmacogenetics knowledge base. Nucleic acids research. 2002;30:163-5. [11] Hamosh A, et al. Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic acids research. 2005;33:D514-D7. [12] Firth HV, et al. DECIPHER: database of chromosomal imbalance and phenotype in humans using ensembl resources. The American Journal of Human Genetics. 2009;84:524-33. [13] Rath A, et al. Representation of rare diseases in health information systems: the Orphanet approach to serve a wide range of end users. Human mutation. 2012;33:803-8. [14] Dasgupta S, et al. SMARTSPACE: Multiagent based distributed platform for semantic service discovery. IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2014;44:805-21. [15] Vaka P, et al. PEMAR: A pervasive middleware for activity recognition with smart phones. 2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops): IEEE; 2015. p. 409-14. [16] Zhang Y, et al. An integrative computational approach to identify diseasespecific networks from PubMed literature information. 2013 IEEE International Conference on Bioinformatics and Biomedicine: IEEE; 2013. p. 72-5. [17] Shen F, et al. SAMAF: Situation aware mobile apps framework. 2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops): IEEE; 2015. p. 26-31. [18] Shen F. Situation Aware Mobile Apps Framework: University of Missouri-Kansas City; 2012. [19] Peng S, et al. Leveraging Association Rule Mining to Detect Pathophysiological Mechanisms of Chronic Kidney Disease Complicated by Metabolic Syndrome. 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): IEEE; 2018. p. 1302-9."
1902.07688,"data, database",348,,0,"Cytoscape: a software environment for integrated models of biomolecular interaction networks. Genome research. 2003;13:2498-504. [46] Shen F, et al. Biobroker: Knowledge discovery framework for heterogeneous biomedical ontologies and data. Journal of Intelligent Learning Systems and Applications. 2018;10:1. [47] Shen F, et al. Knowledge discovery from biomedical ontologies in cross domains. PloS one. 2016;11:e0160005. [48] Shen F. A pervasive framework for real-time activity patterns of mobile users. 2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops): IEEE; 2015. p. 248-50. [49] Shen F. A Graph Analytics Framework For Knowledge Discovery 2016. [50] Li D, et al. Towards a multi-level framework for supporting systematic review— A pilot study. 2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): IEEE; 2014. p. 43-50. [51] Li D, et al. A text-mining framework for supporting systematic reviews. American journal of information management. 2016;1:1. [52] Shen F, et al. Phenotypic Analysis of Clinical Narratives Using Human Phenotype Ontology. Studies in health technology and informatics. 2017;245:581-5. [53] Shen F, et al. Using Human Phenotype Ontology for Phenotypic Analysis of Clinical Notes. Studies in health technology and informatics. 2017;245:1285-. [54] Son JH, et al. Deep phenotyping on electronic health records facilitates genetic diagnosis by clinical exomes. The American Journal of Human Genetics. 2018;103:58-73. [55] Rastegar-Mojarad M, et al. Need of informatics in designing interoperable clinical registries. International journal of medical informatics. 2017;108:78-84. [56] Chen Z, et al. Collaborative mobile-cloud computing for civil infrastructure condition inspection. Journal of Computing in Civil Engineering. 2013;29:04014066."
1902.07688,"data, database",350,,0,"[3] Belleau F, et al. Bio2RDF: towards a mashup to build bioinformatics knowledge systems. Journal of biomedical informatics. 2008;41:706-16. [4] Schriml LM, et al. Disease Ontology: a backbone for disease semantic integration. Nucleic acids research. 2011;40:D940-D6. [5] Ashburner M, et al. Gene Ontology: tool for the unification of biology. Nature genetics. 2000;25:25. [6] Wishart DS, et al. DrugBank: a comprehensive resource for in silico drug discovery and exploration. Nucleic acids research. 2006;34:D668-D72. [7] Köhler S, et al. The Human Phenotype Ontology project: linking molecular biology and disease through phenotype data. Nucleic acids research. 2013;42:D966D74. [8] Smith CL, et al. The mammalian phenotype ontology: enabling robust annotation and comparative analysis. Wiley Interdisciplinary Reviews: Systems Biology and Medicine. 2009;1:390-9. [9] Smith CL, et al. The Mammalian Phenotype Ontology as a tool for annotating, analyzing and comparing phenotypic information. Genome biology. 2005;6:R7. [10] Hewett M, et al. PharmGKB: the pharmacogenetics knowledge base. Nucleic acids research. 2002;30:163-5. [11] Hamosh A, et al. Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic acids research. 2005;33:D514-D7. [12] Firth HV, et al. DECIPHER: database of chromosomal imbalance and phenotype in humans using ensembl resources. The American Journal of Human Genetics. 2009;84:524-33. [13] Rath A, et al. Representation of rare diseases in health information systems: the Orphanet approach to serve a wide range of end users. Human mutation. 2012;33:803-8."
1902.07688,database,194,,0,"For example, in life science research, a researcher expertise on disease A wants to find the corresponding ensemble genome G for the gene symbol and marker that re       lated to disease A as well as the drug D for disease A. In this use case, A, G and D represent three different domains, such as Disease Ontology [4], Gene Ontology [5], and DrugBank [6]. More knowledge bases and ontology exist in the life science domain, including the Human Phenotype Ontology [7], the Mammalian Phenotype Ontology [8, 9], the Pharmacogenetics Knowledge Base (PharmGKB) [10], the Onine Mendeian Inheritance in Man (OMIM) [11], the Database of Chromosomal Imbalance and Phenotypc in Humans Using Ensembl Resources (DECIPHER) [12], the Orphanet [13] and so on. With tremendous heterogeneous knowledge graphs, it is very difficult for researchers who only know disease domain to make a query to discover knowledge across different domains in a comprehensive way."
1902.07688,dataset,5,,0,Table 1: Bio2RDF Datasets
1902.07688,dataset,169,,0,"After finalizing Fuzzy C-means with a probability based similarity score as the measurement approach, we apply a hierarchical Fuzzy C-means (HFCM) algorithm to the dataset trying to partition each cluster into an indivisible unit. For each level of processing, we still used silhouette width to determine the proper number of clusters.    As Figure 2 shows, in first level running of HFCM, we get five clusters. Then in second level running of HFCM, only cluster 2 (C2) and cluster 3 (C3) can be further clustered. According to silhouette width, we get the best number of cluster 2 for C2 and 4 for C3 respectively. As a result, after level 2 running, we get 9 different clusters. Because the number of cluster does not change as we run the 3rd level HFCM, the algorithm stops here. The solutions from the algorithm yield the number of clusters as 9."
1902.07688,"dataset, data",85,,0,"Today, the main challenge we are facing in knowledge discovery research is the big data problem associated with large, complex, and dynamic variation of format. Increasingly, we are also seeing the emergence of cross domain among different datasets. In this drive, the large amounts of data have been specified and shared via machine-readable formats, such as a Resource Description Framework (RDF) [1] and Ontology Web Language (OWL) [2]."
1902.07688,"dataset, open-source data, data",149,,0,"However, with increasing research in big data, more and more datasets from different domains have been added to the existing linked open data (e.g., Bio2RDF [3]), which makes the highly complex relationships and condensed interlinks among the large number of these knowledge bases. To some extent, the speed of data growing in terms of multiple domains is much faster than that of the large amount of knowledge people can acquire and consume in their daily lives. In other words, since cross domain datasets are physically grouped instead of semantically clustered, it is extremely difficult for people without expertise to extract knowledge from various domains nowadays. Therefore, there is a big gap between human limited knowledge and the large amount of cross domain knowledge that can be discovered from this huge amount of data."
1903.06464,"code, python",126,,0,"We compared the performance by independently reproducing the Python code related in the CACR paper. There was no detailed experimental information such as frequency in the actual paper. Since no frequency was mentioned, we assume that the performance described in the CACR paper is based on a frequency of one, and we compare our performance with the performance described in the paper of CACR as shown in Table 2. For MAP, MRR, and Recal@10, our model outperform, but after Recall@10, it does underperform. Based on experience, we guessed this to be a phenomenon that occurs when the classiﬁcation label value is returned with a high frequency of cited papers."
1903.06464,"data, dataset provided",116,,0,"The data actually collected was noisy because the latex documents were not consistent format. After we automatically collected the necessary data, we removed the noisy data manually. For example, in the case of CiteseerX 6, the citation symbol corresponding to the placeholder is left in the context and data is provided. However, in this case, the placeholder text itself was used for overﬁtting learning, so the text can be used to tell the correct answer. Exactly, the reminded placeholder can be used as crucial evidence of prediction. Therefore, our ﬁnal work was post-processed manually because this noise may remain when mechanically collected."
1903.06464,dataset,1,,0,Dataset
1903.06464,dataset,2,,0,Dataset Model
1903.06464,dataset,3,,0,2 Proposed Dataset
1903.06464,dataset,3,,0,2.1 Dataset overview
1903.06464,dataset,22,,0,Dataset name # of total papers # of base papers # of cited papers # of citation context # Paper published year
1903.06464,dataset,34,,0,"• We measure performance according to paper occurrence in the aggregate dataset. Citation of speciﬁc papers is rare; however, we need to understand how our model performs when it happens."
1903.06464,dataset,83,,0,"[Kang et al., 2018] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. A dataset of peer reviews (peerread): Collection, insights and nlp applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1647–1661, 2018."
1903.06464,dataset,90,,0,"4 Experiments 4.1 Experiments overview We compare the proposed model with CACR [Yang et al., 2018], one of the existing SOTA models, with a focus on performance. We use the AAN and FullTextPeerRead (FTPR) datasets in our experiments and use Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Recall@K as evaluation metrics. The purpose of our experiments is to investigate the following topics, including the overall performance of our model."
1903.06464,dataset,90,,0,"The structure of the proposed model is linked to the baseline CACR [Yang et al., 2018]. CACR has both a paper encoder and a citation context encoder. CACR demonstrates the performance of SOTA as the most recent context-aware citation recommendation model using an AAN dataset and an LSTM model. In the CACR model, a paper encoder was constructed using author, venue, and abstract information in the paper. Our model constructed the citation encoder with GCN solely using citation information."
1903.06464,dataset,116,,0,"We constructed two new datasets for the context-aware citation recommendation task. We suggested revisions of the existing datasets, AAN [Radev et al., 2013] and FullTextPeerRead which is an expansion of PeerRead dataset [Kang et al., 2018]. AAN and PeerRead datasets have well-organized bibliometric information. The PeerRead dataset mainly provides peer reviews of submitted papers in top-tier venues of the artiﬁcial intelligence ﬁeld, along with bibliometric information. Since all existing datasets of the task lack information in the citation context, our focus is on gathering context information with metadata. Therefore, the AAN and PeerRead datasets were reprocessed to create the dataset."
1903.06464,dataset,172,,0,"2.3 Statics of dataset Finally, the statics of the dataset we built are as shown in Table 1. The number of extracted datasets were less than the original number of AAN or PeerRead dataset because we needed to remove paper the PDFs that did not use latex or were very noisy from being processed with arXiv Vanity. In Table 1 below, Total ”# of base papers” is a paper that cites other researches. We have metadata information of the paper that is used as an input for the classiﬁcation task. ”# of cited papers” is a cited paper. In addition, we extracted paragraph units on both sides of the citation symbol, and ”# of citation context” means the sum of the number of sentences which are in the extracted paragraph. Further, ”# of total papers” refers to the total number of papers covered by base paper and cited paper, excluding duplicates."
1903.06464,"dataset, data",36,,1,"Regarding the context-aware citation recommendation study, existing datasets are not up-to-data and there is no clear context detection. To address this problem, we devised and released the FullTextPeerRead dataset. The proposed dataset"
1903.06464,"dataset, data",199,,0,"The purpose of this study is to provide datasets and stateof-the-art models suitable for the context-aware paper recommendation task research, and in turn provide researchers with an improved paper writing environment. The main contributions of this study are as follows: First, we built reproducible benchmarking datasets for the task. We preprocessed the existing AAN dataset [Radev et al., 2013; Radev et al., 2009; Dragomir R. Radev, 2009] to ﬁt the task, and constructed new dataset called FullTextPeerRead using PeerRead 4 [Kang et al., 2018]. Second, we constructed the state-of-the-art model for the task using BERT [Devlin et al., 2018] and Graph Convolution Networks (GCN) [Kipf and Welling, 2016a]. Because scientiﬁc papers contain textual contents data, and metadata that can be represented as a graph, we use BERT, which recently proved to have the highest performance level in the ﬁeld of Natural Language Processing (NLP) for textual data, and GCN for network-based metadata. Finally, we investigated various factors to affect task performance through experiments."
1903.06464,"dataset, data",202,,0,"4.2 Experiments setting Experimental dataset In the experiment, the AAN dataset used data published before 2014, whereas the FullTextPeerRead dataset comprised paper data published before 2018. After interpreting the data, with sufﬁcient context, and the metadata among datasets, AAN and FullTextPeerRead yielded 6,500 and 4,898 papers, respectively. The datasets were divided into two parts: the AAN dataset used 5,806 pre-2013 papers for the training set, and the remaining 973 others for the test set prior to 2013, and the test set 973 for the test set. Regarding the FullTextPeerRead dataset, 3,411 pre-2017 papers were used for the training set, and 2,559 papers from 2017 were used for the test set. We applied two text lengths, namely 50 and 100, to the citation context features, and measured the related shift in performance. Furthermore, in order to test performance according to the text input characteristics, we conducted an experiment comparing single and pair context characteristics. For this purpose, we used a single context consisting of a 100 text length on the left side of a citation placeholder, and the pair"
1903.06464,"dataset, data",252,,1,"With the tremendous growth in the number of scientiﬁc papers being published, searching for references while writing a scientiﬁc paper is a timeconsuming process. A technique that could add a reference citation at the appropriate place in a sentence will be beneﬁcial. In this perspective, context-aware citation recommendation has been researched upon for around two decades. Many researchers have utilized the text data called the context sentence, which surrounds the citation tag, and the metadata of the target paper to ﬁnd the appropriate cited research. However, the lack of wellorganized benchmarking datasets and no model that can attain high performance has made the research difﬁcult. In this paper, we propose a deep learning based model and well-organized dataset for contextaware paper citation recommendation. Our model comprises a document encoder and a context encoder, which uses Graph Convolutional Networks (GCN) layer and Bidirectional Encoder Representations from Transformers (BERT), which is a pretrained model of textual data. By modifying the related PeerRead dataset, we propose a new dataset called FullTextPeerRead containing context sentences to cited references and paper metadata. To the best of our knowledge, This dataset is the ﬁrst well-organized dataset for context-aware paper recommendation. The results indicate that the proposed model with the proposed datasets can attain state-of-the-art performance and achieve a more than 28% improvement in mean average precision (MAP) and recall@k."
1903.06464,"dataset, database, data",130,,0,"2.2 Data acquistion We used arXiv Vanity 5 to create the dataset. arXiv Vanity is a site that converts a latex-based PDF ﬁle to HTML document. Our goal is to extract the context information on both sides of the citation symbol, such as [1] or [Choi et al. 2016], together with the reference paper information. To do this, we parsed latex into HTML via arXiv Vanity, and used a regular expression to recognize the citation symbol in the document. Next, the sentences on both sides of the citation symbol were stored on a database with reference article information. We stored the collected information together with the existing metadata and build it into a database."
1903.06464,"dataset, dataset provided, data",97,,0,"Even though the task has been a relatively constantly researched area, one of the most challenging aspects of this study is that there is no benchmarking dataset against which proper performance can be measured. In general, this task needs to use metadata along with the context surrounding the cited paper. To the best of our knowledge, suitable datasets have not been disclosed. Among the commonly used data, the ACL Anthology Network (AAN) 1 dataset does not provide paper sentences and metadata in a preprocessed form, and the"
1903.06464,"dataset, open-source data, data",34,,0,"Table 1: Dataset description. Because the AAN policy prohibits disclosure of modiﬁed data, we cannot disclose it. We will open the dataset after receiving a grant from the AAN"
1903.06464,github,12,,0,2https://dblp.uni-trier.de/xml/ 3https://psu.app.box.com/v/refseer 4https://github.com/allenai/PeerRead 5https://www.arxiv-vanity.com/
1903.06464,"used dataset, dataset, dataset provided",52,,0,"DBLP dataset 2 only provides bibliographic information. In a recently published [Huang et al., 2015], CiteseerX datasets 3 only provided context and citation information and did not provide meta information simultaneously. As a result, related studies have failed to use the same benchmarking dataset."
1904.04307,"data available, data, data https, dataset, github",164,,1,"The main contributions of this work include (i) the development of linguistic resources for a low-resourced non-English language, (ii) the translation and rating of three datasets for Thai based on English WordSim-353, SimLex-999 and SemEval-500, (iii) the provision of the datasets including accompanying data such as the ﬁnegrained annotator data and IAA computations, (iv) an evaluation tool which makes it very easy to evaluate any Thai word embedding model with the new datasets, (v) extensive baseline evaluations and a basic variant of dealing with OOV words, and ﬁnally, (vi) the analysis and discussion of the speciﬁcs of Thai language, esp. with regards to OOV words. The datasets (and accompanying data) are available at: https://github.com/gwohlgen/thai_word_ similarity, and the evaluation tool for easy evaluation of Thai embeddings can be found at: https://github. com/gwohlgen/word-embeddings-benchmarks."
1904.04307,database,18,,0,"[20] G. A. Miller, “Wordnet: a lexical database for english,” Communications"
1904.04307,dataset,3,,0,B. ENGLISH-LANGUAGE DATASETS
1904.04307,dataset,3,,0,C. DATASET CREATION
1904.04307,dataset,4,,0,1) Dataset Translation
1904.04307,dataset,5,,0,A. WORD SIMILARITY DATASETS
1904.04307,dataset,7,,0,III. CONSTRUCTION OF THAI LANGUAGE DATASETS
1904.04307,dataset,9,,0,Word Similarity Datasets for Thai: Construction and Evaluation
1904.04307,dataset,11,,0,P. Netisopakul et al.: Word Similarity Datasets for Thai
1904.04307,dataset,12,,0,1. Comparison between English and Thai datasets with examples from WordSim-353
1904.04307,dataset,16,,0,"INDEX TERMS dataset creation, distributional semantics, Thai language, word embeddings, word similarity"
1904.04307,dataset,29,,0,"Given the newly created word similarity datasets, in this section we provide baseline evaluations for Thai word embedding models, as well as a discussion of results."
1904.04307,dataset,30,,0,"A considerable number of English-language word similarity datasets has been released in the last decades, which differ in size, difﬁculty, rating scale, and other features."
1904.04307,dataset,31,,0,"In this section, we introduce the word similarity datasets, the translation process, as well as the rating of word pairs for the Thai language and inter-annotator agreement."
1904.04307,dataset,31,,0,is now at 22.1%. This simple approach helps to raise the correlation scores to over 0.5 for all datasets. The model Kyu-ft consistently provides the best results.
1904.04307,dataset,33,,0,"Table 2. Inter-annotator agreement statistics for the three datasets. Spearman ρ (S), Pearson ρ (P), and harmonic mean of the two (HM)."
1904.04307,dataset,35,,0,"[9] C. Chen and W. Ma, “Word embedding evaluation datasets and wikipedia title embedding for chinese,” in LREC. European Language Resources Association (ELRA), 2018."
1904.04307,dataset,37,,0,"[22] Y. Sakaizawa and M. Komachi, “Construction of a japanese word similarity dataset,” in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018."
1904.04307,dataset,39,,0,"The outline of this publication is as follows: In Section II we discuss existing work related to word similarity datasets and their translation. Section III explains the individual datasets, and the dataset construction process (translation"
1904.04307,dataset,39,,0,"We use the newly created datasets to provide baseline evaluations for Thai word embedding models. For this purpose, we use pretrained models, which were found by search engine queries and by asking in Thai NLP groups"
1904.04307,dataset,44,,0,"RG-65 is a classic word similarity dataset presented by Rubenstein and Goodenough [6] already in 1965. It contains only 65 word pairs, and focuses on similarity rather than relatedness. 15 annotators rated the similarity of each word pair."
