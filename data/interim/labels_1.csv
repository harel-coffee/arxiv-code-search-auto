id,pattern,token_count,update_date,label,para
1401.8008,"code, package, code available, github, code package",56,04/27/22,2,"So we can solve the dual comparison problem (18) using any eﬃcient SVM solver, such as libsvm (Chang & Lin 2011). We used the R interface in the kernlab package (Karatzoglou et al. 2004), and our code is available in the rankSVMcompare package on Github."
1401.8008,data,103,,,"Comparison data also results when considering subjective human evaluations of pairs of items. For example, if each item is a movie, a person might say that Les Misérables is better than Star Wars, and The Empire Strikes Back is as good as Star Wars. Another example is rating food items such as wine, in which a person may prefer one wine to another, but not be able to perceive a diﬀerence between two other wines. In this context, it is important to use a model which can predict no diﬀerence between two items."
1401.8008,data,11,,,4. Comparison to SVMrank in sushi and simulated data sets
1401.8008,data,112,,,"Fig. 6. Test AUC for each model used after training on the ﬁrst 4 months of match data in the 8 different 12-month periods. All SVM model AUCs are shown in addition to AUC of the ELO, Glicko scores and trivial model. The plots in black show the trivial AUC calculated by predicting the most common positive label. The plots in dark blue are AUC values obtained from using Glicko or ELO scores only. The plots in light blue show the AUC distribution from models using only ELO and Glicko features and plots in white are AUC values from models using all computed features."
1401.8008,data,113,,,"The goal of learning to compare is to accurately predict a test set of labeled pairs (2), which includes equality yi = 0 pairs. We test the SVMcompare algorithm alongside two baseline models that use SVMrank (Joachims 2002). We chose SVMrank as a baseline because of its similar large-margin learning formulation, to demonstrate the importance of directly modeling the equality yi = 0 pairs. SVMrank does not directly model the equality yi = 0 pairs, so we expect that the proposed SVMcompare algorithm makes better predictions when these data are present. The diﬀerences between the algorithms are summarized in Table 2:"
1401.8008,data,115,,,"Lemma 3.1 establishes the fact that one can learn a ranking function r and a corresponding comparison function c1, deﬁned in (5), by solving either the LP (7) or the QP (12). To make corresponding learning problems for non linearly-separable data as deﬁned by the linear separability test in this section, one can add slack variables to either the QP or the LP. In the next subsection, we pursue only the QP, since it leads to a dual problem with a sparse solution that can be solved by any standard SVM solver such as libsvm (Chang & Lin 2011)."
1401.8008,data,138,,,"In this subsection, we assume the data are not linearly separable, and want to learn a nonlinear ranking function. We deﬁne a positive deﬁnite kernel κ : Rp × Rp → R, which implicitly deﬁnes an enlarged set of features Φ(x) (middle panel of Figure 1). As in (9), we learn a function f (x) = β + u⊺Φ(x) which is aﬃne in the feature space. Let α, α′ ∈ Rm be coeﬃcients such that u = i), and so we have m i=1(αiκ(˜xi, x) + α′ f (x) = β + i, x)). We then use Lemma 3.1 to deﬁne the ranking function"
1401.8008,data,15,,,"Overall from the sushi data, it is clear that the proposed SVMcompare model performs"
1401.8008,data,15,,,"generalizes to a test set of data, as measured by the zero-one loss:"
1401.8008,data,161,,,"In Figure 5 we ﬁxed the number of training pairs n = 400 and varied the proportion ρ of equality pairs for the three simulated squared norm ranking functions r. We select the model with maximum area under the validation set ROC curve, then use test set AUC to evaluate the learned models. All methods perform close to the optimal true ranking function when r(x) = ||x||2 2. For the other patterns, it is clear that all the methods perform similarly when there are mostly inequality pairs (ρ = 0.1), since SVMrank was designed for this type of training data. In contrast, when there are mostly equality pairs (ρ = 0.9), the compare and rank2 methods clearly outperform the rank method, which ignores the equality pairs. It is also clear that the rank2 and compare methods perform similarly in terms of test AUC."
1401.8008,data,19,,,"Joachims, T. (2002), Optimizing search engines using clickthrough data, in ‘KDD’."
1401.8008,data,199,,,"In the supervised learning to rank problem (Li 2011), we are given labeled pairs of items x, x′, where the label y ∈ {−1, 1} indicates which item in the pair should be ranked higher. The goal is to learn a ranking function r(x) ∈ R which outputs a real-valued rank for each item. In this paper we consider a related problem in which the expanded label space y ∈ {−1, 0, 1} includes the y = 0 label which indicates that there should be no rank diﬀerence. In this context the goal is to learn a comparison function c(x, x′) ∈ {−1, 0, 1}. Comparison data naturally arise from competitive two-player games in which the space of possible outcomes includes a draw (neither player wins). In games such as chess, draws are a common result between highly skilled players (Elo 1978). To accurately predict the outcome of such games, it is thus important to learn a model that can predict a draw."
1401.8008,data,2,,,training data
1401.8008,data,23,,,"predicted (Shashua & Levin 2003). In this article we propose SVMcompare, a support vector algorithm for these data."
1401.8008,data,29,,,"Overall, our analysis of the chess match data suggests that the proposed SVMcompare model performs with a higher AUC than the existing state-of-the-art ELO and Glicko results."
1401.8008,data,31,,,"Overall from the simulations, it is clear that when the data contain equality pairs, it is advantageous to use a model such as the proposed SVMcompare method which learns"
1401.8008,data,32,,,"Taken together, these imply ˆµ = −1/β. Now, by deﬁnition of the ﬂipped data (8), we can re-write the max margin QP (9) as"
1401.8008,data,51,,,"These data are geometrically represented in the top panel of Figure 1. Pairs with equality labels yi = 0 are represented as line segments, and pairs with inequality labels yi = {−1, 1} are represented as arrows pointing to the item with the higher rank."
1401.8008,data,57,,,"For future work, it will be interesting to see if the same results are observed in learning to rank data from search engines. For scaling to these very large data sets, we would like to try algorithms based on smooth discriminative loss functions, such as stochastic gradient descent with a logistic loss."
1401.8008,data,67,,,"The optimal decision boundaries r(x) ∈ {−1, 1} and margin boundaries r(x) ∈ {−1 ± µ, 1 ± µ} are drawn in Figure 2. Note that ﬁnding a feasible point for this LP is a test of linear separability. If there are no feasible points then the data are not linearly separable."
1401.8008,data,7,,,3.2. Kernelized QP for non-separable data
1401.8008,data,77,,,"Fig. 2. The separable LP and QP comparison problems. Left: the difference vectors x′ − x of the original data and the optimal solution to the LP (7). Middle: for the unscaled ﬂipped data ˜x′ − ˜x (8), the LP is not the same as the QP (9). Right: in these scaled data, the QP is equivalent to the LP."
1401.8008,data,89,,,"1 where x ∈ R2. Left: the training data Fig. 3. Application to a simulated pattern r(x) = ||x||2 are n = 100 pairs, half equality (segments indicate two points of equal rank), and half inequality (arrows point to the higher rank). Others: level curves of the learned ranking functions. The rank model does not directly model the equality pairs, so the rank2 and compare models recover the true pattern better."
1401.8008,data,89,,,"Our experimental results on simulated and real data clearly showed the importance of directly modeling the equality pairs, when they are present. We showed in Figure 5 that when there are few equality pairs, as is the usual setup in learning to rank problems, the baseline SVMrank algorithm performs as well as our proposed SVMcompare algorithm. However, when there are many equality pairs, it is clearly advantageous to use a model such as SVMcompare which directly learns from the equality pairs."
1401.8008,data,94,,,"3.1. LP and QP for separable data In our learning setup, the best comparison function is the one with maximum margin. We will deﬁne the margin in two diﬀerent ways, which correspond to the linear program (LP) and quadratic program (QP) discussed below. To illustrate the diﬀerences between these max-margin comparison problems, in this subsection we assume that the training data are linearly separable. Later in Section 3.2, we propose an algorithm for learning a nonlinear function from non linearly-separable data."
1401.8008,"data, database",181,,,"Ranking data sets are often described not in terms of labeled pairs of inputs (xi, x′ i, yi) but instead single inputs xi with ordinal labels yi ∈ {1, . . . , k}, where k is the number of integral output values. Support Vector Ordinal Regression (Chu & Keerthi 2005) has a large-margin learning formulation speciﬁcally designed for these data. Another approach is to ﬁrst convert the inputs to a database of labeled pairs, and then learn a ranking model such as the SVMcompare model we propose in this paper. Van Belle et al. (2011) observed that directly using a regression model gives better performance than ranking models for survival data. However, in this paper we limit our study to models for labeled pairs of inputs, and we focus on answering the question, “how can we adapt the Support Vector Machine to exploit the structure of the equality yi = 0 pairs when they are present?”"
1401.8008,"data, dataset",101,,,"The notation and organization of this article is as follows. We use bold uppercase letters for matrices such as X, K, and bold lowercase letters for their row vectors xi, ki. In Section 2 we discuss links with related work on classiﬁcation and ranking, then in Section 3 we propose a new algorithm: SVMcompare. We show results on 3 illustrative simulated data sets and 2 real by learning to rank a sushi data set and a chess dataset in Section 4 and 5. We then discuss future work in Section 6."
1401.8008,"data, dataset",159,,,"In ranking problems, the goal is to learn a ranking function r(x) ∈ R from Summary. labeled pairs x, x′ of input points. In this paper, we consider the related comparison problem, where the label y ∈ {−1, 0, 1} indicates which element of the pair is better (y = −1 or 1), or if there is no signiﬁcant difference (y = 0). We cast the learning problem as a margin maximization, and show that it can be solved by converting it to a standard SVM. We use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed SVMcompare algorithm outperforms SVMrank when there are equality y = 0 pairs. In addition, we show that SVMcompare outperforms the ELO rating system when predicting the outcome of chess matches."
1401.8008,"data, dataset",68,,,"Fig. 5. Area under the ROC curve (AUC) for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. For each data set we picked n = 400 pairs, varying the proportion ρ of equality pairs. We plot mean and standard deviation of AUC over 4 test sets."
1401.8008,"data, dataset",78,,,"Another way to formulate the comparison problem is by ﬁrst performing a change of variables, and then solving a binary SVM QP. The idea is to maximize the margin between signiﬁcant diﬀerences yi ∈ {−1, 1} and equality pairs yi = 0. Let Xy, X′ y be the |Iy| × p matrices formed by all the pairs i ∈ Iy. We deﬁne a new “ﬂipped” data set with"
1401.8008,"data, dataset",86,,,"2.2. SVMrank for comparing In this subsection we explain how to apply the existing SVMrank algorithm to a comparison data set. The goal of SVMrank is to learn a ranking function r : Rp → R. When r(x) = w⊺x (where ⊤ denotes the transpose) is linear, the primal problem for some cost parameter C ∈ R+ (where R+ is a set of all non-negative real numbers) is the following quadratic program (QP):"
1401.8008,"data, dataset",87,,,"Fig. 4. Test error for 3 different simulated patterns r(x) where x ∈ R2 and one real sushi data set where x ∈ R14. We randomly generated data sets with ρ = 1/2 equality and 1/2 inequality pairs, then plotted test error as a function of data set size n (a vertical line shows the data set which was used in Figure 3). Lines show mean and shaded bands show standard deviation over 4 test sets."
1401.8008,"data, dataset",95,,,"For each experiment, there are train, validation, and test sets each drawn from the same data set of examples. We ﬁt a 10 × 10 grid of models to the training set (cost parameter C = 10−3, . . . , 103, Gaussian kernel width 2−7, . . . , 24), and select the model using the validation set. We use two evaluation metrics to judge the performance of the models: zero-one loss and area under the ROC curve (AUC)."
1401.8008,"data, dataset, data https",164,,,"5.1. Data source and processing We downloaded the chess match dataset from Chessmetrics (http://www.chessmetrics.com/cm/), containing 1.8 million games played over the 11year period from 1999–2009 by 54205 chess players. For each of the years 1999–2006, we consider the ﬁrst four months (Jan–Apr) as a train set, and the last eight months as a test set (May–Dec). We removed all matches containing a player who had less than 10 matches against other players in the train set, to prevent our data set from containing players with very little information. We also removed all matches that contained a player’s ﬁrst match from the train set as we would have no information about this player. Before pre-processing, 30.9% of matches were a draw and 69.1% of matches resulted in a win or loss. After pre-processing, the median percentage of draws and win-loss results"
1401.8008,"data, dataset, data https",233,,,"4.2. Learning to rank sushi data We downloaded the sushi data set of Kamishima et al. (2010) from kamishima (http://www.kamishima.net/sushi). We used the sushi3b.5000.10.score from kamishima, which consist of 100 diﬀerent sushis rated by 5000 diﬀerent people. Each person rated 10 sushis on a 5 point scale, which we convert to 5 preference pairs, for a total of 17,832 equality yi = 0 and 7,168 inequality yi ∈ {−1, 1} pairs. For each pair i we have features i ∈ R14 consisting of 7 features of the sushi and 7 features of the person. Sushi xi, x′ features are style, major, minor, oily, eating frequency, price, and selling frequency. Person features are gender, age, time, birthplace and current home (we converted Japanese prefecture codes to latitude/longitude coordinates). As in the simulations of Section 4.1, we picked train, validation, and test sets, each with the same number of pairs n and the same proportion ρ of equality pairs. We ﬁt a grid of models to the training set, select the model with minimal zero-one loss on the validation set, and then use the test set to estimate the generalization ability of the selected model."
1401.8008,"dataset, package",127,,,"was 44.7% and 55.3% respectively over each of the 8 datasets. For each match i, we i ∈ R16 consisting of ELO scores, Glicko scores, if the player had computed features xi, x′ the initial move, the percentage of instances where a player either lost to a lower ranked player, or won against a higher ranked player, the average score diﬀerence of opponents, win/loss/draw/games played raw values and percentages in addition to various other statistics. ELO scores were initially set at 1200 for all players and FIDE rules were applied to score calculations. ELO and Glicko scores were updated after every match using the PlayerRatings R package (Stephenson & Sonas 2016)."
1401.8008,github,3,,,https://github.com/tdhock/compare-paper
1401.8008,package,13,,,"Player Ratings Estimation (R package version 1.0-1)’, CRAN ."
1401.8008,package,26,,,"Karatzoglou, A., Smola, A., Hornik, K. & Zeileis, A. (2004), ‘kernlab – an S4 package"
1605.07495,code,14,,,Algorithm 1: Pseudo code of selection method based on non-dominated relative crowding distance
1605.07495,code,16,,,Algorithm 3: Pseudo code of calculation methods of CR and LR in (15)
1605.07495,code,233,,,"(i.e., ξcd) and the relative crowding distance (i.e., ξrcd) of all the 6 solutions are given out in TABLE II. Then, with nondominated CDV, the Θ2, Θ3 and Θ4 are qualiﬁed to be the candidates. Thus, the global best solution will be selected out according to their ξrcd (i.e., the selected ones will be marked with circle). For instance, since with the largest ξrcd, the Θ4 has the highest priority to be selected as global best in any case, while the Θ2 is with the lowest priority (only when Y = 3) for its smallest ξrcd. Finally, we assume there are 100 particles in total and we give out the result of particle dividing when Y = 1, 2, 3 (i.e., the numbers of particles in the group assigned to each selected solution are given out after each circle). We notice that the Θ5 is not be selected for any case though its relative crowding distance higher than that of the Θ2, it is because that the CDV of the Θ5 is dominated by the Θ3, which excluding its possibility to be selected as global best. We also give out the pseudo code of this selection method in Algorithm 1."
1605.07495,code,65,,,"more non-dominated solutions. Meanwhile, each sub-swarm aims to ﬁnd better values for its assigned objective function and all sub-swarms work together in order to get a bigger PF . The pseudo code of MOPSO-NRCD is given at Algorithm 2 (the calculation method of objective functions is given in Algorithm 3, whose detail will be given in the next section)."
1605.07495,code,7,,,Algorithm 2: Pseudo code of MOPSO-NRCD
1606.01039,data,10,,,(b) Signal used for ﬁlling missing-data gaps.
1606.01039,data,106,,,Results using (16) are presented in Figures 5(e)-5(f). We see that in Figure 5(f) the posterior mean describes properly the periodic behaviour and amplitude envelope smooth evolution of the modelled signal. We observe that prediction on the decay gap using (16) is closer to the actual data (red dots) than the results obtained with (15) as well as (14). This is reﬂected in the smallest RMS error in table 2. This is because (16) allows to describe periodic functions that
1606.01039,data,119,,,"To face the issue of modelling time dynamics we modiﬁed the previous covariance function (15), by multiplying it with an exponentiated quadratic kernel (14). This allows to “smooth” the strictly periodic behaviour of (15). The resulting kernel corresponds to (16). From Figure 4(b) we see that although the posterior mean of the predictive distribution does not exactly ﬁt the data, the model is able to learn the pitch of each of the three sound events with a smaller RMS error (Table 1), as well as the time dynamics or variations in the amplitude envelope of the signal."
1606.01039,data,131,,,"Experiments were done over real audio. We evaluated diﬀerent kernel conﬁgurations on a pitch estimation task, and on a missing data imputation task. All experiments assume we previously know the number of change-windows and its locations. In the pitch estimation task all the parameters of the covariance function are known, except those related with the fundamental frequency of each sound event, i.e. the value of ωm in (15) and (16) when using these kernels in the general model (10). Thus, we focus on optimizing only these model hyperparameters from the data. In the missing data imputation task the score of the modelled piece of music audio is used for tuning manually the model hyperparameters."
1606.01039,data,132,,,"In music information research, the aim of audio content analysis is to estimate musical concepts which are present but hidden in the audio data [17]. With this purpose, diﬀerent signal processing techniques are applied to music signals for extracting useful information and descriptors related to the musical concepts. Here, musical concepts refers to parameters related to written music, such as pitch, melody, chords, onset, beat, tempo and rhythm. Then, perhaps the most general application is one which involves the prediction of several musical dimensions, that of recovering the score of a music track given only the audio signal [10]. This is known as automatic music transcription (AMT) [5]."
1606.01039,data,132,,,"We performed regression on the signal shown in Figure 3(a) using the kernel (15). Figure 4(a) shows the posterior mean of the predictive distribution after training (blue continuous line). The black circle points correspond to observed data. We see the trained model is able to estimate the pitch for each sound event with a RMS error of 0.6282 semitones (Table 1). On the other hand, the amplitudeenvelope evolution of the signal is beyond the scope of the structure that this kernel can model. This is because this covariance function can only describe constant amplitude-envelope, periodic signals, with a fundamental frequency and several harmonics (see Figure 2(f))."
1606.01039,data,142,,,"In this study we used two short audio excerpts, in order to explore the method, so that we can eﬃciently ﬁt models and search in the hyperparameter space. The excerpt used for pitch estimation experiments corresponds to 0.7 seconds of the song Black Chicken 37 by Buena Vista Social Club. This segment of audio contains three notes of a bass melody (Figure 3(a)). In the missing data imputation task we used polyphonic audio corresponding to 1.14 seconds of Chopin’s Nocturne Op. 15 No. 1, where more than one note occur at the same time. The segments of signal in red in Figure 3(b) represent gaps of missing data. We reduced the sample frequency of both audio excerpts from 44.1KHz to 8KHz."
1606.01039,data,144,,,"In this article we discussed a Gaussian processes regression framework for modelling music audio. We compared diﬀerent models in pitch estimation as well as in prediction of missing data. We showed which kernels were more appropriate for describing properties of music signals, speciﬁcally: nonstationarity, dynamics, and spectral harmonic content. The advantage of this approach is that by designing a proper kernel we can introduce prior knowledge and beliefs about the properties of music signals, and use all that prior information to improve prediction. The presented work could be extended using eﬃcient representations of GPs in order to model larger audio signals. Other kernels could be studied, as the spectral mixture for modelling harmonic content [2], and Latent Force models [1] for describing mechanistic characteristics of the signal."
1606.01039,data,153,,,"We compared three diﬀerent models predicting missing-data gaps. We studied kernels (14), (15), and (16). In Figure 3(b) ﬁrst gap (red segment) contains the transient (onset and attack [3]) of a sound event, whereas the second gap is located in a more stable segment of the data (smooth decay). Figures 5(a)5(b) depict the prediction using (14). These ﬁgures correspond to zoom in small sections of the signal where the gaps occur (Figure 3(b)). We see that the model using this kernel overﬁts the data, i.e. the posterior mean (blue line) ﬁts all the observed data (black dots) with high conﬁdence (grey shaded area), but the"
1606.01039,data,183,,,"Figures 5(c)-5(d) show the prediction using covariance function (15). In the transient gap (Figure 5(c)) the posterior mean (blue line) does not follows the data, this is because transients are short intervals during which the signal evolves in a nonstationary, nontrivial and unpredictable way [3]. opposite to this, the model using kernel (15) can only describe the behaviour of constant amplitudeenvelope periodic stochastic functions. In the second gap (Figure 5(d)) the posterior mean describes properly the periodic behaviour of the data, but it does not follow the amplitude-envelope of the observations. This is because this covariance function is able to describe periodic functions that have several harmonic components. The drawback of this kernel is that it assumes constant the amplitude of the periodic stochastic functions that describes. These diﬀerent performance on the prediction is reﬂected on the RMS error obtained for each gap (Table 2)."
1606.01039,data,2,,,3.1 Data
1606.01039,data,219,,,"R where the time input variable t ∈ R, we model the whole function f (t) as a GP. That is, instead of putting a prior over the function parameters η, we introduce a prior over the function f (t) itself [16]. Learning in GP regression corresponds to computing the posterior distribution over the function f (t) conditioned on the observed data y = [y1, · · · , yN ] [15, 13]. The underlying idea in GP regression is that the correlation function introduces dependences between function f (t) values at diﬀerent inputs. Thus, the function values at the observed points give information also of the unobserved points [14]. The structure of the kernel (1) captures high-level properties of the unknown function f (t), which in turn determines how the model generalizes or extrapolates to new test time instants [9]. This is quite useful because we can introduce prior knowledge about what we believe the proprieties of music signals are, by choosing a proper kernel that reﬂects those characteristics. In section 2.2 we study in more detail the design of kernels."
1606.01039,data,264,,,"In [20] GPs are used for time-frequency analysis as probabilistic inference. Natural signals are assumed to be formed by the superposition of distinct timefrequency components, with the analytic goal being to infer these components by applying Bayes’ rule [20]. GPs have also been used for underdetermined audio source separation. In [8] the mixture signal is modelled as a linear combination of independent convolved versions of latent GPs or sources. The model splits the mixture signal in frames also considered independent, by using weightfunctions. Thus each source is modelled as a series of concatenated locally stationary frames, each one with its corresponding covariance function. With this assumption the resulting signal is supposed to be non-stationary [8]. On the other hand, despite the approach we present also assumes the latent GPs fm in (10) as non-correlated, the observed signal is not framed into independent segments. Instead of using weight-functions that act over the observed data, we introduce change-windows φm inﬂuencing each latent GP ending up with latent processes representing speciﬁc sound events that happen at certain segments of time. Therefore the proposed model keeps the correlation between the observations throughout all the signal. That is what allows to make prediction in gaps of missing data (section 4.2). GPs have been used also for estimating spectral envelope and fundamental frequency of singing voice [21], and for time-domain audio source separation [22]."
1606.01039,data,42,,,"Figure 3: (a) analysed audio (blue line), change-windows (dashed lines). (b) observed data (blue line), missing-data gaps (red line), change-windows (dashed lines)."
1606.01039,data,45,,,"Figure 5: Zoom in a portion of missing-data gaps. In each ﬁgure the continuous blue line represent the posterior mean, grey shaded areas correspond to the posterior variance, red dots are missing data, whereas black dots are observed data."
1606.01039,data,8,,,4.2 Filling gaps of missing data in audio
1606.01039,data,8,,,"and Data Analysis. Wiley, 1988."
1606.01039,data,91,,,"The covariance function (1) used for computing the prior distribution (5) allows us to introduce in the model all the knowledge and beliefs we have about the properties of the data. We are trying to model music signals, and some of the broad properties of audio signals are non-stationarity, rich spectral content, dynamics (locally periodic, non constant amplitude envelope), mechanistic behaviour, and music structure. Therefore we seek covariance functions that can describe or reﬂect these properties."
1606.01039,"data, data available",57,,,"conﬁdence decreases and the prediction is quite poor in the input space zones where the data is not available (red dots). Also, we see that the model using (14) does not expect any periodic behaviour in the gaps. The RMS error for both gaps is presented in Table 2."
1606.01039,"data, dataset",117,,,"The regression problem concerns the prediction of a continuous quantity [12], here a function f (t), given a data set D = {(ti, yi)}N i=1, where yi are assumed as noisy measurements of f (t) at typically regularly-spaced time instants ti (though GP regression framework allows for irregular sampling or missing data), i.e. yi = f (ti)+ǫi, where ǫi ∼ N (0, σ2 noise). In GP regression for mono channel audio signals, instead of estimating parameters η of ﬁxed-form functions f (t, η) : R 7→"
1608.04885,code,13,,,in the synthesis of code which mimics proposed service behaviour replacing the real
1608.04885,code,13,,,"– First line speciﬁes HTTP version, three-digit code and a text string"
1608.04885,code,14,,,Listing A.2 contains the Java code for the symmetric ﬁeld identiﬁcation class which can
1608.04885,code,14,,,indicates the operation code that the client requests to or to which the server
1608.04885,code,14,,,"interact with distributed software systems, while it interacts with local code in lieu"
1608.04885,code,15,,,The java source code of the implementation of modifying the centroid response is listed in
1608.04885,code,15,,,the code of mock objects are typically coupled with the code of the software under
1608.04885,code,17,,,for both identiﬁed symmetric ﬁelds are shown in Table 5.3. The java source code of the
1608.04885,code,2,,,Java Code
1608.04885,code,27,,,Listing A.3 contains the Java code for the ﬁeld substitution method that performs sym metric ﬁeld substitution to modify a recorded response for generating a response.
1608.04885,code,3,,,A Java Code
1608.04885,code,4,,,APPENDIX A. JAVA CODE
1608.04885,"code, code available",14,,,environment with the code under investigation is only possible if the code itself is
1608.04885,"code, code package, code available",13,,,Listing A.1 contains the Java code for describing the symmetric ﬁeld information.
1608.04885,data,1,,,data
1608.04885,data,10,,,Table 7.8: Response Accuracy for Clusters with Noisy Data
1608.04885,data,10,,,deﬁnes temporal rules expressing data dependencies among exchanged messages.
1608.04885,data,10,,,signiﬁcant diﬀerence between this form and ASN.1 on-the-wire data representation
1608.04885,data,11,,,EncodingDecodingApplication data representationmessageApplication data representationmessagemessageCHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,11,,,attach location data to tweets and discover tweets & locations.
1608.04885,data,11,,,"hierarchical data structure, for instance, a tree structure."
1608.04885,data,12,,,approach. This approach adopted clustering techniques and data mining techniques to
1608.04885,data,13,,,A reordered dissimilarity matrix image indicates cluster tendency in the data by dark
1608.04885,data,13,,,One of the most common transformation of network data is from the representation
1608.04885,data,13,,,and management of data in distributed applications. The middleware service protocol is
1608.04885,data,13,,,databases. In Proceedings of the 20th International Conference on Very Large Data
1608.04885,data,13,,,"twitter data, rather than changing/updating data. Although the current client program"
1608.04885,data,13,,,"– Compound data types can be constructed by nesting primitive types, shown"
1608.04885,data,13,,,• Built on a client-server architecture and uses separate control and data connections
1608.04885,data,14,,,3. A variable-length SMB Data contain 2-byte length ﬁeld that indicates the size
1608.04885,data,14,,,Textual protocols are built around the notion of message ﬁelds encoded with text data
1608.04885,data,14,,,and transmitting structured data over a network connection. It is used primarily to
1608.04885,data,14,,,"application integration, data integration, message oriented middleware (MOM), object"
1608.04885,data,14,,,binary and textual methods. The binary encoding method targets to a standardized data
1608.04885,data,14,,,"command type identiﬁer, and value represents data for the command. The most"
1608.04885,data,14,,,how primitive data types and compound data structure are encoded so they can be
1608.04885,data,14,,,individual machines. The same issues of complex setup and data availability exist.
1608.04885,data,14,,,primitive data types and compound data structures. Both X DR and ASN.1 specify
1608.04885,data,14,,,"speciﬁc data representation, we need to design speciﬁc rules to obtain the embedded"
1608.04885,data,14,,,the data it wants to transmit from representation it uses internally into a message
1608.04885,data,14,,,"the oﬄine processing, utilising data mining techniques. It can dramatically reduce the"
1608.04885,data,14,,,"transmit data between a server and web applications, serving as an alternative of"
1608.04885,data,14,,,• Uses ASN.1 to deﬁne the data types used to build an SNMP message
1608.04885,data,15,,,"In this chapter, we have utilised data mining techniques in a large enterprise software"
1608.04885,data,15,,,"data representations, where ASN.1 and XDR are the most popular ones. For a"
1608.04885,data,15,,,library into clusters of interactions of the same type. The data mining techniques were
1608.04885,data,15,,,"schema, which is a speciﬁcation for what JSON data is required for a given"
1608.04885,data,15,,,• Streaming: Give developers low latency access to Tweet data and other events have
1608.04885,data,16,,,External Data Representation (XDR) [22] is the network format used to transfer
1608.04885,data,16,,,JavaScript Object Notation (JSON) [21] is a text-based data interchange format.
1608.04885,data,16,,,Most of Twitter API operations are used to provide the twitter data for 3rd party Twitter
1608.04885,data,16,,,Network Data Management Protocol (NDMP) [14] is used to transport data between
1608.04885,data,16,,,"data on that directory. Once the server received an unbind request, it must unbind"
1608.04885,data,16,,,"data reorganization by a clustering technique. Operations Research, 20(5):993–1009,"
1608.04885,data,16,,,"than the Whole Library approach, even though the latter uses all the available data points"
1608.04885,data,16,,,"• XML deﬁnes a basic syntax for mixing markup with data text, but the designer"
1608.04885,data,17,,,[82] M. K. Jiawei Han. Data Mining: Concepts and Techniques. Morgan Kaufmann
1608.04885,data,17,,,"name in their request, followed by a payload, containing the data the service is expected"
1608.04885,data,17,,,"visual cluster analysis. In Data Mining, 2008. ICDM’08. Eighth IEEE International"
1608.04885,data,18,04/21/22,0,"[122] A. A. Sofokleous and A. S. Andreou. Automatic, Evolutionary Test Data Generation"
1608.04885,data,18,,,and semantics of the data. The model step is the ﬁnal stage where we specify how virtual
1608.04885,data,18,,,"encode data, which is intended or expected to be read by a machine rather than a human"
1608.04885,data,18,,,"fast as a real service can. To answer this question, we propose and implement a data"
1608.04885,data,19,,,"2For each LDAP operation, we demonstrate its raw data, as well as the textual representation, which"
1608.04885,data,19,,,"Data Units (PDU)), exchange of interactions (via service primitives) with service users at"
1608.04885,data,19,,,"following, we ﬁrst describe how we collect data of six case study protocols in Section. 7.3.1."
1608.04885,data,19,,,"that can be transmitted over the network; that is, the data is encoded into a message."
1608.04885,data,19,,,"– Represents each data item with a triple of the form <tag, length, value>,"
1608.04885,data,20,,,"[62] S. Elbaum, G. Rothermel, S. Karre, and M. F. II. Leveraging user-session data"
1608.04885,data,20,,,work in pattern matching sequences of data. One is the n − gram approach [72] and the
1608.04885,data,21,,,VAT [35][148] is a technique that exists in data mining for the visual assessment of cluster
1608.04885,data,21,,,approach (cf. Section 5.4.2 in Chapter 5) to data sets of all six case study protocols. Having
1608.04885,data,22,,,"Given a trace library (cf. Deﬁnition 5 in Chapter 4.2), shown as Table 6.1, a data clustering"
1608.04885,data,22,,,"[147] D. Yuan, Y. Yang, X. Liu, and J. Chen. A data placement strategy in scientiﬁc"
1608.04885,data,23,,,"protocols, they can be further divided based on the multi-byte order and data representa tions, which are illustrated as follows:"
1608.04885,data,24,,,7.8 Response Accuracy for Clusters with Noisy Data . . . . . . . . . . . . . . . 118
1608.04885,data,25,,,Abstract Syntax Notation One (ASN.1) [20] is an standard that deﬁnes a rep resentation for data sent over a network.
1608.04885,data,26,,,"[97] G. J. McLachlan, K.-A. Do, and C. Ambroise. Analyzing Microarray Gene Expres sion Data. Wiley-Interscience, 2004."
1608.04885,data,27,,,3.2 Encoding and decoding application data . . . . . . . . . . . . . . . . . . . . 34
1608.04885,data,28,,,"Most application-level protocols deﬁne message structures containing some form of oper ation or service name in their requests, followed by a payload on what data this service"
1608.04885,data,28,04/21/22,0,"interaction models. Moreover, by utilizing data mining techniques, the eﬃciency of re sponse generation in the emulation environment has been greatly improved. However the"
1608.04885,data,29,,,"integrated with many other systems for managing and interpreting data from many busi ness activities. The other systems (called services) include a legacy mainframe program,"
1608.04885,data,3,,,of data.
1608.04885,data,30,,,"Binary protocols rely on speciﬁc data structure; and hence, transmitted messages usu ally resort to ﬁxed-length ﬁelds or to a special notation to indicate the length of variable"
1608.04885,data,30,,,"There are two popular network data representations (i.e. External Data Represen tation (XDR) and Abstract Syntax Notation One (ASN.1)), proposed to encode"
1608.04885,data,31,,,"• XML syntax provides for a nested structure of tag/value pairs, which is equiv alent to a tree structure for the represented data. This is similar to XDR and"
1608.04885,data,34,,,"[104] R. T. Ng and J. Han. Eﬃcient and eﬀective clustering methods for spatial data min ing. In Proceedings of the 20th International Conference on Very Large Data Bases,"
1608.04885,data,34,,,"[140] L. Wang, X. Geng, J. Bezdek, C. Leckie, and R. Kotagiri. Enhanced visual anal ysis for cluster tendency assessment and data partitioning. Knowledge and Data"
1608.04885,data,35,,,"[138] L. Wang, J. C. Bezdek, C. Leckie, and R. Kotagiri. Selective sampling for approxi mate clustering of very large data sets. International Journal of Intelligent Systems,"
1608.04885,data,43,,,"as AgileLoad [1] and Selenium [10]. AgileLoad [39] features functions like auto matic modeling, real time data correlation, anomaly diagnostic and recommenda tions, enabling automatic identiﬁcation of performance bottlenecks in the systems."
1608.04885,data,5,,,2. Network data representations
1608.04885,data,7,,,• External Data Representation (XDR)
1608.04885,data,8,,,3.2.2 Network Data Management Protocol (NDMP)
1608.04885,data,8,,,Figure 3.2: Encoding and decoding application data
1608.04885,data,8,,,data between diﬀerent kinds of computer systems.
1608.04885,data,83,,,Protocol Taxonomy      NFS NDMP SNMP LDAP SMB HTTP FTP SMTP POP3 IRC JMS SOAP Message Format Textual   √ √ √ √ √ √ Binary √ √ √ √ √   √ Multi-byte Transmission Order Big-Endian √ √ √ √     Little-Endian   √    Data Representation XDR √ √      ASN.1  √ √     Stateful Stateful v4 √ √ √ √ √ √ √  Stateless v1-v3 √  √    Layer Low √ √ √ √ √ √ √ √ √ √  High      √ √ CHAPTER 3. APPLICATION-LAYER PROTOCOL STUDY
1608.04885,data,9,04/21/22,0,data is transformed into messages sent and received.
1608.04885,data,9,,,"– Unstructured body data follows, with speciﬁed size"
1608.04885,data,9,,,• All requests are four characters followed by data
1608.04885,"data, data available",12,,,require physical relocation and manual reconﬁguration. Real system data may not
1608.04885,"data, data https",14,,,[22] XDR: External Data Representation Standard. 2006. http://tools.ietf.org/
1608.04885,"data, data https",17,,,[14] NDMP: Network Data Management Protocol. Network Working Group. 1996. http:
1608.04885,"data, dataset",16,,,"As shown in Figure 5.4, we randomly partitioned the original interactions’ data set into"
1608.04885,"data, dataset",16,,,a statistical analysis will generalise to an independent data set. For the purpose of our
1608.04885,"data, dataset",16,,,results of a statistical analysis will generalise to an independent data set. For the purpose
1608.04885,"data, dataset",18,,,"similar search requests in our data set, some of them resulting in responses with zero or one"
1608.04885,database,12,,,widely utilised in distributed database systems for the vertical partition of large
1608.04885,database,13,,,of relationships between proposed services and a database. This information is used
1608.04885,database,15,,,which is a database term for a speciﬁcation of how to interpret a collection of
1608.04885,database,16,,,[130] M. Tamer ˝Ozsu and P. Valduriez. Principles of Distributed Database Systems.
1608.04885,dataset,13,,,Our experimental results using the 6 message trace datasets demonstrate that our approach
1608.04885,dataset,13,,,has two additional datasets: a dataset with textual representation converted from the
1608.04885,dataset,13,,,• The datasets were obtained by randomly generating client requests for services of
1608.04885,dataset,14,,,accuracy overall for the datasets tested. The combined approach achieves 100% accuracy
1608.04885,dataset,14,,,datasets. The Accuracy Ratio column is calculated by dividing the number of valid
1608.04885,dataset,15,,,client-to-server load. They require detailed knowledge of target protocols and suit able datasets.
1608.04885,dataset,15,04/21/22,0,"represent the average times spent generating requests, for all the requests in the datasets"
1608.04885,dataset,15,,,than those of our datasets. Further testing on real system interactions are warranted.
1608.04885,dataset,15,,,• Our evaluation was performed on six datasets from four protocols. Given the great
1608.04885,dataset,16,,,The impact of the entropy weightings can only be observed for the LDAP binary dataset.
1608.04885,dataset,18,,,higher accuracy than f = 1. For the other datasets the threshold had no impact on the
1608.04885,dataset,18,,,the LDAP (binary) dataset the thresholds of f = 0.5 and f = 0.8 produced signiﬁcantly
1608.04885,dataset,19,,,(i.e. 0% noise) of interaction messages by operation type for the six datasets tested.
1608.04885,dataset,19,,,"for four of the datasets, and 99.95% and 99.34% for the remaining two (LDAP binary"
1608.04885,dataset,20,,,"binary dataset (denoted by LDAP text (1)), and another textual dataset that was used in"
1608.04885,dataset,21,,,"accuracy stays above 97% for all datasets, when the noise ratio is 5%. As the noise ratio"
1608.04885,dataset,29,,,"datasets, no impact from the weightings can be observed, as the consensus sequence pro totype by itself (Consensus Only) already produces 99-100% accuracy."
1608.04885,dataset,31,,,"responses, indeed much faster than the real services being emulated. The response genera tion time is comparable to the Cluster Centroid approach, being faster for some datasets,"
1608.04885,dataset,35,,,"from the trace library (for three datasets Consensus+Weighting is signiﬁcantly more ac curate, for two it has the same accuracy, for one it is slightly lower). The reason for the"
1608.04885,dataset,4,,,the other datasets.
1608.04885,dataset,6,,,7.5 Sample protocol message trace datasets
1608.04885,dataset,8,,,Table 7.5: Sample protocol message trace datasets
1608.04885,"dataset, used dataset",17,,,"We have used one message trace dataset for each of these protocols. In addition, LDAP"
1608.04885,github,9,,,[9] Mockery. https://github.com/padraic/mockery.
1608.04885,open-source,13,,,ActiveMQ is an open source message broker which fully implements the Java Message
1608.04885,open-source,22,,,"[142] T. Wang, G. Yin, X. Li, and H. Wang. Labeled topic detection of open source"
1608.04885,package,12,,,1 package com . ca . calabs . bilby . substitution ;
1608.04885,publicly available,13,,,"However, the disadvantage is that the communication contract is not always publicly"
1608.04885,publicly available,15,,,publicly available so that we can use this knowledge to deﬁne criteria for validation.
1608.04885,python,20,,,"including Java, C#, Groovy, Perl, PHP, Python and Ruby so that the tests can"
1701.07853,code,6,,,"Elisa Mussumecia, Fl´avio Code¸co Coelhoa"
1701.07853,data,10,,,empirical data. The adjacency matrix A is given by
1701.07853,data,133,,,"In this work we decided to look at the spread of news stories over the internet characterizing the resulting spread network and the dynamics of the spread. We start by looking at an actual case of news spread, and estimate the spread network by applying ideas of temporal networks and topic Modeling, connecting similar articles within the bounds of temporal window of inﬂuence. Then we postulate that the spread dynamics approximates an epidemic process and model it using a Network SIR model[3]. The spread of ideas as an epidemic process is not a new idea[4], but here we Propose new tools to estimate the spread network from data and compare it with simulated networks produced by an SIR epidemic model."
1701.07853,data,28,,,"From the simulation (ﬁgure 11) we obtain the state matrix, which we use to compare the simulated infection distribution with the original data. Then"
1701.07853,data,4,,,2.1. Data sources
1701.07853,"data, database",91,,,"The data used for this study was obtained from the Media Cloud Brasil project (MCB) which collects news articles from thousands of sources in the Brazilian Internet since 2013. From the MCB database we obtained 2129 articles talking about the Charlie Hebdo terrorist attack in February 2015. The articles span from the day of the attack to the end of march of 2015. The data include the full text of the article, the URL of publication and the date and time of the publication."
1701.07853,dataset,52,,,"Figure 10: Total number of articles infected between 0 < λ < 0.00005. The blue area is the area where the peak of the simulation is the same as the peak of the dataset distribution, threfore is the area where the λ values were tested for our simulation."
1701.07853,dataset,63,,,"where NXY is the number of times an article from publisher X (the publisher of article i), has infected an article from publisher Y (the publisher of article j) and NY Is the total number of articles from publisher Y that have been infected, regardless of publisher. These counts are derived from the empirical dataset."
1701.07853,"dataset, database",51,,,"The dataset used is the result of a very speciﬁc search on a news articles database, therefore we can expect to the articles to display a great similarity among themselves. Figure 4, shows the distribution of pairwise similarities that were used to construct the empirical inﬂuence network."
1803.06456,code,61,,,"Here, the baselines are the same as the TE schema. However, we are not able to compare the PAN winner methods with our model as they have not published their code and many implementation details are left unknown to us in their reports to reimplement their methods. We call our second model PRNN in our reports."
1803.06456,data,140,,,"methods as long as we keep the test and training sets the same as theirs. TE methods: We perform the Transformation Encoder (TE) on a problem P = (DS, DT ) that its documents are represented under one feature set with one feature value assignment to compute the transformation error. We then leverage the error rates taking from (at most) F = 7 feature sets (Section 3.2) of TE to form the ﬁnal TE feature vector (V ). Indeed, Each of the dimensions captures the transformation loss of one feature set. We apply the TE to both training and test data. Two well-known GNB and DT classiﬁers are used for veriﬁcation. We indicate them as TE+GNB and TE+DT respectively in our experiments."
1803.06456,data,176,,,"where zs ∈ Rd is the reconstructed input and must be transformed into the target (zs ≈ xt). This can be done by setting TE’s objective function as the minimization of the transformation loss. We set the TE transformation loss Er to be the cross-entropy between reconstructed input (zs) and the target input (xt) as: Er(xt, zs) = − (cid:80)d i + (1 − xt Now, we assign our authorship veriﬁcation problem into the proposed Transformation Encoder. It is intuitively expected that TE shows diﬀerent manner when it transforms the source into the target while both having many features in common compared to the case where they have less common features. Here, the goal is to utilize TE for the AV problems that suﬀer from restricted labeled data. So, we put a document expansion method on top of TE as an initial step to overcome the restriction to some extent."
1803.06456,data,236,,,"In this paper we deﬁne two diﬀerent schemas to study the AV problem. Under the ﬁrst schema we address the following challenges: 1- writing samples of available authors are quite limited during the training step as the length of the given text documents is short (a few hundred to a few thousand words) and size of the training set is so small (from 10 to 200 examples). So, it is quite hard to infer the same or diﬀerent-authorship status of given pairs. 2- The test and train documents are from diﬀerent genera and/or topics which makes the learning and prediction process much harder as the word distribution might diﬀer considerably. 3- No writing samples of the future authors is speciﬁed to us during the training and we may have seen no samples by the future authors at all. Under the second schema the scale of the training data is larger compared to the ﬁrst schema. However, we address the problem of identifying the diﬀerence in documents from identical domains in two ways: 1- authorship diversity in similar contents by utilizing Amazon reviews from 300 distinct authors. 2- Scientiﬁc documents from the same area of research by diﬀerent authors who have almost identical level of expertise in the ﬁeld. It also can be considered as an application of plagiarism detection."
1803.06456,data,4,,,1 http://pan.webis.de/data.html
1803.06456,data,43,,,"of the two classes decreases as the transformation encoder updates its weights in each epoch. However, there is a diﬀerence between the transformation loss of the positive and negative data in both diagrams. The loss of negative data is less"
1803.06456,data,44,,,"Fig. 4: t-SNE plot of two folds of output of the fusion layer for PAN2015 in 5-fold CV. +: positive training data, ×: positive test data, ◦: negative training data, •: negative test data"
1803.06456,"data, dataset",188,,,"Document Expansion for Small Scale Datasets Neural networks need suﬃcient amounts of data during their learning process to avoid the over-ﬁtting problem to produce the desired output. So, we propose a document expansion method to make use of the existing labeled training data of small scale datasets such as PAN to a great extent. A sliding window with the length of l sentences moves forward through each text document by one sentence per step making a smaller document each time. More speciﬁcally, a document with n sentences will be distributed into n − l + 1 smaller documents. New line characters, as well as empty sentences, are ignored here. So, using this expansion technique each problem P = (S, T ) in the small datasets will be converted to P = (DS, DT ) j}lT where DS = {ds j=1 are the set of all shorter documents after expansion of S and T (source and target). lS and lT denote the size of DS and DT respectively."
1803.06456,"data, dataset",330,,,"However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy. The best accuracy belongs to the Amazon dataset where we have the largest dataset. It can be inferred that when the scale of the underlying dataset is large enough, the network learns the relation between the two language models of its given inputs well. It should be noted that for the two PAN2013 and PAN2014E even after CV the network cannot converge and the validation loss increases after each epoch. To avoid it we increase the total number of document pairs by splitting each document into two smaller ones with an equal number of sentences and making new pairs. This technique decreases the validation loss during training. However, it still suﬀers from lack of labeled examples and causes weakest results compared to the other larger datasets. To illustrate how PRNN discriminate writing styles we provide the t-SNE plot of the output of the fusion layer in a 5-fold CV classiﬁcation for two folds of PAN2015 (Figure 4). According to Figure 4, both classes have almost similar distribution in the test and training data. But, in some rare parts, the positive and negative points are close. They are probably the portion of the data that mislead the classiﬁer during the training step or will be misclassiﬁed in predictions."
1803.06456,"data, dataset",344,,,"than the positive’s. In other words, it is easier to transform one document into the other while they are a diﬀerent-authorship pair (negative pair) compared to a same-authorship pair (positive pair). It makes the results of reconstruction loss to be counterintuitive. The reason is that we represent both documents of each problem under vector space model and only based on the vocabulary of the source document. So, the exclusive features of the target document, the features that only belong to the target but no to the source document, will be ﬁltered under this document representation model. Moreover, it is expected that the documents written by diﬀerent authors have fewer features in common and have more exclusive features than the ones written by the same author. This fact makes the target document of diﬀerent-authorship pair sparser than that of the same-authorship pair. And transforming the source document into a sparse document (its vector is sparse) makes less error than to a dense document (its vector is dense). This feature diﬀerentiates the positive and negative data and exists for both training and test sets and makes the transformation loss a distinctive feature for the veriﬁcation. A more direct way to classify the documents (instead of using classiﬁers) is to simply thresholding the reconstruction error. However, it is only precise for one or two feature sets. So, we employed the NB and DT for classiﬁcation. We also tried SVM for this step but its results were not as precise as the two DT and GNB classiﬁers. One reason might be that SVM cannot recognize the decision boundary for very low dimensional TE error vectors (at most 7 feature sets). PRNN schema The comparison results are reported in Table 4. According to it, PRNN beats all baselines for all datasets except PAN2013 where it achieves the second highest accuracy."
1803.06456,"data, dataset",46,,,"Fig. 3: Transformation loss for 50 epochs: averaged over all problems in the PAN2015 dataset. (A): training data (100 problems), (B): test data (500 problems), Feature set: unigram."
1803.06456,"data, dataset",84,,,"Neural Network (PRNN) for small and large scale datasets. TE transforms one document of the pair into the other and observes the transformation loss as a distinctive feature for classiﬁcation. PRNN investigates the diﬀerence between the language models of documents. Experiments show that TE can achieve stable results in all four PAN datasets with various size, genre and/or topics. Also, PRNN beats almost all baselines avoiding over-ﬁtting problem by a reasonable amount of training data."
1803.06456,"data, dataset, dataset provided",344,,,"For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes. The underlying feature set is unigram. The ﬁgures show that in both training and test sets the TE loss"
1803.06456,"data, dataset, dataset provided",349,,,"TE schema The classiﬁcation results are compared in Table 3. The highest accuracy is indicated in bold and the second highest is underlined. For PAN2014N (Table 3(A)) TE+GNB and TE+DT methods beat the best baseline accuracy (SVM)by more than 11 percent and approach to the PAN winner by 1.5 percent. For PAN2014E, TE methods overcome half of baselines and FCMC in terms of accuracy. This might be because of PAN2014E short documents compared to the PAN2014N making it harder to discriminate the employed transformation loss for positive and negative data. However, no PAN winner stays stable at the ﬁrst rank in both Essays and Novels datasets. While FCMC works quite appropriately for the PAN2014N and is the winner of that year, it achieves the lowest accuracy compared to the baselines for PAN2014E. Furthermore, none of the best 2014 PAN methods (FCMC and OCT) works stably for the two genres (Novels and Essays) simultaneously. They gain the highest score in only one genre and have a large diﬀerence with the winner indicating the dependency of their method on the context while the two TE methods get close to the winners reasonably. The results for PAN2013 and PAN2015, the dataset with the largest test set (500 pairs), are provided in (Table 3(B)). According to them, IM and TE+GNB beat the other results although most results for the PAN2013 dataset are almost close to each other in terms of accuracy. For PAN2015, Both TE+GNB and TE+DT can beat the accuracy of all the baselines. TE+GNB also shows a reasonable result while its accuracy is 1.2 percent less than the best method (MRNN) and again stays at the second accuracy and score rank. Figure 3 shows the TE transformation loss averaged over 100 problems of the training set and 500 problems of the test set of PAN2015 for both classes."
1803.06456,dataset,10,,,Experimental results on evaluation datasets show that both methods achieve
1803.06456,dataset,153,,,"Let P = (S, T ) denotes a pair of documents, indicating S as the source and T as the target. Here, the task is to investigate whether S and T are written by the same author. We map this problem into a binary classiﬁcation paradigm. Accordingly, if S and T are authored by the same person, P belongs to the positive class. Nevertheless (S and T have diﬀerent authors) P belongs to the negative class. In the ﬁrst step, we explain the Transformation Encoder which is a feature extraction-based method designed for the small-scale datasets with 200 labeled samples at most. However, many AV problems might have a larger scale with much more examples. So, we introduce the Parallel Recurrent Neural Network (PRNN) for large scale datasets in the second step."
1803.06456,dataset,167,,,"We analyze authorship veriﬁcation on several datasets with binary structure. To our knowledge this amount of analysis has not been done in authorship veriﬁcation on diverse types of datasets. Two models are proposed. First, a Transformation Encoder (TE) to model error feature vectors for classiﬁcation inspired by the idea of autoencoders. TE is compatible with the AV problems with small-scale training sets. Giving a pair of input documents, TE transforms one input into the other. In this process, the transformation loss is observed as a reasonable measure of closeness of the two inputs to be used by a classiﬁer. The second model is a parallel recurrent neural network (PRNN) that is inspired by the popular similarity measures in Statistical Machine Learning (ML). Being based on language models, it is mostly applicable for relatively larger datasets. PRNN compares the proximity of the language model of its two input sequences"
1803.06456,dataset,2,,,3.1 Dataset
1803.06456,dataset,2,,,Dataset Train
1803.06456,dataset,222,,,"Abstract. We propose two models for a special case of authorship veriﬁcation problem. The task is to investigate whether the two documents of a given pair are written by the same author. We consider the authorship veriﬁcation problem for both small and large scale datasets. The underlying small-scale problem has two main challenges: First, the authors of the documents are unknown to us because no previous writing samples are available. Second, the two documents are short (a few hundred to a few thousand words) and may diﬀer considerably in the genre and/or topic. To solve it we propose transformation encoder to transform one document of the pair into the other. This document transformation generates a loss which is used as a recognizable feature to verify if the authors of the pair are identical. For the large scale problem where various authors are engaged and more examples are available with larger length, a parallel recurrent neural network is proposed. It compares the language models of the two documents. We evaluate our methods on various types of datasets including Authorship Identiﬁcation datasets of PAN competition, Amazon reviews and machine learning articles. Experiments show that both methods achieve stable and competitive performance compared to the baselines."
1803.06456,dataset,234,,,"case no writing samples of a questioned author are speciﬁed and they are unknown to us. No general solution has been oﬀered for the veriﬁcation problem under this assumption till 2014 [7]. Since then, a few works can be found in the literature: Koppel and Winter [7] propose an almost unsupervised method for the blog corpus dataset using “impostors” method. Optimized Classiﬁcation Trees, the winner method of PAN2014 Essays dataset, optimizes a decision tree based on various types of features and diﬀerent comparison methods including cosine similarity, correlation coeﬃcient and euclidean distance [8]. Multi-headed RNN is a character-level RNN and contains a common recurrent state among all authors with an independent softmax output per author [9]. Fuzzy C-Means clustering, the winner of the PAN2014 competition for novels dataset, adopts C-Means clustering and lexical features for the task [10]. Recently, an approach based on the compression models has been evaluated on PAN datasets [11]. Their method achieves promising results for the two years of PAN competitions but not for the other two datasets. Our methods is similar to these methods and considers the problems with the binary structure but we examine them on all PAN small-scale datasets as well as two large scale datasets."
1803.06456,dataset,260,,,"for PAN datasets. All other parameters are selected based on pilot experiments. We report accuracy, the Area Under Receiver Operating Characteristic (ROC) curve [4] (AUC) and Score=AUC× Acc in TE experiments. The higher AUC and Score indicate more eﬀective classiﬁcation. PRNN schema The plain text of each document is used as the input of PRNN. The features sets for the baselines are the same as the TE baselines. However, we did not use the original training and test sets of the PAN datasets as the size of the training set is too small to be used for PRNN. To avoid overﬁtting problem we perform 5-fold Cross Validation (CV) for the PAN2015, Amazon and MPLA* where we have suﬃcient amount of examples in training folds. And for the PAN2013, PAN2014E and PAN2014N datasets that are relatively smaller we perform 10-fold CV to increase the size of the training folds. This setting is applied for PRNN as well as the baselines. We use Theano to implement PRNN. All classiﬁer’s parameters are the same as the TE schema. The back-propagation is done using stochastic gradient descent with learning rate=0.001, batch size=1, and dropout rate=0.2. We use the Glove pre-trained vectors5 as an initial value for the embedding vectors when there is a match. Otherwise, a random vector from a continuous uniform distribution over [0, 1) is used."
1803.06456,dataset,3,,,Dataset Positive Negative
1803.06456,dataset,30,,,Table 4: Classiﬁcation accuracy for PRNN schema using 5 and 10-fold CV across different datasets. The input for the baselines are empowered by the proposed similarity vector.
1803.06456,dataset,334,,,"Here, the comparison methods are presented in three categories: baseline, PAN winners and our TE method. The details are provided as follows. Baseline: We connect several Machine Learning reliable classiﬁers widely used in the area with the seven similarity measures to set strong baselines (Table2). Since each example in our underlying dataset structure comprises two documents, we need to adapt it to the structure of an ordinary classiﬁer input by converting them to one single entity. A simple direct way is to concatenate their feature vectors. However, our experiments show it provides weak results mostly equal to the random label assignment. So, we deﬁne the summary vector as a single unit representative of each example/problem P = (DS, DT ) by utilizing several similarity measures. The summary vector comprises a class of several metrics each measures one aspect of the closeness of the two documents (DS and DT ) of the pair for all underlying feature sets. For any two feature vector documents x, y their summary vector is sum(x, y) = [simj i (x, y)1≤i≤M,1≤j≤F computes the ith similarity metric of M metrics in Table 2 under jth of F = 7 feature sets (Section 3.2) between x, y. Then, we use a classiﬁer including SVM, Gaussian Naive Bayes (GNB), K-Nearest Neighbor (KNN), Logistic Regression (LR), Decision Tree (DT) and Multi-Layer Perception (MLP) to predict the class label. PAN winners: We compare our method with the top methods of PAN AV competition between 2013 and 2015. The results of each method for one year of the competition are available and we report them here. So, our comparisons are not impacted by diﬀerent parameter setting and implementation details of these"
1803.06456,dataset,348,,,"PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random. On the other hand, the negative candidate set is made of all unique and possible combinations of review pairs having diﬀerent authors. The negative class having equal size with the positive class is created by random selection from the negative candidate set."
1803.06456,dataset,348,,,"for PAN2013), and literally the second document includes one piece of writing. Two documents of a pair might be from signiﬁcantly various genres and topics. The length of a document changes from a few hundred to a few thousand words. PAN2014 includes two datasets: Essays and Novels. The paired documents in PAN datasets are used for our experiments. So, for a problem P = (S, T ), S (source) is the ﬁrst document and T (target) is the second document of a PAN problem. PRNN schema We evaluate PRNN on new schemas of MPLA-400 2 and Amazon reviews. The schemas are deﬁned similarly to the PAN-style explained above. MPLA-400 dataset contains 20 articles by each of the top-20 authors by citation in Machine Learning. We create its new schema, MPLA*, by selecting publications from MPLA-400 that are written by a single author and have no co-authors. To keep the distribution of authors and classes balanced in MPLA*, we select an equal number of single-authorship articles from all existing 20 authors and map it to the PAN-style (there are at most 9 single-author publications by each author). Here, the positive class consists of the pairs which are made up of all possible combinations of same-authorship articles (20 × (cid:0)9 (cid:1) = 720). And the same size negative class includes the pairs that are randomly selected from the set of all unique combinations of diﬀerent-authorship articles. We apply the similar method to Amazon review dataset to deﬁne its new PAN-style schema. We select 300 authors with at least 40 reviews to make the positive and negative candidate sets. Then, for each author, the positive candidate set is all possible and unique combinations of the author’s reviews. To make a positive class we choose 4500 review pairs from this positive candidate set at random."
1803.06456,dataset,5,,,Table 1: Datasets information
1803.06456,dataset,86,,,TE schema To evaluate the Transformation Encoder we use all available authorship identiﬁcation datasets released by PAN 1 (Table 1). Each PAN dataset consists of a training and test corpus and each corpus has a various number of distinct problems. One problem is a pair of two documents: the ﬁrst document of a problem composed of up to ﬁve writings (even as few as one) by a single person (implicitly disjoint For PAN2014 and PAN2015 and explicitly disjoint
1803.06456,dataset,88,,,"PRNN is designed to solve the AV problem for relatively large scale datasets. The structure of the problem is the same as TE’s. We model a pair of documents using a simple parallel recurrent architecture. The overall model is shown in Figure 2. In general, PRNN consists of three components: two parallel columns of identical layers, one shared fusion layer and a SoftMax layer as the output. We proceed to describe the network in the following paragraphs."
1803.06456,dataset,93,,,"to investigate their authorship. We also propose the summary vector to adapt our problem to a common binary classiﬁcation style to create strong baselines as there are limited studies in authorship veriﬁcation according to the literature. Applying this adaptation we are able to employ the recognized classiﬁers as well as similarity measures that are widely used in ML to build our baselines. Besides, the two pre-existing datasets, Amazon reviews and MPLA-400, are mapped to the binary structure to be used for our large scale AV problem."
1803.06456,"dataset, github",17,,,2 https://github.com/dainis-boumber/MLP-400-datasets 3 we use scikit-learn software for all linguistic features 4 http://deeplearning.net/software/theano/
1803.06456,python,106,,,"3. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. Journal of Machine Learning Research 12 (2011) 2825–2830 4. Egan, J.P.: Signal detection theory and {ROC} analysis. Academic Press (1975) 5. Japkowicz, N., Myers, C., Gluck, M., et al.: A novelty detection approach to"
1807.01857,code,147,,,"Fig. 1 shows the schematic diagram of our proposed approach for IDE-based web search. Once the developer selects an exception from the Error Log or Console View of Eclipse IDE, our approach collects necessary information about it such as error message, stack trace and source code context. Then, it collects the results from three reliable search engines (e.g., Google, Bing and Yahoo) and one programming Q & A site (e.g., StackOverﬂow) through API endpoints against the error message and develops the corpus. The proposed approach then considers the context of the occurred error or exception, popularity and search engine recommendation of the collected results and calculates the proposed metrics to determine their acceptability and relevance with the target exception. Once the ﬁnal scores are calculated from those metrics, the results"
1807.01857,code,148,,,"A sites, forums or discussion boards in their program directly or with minor modiﬁcations. Therefore, a result link containing source code snippet similar to the surrounding code block of the selected error or exception location is likely to discuss relevant issues that the developer needs. We consider three lines before and after the affected line in the source ﬁle as the source code context of the error or exception and extract the code snippets from result links though HTML scrapping. Then, we apply SimHash Algorithm on both code contexts and generate their SimHash values. We use equation (1) to determine Source Code Context Matching Score for each result link. The score values from zero to one and it indicates the relevance of the result link with the target error in terms of the context of source code."
1807.01857,code,252,,,"To summarize, we propose a novel IDE-based web search solution that (1) exploits the search and ranking capabilities of three reliable search engines and a programming Q & A site through their API endpoints, (2) considers not only the content of the search (i.e., query keywords) but also the problem context such as stack trace and source code context, link popularity and link recommendation from the search engines, and (3) provides search result within the context of IDE with web browsing capabilities. We conduct an experiment with 25 runtime errors and exceptions related to Eclipse plugin development. Our approach recommended solutions with 96% accuracy which necessarily outperforms the traditional keyword-based search. In order to validate the results, we conduct a user study involving ﬁve prospective participants which gave a response agreement of 64.28%. Given that the relevance checking of a solution against the selected error is completely a subjective process, the preliminary results are promising. However, the proposed approach needs to be further validated with more errors and exceptions followed by an extensive user study to establish itself as a complete IDE-based web search solution. We also have plans to enable multiprocessing for the application and host it as a web service API so that others can readily use it with real time experience and also can use the API in their own IDEs rather than Eclipse."
1807.01857,code,47,,,"[2] J. Brandt, P. J. Guo, J. Lewenstein, M. Dontcheva, and S. R. Klemmer. Two studies of opportunistic programming: interleaving web foraging, learning, and writing code. In Proc. SIGCHI, pages 1589–1598, 2009."
1807.01857,code,66,,,"Title Matching Score measures the content similarity between search query and result title. Stack Trace Matching Score and Source Code Context Matching Score determine the relevance of the result link based on its contextual similarity with that of the selected error or exception; therefore, they constitute the Context Relevance Score, Scxt. We get this score using equation (5)."
1807.01857,code,81,,,"[5] M. Goldman and R. C. Miller. Codetrail: Connecting source code and web resources. J. Vis. Lang. Comput., 20(4):223–235, August 2009. [6] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns. What makes a good code example?: A study of programming Q & A in Stack Overﬂow. In Proc. ICSM, pages 25–34, 2012."
1807.01857,code,86,,,"4) Source Code Context Matching Score (Scc): Sometimes, stack trace may not be enough for problem ﬁxation and developers post related source code in forums and discussion boards for clariﬁcation. We are interested to check if the source code contexts of the discussed errors or exceptions in the result links are similar to that of the selected exception from IDE. The code contextual similarity is possible; because, the developers often reuse code snippets from programming Q &"
1807.01857,code,98,,,"In our experiment, we select 25 runtime errors and exceptions related to Eclipse plug-in development, and collect associated information such as error or exception messages, stack traces and source code context. We then use those information (e.g., error content and context) to search for solution using our approach. We also perform extensive web search manually with different available search engines and ﬁnd out the solutions for all errors and exceptions. We should note that we choose the most appropriate solution as the accepted one for each exception or error"
1807.01857,"code, package",187,,,"In this paper, we propose an Eclipse IDE-based search solution called SurfClipse to the encountered errors or exceptions which addresses the concerns identiﬁed in case of existing approaches. We package the solution as an Eclipse plug-in which (1) exploits the search and ranking algorithms of three reliable web search engines (e.g., Google, Bing and Yahoo) and a programming Q & A site (e.g., StackOverﬂow) through their API endpoints, (2) provides a content (e.g., error message), context (e.g., stack trace and surrounding source code of the subject error), popularity and search engine recommendation (of result links) based ﬁltration and ranking on the extracted results of step one, (3) facilitates the most recent solutions, accesses the complete and extensible solution set and pulls solutions from numerous forums, discussion boards, blogs, programming Q & A sites and so on, and (4) provides a real web surﬁng experiences within the IDE context using Java based browser."
1807.01857,data,102,,,"discussion boards and Q & A sites with the help of search enginies, but also ensures the access to the most recent content of StackOverﬂow through API access. However, the existing approaches by Cordeiro et al. [4] and Ponzanelli et al. [7] provide results from a single and ﬁxed sized data dump of StackOverﬂow and therefore, the results do not contain the most recent posts (i.e., discussing the most recent errors or exceptions) from StackOverﬂow as well as the promising solutions from other programming Q & A sites."
1807.01857,data,103,,,"8) Search Trafﬁc Rank Score (Sstr): The amount of search trafﬁc to a site can be considered as an important indicator of its popularity. In this research, we consider the relative popularity of the result links found in the corpus. We use the statistical data from two popular site trafﬁc control companiesAlexa and Compete through their provided APIs and get the average ranking for each result link. Then, based on their ranks, we provide a normalized Search Trafﬁc Rank Score between zero and one considering minimum and maximum search trafﬁc ranks found."
1807.01857,data,216,,,"Abstract—Traditional web search forces the developers to leave their working environments and look for solutions in the web browsers. It often does not consider the context of their programming problems. The context-switching between the web browser and the working environment is time-consuming and distracting, and the keyword-based traditional search often does not help much in problem solving. In this paper, we propose an Eclipse IDE-based web search solution that collects the data from three web search APIs– Google, Yahoo, Bing and a programming Q & A site– StackOverﬂow. It then provides search results within IDE taking not only the content of the selected error into account but also the problem context, popularity and search engine recommendation of the result links. Experiments with 25 runtime errors and exceptions show that the proposed approach outperforms the keyword-based search approaches with a recommendation accuracy of 96%. We also validate the results with a user study involving ﬁve prospective participants where we get a result agreement of 64.28%. While the preliminary results are promising, the approach needs to be further validated with more errors and exceptions followed by a user study with more participants to establish itself as a complete IDE-based web search solution."
1807.01857,data,282,,,"Traditional web search forces the developers to leave the working environment (i.e., IDE) and look for the solution in the web browsers. In contrast, if the developer chooses SurfClipse, it allows to check the search results within the context of IDE (e.g., Fig. 1-(b)). Once she selects an error message using context menu option (e.g., Fig. 1-(a)), the plugin pulls results from three reliable search engines and one programming Q & A site against that error message. Then, it calculates the proposed metrics for each result related to the error content, error context, popularity and search engine recommendation to determine its relevance with the occurred error or exception, and then sorts and displays the results. Moreover, the plug-in allows the developer to browse the solution on a Java-based web browser (e.g., Fig. 1-(c)) without leaving the context of the IDE which makes it time-efﬁcient and ﬂexible to use. The plug-in by Cordeiro et al. [4] also shows the results within the context of the IDE; however, (1) the result set is limited (i.e., only from StackOverﬂow and does not consider the whole web), (2) cannot address newly introduced issues (i.e., ﬁxed corpus and subjected to the availability of StackOverﬂow data dump), (3) only considers stack trace information as problem context, and (4) the developer cannot enjoy the web browsing experience."
1807.01857,data,318,,,"1) Search Engine Weight Based Score (Ssew): According to Alexa1, one of the widely recognized web trafﬁc data providers, Google ranks second, Yahoo ranks fourth and Bing ranks sixteenth among all websites in the web this year. While these ranks indicate their popularity (e.g., site trafﬁc) and reliability (i.e., users’ trust) as information service providers, it is reasonable to think that search results from different search engines of different ranks have different levels of acceptance. We conduct an experiment with 75 programming task and exception related queries2 against those search engines and a programming Q & A site (e.g., StackOverﬂow) to determine the relative weights or acceptance. We collect the top 15 search results for each query from each search tool and get their Alexa ranks. Then, we consider the Alexa ranks of all result links provided by each search tool and calculate the average rank for a result link provided by them. The average rank for each search tool is then normalized and inversed which provides a value between 0 and 1. We get a normalized weight of 0.41 for Google, 0.30 for Bing, 0.29 for Yahoo and 1.00 for StackOverﬂow. The idea is that if a result link against a single query is found in all three search engines, it gets the search engine scores (i.e., conﬁdence) from all three of them which sum to 1.00. StackOverﬂow has drawn the attention of a vast community (1.7 million3) of programmers and software professionals, and it also has a far better average Alexa rank than that of the search engines; therefore, the results returned from StackOverﬂow are provided a search engine score (i.e., conﬁdence) of 1.00."
1807.01857,"data, code",117,,,"Existing studies related to our research focus on integrating commercial-off-the-shelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying within IDE environment [4, 7], embedding web browser inside the IDE [3] for code example recommendation and so on. In this paper, we propose a novel approach that exploits result data from the state of art web search APIs and provides ﬁltered and ranked search results taking problem content, context, result link’s popularity and search engine recommendation about the result links into consideration. Our proposed approach not only collects solution posts from a large set of forums,"
1807.01857,"data, code, dataset provided",293,,,"for the solution in the web browsers. The context-switching between IDE and the web browser is distracting and timeconsuming. Moreover, checking relevance from hundreds of search results is a cognitive burden on the novice developers. Existing studies focus on integrating commercial-off-theshelf (COTS) tools into Eclipse IDE [8], recommending StackOverﬂow posts and displaying them within the IDE environment [4], embedding web browser inside the IDE [3] and so on. Cordeiro et al. [4] propose an IDE-based recommendation system for runtime exceptions. They extract the question and answer posts from StackOverﬂow data dump and suggest posts relevant to the occurred exceptions considering the context from the stack trace information generated by the IDE. They also suggest a nice solution to the context-switching issue through visualization of the solution within the IDE. However, the proposed approach suffers from several limitations. First, they consider only one source (e.g., StackOverﬂow Q & A site) rather than the whole web for information and thus, their search scope is limited. Second, the developed corpus cannot be easily updated and is subjected to the availability of the data dump. For example, they use the StackOverﬂow data dump of September 2011, that means it cannot provide help or suggestions to the recently introduced software bugs or errors after September 2011. Third, the visualization of the solutions is not efﬁcient as it uses plain text to show the post contents such as source code, stack trace and discussion. Thus the developers do not really experience the style and presentation of a web page."
1807.04488,code,115,04/21/22,0,"Baseline Query Selection: We select the title of a bug report as the baseline query for our experiments, as was also selected by earlier studies [21, 28, 49]. However, we discard such queries that (i.e., in verbatim titles) already return their ﬁrst correct results within the Top-10 positions, they possibly do not need query reformulation [21]. Finally, we ended up with a collection of 1,675 baseline queries. We perform the same preprocessing steps as were done on the source documents (Section II-C), on the queries before using them for code search in our experiments."
1807.04488,code,117,04/21/22,0,"Answering RQ3–Do Document Structures Matter? While most of the earlier reformulation techniques miss or ignore the structural aspect of a source document, we consider such aspect as an important paradigm of our technique. We consider a source document as a collection of structured entities (e.g., signatures, methods, ﬁelds) [38] rather than a regular text document. Thus, we make use of method signatures and ﬁeld signatures rather than the whole source code for query reformulation given that they are likely to contain more salient terms and less noise [23]. Fig. 4 demonstrates how incorpora Method signature Field signature Both signatures Both signatures"
1807.04488,code,13,,,"Code Elements in Informal Documentation. ICSE, pages 832–841, 2013."
1807.04488,code,132,,,"earlier [32], we apply a heuristic threshold of 0.0001 for the convergence checking. The algorithm captures importance of a source term not only by estimating its local impact but also by considering its global inﬂuence over other terms. For example, the term, “Classpath”, Fig. 1, occurs in multiple structured tokens (Listing 1), complements the semantics of ﬁve other terms, and thus is highly important within the term graph (i.e., Fig. 1). Once the iterative computation is over, each of the terms from the graph is found with a numeric score. We consider these scores as the relative weight or importance of the corresponding terms from the source code."
1807.04488,code,146,04/21/22,0,"To summarize, we propose a novel technique–ACER–for improved query reformulation for concept location. It takes an initial query as input, identiﬁes appropriate search terms from the source code using a novel term weight, and then suggests the best reformulation to the initial query using document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight systems report that our technique can improve 71% of the baseline queries and preserve 26% of them, which are highly promising. Comparison with ﬁve closely related approaches including the state-of-the-art not only validates our empirical ﬁndings but also demonstrates the high potential of our technique. In future, we plan to apply our term weighting method, CodeRank, to other SE text retrieval tasks involving source code such as bug localization and traceability recovery."
1807.04488,code,146,04/21/22,0,"source subject systems show that our technique can improve 71% (and preserve 26%) of the baseline queries which are highly promising according to relevant literature [13, 21, 34]. Our suggested queries return correct results for 64% of the queries in the Top-100 results. Our ﬁndings report that CodeRank is a more effective term weighting method than the traditional methods (e.g., TF, TF-IDF) for search query reformulation in the context of source code. Our ﬁndings also suggest is an important paradigm for both term weighting and query reformulation. Comparison with ﬁve closely related existing approaches [13, 21, 23, 43, 49] not only validates our empirical ﬁndings but also demonstrates the superiority of our technique. Thus, the paper makes the following contributions:"
1807.04488,code,149,,,"In this paper, we propose a novel technique–ACER–for automatic query reformulation for concept location in the context of software change tasks. We (1) ﬁrst introduce a novel graph-based term weight –CodeRank– for identifying important terms from the source code, and then (2) apply that term weight and source document structures (e.g., method signatures) to our technique for automatic query reformulation. CodeRank identiﬁes important terms not only by analyzing salient structured entities (e.g., camel case tokens), but also by exploiting the co-occurrences among the terms across various entities. Our technique–ACER–accepts a natural language query as input, develops multiple candidate queries from two different important contexts, (1) method signatures and (2) ﬁeld signatures of the source documents independently using CodeRank, and then suggests the best reformulation ( based"
1807.04488,code,163,,,"[14] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker. Mining Source Code to Automatically Split Identiﬁers for Software Analysis. In Proc. MSR, pages 71–80, 2009. [15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The Vocabulary Problem in Human-system Communication. Commun. ACM, 30(11):964–971, 1987. [16] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the Use of Relevance Feedback in IR-based Concept Location. In Proc. ICSM, pages 351–360, 2009. [17] S. Haiduc and A. Marcus. On the Use of Domain Terms in Source Code. In Proc. ICPC, pages 113–122, 2008. [18] S. Haiduc and A. Marcus. On the Effect of the Query In Proc. ICPC, pages"
1807.04488,code,167,,,"CodeRank: PageRank [10] is one of the most popular algorithms for web link analysis which was later adapted by Mihalcea and Tarau [32] for text documents as TextRank. In this research, we adapt our term weighting method from TextRank [9, 32, 41] for source code, and we call it CodeRank. To date, only traditional term weights (e.g., TF, TFIDF [21, 43, 49]) are applied to source code which were originally proposed for regular texts [26] and are mostly based on isolated frequencies. On the contrary, CodeRank not only analyzes the connectivity (i.e., incoming links and outgoing links) of each source term, but also the relative weight of the connected terms from the graph recursively, and calculates the term weight, S(Vi), as follows (Step 6, Fig. 2):"
1807.04488,code,168,,,"There exist a number of studies in the literature that reformulate a given query for concept location in the context of software change tasks. Existing studies apply relevance feedback from developers [16], pseudo-relevance feedback from IR tools [21], partial phrasal matching [23, 44], and machine learning [21, 34] to query reformulation. They also make use of context of query terms from source code [25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Hill et al. [23] consider the presence of query terms in the method or ﬁeld signatures as an indicator of their relevance, and suggest natural language phrases from them as reformulated queries. Sisman and Kak [49] choose such terms for query reformulation that frequently co-occur with query terms within"
1807.04488,code,17,,,Fig. 1. An example term graph generated by CodeRank for source code of Listing 1
1807.04488,code,17,,,Listing 1. Source code used for automatic query reformulation (abridged from [3])
1807.04488,code,179,,,"In order to suggest meaningful reformulations to an initial query, feedback on the query is required. Gay et al. [16] ﬁrst reformulate queries based on explicit feedback from the developers. Although such feedback could be useful, gathering them is often time-consuming and sometimes infeasible. Hence, a number of recent studies [13, 21, 40, 41] apply pseudorelevance feedback as a feasible alternative. The top ranked results returned by the code search tool for an initial query are considered as the pseudo-relevance feedback for the query. We ﬁrst reﬁne an initial query by removing the punctuation marks, numbers, special symbols and stop words (Step 1, Fig. 2). Then we collect the Top-K (i.e., K = 10, best performing heuristic according to our experiments) search results returned by the query, and use them as the source for our candidate terms for query reformulation (Steps 2, 3, Fig. 2)."
1807.04488,code,185,04/21/22,0,"Abstract—During software maintenance, developers usually deal with a signiﬁcant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique –ACER– that takes an initial query, identiﬁes appropriate search terms from the source code using a novel term weight –CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with ﬁve closely related existing techniques in query reformulation not only validates our empirical ﬁndings but also demonstrates the superiority of our technique."
1807.04488,code,190,,,"Stop word and Keyword Removal: Since our structured tokens comprise of natural language terms, we discard stop words from them as a common practice (Step 4, Fig. 2). We use a standard list [6] hosted by Google for stop word removal. Programming keywords can often be considered as the equivalence of stop words in the source code which are also discarded from our analysis. Since we deal with Java source code, the keywords of Java are considered for this step. As suggested by earlier study [21], we also discard insigniﬁcant source terms (i.e., having word length< 3) from our analysis. Stemming: It extracts the root (e.g., “send”) out of a word (e.g., “sending”). Although existing studies suggest contradictory [28, 45] or conﬂicting [24] evidences for stemming with the source code, we investigate the impact of stemming with RQ4 where Snowball stemmer [24, 37] is used for stemming."
1807.04488,code,219,,,"Thus, to answer RQ1, the reformulation of ACER improves the baseline queries signiﬁcantly both in terms of query effectiveness and retrieval performance. ACER improves 71% of the baseline queries with 64% Top-100 retrieval accuracy. Answering RQ2–CodeRank vs. Traditional Term Weighting Methods: Table VII shows the comparative analysis between CodeRank and two traditional term weights– TF and TF-IDF– which are widely used in the text retrieval contexts [13, 28, 43]. While TF estimates the importance of a term based on its occurrences within a document, TF-IDF additionally captures the global occurrences of the term across all the documents of the corpus [26]. On the contrary, CodeRank employs a graph-based scoring mechanism that determines the importance of a term based on its co-occurrences with other important terms within a certain context. From Table VII, we see that CodeRank performs signiﬁcantly better than both TF (i.e., paired t-test, p-value=0.005<0.05) and TF-IDF (i.e., p-value<0.001) in identifying important search terms from source code, especially from the method signatures. Considering the whole source code rather than signatures improves the performance of both TF (i.e., 56% query improvement) and"
1807.04488,code,228,,,"Fig. 3 shows how CodeRank and traditional term weights perform in reformulating the baseline queries with their (a) Top-10 and (b) Top-30 terms. We see that TF reaches its peak performance pretty quickly (i.e., K = 3), and then shows a stationary or irregular behaviour. That means, TF identiﬁes frequent terms for query reformulation, and few of them (e.g., Top-3) could be highly effective. On the contrary, our method– CodeRank– demonstrates a gradual improvement in the performance up to Top-12 terms (i.e., K=12, Fig. 3-(b)), and crosses the performance peak of TF with a large margin (i.e., paired t-test, p-value=0.004<0.05, Cohen’s D=3.77>1.00 (large)), for K=10 to K=15). CodeRank emphasizes on the votes from other important terms (i.e., by leveraging co-occurrences) for determining weight of a term, and as demonstrated in Fig. 3, this weight is found to be more reliable than TF. TF-IDF is found relatively less effective according to our investigation. Thus, to answer RQ2, CodeRank performs signiﬁcantly better than traditional methods in identifying effective terms for query reformulation from the source code."
1807.04488,code,23,04/21/22,0,• RQ3: Does employment of document structure improve ACER’s suggestion on good quality search terms from the source code?
1807.04488,code,231,,,"Candidate Token Mining: Developers often express their intent behind the code and encode domain related concepts in the identiﬁer names and comments [17]. However, code comments are often inadequate or outdated [51]. All identiﬁer types also do not have the same level of importance. For example, while the signature of a method encodes the high level its body focuses on granular level implementation details and thus possibly contains more noisy terms [23]. In fact, Hill et al. [23] ﬁrst analyze method signatures and ﬁeld signatures to suggest natural language phrases as queries for code search. In the same vein, we thus also consider method signatures (msig) and ﬁeld signatures (f sig) as the source for our candidate reformulation terms. We extract structured identiﬁer names from these signatures using appropriate regular expressions [42] (Step 4, Fig. 2). Since different contexts of a source document might convey different types or levels of semantics (i.e., developers’ intent), we develop a separate candidate token set (CTsig) for each of the two signature types (sig ∈ {msig, f sig}) from the feedback documents (∀d ∈ DRF ) as follows: CTsig ="
1807.04488,code,248,,,"Candidate Reformulation Selection: Algorithms 1 and 2 show the pseudo-code of our query reformulation technique– ACER–for concept location. We ﬁrst collect pseudo-relevance feedback for the initially provided query (Q) where Top-K source documents are returned (Lines 3–5, Algorithm 1). Then we collect method signatures and ﬁeld signatures from each of the documents (∀d ∈ DRF ), and extract structured tokens from them. We prepare three token sets–CTmsig, CTf sig and CTcomb from these signatures (Lines 6–12, Algorithm 1, Step 4, Fig. 2) where CTcomb combines tokens from both signatures. Then we perform limited natural language preprocessing on each token set where Samurai algorithm [14] is used for token splitting. We develop separate term graph for each of these token sets where individual terms are represented as vertices, and term co-occurrences are encoded as connecting edges (Lines 3–7, Algorithm 2, Step 5, Fig. 2). We apply CodeRank term weighting to each of the graphs which provides a ranked list of terms based on their relative importance. Then we select Top-K (e.g., K = 10) important terms from each of the three graphs, and prepare three reformulation candidates (Lines 8– 12, Algorithm 2, Steps 6, 7, 8, Fig. 2). Algorithm 1 ACER: Proposed Query Reformulation"
1807.04488,code,26,,,[3] Example code snippet. URL https://goo.gl/WSZHiC. [4] Samurai preﬁx and sufﬁx list. URL https://hiper.cis.udel.
1807.04488,code,263,,,"Once candidate tokens are extracted from method signatures and ﬁeld signatures, and are splitted into candidate terms, we develop source term graphs (e.g., Fig. 1) from them (Step 5, Fig. 2). Developers often encode their intent behind the code and domain vocabulary into the carefully crafted identiﬁer names where multiple terms are concatenated. For example, the method name–getChatRoomBots–looks like a natural language phrase–“get chat room bots”–when splitted properly. Please note that each of these three terms–“chat”, “room” and “bots”– co-occur with each other to convey an important concept– a robotic technology, and thus, they are semantically connected. On the other hand, the remaining term–“get”– cooccurs with them due to a temporal relationship (i.e., develops a verbal phrase). Similar phrasal representations (reﬁned with lexical matching) were directly returned by Hill et al. for query reformulation. However, their approach could be limited due to the added constraint (e.g., warrants query terms in signatures). We thus perform further analysis on such phrases, and exploit the co-occurrences among the terms for our graph based term weighting. In particular, we encode the term co-occurrences into connecting edges (E) in the term graph (G(V, E)) where the individual terms (Vi) are denoted as vertices (V )."
1807.04488,code,281,,,"Studies show that about 80% of the total efforts is spent in software maintenance [36] where developers deal with a signiﬁcant number of software issues [35, 45, 52]. Software issue reports (a.k.a., change requests) discuss both unexpected (or erroneous features such as bugs) and expected but nonexistent features (e.g., new functionality). For both bug resolution and new feature implementation, a developer is required to map the concepts discussed in the issue report to appropriate source code within the project which is widely known as concept location [29, 31, 40]. Developers generally choose one or more important keywords from the report texts, and then use a search method (e.g., regular expression) to locate the source code entities (e.g., classes, methods) that need to be changed. Unfortunately, as the existing studies [28, 30] report, developers regardless of their experience perform poorly in choosing appropriate search terms for software change tasks. According to Kevic and Fritz [28], only 12.20% of the search terms chosen by the developers were able to locate relevant source code entities for the change tasks. Furnas et al. [15] also suggest that there is a little chance (i.e., 10%–15%) that developers guess the exact words used in the source code. One way to assist the developers in this regard is to automatically suggest helpful reformulations (e.g., complementary keywords) to their initially chosen queries."
1807.04488,code,29,,,"• A novel term weighting method –CodeRank– for source code that identiﬁes the most important terms from a given code entity (e.g., class, method)."
1807.04488,code,29,,,"• RQ2: Does CodeRank perform better than traditional term weighting methods (e.g., TF, TF-IDF) in identifying effective search terms from the source code?"
1807.04488,code,3,04/21/22,0,Preprocessing Code search
1807.04488,code,31,,,"[49] B. Sisman and A. C. Kak. Assisting Code Search with Automatic Query Reformulation for Bug Localization. In Proc. MSR, pages 309–318, 2013."
1807.04488,code,311,,,"Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]. Thus, they might also fail to identify the appropriate terms from the structured source documents for query reformulation."
1807.04488,code,320,,,"We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig. 5 shows how (a) Top-10 and (b) Top-30 reformulation terms improve the baseline queries. We see that"
1807.04488,code,324,,,"Table I shows an example change request [2] submitted for eclipse.jdt.debug system, and it refers to “debugger source lookup” issue of Eclipse IDE. Let us assume that the developer chooses important keywords from the request title, and formulates a generic initial query–“debugger source lookup.” Unfortunately, the query does not perform well, and the 79th position of the returns the ﬁrst correct result at result list. Further extension–“debugger source lookup work variables”–also does not help, and returns the result at the 77th position. The existing technique – RSV [13]– extends the query as follows–“debugger source lookup work variables launch conﬁguration jdt java debug”–where the new terms are collected from the project source using TF-IDF based term weight. This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig."
1807.04488,code,328,,,"This query returns the correct result at the 30th position which is also far from ideal unfortunately. The query of Sisman and Kak [49]–“debugger source lookup work variables test exception suite core code”–also returns the correct result at the 51st position. On the other hand, our suggested query– “debugger source lookup work variables launch debug problem resolve required classpath”–returns the correct result at the 2nd position which is highly promising. We ﬁrst collect structured tokens (e.g., resolveRuntimeClasspathEntry) from method signatures and ﬁeld signatures of the source them into simpler terms code (e.g., Listing 1), and split (e.g., resolve, Runtime, Classpath and Entry). The underlying idea is that such signatures often encode high level intents and important domain terms while the rest of the code focuses on more granular level implementation details, and thus possibly contains more noise [23, 48]. We develop individual term graph (e.g., Fig. 1) based on term co-occurrences from each signature type, apply CodeRank term weighting, and extract multiple candidate reformulations with the highly weighted terms (e.g., orange coloured, Fig. 1). Then we analyze the quality of the candidates using their quality measures [19], apply machine learning, and suggest the best reformulation to the initial query. Thus, our technique (1) ﬁrst captures salient terms from the source documents by analyzing their structural aspects (i.e., unlike bag of words approaches [46]) and an appropriate term weight–CodeRank, and (2) then suggests the best query reformulation using document structures (i.e., multiple candidates derived from various signatures), query quality analysis and machine learning [19]. Experiments using 1,675 baseline queries from eight open"
1807.04488,code,33,,,"[38] M. M. Rahman and C. K. Roy. On the Use of Context in Recommending Exception Handling Code Examples. In Proc. SCAM, pages 285–294, 2014."
1807.04488,code,335,,,"* = Statistically signiﬁcant difference between two measures from the same signature, MRD = Mean Rank Difference between ACER and baseline queries tion of document structures into a technique could be useful for query reformulations. We see that reformulations using method signatures and ﬁeld signatures improve two different sets of baseline queries, and this happens with both term weighting methods–(a) CodeRank and (b) TF. While these sets share about half of the queries (49%–57%), reformulations based on each signature type also improve a signiﬁcant amount (i.e., 19% (73+136+24) – 25% (105+152+46)) of unique baseline queries. In Fig. 4-(c), when these signatures (i.e., along with ACER) are contrasted with the whole source code (i.e., along with TF), we even found that the signature-based reformulations outperform the whole code-based reformulations by a large margin (i.e., (25.2%–8.39%) ≈ 17% more query the use of the whole source code improvement). That introduces additional noise, and diminishes the strength or salience of the individual structures (i.e., signatures). Most of the existing methods [16, 21, 40] suffer from this limitation. On the contrary, our technique ACER exploits document structures (i.e., signatures), and carefully chooses the best among all the candidate reformulations derived from such structures using query quality analysis and machine learning. Thus, to answer RQ3, document structures improve the suggestion of query reformulation terms from the source code. Answering RQ4– Impact of Stemming, Query Length, and Relevance Feedback: From Table VIII, we see that stemming generally degrades the effectiveness of our reformulated queries. Similar ﬁndings were also reported by earlier studies [28, 45]. Fig."
1807.04488,code,342,,,"reformulation tasks. They also make use of context of query terms from source code [23, 25, 40, 49, 53], text retrieval conﬁguration [21, 34], and quality of queries [19, 20] in suggesting the reformulated queries. Gay et al. [16] capture explicit feedback on document relevance from the developers, and then suggest reformulated queries using Rocchio’s expansion [43]. Haiduc et al. and colleagues [18, 19, 20, 21, 22] take quality of a given query (i.e., query difﬁculty) into consideration, and suggest the best reformulation strategy for the query using machine learning. While all these above techniques are reported to be novel or effective, most of them also share several limitations. First, source documents contain both structured items (e.g., method signatures, formal parameters) and unstructured items (e.g., code comments). Unfortunately, many of the above reformulation approaches [16, 21, 49] treat the source documents as simple plain text documents, and ignore most of their structural aspects except structured tokens. Such inappropriate treatment might lead to suboptimal or poor queries. In fact, Hill et al. [23] ﬁrst consider document structures, and suggest natural language phrases from method signatures and ﬁeld signatures for local code search. However, since they apply only simple textual matching between initial queries and the signatures, the suggested phrases are subject to the quality of not only the given queries and but also of the identiﬁer names from those signatures. Second, many of these approaches often directly apply traditional metrics of term importance (e.g., avgIDF [20], TF-IDF [43]) to source code which were originally targeted for unstructured regular texts (e.g., news article) [26]."
1807.04488,code,35,04/21/22,0,"Fig. 2 shows the schematic diagram of our proposed technique–ACER–for automatic query reformulation. We use a novel graph-based metric of term importance–CodeRank– for source code, and apply source document structures, query"
1807.04488,code,36,,,"[23] E. Hill, L. Pollock, and K. Vijay-Shanker. Automatically Capturing Source Code Context of NL-queries for Software Maintenance and Reuse. In Proc. ICSE, pages 232–242, 2009."
1807.04488,code,37,04/21/22,0,"[25] M. J. Howard, S. Gupta, L. Pollock, and K. VijayShanker. Automatically Mining Software-based, Semantically-Similar Words from Comment-Code Mappings. In Proc. MSR, pages 377–386, 2013."
1807.04488,code,38,,,"[31] A. Marcus, A. Sergeyev, V. Rajlich, and J.I. Maletic. An Information Retrieval Approach to Concept Location in Source Code. In Proc. WCRE, pages 214–223, 2004."
1807.04488,code,4,,,C. Source Code Preprocessing
1807.04488,code,4,,,URL https://code.google.com/p/
1807.04488,code,49,,,"[22] S. Haiduc, G. De Rosa, G. Bavota, R. Oliveto, A. De Lucia, and A. Marcus. Query Quality Prediction and the Refoqus Reformulation for Source Code Search: Tool. In Proc. ICSE, pages 1307–1310, 2013."
1807.04488,code,61,04/21/22,0,"[43] J.J. Rocchio. The SMART Retrieval System—Experiments in Automatic Document Processing. Prentice-Hall, Inc. [44] M. Roldan-Vega, G. Mallet, E. Hill, and J. A. Fails. CONQUER: A Tool for NL-based Query Reﬁnement and In Proc. ICSM, Contextualizing Code Search Results. pages 512–515, 2013."
1807.04488,code,7,,,B. Corpus Indexing & Source Code Search
1807.04488,code,7,,,Tasks to Source Code. 2014.
1807.04488,code,7,04/21/22,0,that structure of a source code document
1807.04488,code,77,,,"[51] C. Vassallo, S. Panichella, M. Di Penta, and G. Canfora. CODES: Mining Source Code Descriptions from Developers Discussions. In Proc. ICPC, pages 106–109, 2014. [52] I. Vessey. Expertise in Debugging Computer Programs: An Analysis of the Content of Verbal Protocols. TSMC, 16(5):621–637, 1986. [53] J. Yang and L. Tan."
1807.04488,code,87,,,"Here sig(d) extracts all tokens from method signatures or ﬁeld signatures, and structured(t) determines whether the token t ∈ Tsig is structured or not. Although we deal with Java source code in this research where the developers generally use camel case tokens (e.g., MessageType) or occasionally might use same case tokens (e.g., DECIMALTYPE), our approach can be easily replicated for snake case tokens (e.g., reverse traversal) as well."
1807.04488,data,130,,,"Threats to internal validity relate to experimental errors and biases [55]. Although CodeRank and document structures play a major role, the data resampling step (Section II-F, Step 9, Fig. 2) has a signiﬁcant role behind the high performance of our technique. Unfortunately, to the best of our knowledge, Refoqus [21] does not have such a step. Thus, the performance comparison might look like a bit unfair. Besides, models based on data resampling are sometimes criticized for intrinsic biases [5]. However, we apply data resampling to Refoqus as well (i.e., Refoqussampled), and demonstrate that our technique still performs better in terms of worsening ratio."
1807.04488,data,19,,,"Index Terms—Query reformulation, CodeRank, term weight ing, query quality analysis, concept location, data resampling"
1807.04488,data,328,,,"Haiduc et al. [19] suggest that quality of a query with respect to the corpus could be determined using four of its statistical properties– speciﬁcity, coherency, similarity and term relatedness–that comprise of 21 metrics [11]. They apply machine learning on these properties, and separate high quality queries from low quality ones. We thus also similarly apply machine learning on our reformulation candidates (and their metrics), and develop classiﬁer model(s) where Classiﬁcation And Regression Tree (CART) is used as the learning algorithm [19]. Since only the best of the four reformulation candidates (i.e., including baseline) is of our interest, the training data was inherently skewed. We thus perform bootstrapping (i.e., random resampling) [27, 50] on the data multiple times (e.g., 50) with 100% sample size and replacement (Step 9, Fig. 2), train multiple models using the sampled data, and then record their output predictions. Then, we average all the predictions for each test instance from all models, and determine their average probability of being the best candidate reformulation. Thus, we identify the best of the four candidates using our models, and suggest the best reformulation to the initial query (Lines 16–20, Algorithm 1, Steps 10, 11, Fig. 2). Bassett and Kraft [8] suggest that repetition of certain query terms might improve retrieval performance of the query. If none of the candidates is likely to improve the initial query according to the quality model (i.e., baseline itself is the best), we repeat all the terms from the initial query as the reformulation. Algorithm 2 getQRCandidate: Get a candidate reformulation (cid:46) CTsig: extracted"
1807.04488,data,33,,,"[54] J. Yao, B. Cui, L. Hua, and Y. Huang. Keyword Query Reformulation on Structured Data. In Proc. ICDE, pages 953–964, 2012."
1807.04488,data,36,,,"[50] M. Tan, L. Tan, S. Dara, and C. Mayeux. Online Defect Prediction for Imbalanced Data. In Proc. ICSE, volume 2, pages 99–108, 2015."
1807.04488,data,37,04/21/22,0,"[7] A. Bachmann and A. Bernstein. Software Process Data Quality and Characteristics: A Historical View on Open and Closed Source Projects. In Proc. IWPSE, pages 119– 128, 2009."
1807.04488,data,4,04/21/22,0,Quality metric data resampling
1807.04488,"data, code",319,,,"a ﬁxed size of window in the code. Rocchio [43] and RSV [13] determine importance of a term using TF-IDF based metrics. Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source."
1807.04488,"data, code",338,,,"Haiduc et al. [21] identify the best of four reformulation candidates for any given query using a machine learning model with 28 metrics. All these ﬁve studies are highly relevant to ours, and we directly compare with them using experiments. Readers are referred to Section III-E for comparison details. Other related studies [39, 41, 54] explore graph-based methods for term weighting. Rahman and Roy [39, 41] simply use TextRank on change request texts for suggesting initial queries for concept location. Yao et al. [54] build a term augmented tuple graph and use a random walk approach to reformulate queries for structured bibliographic DBLP Data (i.e., nonsource code). Ours is signiﬁcantly different from these studies in the sense that we reformulate the initial queries not only by employing our term weighting method–CodeRank for source code, but also by applying source code document structures, query quality analysis and machine learning. Besides, their reported best performance (i.e., 58%–62% query improvement over baseline [41]) is quite lower than our performance (i.e., 71%, even with difﬁcult queries). Given that reformulation is often performed on the initial queries, our technique can potentially complement theirs. Howard et al. [25] map method signatures to associated comments for query reformulation, and thus, might not work well with source code without comments. Rahman and Roy [40] exploit crowd sourced knowledge for query reformulation, and their method is also subject to the availability of a third party information source. Thus, while earlier studies adopt various methodologies or information sources, our technique not only employs a novel and promising term weight –CodeRank, but also exploits structures of the source documents for identifying the best reformulation to a given query for improved concept location."
1807.04488,"data, data https",32,04/21/22,0,[1] ACER experimental data. URL https://goo.gl/ZkaNvd. [2] Debbugger source lookup does not work with variables. URL https://bugs.eclipse.org/bugs/show bug.cgi? id=31110.
1807.04488,"data, data https",8,04/21/22,0,Replication: All experimental data and relevant materials
1807.04488,"data, dataset",308,04/21/22,0,"From Table IX, we see that RSV and Refoqus perform better than the other existing approaches. They improve about 55% and about 53% of the baseline queries respectively. Such ratios are also pretty close to the originally reported performances by Haiduc et al. on a different dataset, which possibly validates the correctness of our implementation. While 55% query improvement is the maximum performance provided by any of the existing approaches, our technique–ACER–improves about 70% of the baseline queries (i.e., 1% difference between Table V and Table IX due to rounding error) which is signiﬁcantly higher, i.e., paired t-test, p-value=6.663e-06<0.05, Cohen’s D=2.43>1.00 (large). Refoqus adopts a similar methodology like ours. Unfortunately, the approach is limited due to possibly the low performance of its candidate reformulations. One might argue about the data resampling step (i.e., Step 9, Fig. 2) of ACER for the high performance. However, we also apply data resampling to Refoqus using the same settings as ours for further investigation. We see that Refoqussampled has a similar improvement ratio like ours, but it still worsens a signiﬁcant amount of queries, 29%, compared to our 3.40%. Thus, our technique still performs better than Refoqus in the equal settings. Our quantile measures and mean ranks are more promising than those from the baseline or competing methods as reported in Table IX. Table V and RQ1 also suggest that our queries have high potential for reducing human efforts. We also experiment with an extended dataset (i.e., 1,755=1,675 + 8x10) containing 80 very good queries. As reported in Table"
1807.04488,"data, dataset, open-source",55,,,"Data Collection: We collect a total of 1,675 bug reports from eight open source subject systems (i.e., ﬁve Eclipse systems and three Apache systems) for our experiments. Table III shows the experimental dataset. We ﬁrst extract resolved bug reports (i.e., marked as RESOLVED) from BugZilla and"
1807.04488,database,32,,,"[55] T. Yuan, D. Lo, and J. Lawall. Automated Construction of a Software-speciﬁc Word Similarity Database. In Proc. CSMR-WCRE, pages 44–53, 2014."
1807.04488,dataset,13,04/21/22,0,TABLE III EXPERIMENTAL DATASET System #CR #Classes ecf–279.279 log4j–1.2.18 sling–9.0 tomcat70–7.0.73
1807.04488,dataset,4,,,A. Experimental Dataset
1807.04488,"dataset, code, github",133,04/21/22,0,"JIRA repositories, and then collect corresponding bug-ﬁxing commits from GitHub version control histories of these eight systems. Such approach was regularly adopted by the relevant literature [8, 21, 41, 49], and we also follow the same. In order to ensure a fair evaluation or validation, we discard the bug reports from our dataset for which no source code ﬁles (e.g., Java classes) were changed or no relevant source ﬁles exist in the system snapshot collected for our study. We also discard such bug reports that contain stack traces using appropriate regular expressions [33]. They do not represent a typical change request (i.e., mostly containing natural language texts) from the regular software users."
1807.04488,open-source,7,,,from eight open source subject systems.
1807.04488,publicly available,103,04/21/22,0,"that their implementations are not publicly available. In the case of Refoqus, we implement 27 metrics (20 pre-retrieval [19] and 7 post-retrieval [21]) that estimate query difﬁculty. We develop a machine learning model using CART algorithm (i.e., as used by them) and 10-fold cross validation. Then, we use the model to return the best reformulation out of four candidates of Refoqus– query reduction, Dice expansion, Rocchio’s expansion and RSV expansion–for each baseline query. Table IX and Fig. 6 summarize our comparative analyses."
1810.03977,data,100,,,The images are normalized and then given to the model for training.First convolution layer the kernel size used is 3×3 with input shape 32×56×56 with RELU (Rectiﬁed Linear Unit) activation function in the ﬁrst convolution layer and then with max pooling layer of size 32×27×27 we are down sampling the data to half of the original dimension and subsequent layers follow the similar pattern.The brief description of the CNN layers architecture along with the output shape is described in table 1.drop out is 0.25 which means we randoms abandon some of the weights to avoid the over ﬁtting
1810.03977,data,173,,,"Support vector machines(SVM’s) are the most used machine learning algorithms for the image spam detection and because of high accuracy and robustness to misclassiﬁcations is the reason researcher prefer SVM[6]. SVM is a supervised learning algorithm used for the classiﬁcation of data.It consists of support vectors which divide and classiﬁes the data.It classiﬁes the non linear data using kernel trick in which the non linear data is projected to higher dimensions to make it linearly separable by a plane which is generally referred as hyper plane.It does this using a kernel function and their are diﬀerent types of kernel functions like linear,polynomial,radial basis function(RBF) and GausImage spam detection is a binary classian kernels. siﬁcation problem and two classes are spam and not spam.Using the training data that is collected and labeled according to respective classes model is trained and then the model is tested by giving the test data and performance of model is evaluated."
1810.03977,data,39,,,"[8] R. Vinayakumar, P. Poornachandran, K. P. Soman, Scalable Framework for Cyber Threat Situational Awareness Based on Domain Name Systems Data Analysis, Springer Singapore, Singapore, 2018, pp. 113–142."
1810.03977,data,50,,,"Initially, neural networks are used for image spam detection and then now research has shifted focus on applying the deep learning algorithms. Deep learning consists of neural network layers which automatically extracts the features from the data in hierarchical pattern and then predicts and classiﬁes the data."
1810.03977,data,81,,,Convolutional neural networks (CNN’s) are one of the highly eﬃcient deep learning algorithms used for classifying data (particularly image data) using supervised learning technique. They consist of an Input layer and convolution layer followed by pooling layer and again convolution and pooling layers alternatively based on the size and architecture of the network. The ﬁnal layer is a fully connected layer. Fully connected layer converts the ﬁnal scalar outputs of individual classes
1810.03977,"data, dataset",10,,,Table 2: Results metrics evaluated on test data set
1810.03977,"data, dataset",144,,,In this research we have used the convolutional neural network(CNN) which is a deep learning network architecture for image spam detection.The deep learning approach gives better accuracy when compared with the machine learning and other conventional image processing based methods and also avoids the manual feature extraction task by automatically identifying the features by itself reducing the time and eﬀort.Binary classiﬁcation of image is performed the model is trained with existing labelled data set and then tested with the test data then metrics are evaluated.Further research can be carried out by exploring other deep learning algorithms like RNN and LSTM and tuning the architecture and hyper parameters may provide interesting insights.Capsule networks can also be tested on the data set which are giving promising results recently when compared with the convolutional neural networks for image related techniques[10].
1810.03977,"data, dataset",24,,,"features[5]. Features like sender,meta data,message header are extracted and training dataset is prepared and labeled."
1810.03977,"data, dataset",4,,,4. Data set
1810.03977,"data, dataset",47,,,"Total images are split in ratio of 80 percent training data and 20 percent testing data.The model is evaluated after the convolutional neural network is trained on training dataset.It is then tested on the testing data set and result metrics accuracy,precision,recall and f1score"
1810.03977,dataset,32,,,Dataset is subdivided into both training and testing datasets. Training dataset consists of 742 spam images and 648 normal images.Testing dataset consists of 186 spam images and 162 normal images.
1810.03977,dataset,47,,,The dataset used in the experiment consist of 928 spam images and 810 normal images which collected from diﬀerent sources[9] and all are RGB images in various dimensions which in preprocessing are reshaped to 56×56 images.The sample images are shown in ﬁgure.1 and ﬁgure.2
1810.03977,dataset,8,,,Figure 1: Sample spam image from dataset
1810.03977,dataset,8,,,Figure 2: Sample non-spam image from dataset
1810.03977,dataset,96,,,Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classiﬁcation achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques.
1812.05067,code,133,,,"Experimental evaluation We have used our implementation to typecheck a variety of examples, including all the examples from the RelCost paper. Some of the examples, such as the relational analysis of merge sort (msort), have rather complex paper proofs. However, in all cases, the total typechecking time (including existential elimination and SMT solving) is less than 1s, suggesting that the approach is practical. Table 1 shows the experimental results over a subset of our example programs (our appendix lists all our example programs, including their code and experimental results). A “-” indicates a negligible value. Our experiments were performed on a 3.20GHz 4-core Intel Core i5-6500 processor with 16 GB of RAM."
1812.05067,code,239,,,"Heuristics illustrated with merge sort We explain how our implementation types one example—the standard merge sort function— relationally. The goal of this exercise is primarily to illustrate some of our heuristics. The merge sort function, msort, splits a list into two nearly equal-sized sublists using an auxiliary function we call bsplit, recursively sorts each sublist and then merges the two sorted sublists using another function merge. In their paper on RelCost, Çiçek et al. [14] show that the relative cost of two runs of msort with two input lists of length n that diﬀer in at most α positions is upper-bounded by Q(n, α) = ) · min(α, 2H −i ), where H = ⌈log2(n)⌉ and h is a speciﬁc linear function. This open-form expression lies in O(n · (1 + log2(α))).1 Next, we explain at a high-level how this relative cost is typechecked bidirectionally. We show below the code of the top-level merge sort function msort. ﬁx msort(_).Λ.Λ.λl .case l of nil → nil | h1 :: tl1 → case tl1 of nil → cons(h1, nil) | _ :: _ → let r = bsplit ()[ ] [ ] l in"
1812.05067,code,36,,,"We do not show the code of the helper functions bsplit and merge, but they have the following types (these types are also checked with BiRelCost; we omit those details here):"
1812.05067,data,154,,,"Next, we extend relSTLC in two steps inspired by the features of previously proposed relational type systems. Our ﬁrst step, named RelRef, adds relational reﬁnement types over lists (as an example of an inductive data type), and a comonadic type that represents syntactic equality of two values. Our second step, named RelRefU, adds to RelRef the possibility to relate arbitrary programs of possibly dissimilar syntactic structure, thanks to the possibility to switch to a complementary unary type system. Both these extensions add intrinsic nondeterminism to the type system to allow a programmer ﬂexibility in writing programs. The source of nondeterminism in both these systems is non-syntax-directed typing and subtyping rules. RelRef has such rules for relational reﬁnement types and for subtyping, while RelRefU has such a rule for switching to unary typing and more such rules for subtyping."
1812.05067,data,98,,,"Relational eﬀects [8, 14, 16, 28, 39] are often of a quantitative nature and measure some quantitative diﬀerence between two executions of the two expressions. These relational eﬀects are similar in spirit to their standard unary counterpart [13, 31, 32, 34] but their interpretation is a relation between the eﬀects of the two executions. For example, in diﬀerential privacy, a relational eﬀect is used to measure the level of indistinguishability between the observable outputs on two inputs diﬀering in one data element."
1904.11228,data,112,,,"The feature matrix of data in the vth view is denoted as Xv = [xv N ]T ∈ RN ×dv , xv 1 ∈ Rdv×1, dv is the dimension of feature in the vth view, N is the number of data samples. We pack the feature matrices in V views {Xv}V v=1 and the overall feature matrix of data can be represented as X = [X1, X2, ..., XV ] ∈ RN ×d, (cid:80)V v=1 dv = d. The objective of unsupervised multi-view feature selection is to identify l most valuable features with only X."
1904.11228,data,145,,,"The selected features should preserve the dynamically learned similarity structure. Conventional approaches separate the similarity structure construction and feature selection into two independent processes, which will potentially lead to sub-optimal performance. In this paper, we learn the collaborative similarity structure dynamically and further integrate it with feature selection into a uniﬁed framework. Specifically, based on the collaborative similarity structure learning in Eq.(3), we employ sparse regression model to learn a projection matrix P ∈ Rd×k, so that the projected lowdimensional data XP can approximate the relaxed cluster indicator F. To select the features, we impose l2,1 norm penalty on P to force it with row sparsity. The importance of features can be measured by the l2 norm of each row feature in P. The overall optimization formulation can be derived as"
1904.11228,data,15,,,"ture selection for big data analytics. 32(2):9–15, 2017."
1904.11228,data,173,,,"N Sj = 1, Sj ≥ 0, WT where Sj ∈ RN ×1 characterizes the similarities between any data points with j, it should be subjected to the constraint that 1TSj = 1, Sj ≥ 0, Wj = [w1 j ]T ∈ RV ×1 is comj , w2 prised of view weights for the jth column of similarities, it is constrained with WT j 1V = 1, W = [W1, W2, ..., WN ] ∈ RV ×N is view weight matrix for all columns in the similarity structures. As indicated in recent work [Nie et al., 2014], a theoretically ideal similarity structure for clustering should have the property that the number of connected components is equal to the number of clusters. The similarity structure with such neighbor assignment could beneﬁt the subsequent feature selection. Unfortunately, the similarity structure learned from Eq.(1) does not have such desirable property."
1904.11228,data,176,,,"hand, with multi-view features, the data could be characterized more precisely and comprehensively from different perspectives. On the other hand, high-dimensional multiview features will inevitably generate expensive computation cost and cause massive storage cost. Moreover, they may contain adverse noises, outlying entries, irrelevant and correlated features, which may be detrimental to the subsequent learning process [Zhu et al., 2016b; Zhu et al., 2016a; Zhu et al., 2017a]. Unsupervised multi-view feature selection [Wang et al., 2016; Li and Liu, 2017] is devised to alleviate the problem. It selects a compact subset of informative features from the original features by dropping irrelevant and redundant features with advanced unsupervised learning. Due to the independence on semantic labels, high computing efﬁciency and well interpretation capability, unsupervised multiview feature selection has received considerable attention in It becomes a prerequisite component in various literature. machine learning models [Li et al., 2017]."
1904.11228,data,204,,,"The key problem of multi-view feature selection is how to effectively exploit the diversity and consistency of multi-view features to collaboratively identify the feature dimensions, which could retain the key characteristics of the original features. Existing approaches can be categorized into two major families. The ﬁrst kind of methods ﬁrst concatenates multiview features into a vector and then directly imports it into the conventional single-view feature selection model. The candidate features are generally ranked based on spectral graph theory. Typical methods of this kind include Laplacian Score (LapScor) [He et al., 2005], spectral feature selection (SPEC) [Zhao and Liu, 2007] and minimum redundancy spectral feature selection (MRSF) [Zhao et al., 2010]. Commonly, the pipeline of these methods follows two separate processes: 1) Similarity structure is constructed with ﬁxed graph parameters to describe the geometric structure of data. 2) Sparsity and manifold regularization are employed together to identify the most salient features. Although these methods are reported to achieve certain success, they treat features from different views independently and unfortunately neglect the important view correlations."
1904.11228,data,205,,,"Another family of methods considers view correlation when performing feature selection. Representative works in       clude adaptive multi-view feature selection (AMFS) [Wang et al., 2016], multi-view feature selection (MVFS) [Tang et al., 2013] and adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013]. These methods ﬁrst construct multiple view-speciﬁc similarity structures1 and then perform the subsequent feature selection based on the collaborative (combined) similarity structure. These two processes are separate and independent. The collaborative similarity structure remains ﬁxed during feature selection. The latently involved data noises and outlying entries in the view-speciﬁc similarity structures will adversely reduce the reliability of the ultimate collaborative similarity structure for feature selection. Furthermore, conventional approaches generally employ knearest neighbors assignment to construct the view-speciﬁc similarity structures and the simple weighted combination for ultimate similarity structure generation. This strategy can hardly achieve the ideal state for clustering that the number of connected components in the ultimate similarity structure is equal to the number of clusters [Nie et al., 2014]. Thus, suboptimal performance may be caused under such circumstance."
1904.11228,data,206,,,"the important correlation of different feature views. Another kind of methods directly tackles the multi-view feature selection. They consider view correlations when performing feature selection. Adaptive multi-view feature selection (AMFS) [Wang et al., 2016] is an unsupervised feature selection approach which is developed for human motion retrieval. It describes the local geometric structure of data in each view with local descriptor and performs the feature selection in a general trace ratio optimization. In this method, the feature dimensions are determined with trace ratio criteria. Adaptive unsupervised multi-view feature selection (AUMFS) [Feng et al., 2013] addresses the feature selection problem for visual concept recognition. It employs l2,1 norm [Nie et al., 2010] based sparse regression model to automatically identify discriminative features. In AUMFS, data cluster structure, data similarity and the correlations of different views are considered for feature selection. Multi-view feature selection (MVFS) [Tang et al., 2013] investigates the feature selection for multi-view data in social media. A learning framework is devised to exploit the relations of views and help each view select relevant features."
1904.11228,data,38,,,"[Tang et al., 2013] Jiliang Tang, Xia Hu, Huiji Gao, and Huan Liu. Unsupervised feature selection for multi-view data in social media. In SDM, pages 270–278, 2013."
1904.11228,data,69,,,"[He et al., 2005] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS, pages 507–514, 2005. [Huang et al., 2015] Jin Huang, Feiping Nie, and Heng Huang. A new simplex sparse learning model to measure data similarity for clustering. In IJCAI, pages 3569–3575, 2015."
1904.11228,data,75,,,"As mentioned above, the data points can be directly partitioned into k clusters if the number of components in the similarity structure S is exactly equal to k. Theorem 1 indicates that this condition can be achieved if the rank of Laplacian matrix is equal to n − k. With the analysis, we add a reasonable rank constraint in Eq.(1) to achieve the condition. The optimization problem becomes"
1904.11228,data,76,,,"With the advent of big data, multi-view features with high dimensions are widely employed to represent the complex data in various research ﬁelds, such as multimedia computing, machine learning and data mining [Liu et al., 2016; Liu et al., 2017; Zhu et al., 2017b; Zhu et al., 2015; Cheng and Shen, 2016; Cheng et al., 2016]. On the one"
1904.11228,"data, dataset",295,,,"4 Experiments 4.1 Experimental Datasets 1) MSRC-v1 [Winn and Jojic, 2005]. The dataset contains 240 images in 8 class as a whole. Following the setting in [Grauman and Darrell, 2006], we select 7 classes composed of tree, building, airplane, cow, face, car, bicycle and each class has 30 images. We extract 5 visual features from each image: color moment with dimension 48, GIST with 512 dimension, SIFT with dimension 1230, CENTRIST feature with 210 dimension, and local binary pattern (LBP) with 256 dimension. 2) Handwritten Numeral [van Breukelen et al., 1998]. This dataset is comprised of 2,000 data points from 0 to 9 digit classes. 6 features are used to represent each digit. They are 76 dimensional Fourier coefﬁcients of the character shapes, 216 dimensional proﬁle correlations, 64 dimensional Karhunen-love coefﬁcients, 240 dimensional pixel averages in 2 × 3 windows, 47 dimensional Zernike moment and 6 dimensional morphological features. 3) Youtube [Liu et al., 2009]. This real-world dataset is collected from Youtube. It contains intended camera motion, variations of the object scale, viewpoint, illumination and cluttered background. The dataset is comprised of 1,596 video sequences in 11 actions. 4) Outdoor Scene [Monadjemi et al., 2002]. The outdoor scene dataset contains 2,688 color images that belong to 8 outdoor scene categories. 4 visual features are extracted from each image: color moment with dimension 432, GIST with dimension 512, HOG with dimension 256, and LBP with dimension 48."
1904.11228,dataset,1,,,Dataset
1905.12665,code,82,,,"This work was ﬁnanced in part by the S˜ao Paulo Research Foundation (FAPESP) under grants No. 2016/199476 and No. 2017/16597-7, the Brazilian National Council for Scientiﬁc and Technological Development (CNPq) under grant No. 307425/2017-7, and the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior—Brasil (CAPES)—Finance Code 001. We acknowledge the support of NVIDIA Corporation for the donation of a Titan X Pascal GPU used in this research."
1905.12665,"code, code available",1,,,Code
1905.12665,data,102,,,"Abstract Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classiﬁcation, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings."
1905.12665,data,22,,,Presented at the ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data Copyright 2019 by the author(s).
1905.12665,data,37,,,"Berg, R. v. d., Kipf, T. N., and Welling, M. Graph convolutional matrix completion. ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), 2018."
1905.12665,data,49,,,"Donnat, C., Zitnik, M., Hallac, D., and Leskovec, J. Learning structural node embeddings via diffusion wavelets. In ACM Conf. Knowl. Discov. Data Min. (ACM SIGKDD), pp. 1320–1329. ACM, 2018."
1905.12665,data,73,,,"Bai, Y., Ding, H., Bian, S., Chen, T., Sun, Y., and Wang, W. SimGNN: A neural network approach to fast graph similarity computation. In ACM Inter. Conf. Web Search Data Min. (WSDM), WSDM ’19, pp. 384–392, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-5940-5."
1905.12665,data,94,,,"Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M. Modeling relational data with graph convolutional networks. In Gangemi, A., Navigli, R., Vidal, M.-E., Hitzler, P., Troncy, R., Hollink, L., Tordai, A., and Alam, M. (eds.), Semantic Web Conf. (ESWC), pp. 593–607, Cham, 2018. Springer International Publishing."
1905.12665,dataset,140,,,"In this work, we evaluate our model as an edge classiﬁer, and simulate its performance as a graph generator by inputting noise as features and predicting on them. We perform experiments on three synthetic datasets that consist of images with Geometric Figures for segmentation, 3D surface function, and Community dataset (see Appendices A.1, A.2, and A.3, respectively). For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. Our evaluation metric is the Maximum Mean Discrepancy (MMD) measure (You et al., 2018), which measures the Wasserstein distance over three statistics of the graphs: degree (Deg), clustering coefﬁcients (Clus), and orbits (Orb)."
1905.12665,dataset,147,,,"For our experiments, we used 80% of the graphs in each dataset for training, and test on the rest. For both models, we use the following settings. Our activation functions, σl, are sigmoid for all layers, except for the Eq. 7 where σl is a hyperbolic tangent. We use L = 5 layers to extract the ﬁnal adjacency and embeddings. The feature dimension, dl, is 32 for all layers. The learning rate is set 10−5 for the Community dataset, and in the rest of datasets, the learning rate is set 5 × 10−6. Additionally, the number of epochs changes depending on the experiment. Thus in the experiments of Communities, Surfaces and Geometrical Figures we use 150, 200 and 150 times respectively and, the number"
1905.12665,dataset,2,,,A. Datasets
1905.12665,dataset,39,,,"Figure 2. Results of the dissimilarity (MMD) between the prediction and ground truth (smaller values are better) while varying the number of recurrent steps, on the 3D Surface dataset (Surf400)."
1905.12665,dataset,4,,,A.3. Community Dataset
1905.12665,dataset,40,,,"We made the Geometric Figures dataset for the task of image segmentation within a controlled environment. Segmentation is given by the connected components of the graph ground-truth. Here, we provide RGB images and their expected segmentations."
1905.12665,dataset,44,,,"Additionally, in Table 2, we present an ablation analysis of our model’s loss functions and regularization components on the Geometric Figures dataset. We emphasize a stable training and a fast convergence when we minimize both loss functions simultaneously."
1905.12665,dataset,5,,,A.1. Geometric Figures Dataset
1905.12665,dataset,5,,,A.2. 3D Surfaces Dataset
1905.12665,dataset,52,,,"The Geometric Figures dataset contains 3000 images of size n×n, that are generated procedurally.1 Each image contains circles, rectangles, and lines (dividing the image into two parts). We also add white noise to the color intensity of the images to perturb and mixed their regions."
1905.12665,dataset,56,,,"We generated 200 versions of each surface by randomly applying a set of transformations (from scaling, translation, rotation, reﬂection, or shearing) to the curve, moreover, two versions of the Surface dataset were created, Surf100 and Surf400 that use 100 and 400 vertices per surface, respectively."
1905.12665,dataset,68,,,"We perform experiments on a synthetic dataset (Community dataset) that comprises two sets with C = 2 and C = 4 communities with 40 and 80 vertices each, respectively, created with the caveman algorithm (Watts, 1999), where each community has 20 people. Besides, Community C = 4 and C = 2 have 500 and 300 samples respectively."
1905.12665,dataset,69,,,"Table C.1. Comparison of GLN, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric are accuracy (Acc), intersection-over-union (IoU), Recall (Rec), and Precision (Prec) shown row-wise per method, where larger numbers denote better performance."
1905.12665,dataset,73,,,"Finally, in Fig. F.1, we present an application, even fundamental, on segmentation where each of the connected components represents different objects. For this, we apply our GLN model on Geometric Image dataset, using size image of 20 × 20. Besides, the white edges represent correct predictions, and light blue dashed edges are false negatives (i.e., not predicted edges)."
1905.12665,dataset,78,,,"Figure E.1. Results on Community dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,78,,,"Knowing the depth of the recursive model (i.e., the number of iterations) is not a trivial task since we must ﬁnd a tradeoff between the efﬁciency and effectiveness of the model. In Fig. 2, we show the dissimilarity metrics (MMD) while varying the number of applications of our proposed block on the 3D Surface dataset. According to our experiment, using ﬁve recurrent steps provides the right trade-off."
1905.12665,dataset,79,,,"Figure D.1. Results on 3D Surface dataset predictions for the proposed methods, and the learned latent space, used for build adjacency matrix in the prediction. The blue edges represent false negatives (i.e., not predicted edges), red edges represent false positives (i.e., additional predicted edges), and black edges are correctly predicted ones. The graphs were normalized (w.r.t. scale and translation) for better visualization."
1905.12665,dataset,81,,,"To evaluate our method we needed a highly structured dataset with intricate relations and with easily understandable features. Hence, we convert parts of 3D surfaces into a mesh by sampling them. Each point in the mesh is translated into a node of the graph, with its position as a feature vector. We have a generator2 that creates different conﬁgurations for this dataset based on a number of nodes per surface, and transformation on it."
1905.12665,dataset,83,,,"In Fig. D.1, we show the qualitative result of GLN for the 3D Surface dataset. We show the prediction on the elliptic hyperboloid, elliptic paraboloid, torus, saddle, and ellipsoid, all using 100 nodes (Surf100). We normalized the graphs (w.r.t. scale and translation) for better visualization. Besides, the red edges represent false negatives (i.e., not predicted edges) and black edges are correctly predicted ones."
1905.12665,dataset,83,,,"In Fig. E.1, we predict the adjacency matrix the of Community dataset on two and four communities, C = 2 and C = 4 respectively (even rows). Note, our node embedding obtained after apply the λl function, shows a good grouping of individuals in the hyperspace (odd rows). Furthermore, the red edges represent false negatives (i.e., not predicted edges), and black edges are correctly predicted ones."
1905.12665,dataset,83,,,"structure given a set of points and their feature embeddings, respectively. (ii) A recurrent architecture that deﬁnes our iterative process and our prediction functions. (iii) An endto-end learning framework for predicting graphs’ structure given a family of graphs. (iv) Additionally, we introduce a synthetic dataset, i.e., 3D surface functions, that contains patterns that can be controlled and mapped into graphs to evaluate the robustness of existing methods."
1905.12665,dataset,84,04/21/22,0,"Table 1. Comparison of GLN against deep generative models, GraphRNN (G.RNN), Kronecker (Kron.), and MMSB, on the Community (C = 2 and C = 4), on all sequences of Surf100 and Surf400, and Geometric Figures datasets. The evaluation metric is MMD for degree (D), cluster (C), and orbits (O) shown row-wise per method, where smaller numbers denote better performance."
1905.12665,github,4,,,at https://gitlab.com/mipl/
1905.12665,supplementary data,10,,,Graph Learning Network: A Structure Learning Algorithm SUPPLEMENTARY MATERIAL
1906.08554,data,105,,,"The smart devices are increasing exponentially day by day in the whole world. They provide much more facility to the end users and also attach with their daily life. Smart devices can connect to the internet easily for sending and receiving data within the network. The smart devices are not just smart phones, it may be smart refrigerator, Smart home automation entry point, smart air conditioners, Smart hubs, Smart thermostat, Color changing smart LEDs, Smart Watches and smart Tablets etc. in internet of things framework they are connected to each other through internet."
1906.08554,data,119,,,"The objective of this research is to create the new reliable communication framework for the smart cities using the Tactile Internet a next revolution of internet of things. This research is based on low-latency, ultra-high availability and high-performance concepts of Tactile Internet. The framework provides QoS through reducing the latency (1ms in round trip) also the variety of the quantity of smart devices. In this research we consider idle state in order to makes our examination more efficient, at that point the general execution regarding the overall performance of the framework is evaluated. The framework will monitor and analyze the real-time data collected from network and then taking the action."
1906.08554,data,12,,,Table.2: Comparison of peak data rate and latency [3]
1906.08554,data,127,,,"The proposed research entitled “Tactile Internet based reliable communication framework for Smart cities in 5G” is a step forward in wireless networking and IoT where we propose new reliable framework based on Tactile Internet. The Wireless communication is the key of Internet of things and Tactile Internet. It is expected to exceed 50 billion connected devices by 2020 and most of these nodes cannot be connected by wireline. In order to enable critical applications such as smart factories or smart buildings, the networking protocols have to deal with the non-deterministic nature of wireless links. In the 5th generation communication system, the secure and reliable data packets will rely on the network with high availability and low latency."
1906.08554,data,169,,,"The proposed research plan builds research on extending the performance of communication in internet of things using tactile internet. The transfer data from one configuration to another using wireless networks starts from 1973 in the form of packets radio network. They were able to communicate with another same configuration devices. Recent work is continuing on a project called the Serval Project. It provides networks facility to android devices for communication in infrastructure less network. Whereas our research is concerned about the high-performance communication in internet of smart devices for smart cities. The main contribution of this research is the creation of the reliable communication framework and provide secure, reliable and fast communication using Tactile Internet among the internet of smart devices. The previous studies have been focused on the creation and optimization the framework for communication, but such research doesn’t perform the full framework for secure and reliable communication among internet of smart devices for smart cities."
1906.08554,data,270,,,"The main contribution of this research is designing a framework for ultra-reliable, low latency and high availability communication in Internet of smart devices for future smart cities using the Tactile Internet. The proposed framework is specifically appropriate for applications in which data is periodically transmitted in internet of smart devices environment. In these applications, on one hand, packets are being produced based on a certain periodic time pattern. On the other hand, service time is always a random variable with general distribution. Therefore, service time might temporarily exceed the period time which, as an inevitable consequence some packets might encounter a busy channel and be dropped. We solve this problem by proposing the new communication framework. We demonstrate that proposed reliable framework, not only increases the throughput, but also the direct connection between the generation (sensors) and communication packet systems are eliminated which make the system far more stable. Moreover, in order to enhance the proposed model, we have employed retransmission scheme, variable packet length, and saturated traffic condition. The solution of this research is summarized as follows. The implementation of proposed framework for communication among internet of smart devices in 5G will be programmed to execute on to the internet of things using Tactile Internet concepts. The idea will focus into three main concepts, these concepts are Reliability, Security and availability. The proposed study supports the wireless networking technology to establish a reliable framework among internet of devices for smart cities."
1906.08554,database,110,,,"[27]. Aljohani, Mohammed, and Tanweer Alam. ""An algorithm for accessing traffic database using wireless technologies."" In Computational Intelligence and Computing Research (ICCIC), 2015 IEEE International Conference on, pp. 1-4. IEEE, 2015. DOI: https://doi.org/10.1109/iccic.2015.7435818  [28]. Alam, Tanweer, and Mohammed Aljohani. ""Design a new middleware for communication in ad hoc network of android smart devices."" In Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies, p. 38. ACM, 2016. DOI: https://doi.org/10.1145/2905055.2905244"
1912.0313,data,125,,,"We observe that the ST-DIM based pre-trained models can easily be ﬁne-tuned only with small amount of downstream data. In this classiﬁcation task, our model can classify a randomly chosen time-series as a sample of VAR or SVAR. Note, with very few samples, models based on the pre-trained encoder (FPT and UFPT) outperform supervised models. However, as the number of samples grows, the accuracy achieved with or without pre-training levels out. We also notice that autoencoder based self-supervised pretraining does not assist in VAR vs. SVAR classiﬁcation. Consequently, we use only ST-DIM based pre-training for all the real data experiments. Refer to Figure 4 for the results of simulation experiments."
1912.0313,data,212,,,"Unsupervised pre-training is a well-known technique to get a head start for the deep neural network. It may be considered as a regularizer which compares to classical regularizers (i.e. L1/L2) may not vanish even with more data and could ﬁnd a robust local minima for better generalization [6]. Classical methods are Deep Beliefs Networks (DBMs) [7] and stacked denoising autoencoders (SDAE) [8]. Unsupervised pre-training has broad implications in ﬁelds such as computer vision [9], natural language processing (NLP) (GPT2 [10], BERT [11], Word2Vec [12]) and automatic speech recognition (ASR) (with SDAE [13], with DBN-HMMs [14]). However, this unsupervised pre-training is considered to be less popular in ﬁelds other than NLP [15]. Speciﬁcally, in computer vision, researchers usually use the model which is pre-trained in supervised fashion on Imagenet as a starting point for downstream tasks. Furthermore, given enough data and technical strategies, it is possible to achieve better results on COCO object detection without supervised pre-training on Imagenet [16]."
1912.0313,data,25,,,Simulations Real Data 16 Training Batch Size 200 Validation Batch Size 200 Test Batch Size Initial Learning Rate 3e-4 Learning Rate Scheduler None Max Epochs
1912.0313,data,30,,,Figure 8. ICA time courses are computed from the resting state fMRI data. Results contain statistically independent spatial maps (top) and their corresponding time courses.
1912.0313,data,36,,,"[19] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019."
1912.0313,data,38,,,"namics generalizes across different data distributions, as our model pre-trained on healthy adults shows improvements in children and elderly. The generality of the approach is also demonstrated in an application to the keyword detection problem."
1912.0313,data,42,,,"To simulate the data, we generate multiple 10-node graphs with 10 × 10 stable transition matrices. Using these we generate multivariate time series with autoregressive (VAR) and structural vector autoregressive (SVAR) models [44]."
1912.0313,data,42,,,"[2] Vince D Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adalı. The chronnectome: time-varying connectivity networks as the next frontier in fmri data discovery. Neuron, 84(2):262–274, 2014."
1912.0313,data,45,,,"[28] Vince D Calhoun, Tulay Adali, Godfrey D Pearlson, and JJ Pekar. A method for making group inferences from functional MRI data using independent component analysis. Human brain mapping, 14(3):140–151, 2001."
1912.0313,data,48,,,"[31] R Devon Hjelm, Eswar Damaraju, Kyunghyun Cho, Helmut Laufs, Sergey M Plis, and Vince D Calhoun. Spatio-temporal dynamics of intrinsic networks in functional magnetic imaging data using recurrent neural networks. Frontiers in neuroscience, 12:600, 2018."
1912.0313,data,51,,,"application [33–35] is considered as a way to enable learning from data and thus improve results in downstream classiﬁcation. To achieve improved performance, another idea is the data generating approach [36] which uses synthetic data generator for pre-training, relieving the scarcity of data."
1912.0313,data,61,,,"U. Mahmood1, M. M. Rahman1, A. Fedorov2 , Z. Fu1, V. D. Calhoun1, 2, 3, S. M. Plis1 Tri-institutional Center for Translational Research in Neuroimaging and Data Science: 1Georgia State University, 2Georgia Institute of Technology, 3Emory University Atlanta, GA, USA {umahmood1,mrahman21}@student.gsu.edu afedorov@gatech.edu"
1912.0313,data,70,,,"[50] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer, Mirella Dapretto, et al. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular psychiatry, 19(6):659, 2014."
1912.0313,data,96,,,"As we are more interested in subjects for classiﬁcation task, we feed each time series (ICA time courses) into the encoder in the form of a sequence of windows. The encoder encodes the windows of input data into latent representation. The latent representation of the entire time series is then concatenated and passed to a biLSTM with hidden dimension of size 200. The output of biLSTM is then used as input to a feed forward network of two linear layers with 200 and 2 units to perform binary classiﬁcation."
1912.0313,"data, data https",154,,,"We preprocessed the fMRI data using statistical parametric mapping (SPM12, http://www.ﬁl.ion.ucl.ac.uk/spm/) under MATLAB 2016 environment. A rigid body motion correction was performed using the toolbox in SPM to correct subject head motion, followed by the slice-timing correction to account for timing difference in slice acquisition. The fMRI data were subsequently warped into the standard Montreal Neurological Institute (MNI) space using an echo planar imaging (EPI) template and were slightly resampled to 3 × 3 × 3 mm3 isotropic voxels. The resampled fMRI images were ﬁnally smoothed using a Gaussian kernel with a full width at half maximum (FWHM) = 6 mm. After the preprocessing. We included subjects in the analysis if the subjects have head motion ≤ 3◦ and ≤ 3 mm, and with functional data providing near full brain successful normalization [52]."
1912.0313,"data, data repository",11,,,"2These data were downloaded from the Function BIRN Data Repository,"
1912.0313,"data, data repository",61,,,"[48] David B Keator, Theo GM van Erp, Jessica A Turner, Gary H Glover, Bryon A Mueller, Thomas T Liu, James T Voyvodic, Jerod Rasmussen, Vince D Calhoun, Hyo Jong Lee, et al. The function biomedical informatics research network data repository. Neuroimage, 124:1074–1079, 2016."
1912.0313,"data, data repository, data https",100,,,"Data for Schizophrenia classiﬁcation was used in this study were downloaded from the Function BIRN Data Repository (http://bdr.birncommunity.org:8080/BDR/), supported by grants to the Function BIRN (U24-RR021992) Testbed funded by the National Center for Research Resources at the National Institutes of Health, U.S.A. and from the COllaborative Informatics and Neuroimaging Suite Data Exchange tool (COINS; http://coins.trendscenter.org) and data collection was performed at the Mind Research Network, and funded by a Center of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 from the NIH to Dr.Vince Calhoun."
1912.0313,"data, dataset",106,,,"COBRE The dataset has total 157 subjects — a collection of 68 HC and 89 affected with SZ. Like FBIRN, each subject has 53 non-noise components in its ICA time courses with 140 time points. We use two hold-out sets of size 32 each respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Unlike FBIRN, it has been impossible to increase the number of subjects for downstream training due to insufﬁciency of data."
1912.0313,"data, dataset",111,,,"15255075100Number of Subjects Per Class0.30.40.50.60.70.80.91.0AUCSpeech CommandsFPTUFPTNPTFBIRNCOBREABIDEOASISHCPpre-trainingapplyingautismschizophreniaADdatasets contain labeled Schizophrenia (SZ) and Healthy Control (HC) subjects. FBIRN The dataset has total 311 subjects consisting of 150 HC and 161 affected with SZ. Each subject has 53 non-noise components with 140 time points. We use two hold-out sets with sizes 32 and 64 respectively for validation and test. The remaining data are used for supervised training. With 140 time points, we create 53 × 20-sized windows with 50% overlap along time dimension resulting in 13 windows for each subject. The details of the results are shown in Figure 9."
1912.0313,"data, dataset",111,,,"As we see, the AUC scores of all the three models for 15 subjects is ∼ 0.5, which can be treated as merely random guess. However, as the number of subjects increases, pretrained models gradually start performing better than NPT which, in fact, even with 120 subjects fails to learn from the data. We suspect that the reason why pre-trained models do not work for 15 subjects is that the data set is much different than HCP. The big age gap between subjects of HCP and OASIS is a major difference and 15 subjects are even not enough for pre-trained models."
1912.0313,"data, dataset",126,,,"Figure 7. Datasets used for pre-training and classiﬁcation tasks. Healthy controls from the HCP [5] are used for encoder pre-training guided by data dynamics alone1. The pre-trained encoder is then used in downstream classiﬁcation tasks of 3 different diseases, 4 independently collected datasets, many of which contain data from a number of sites, and consist of populations with signiﬁcant age difference. The age distributions in the datasets have the following means, medians and standard deviations: HCP: 29.31, 29.00, 3.667; ABIDE: 17.04, 15.40, 7.29; COBRE: 37.96, 37, 12.90; FBIRN: 37.87, 38, 11.25; OASIS: 67.67, 68, 8.92."
1912.0313,"data, dataset",131,,,"In this section we study the performance of our model on both, synthetic and real data. To compare and show the advantage of pre-training on large unrelated dataset we use three different kind of models — 1) FPT (Frozen Pre Encoder for simulation experiment consists of 4 1D convolutional layers with output features (32, 64, 128, 64), kernel sizes (4, 4, 3, 2) respectively, followed by ReLU [41] after each layer followed by a linear layer with 256 units. For real data experiments, we use 3 1D convolutional layers with output features (64, 128, 200), kernel sizes (4, 4, 3) re 3"
1912.0313,"data, dataset",136,,,"Recent advances in unsupervised learning using selfsupervised methods with mutual information objectives have reduced the gap between supervised and unsupervised learning on standard computer vision classiﬁcation datasets [17– 21] and scaled pre-training to very deep convolutional networks (e.g., 50-layer ResNet). Furthermore, it inﬂuences the neuroimaging ﬁeld for classiﬁcation of progression to Alzheimer’s disease from sMRI [22], learning useful representation of the states from the frames in Atari games [23] and also from the speech chunks for speaker identiﬁcation [24]. Speciﬁcally, authors [19] have shown that contrastive based self-supervised pre-training can outperform supervised methods by a large margin in case of small data (e.g., 13 images per class in ImageNet [25])."
1912.0313,"data, dataset",164,,,"Differentiating multivariate dynamic signals is a difﬁcult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m (cid:29) n problem. In this paper, we show that the source of the problem—signal dynamics—can be used to our advantage and noticeably improve classiﬁcation performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets."
1912.0313,"data, dataset",17,,,data experiments. Refer to Figure 7 for the details of the datasets for disease classiﬁcation.
1912.0313,"data, dataset",217,,,"subjects provides beneﬁts that transfer across datasets, collection sites, and multiple disease classiﬁcation with varying age gap. Learning dynamics of fMRI helps to improve classiﬁcation results for schizophrenia, autism, Alzheimer’s dieseases and speed up the convergence of the algorithm on small datasets, that otherwise do not provide reliable generalizations. Although the utility of these results is highly promising by itself, we conjecture that direct application to spatio-temporal data will warrant beneﬁts beyond improved classiﬁcation accuracy in the future work. Working with ICA components is a smaller and thus easier to handle space that exhibits all dynamics of the signal, in future we will move beyond ICA pre-processing and work with fMRI data directly. We expect model introspection to yield insight into the spatio-temporal biomarkers of schizophrenia. In future work, we will use the same analogously pre-trained encoder on datasets with various other mental disorders such as MCI and bipolar. We are optimistic about the outcome because the proposed pre-training is oblivious of the downstream use and is done in a manner quite different from the classiﬁer’s work. It may indeed be learning crucial information about dynamics that might contain important clues into the nature of mental disorders."
1912.0313,"data, dataset",221,,,"Mental disorders manifest in behavior that is driven by disruptions in brain dynamics [1, 2]. Functional MRI captures the nuances of spatio-temporal dynamics that could potentially provide clues to the causes of mental disorders and enable early diagnosis. However, the obtained data for a single subject is of high dimensionality m and to be useful for learning, and statistical analysis, one needs to collect datasets with a large number of subjects n. Yet, for any kind of a disorder, demographics or other types of conditions, a single study is rarely able to amass datasets large enough to go out of the m (cid:29) n mode. Traditionally this is approached by handcrafting features [3] of much smaller dimension, effectively reducing m via dimensionality reduction. Often, the dynamics of brain function in these representations vanishes into proxy features such as correlation matrices of functional network connectivity (FNC) [4]. Efforts that pull together data from various studies and increase n do exist, but it is difﬁcult to generalize to study of smaller and more speciﬁc disease populations that cannot be shared to become a part of these pools or are too different from the data in them."
1912.0313,"data, dataset",71,,,"points. We use two hold-out sets of size 100 each respectively for validation and test purpose. The remaining data are used for downstream training i.e., autism vs. HC classiﬁcation. Like COBRE dataset, with 140 time points, we create 53 × 20-sized windows with no overlapping resulting in 7 windows for each subject. Refer to Figure 11 for the details of the experimental results."
1912.0313,"data, dataset",93,,,"For experiments on the downstream tasks, a hold out is selected for testing and is never used through the training/validation phase. For each downstream task, the number of subjects used for supervised training is gradually increased within a range to observe the effectiveness of pretraining in downstream task with varied number of training subjects . For each experiment, 10 trials are performed to ensure random selection of training subjects and, in each case, the performance is evaluated on the hold out dataset (test data)."
1912.0313,"data, dataset",94,,,"For brain data, each of the models (FPT, UFPT, NPT) yields the best results based on its respective gain value of Xavier [43] initialization used for biLSTM. To ﬁnd the best gain value for each model, 20 values in the range 0 − 1 are tried with an increment of 0.05. For each value, 10 experiments are performed and best value is chosen based on the results on validation dataset. Refer to Table 1 for more parametric details of the models."
1912.0313,"data, dataset",98,,,"15306090120Number of Subjects Per Class0.30.40.50.60.70.80.9AUCSTDIM vs AutoencoderNPTUFPT_STDIMFPT_STDIMUFPT_AutoencoderFPT_AutoencoderLibriSpeech+Mel-SpectrogramCoffee ShopBackgroundSpeech Commands    (Cat)Coffee ShopBackground+Mel-SpectrogramPre-TrainingTraining00.511.52Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dB+0 dB1111TTTT/2051015Time0512102420484096HZMel-frequency spectrogram-80 dB-70 dB-60 dB-50 dB-40 dB-30 dB-20 dB-10 dBResearch Excellence) [49] project, from the release 1.0 of ABIDE (Autism Brain Imaging Data Exchange3) [50] and from release 3.0 of OASIS (Open Access Series of Imaging Studies4) [51]. Written informed consent was obtained from all participants of each dataset under protocols approved by the institutional review board (IRB)."
1912.0313,"data, dataset provided",53,,,"Data for Alzheimer’s was provided by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, UL1TR000448, R01AG043434, R01EB009352. AV-45 doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly."
1912.0313,"data, dataset provided",60,,,"Data for healthy subjects was provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University."
1912.0313,"data, dataset provided",82,,,"15255075100120Number of Subjects Per Class0.400.450.500.550.600.650.70AUCOASISFPTUFPTNPTAutism data was provided by ABIDE. We acknowledge primary support for the work by Adriana Di Martino provided by the (NIMH K23MH087770) and the Leon Levy Foundation and primary support for the work by Michael P. Milham and the INDI team was provided by gifts from Joseph P. Healy and the Stavros Niarchos Foundation to the Child Mind Institute, as well as by an NIMH award to MPM (NIMH R03MH096321)."
1912.0313,"data, dataset, publicly available, data available",158,,,"Our goal is to enable the direct study of systems dynamics in smaller datasets. In the case of brain data it, in turn, can enable an analysis of brain function. In this paper, we show how one can achieve signiﬁcant improvement in classiﬁcation directly from dynamical data on small datasets by taking advantage of publicly available large but unrelated datasets. We demonstrate that it is possible to train a model in a self-supervised manner on dynamics of healthy control subjects from the Human Connectome Project (HCP) [5] and apply the pre-trained encoder to a completely different data collected across multiple sites from healthy controls and patients. We show that pre-training on dynamics allows the encoder to generalize across a number of datasets and a wide range of disorders: schizophrenia, autism, and Alzheimer’s disease. Importantly, we show that learnt dy 1"
1912.0313,"data, dataset, used dataset",70,,,"Next, we apply the same unsupervised pre-training method to brain imagining data. For encoder pre-training, we use HCP [5] consortium dataset and apply the pre-trained encoder for further downstream tasks. We apply the same pre-trained encoder for three different types of diseases spanning four datasets to classify schizophrenia, autism and Alzheimer’s diseases. We use resting fMRI data for all brain"
1912.0313,database,11,,,"L. Fei-Fei. Image Database. In CVPR09, 2009."
1912.0313,dataset,10,,,"Learnt dynamics generalizes across tasks, datasets, and populations"
1912.0313,dataset,101,,,"The dataset OASIS [51] has total 372 subjects with equal number (186) of HC and AZ patients. We use two holdout sets each of size 64 respectively for validation and test purpose. The remaining are used for supervised training. Unlike other datasets described earlier, it has only 120 time points though the number of non-noise componets is same (53) as other datasets. With 120 time points, we use six 53 × 20-sized non-overlapping windows for each subject. Refer to Figure 12 for the details of the experiments."
1912.0313,dataset,104,,,"Figure 5. Left: For pre-training, we combine audio ﬁles from LibriSpeech dataset with background noise of coffee shop. T is the length of audio ﬁles which ranges from 1 to 20 seconds. Right: For training, we superimpose a T/2 length audio of word ""cat"" padded with T/2 zeros onto a background noise of coffee shop of length T (T = 2). For both pre-training and training, we calculate the mel-spectrogram of the combined audio ﬁles that results in a matrix of size components × time courses for each audio ﬁle."
1912.0313,dataset,111,,,"First, we generate 50 VAR times series with size 10 × 20000. Then we split our dataset to 50 × 10 × 14000 samples for training, 50 × 10 × 4000 —for validation and 50 × 10 × 2000 — for testing. Using these samples, We pre-train an encoder to learn consecutive windows (positive examples) from the same VAR time series. As mentioned in Section 3.1.2, we also use autoencoder for pre-training the same encoder and show the effectiveness of ST-DIM to learn time-series dynamics in self-supervised manner. After pretraining, we use our pre-trained encoder for complete-time series classiﬁcation."
1912.0313,dataset,141,,,"In most cases, due to practical reasons, researchers in brain imaging are constrained to work with small datasets. In addition, earlier work [26, 27] in brain imaging have been based on unsupervised methods to learn the dynamics and structure of the brain while supervised approaches are used to perform predictions at individual level. Such unsupervised methods include models as linear ICA [28], HMM framework [29]. Moreover, some other nonlinear approaches are also proposed to capture the dynamics. Examples include using Restricted Boltzman Machines (RBMs) [30], RNN modiﬁcation of ICA [31], and reconstructions by recurrent U-Net architecture [32]. In some cases, where dataset is very small, transfer learning as observed in some neuroimaging"
1912.0313,dataset,142,,,"Let D = {(ut, vs) : 1 ≤ t, s ≤ N, t (cid:54)= s} be a dataset of pairs of values at time point t and s sampled from sequence with length N . Then D+ = {(ut, vs) : 1 ≤ t ≤ N − 1, s = t + 1} is called a dataset of positive pairs and D− = {(ut, vs) : 1 ≤ t, s ≤ N, s (cid:54)= t + 1} — of negative pairs. The dataset D+ refers to a joint distribution and D− — a marginal distribution. Eventually, the lower bound with InfoNCE estimator [37] If (D+) is deﬁned as:"
1912.0313,dataset,19,,,[46] Pete Warden. Speech commands: A dataset for limited vocabulary speech recognition. 2018.
1912.0313,dataset,2,,,4.4.1 Datasets
1912.0313,dataset,23,,,"For schizophrenia classiﬁcation, we conduct experiments on two different datasets, FBIRN [48] and COBRE [49]. The"
1912.0313,dataset,30,,,"Four datasets used in this study are collected from the FBIRN (Function Biomedical Informatics Research Network2) [48] project, from the COBRE (Center of Biomedical"
1912.0313,dataset,32,,,"As we have demonstrated, self-supervised pre-training of a spatiotemporal encoder gives signiﬁcant improvement on the downstream tasks in both keyword detection and brain imaging datasets. Pre-training on fMRI of healthy"
1912.0313,dataset,34,,,"Figure 11. AUC scores for all the three models on ABIDE dataset. Like experiments on FBIRN and COBRE, it is evident that the pre-trained models consistently perform better than NPT."
1912.0313,dataset,34,,,"The dataset ABIDE has total 569 subjects, of which, 255 are HC and 314 are affected with autism. Like other datasets, each subject has 53 non-noise components with 140 time"
1912.0313,dataset,43,,,"Our method is two fold. We ﬁrst pre-train our encoder on large unrelated dataset to learn improved representation of the latent factors, and then use the pre-trained encoder for downstream task. We explain both steps in the following sections."
1912.0313,dataset,49,,,"Figure 10. AUC scores for all the three models on COBRE dataset. It is obvious that even with 15 subjects for training, FPT outperforms NPT noticeably, that is, the difference between two median AUC scores is remarkable ((cid:39) 0.15)."
1912.0313,dataset,51,,,"Figure 9. AUC scores for all the three models (Refer to Figure 3) on FBIRN dataset. It is noticeable that even with only 15 subjects for supervised training, the median AUC scores of FPT and NPT differ by a large margin (10%)."
1912.0313,dataset,53,,,"Figure 12. AUC scores for all the models on OASIS dataset. As we continue increasing the number of subjects, the pre-trained models start learning and thus improve their respective scores. However, notice that the NPT model even with 120 subjects didn’t signiﬁcantly improve its predictability."
1912.0313,dataset,56,,,"Figure 4. Area Under Curve (AUC) scores for VAR vs. SVAR time-series classiﬁcation using ST-DIM and autoencoder based pre-training methods. ST-DIM based pre-training greatly improves the performance of downstream task with small datasets. On the other side, autoencoder based pre-training fails to learn dynamics and thus exhibits poor performance."
1912.0313,dataset,59,,,"Out of all the available subjects, we select 416 which have large number of time points (N (cid:62) 20k). We use 300 subjects for training and 116 for validation. For pretraining, we use the algorithm as described in section 3.1.1 and achieve accuracy of ∼ 0.95 on the validation dataset."
1912.0313,dataset,61,,,15255075100Number of Subjects Per Class0.500.550.600.650.700.750.80AUCFBIRNFPTUFPTNPT152540Number of Subjects Per Class0.40.50.60.70.8AUCCOBREFPTUFPTNPT15255075100150Number of Subjects Per Class0.350.400.450.500.550.600.650.70AUCABIDEFPTUFPTNPT7.29 years. Refer to Figure 7 for the demographic information of all the datasets. The dissimilarity in the age range is supposed to cause signiﬁcant difference between these two datasets as the structure of brain and thought process of children is obviously different than adults.
1912.0313,dataset,69,,,"For each dataset, 100 ICA components as shown in Figure 8 are acquired using the same procedure described in [52]. However, only 53 non-noise components as determined per slice (time point) are used in pre-training of encoder and on downstream task. For experiments, including both pretraining and classiﬁcation the fMRI sequence is divided into windows of 20 time points."
1912.0313,dataset,89,,,"Trained): The pre-trained encoder is not further trained on the dataset of downstream task, 2) UFPT (Unfrozen PreTrained): The pre-trained encoder is further trained on the dataset of downstream task and 3) NPT (Not Pre-trained): The encoder is not pre-trained at all and only trained on the small dataset of downstream task. The models are shown in Figure 3. In each experiment, we compare all three models to demonstrate the effectiveness of unsupervised pre-training."
1912.0313,dataset,94,,,"To show the broad implications of unsupervised pretraining, we ﬁrst apply it to a simple problem of keyword detection in audio ﬁles. We choose this problem as it has many practical applications (e.g., virtual assistants in smart phones, robots). We use LibriSpeech ASR corpus [45] for pre-training and Speech Commands Dataset [46] for supervised training. The audio ﬁles of both datasets are combined with a background noise of coffee shop collected from [47] to make pre-training and classiﬁcation harder."
1912.0313,"dataset, used dataset",108,,,"As seen in the ﬁgure, same pre-trained encoder performs reasonably better than NPT for autism vs. HC classiﬁcation and thus reinforces our hypothesis that unsupervised pretraining learns signal dynamics useful for downstream tasks. Note the difference between age ranges of ABIDE and HCP datasets. The age range of ABIDE is much lower than that of HCP dataset used for pre-training. HCP dataset contains subjects of different ages with means 30.01 and 28.48, medians 30 and 28, and standard deviations 3.522 and 3.665 years respectively for female and male, whereas ABIDE dataset has overall mean 17.04, median 15.40 and standard deviation"
1912.09621,"code, code available",88,04/21/22,0,"Images from digital microscopy are captured under different illumination conditions. To aid our classification architecture, we preprocess the images using a color constancy technique [10, 11] to maintain the color contrast across all images. Later, we resize the images to match with the ResNet and GoogLeNet architectures. Figures 4 and 5 present the results obtained using the color constancy preprocessing for different images [10]. The code utilized for color constancy is available at [11]."
1912.09621,data,156,,,"Computer-aided detection has been a research area attracting great interest in the past decade. Machine learning algorithms have been utilized extensively for this application as they provide a valuable second opinion to the doctors. Despite several machine learning models being available for medical imaging applications, not many have been implemented in the real-world due to the uninterpretable nature of the decisions made by the network. In this paper, we investigate the results provided by deep neural networks for the detection of malaria, diabetic retinopathy, brain tumor, and tuberculosis in different imaging modalities. We visualize the class activation mappings for all the applications in order to enhance the understanding of these networks. This type of visualization, along with the corresponding network performance metrics, would aid the data science experts in better understanding of their models as well as assisting doctors in their decision-making process."
1912.09621,data,206,,,"This paper describes a study that supports the apprehension of the results predicted by deep neural networks applied towards medical imaging analysis. Several machine learning and deep learning architectures have been proposed in the literature for automated Computer-Aided Detection (CAD) tools for various applications [1-4]. In the past few years, deep learning networks have been used widely in medical imaging applications [1, 3]. Residual Networks (ResNet) [5] and GoogLeNet [6] are some of the most popular networks used in this field. The availability of a vast variety of networks raises the question of choosing the optimal network for a given disease/condition. In a data science perspective, optimal results could be measured in terms of overall accuracy, confusion matrix, precision, recall, Receiver Operating Characteristics (ROC) curve, or any other performance metric. However, these optimal results might not be satisfactory for the doctors if the results are not interpretable. Determining the Region of Interest (ROI) that contributed to the decision making of the network will enhance the understanding for both data science experts and clinicians."
1912.09621,"data available, dataset",3,,,5.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,,,2.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,,,3.1. Dataset
1912.09621,"data available, dataset, dataset provided",3,,,4.1. Dataset
1912.09621,"data, dataset",213,,,"In addition, we have presented a comprehensive study of these algorithms based on their CAM results. This type of CAD system would let the medical expert analyst choose the algorithm based on their discretion in terms of CAM results, overall accuracy, AUC, or any other performance metric along with computation time and memory consumption. CAM results could be utilized by data science experts to further optimize their respective models in terms of architecture and/or preprocessing techniques. This type of CAM study would also assist the researchers in understanding the discriminative regions determined by various architectures in different imaging modalities. For instance, if an architecture achieves a high CAD detection accuracy for a particular dataset but the discriminative region determined using CAM is present in an unrelated area, performance metrics might mislead both data science researchers and medical expert analysts. Hence, CAM visualization would provide the necessary documentation for the medical expert analysts to enhance the trust in CAD system. This research can be further extended by studying the ROI determined by CAM by changing different hyper-parameters and how they evolve throughout the epochs. Another way to extend this research is by fusing CAM regions determined using different architectures."
1912.09621,"data, dataset",59,,,"For this application, we perform a hold-out validation study. We split the dataset into groups of 80% and 20% for training and testing respectively. We utilize a subset of 10% from our training data for validation purpose in order to fine-tune our hyperparameters. Table 1 presents the distribution of the dataset."
1912.09621,"data, dataset, publicly available",98,,,"To maintain the homogeneity across all these applications, we solely study the performance of transfer learning approaches using GoogLeNet and ResNet. Figure 1 presents the top-level block diagram of the transfer learning methodology adopted in this study. We implement these techniques for publicly available datasets thereby setting a new benchmark for each application. Results presented for the publicly available datasets would grant the capability for researchers to replicate and enhance them further. This type of analysis would assist the doctors and data science experts in selecting the optimal model of their choice."
1912.09621,dataset,115,,,"For this application, due to the limited availability of images, we perform a 10-fold validation study. We believe 10-fold validation would give a better estimate of our performance. We train 10 different networks based on 9 folds and test on the remaining fold in each iteration. For each fold, we train and tune our hyper-parameters solely based on the images from training fold. We make sure to exclude the testing fold in any manner to conduct a rigorous study. Note that we utilize the same set of cases in each fold for the architectures implemented. Overall distribution of the dataset is presented in Table 5."
1912.09621,dataset,135,,,"Figures 21 and 22 present the CAM results obtained for two different cases from the brain tumor dataset. Figure 21 presents the results for the case marked as ‘positive brain tumor’ by the trained clinician and our algorithm accurately predicts the same. The discriminative region is near the tumor portion which would help the doctors pinpoint important features, spatially. Figure 22 presents the results for the case marked as ‘negative brain tumor’ and the visualization behind algorithm’s prediction. CAM visualization for 26 (one test fold) different cases using our approach is available at [17]. This type of automated CAD technology for brain tumor detection would assist the doctors in providing a valuable second opinion and enhancing their workflow."
1912.09621,dataset,147,,,"In this section, we study the CAM results obtained using GoogLeNet and ResNet. Figures 8 and 9 present the CAM results obtained for two different cases from the malaria dataset using our proposed approach. Figure 8 presents the results for the case marked as parasitized by the expert reader, and our algorithm not only accurately predicts the same but also presents the discriminative region that contributed the most to its decision. The ROI is around the red spot containing plasmodium. Figure 9 presents the results for the case marked as uninfected. CAM visualization for 100 different cases using our presented approach is available at [12]. Typically, ResNet CAM converges to a smaller ROI in comparison to GoogLeNet. This type of automated CAD technology for malaria detection would assist the microscopists and enhance their workflow."
1912.09621,dataset,155,,,"Figures 14 and 15 present the CAM results obtained for two different test cases from the APTOS dataset using our proposed approach. Figure 14 presents the results for a case marked as ‘positive DR’ by the expert clinicians. It is also interesting to note that CAM results presented by GoogLeNet and ResNet for DR detection have minimal intersection despite exhibiting similar performance, which could affect an expert clinician’s choice of network. Figure 15 presents the results for a case marked as ‘negative DR’ by an expert clinician and the discriminative region for this case is near the retinal portion. CAM visualization for 60 different cases using our presented approach is available at [15]. This type of automated CAD technology for DR detection could be implemented for immediate solutions and could be applied in places with scarcity of such expert clinicians."
1912.09621,dataset,3,,,Type of Dataset
1912.09621,dataset,49,,,"Similar to brain tumor detection, due to the limited availability of images, we perform a 10-fold validation study. Overall distribution of the dataset is presented in Table 7. There is no additional preprocessing except converting these images into the input size for the network."
1912.09621,dataset,6,,,Table 7: Tuberculosis dataset distribution
1912.09621,dataset,66,,,"Similar to malaria detection (Section 2), we perform a hold-out validation study. We split the dataset into groups of 72%, 8%, and 20% for training, validation, and testing respectively. Table 3 presents the distribution of each dataset. There is no preprocessing except converting these images into the input size of the network."
1912.09621,dataset,67,,,"The remainder of this paper is organized as follows. Sections 2-5 present the results obtained for CAD of malaria, DR, brain tumor, and tuberculosis respectively. In each of these sections, we describe the dataset along with the experimental results obtained in terms of both performance metrics and CAM results. Finally, discussions and conclusions are offered in Section 6."
1912.09621,dataset,7,,,Table 1: Malaria dataset distribution.
1912.09621,dataset,7,,,Table 3: DR dataset distribution.
1912.09621,dataset,8,,,Table 5: Brain Tumor dataset distribution.
1912.09621,dataset,92,,,"MRI scans contain text information for some cases, which are not essential for classification and might mislead our deep neural networks. Hence, we preprocess MRI scans by cropping ROI of brain and removing any additional text from the image using simple morphological operations. In addition, we perform histogram equalization to enhance and maintain the contrast across the dataset. Later, we resize the images to match with the input of ResNet and GoogLeNet architectures. Figure 18 presents the results obtained using these preprocessing techniques."
1912.09621,"dataset, data https",17,,,"[16] Brain Tumor Dataset, https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumordetection, Accessed December 7, 2019."
1912.09621,"dataset, data https",18,,,"[13] Kaggle Diabetic Retinopathy Dataset, https://www.kaggle.com/c/diabetic-retinopathydetection/overview, Accessed December 7, 2019."
1912.09621,"dataset, data https",21,,,"[8] Malaria Dataset, National Institutes of Health, https://ceb.nlm.nih.gov/repositories/malariadatasets/ , Accessed December 8, 2019."
1912.09621,"dataset, publicly available, dataset provided, used dataset",48,,,We make use of the publicly available Shenzhen dataset [19] provided for the classification of chest radiographs. This dataset contains a total of 662 images. Figures 23 and 24 present sample images marked as ‘Normal’ and ‘Tuberculosis’ by radiologists.
1912.09621,"dataset, publicly available, used dataset",136,,,"We make use of a publicly available dataset provided by the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 on Kaggle [14] to detect DR in retinal images. In this dataset, 3662 retinal images are graded by expert clinicians at Aravind Eye Hospital, India into 5 different categories: (i) Negative DR, (ii) Mild DR, (iii) Moderate DR, (iv) Proliferative DR, and (v) Severe DR. In this research, we solely focus on the detection of DR, hence, we merge all the mild, moderate, proliferative and severe cases into a single category ‘positive DR’. Figures 10 and 11 present sample images marked in different categories by expert clinicians."
1912.09621,"dataset, publicly available, used dataset",68,,,We make use of a publicly available dataset provided on Kaggle [16] for the classification of MRI scans. The dataset contains a total of 253 images classified into two different categories ‘positive’ and ‘negative’ brain tumor. Figures 16 and 17 present sample images marked as ‘positive brain tumor’ and ‘negative brain tumor’ by trained clinicians.
1912.09621,"dataset, publicly available, used dataset",84,,,"We make use of a publicly available dataset provided by the National Institutes of Health (NIH) for the classification of cell images [8, 9]. Any cell image that contains plasmodium is marked as ‘parasitized’ by expert analysts and ‘uninfected’ otherwise. The dataset contains a total of 27,558 images with equal distribution of parasitized and uninfected cells. Figures 2 and 3 present sample images marked as parasitized and uninfected by expert readers."
2002.00512,code,55,,,"[32] M. Torrent, F. Jollet, F. Bottin, G. Zérah;, and X. Gonze, Implementation of the projector augmentedwave method in the ABINIT code: Application to the study of iron under pressure, Computational Materials Science, 42 (2008), pp. 337 – 351."
2002.00512,code,94,,,"The numerical results using a Julia [1] homemade code are summarized in the following ﬁgures with Z = 3, R = 1 and L = 5. The atomic PAW function φk are the eigenfunctions of the hydrogenoid atom. For the pseudo atomic function (cid:101)φk, continuity of the function and of the ﬁrst four derivatives are enforced (i.e. d = 5). The lowest eigenvalue is computed using a conjugate-gradient algorithm stopped when the norm of the residual is less than 10−5."
2002.00512,data,43,,,"[17] F. Jollet, M. Torrent, and N. Holzwarth, Generation of Projector Augmented-Wave atomic data: A 71 element validated table in the XML format, Computer Physics Communications, 185 (2014), pp. 1246–1254."
2002.04279,code,15,,,"● Software that checks text-based documents, source code, or both (Chowdhury &"
2002.04279,data,126,,,"Since the analysis and interpretation of the data are quite sensitive, we approached this period with the utmost care. As suggested by Guba and Lincoln (1989), member check is an effective technique for establishing the trustworthiness criteria in qualitative studies. Therefore, having analyzed and reported the data, we sent a preprint of the results to the vendors. Team members closely evaluated the issues raised by the vendors. Not all of them were able to be addressed in this paper, but as many as possible were incorporated. Because of the rigorous efforts to establish the validity of the results and the reliability of the study in this process, this study was further delayed."
2002.04279,data,18,,,"Journal of Data Mining and Knowledge Discovery, 2(2), 50–53. doi: 10.11648/j.ajdmkd.20170202.12"
2002.04279,data,21,,,This research did not receive any external funding. HTW Berlin provided funding for openly publishing the data and materials.
2002.04279,data,36,,,"5. Because of European data privacy laws, for higher education institutions in the EU it must be certain that the companies are only using servers in the EU if they are storing material."
2002.04279,data,5,,,Availability of data and materials
2002.04279,data,64,,,"Sorokina, D., Gehrke, J., Warner, S., & Ginsparg, P. (2006, December). Plagiarism detection in arXiv. In J. Liu & B. W. Wah (Eds.), Proceedings of the Sixth International Conference on Data Mining (ICDM'06). Hong Kong. (pp. 1070–1075). doi: 10.1109/ICDM.2006.147"
2002.04279,"data, data available",125,,,"Our testing took place between Nov 2018 and May 2019. During this time, we tested both coverage and usability. An additional test of multi-source document took place between August and November 2019. Since the present research did not benefit from any funding, the researchers were expected to fulfill their institutional workloads during the research period. Considering the size of the project team from various countries, we could make significant progress only during semester breaks, which explains the length of the testing process. It should be noted that we tested what the systems offered at the time of data collection. We used features that were allowed by the access given to us by the vendors."
2002.04279,"data, publicly available, data available",14,,,Data and materials used in this project are publicly available from http://www.academicintegrity.eu/wp/wg-testing/
2002.04279,database,105,,,"As has been shown in other investigations (Weber-Wulff et al., 2013) translation plagiarism is very seldom picked up by software systems. The worst performance of the systems in this test was indeed the translation plagiarism, with one notable exception—Akademia. This system is the only one that performs semantic analysis and allows users to choose the translation language. Unfortunately, their database—with respect to the languages of our testing—is much smaller than the database of other systems. However, the performance drop between copypaste and translation plagiarism is much smaller for Akademia than for the other systems."
2002.04279,database,107,,,"PlagScan presents itself as a plagiarism checker. It is operated by the German company PlagScan GmbH and was launched in 2009. They state that they have more than 1,500 organizations as customers. Although they focus on higher education, high schools, and businesses, PlagScan is also available for single users. They search the internet using MS Bing, published academic articles, their so-called “Plagiarism Prevention Pool”, and optionally a customer’s own database. PlagScan offers multiple pricing plans for each type of customer, there are apparently also now options for a free trial."
2002.04279,database,108,,,"One aspect of Wikipedia sources that is not adequately addressed by the text-matching software systems is the proliferation of Wikipedia copies on the internet. As discussed in Weber-Wulff et al. (2013), this can lead to the appearance of many smallish text matches instead of one large one. In particular, this can happen if the copy of the ever-changing Wikipedia in the database of the software system is relatively old and the copies on the internet are from newer versions. A careless teacher may draw false conclusions if they focus only on the quantity of Wikipedia similarities in the report."
2002.04279,database,13,,,"2. Text-matching systems that maintain a database of potential sources, employ"
2002.04279,database,139,,,"The Croatian researchers Birkić, Celjak, Cundeković, and Rako (2016) tested four tools that are widely used in Europe and have the possibility to be used at the national and institutional level. They compared such criteria as the existence of an API (application programming interface) and the possibility to integrate it as a plug-in for learning management systems, database scope, size of the user community, and other criteria. The researchers tested the tools using two papers for each type of submission: journal articles, conference papers, master’s and doctoral theses, and student papers. However, they did not include different types of plagiarism and evaluated the checking process with a focus on quote recognition, tool limitations, and interface intuitiveness."
2002.04279,database,151,,,"Turnitin was founded in 1999 by four students and grew to be an internationally known company. In 2014, they acquired the Dutch system Ephorus and “joined forces” (Ephorus, 2015). In 2019 they themselves were taken over by a US investment company, Advance (Turnitin, 2019). With a focus on institutional users only, they are used by 15,000 institutions in 150 countries. Turnitin uses its own crawler to search the web including also an archive of all previously indexed web pages (Turnitin, n.d.). Turnitin further compares the texts against published academic articles, as well as their own database of all assignments which have ever been submitted to the system, and optionally institutional databases. They are also developing many additional software tools for educators to use in teaching and giving feedback."
2002.04279,database,156,,,"Luparenko (2014) tested 22 tools that were selected as popular ones based on an analysis of scientific literature and web sources. She considered many criteria related to functional specification (such as type, availability of free trial mode, need for mandatory registration at a website, number of users that have access to the program, database, acceptable file formats, etc.) and also checked the performance of the tools using one scientific paper in the Ukrainian language and another one in English. Moreover, the checking was done using three different methods: entering the text in the field of website, uploading a file, and submitting the URL of the article. She measured the checking time and evaluated the quality of the report provided by tools, as well as reported the percentage of unique text found in each of the articles."
2002.04279,database,166,,,"As for the general testing, the results are highly consistent with the Wikipedia results which contributes the validity of the single-source and multi-source testing. Again, in single-source documents, Urkund obtained the highest score, while PlagAware is the best performing system in multi-source documents. Dupli Checker, DPV and intihal.net obtained the least scores in both categories. Most of the systems demonstrated better performance for multi-source documents than for single-source ones. This is most probably explained by the chances the systems had for having access to a source. If one source was missing in the tool’s database, it had no chance to identify the text match. The use of multiple sources gave the tools multiple chances of identifying at least one of the sources. This points out quite clearly the issue of false negatives: even if a text-matching tool does not identify a source, the text can still be plagiarized."
2002.04279,database,249,,,"Viper presents itself as a plagiarism checker. It was founded in 2007. Viper focuses on all types of customers; the pricing is based on the pay-as-you-go principle. Currently, it is owned by All Answers Limited (2019), which according to the information at the website, gives an impression of an essay mill. It is interesting to see the progress in the way Viper uses the uploaded content on their “Terms and conditions” page. In 2016 the page stated ""[w]hen you scan a document, you agree that 9 months after completion of your scan, we will automatically upload your essay to our student essays database which will appear on one of our network of websites so that other students may use it to help them write their own essays"" (Viper, 2016). The time span was shortened to 3 months some time afterwards (Viper, 2019a). These paragraphs have been removed from the current version of the page (Viper, 2019b). On a different page, it is noted that ""[w]hen you scan your work for plagiarism using Viper Premium it will never be published on any of our study sites"" (Viper, 2019c). In e-mail communication, Viper claims that they are not using any essay without the author's explicit consent."
2002.04279,database,25,,,Innovation Centre Kosovo (2018). An Albanian Academic Database and a Qualitative AntiPlagiarism System created by Akademia. Retrieved from https://ickosovo.com/news/post/an-albanian-academic-database-and-a-qualitative-antiplagiarism-system-crea
2002.04279,database,90,,,"Source-based coverage testing was made using four types of sources; Wikipedia, openaccess papers, a student thesis and online articles. For many students, Wikipedia is the starting point for research (Howard & Davies, 2009), and thus can be regarded as one of the primary sources for plagiarists. Since a Wikipedia database is freely available, it is expected that Wikipedia texts should easily be identifiable. Testing the tools with Wikipedia texts demonstrates the fundamental ability to catch text matches."
2002.04279,database,92,,,"Copyscape declares itself to be a plagiarism checker. The primary aim is to provide a tool for owners of websites to check if their original content was not used by others. They also provide a service of regular checks and email alerts. Copyscape, which started in 2004 (Greenspan, 2019), is operated by a private company, Indigo Stream Technologies Ltd., which is apparently based in Gibraltar. It does not have its own database but uses Google services to crawl the web."
2002.04279,download,138,,,"The presentation and understandability of the results reported by the systems were evaluated in a second usability criteria group. Since the systems cannot determine plagiarism, the results must be examined by one or more persons in order to determine if plagiarism is present and a sanction warranted. It must be necessary to download the result reports and to be able to locate them again in the system. Some systems rename the documents, assigning internal numbering to them, which makes it extremely difficult to find the report again. Many systems have different formats for online and downloadable reports. It would be useful for the report review if the system kept the original formatting and page numbers of the document being analyzed in order to ease the load of evaluation."
2002.04279,download,88,,,"None of the systems was able to get the highest score in the usability group related to the test results. Two systems (PlagScan and Urkund) support almost all features, but six systems support half or fewer features. The most supported features are the possibility to download result reports and highlighting matched passages in the online report. Less supported features are a side-by-side demonstration of evidence in the downloaded report and in the online report, as well as keeping document formatting."
2002.04279,publicly available,11,,,4–5 pages from a publicly available source in the given language
2002.04279,publicly available,125,,,"Table 2 shows the aggregated results of the language comparisons based on the language sets. It can be seen that most of the systems performed better for English, Italian, Spanish, and German, whereas the results for Latvian, Slovak, Czech, and Turkish languages are poorer in general. The only system which found a Czech student thesis from 2010 which is publicly available from a university webpage, was StrikePlagiarism.com. The Slovak paper in an open-access journal was not found by any of the systems. Urkund was the only system that found an open-access book in Turkish. It is worth noting that a Turkish system, intihal.net, did not find this Turkish source."
2002.04279,publicly available,16,,,● 4–5 pages from any publicly available source in a given language with 1/3 copy &
2005.10539,data,109,,,"The second approach was intended to increase the coordination between each instrument, by combining time (i. e., horizontal) and harmony (i. e., vertically) information. This was achieved by changing the data extraction phase from obtaining separately each complete instrument part to obtain every instrument part at every beat. This allowed us to train a set of instruments at the same time from a concrete set of symphonies. This way, the generated parts present a considerable increment of coordination and it is easier to differentiate each musical phrase, as each instrument part respects or accompanies the others."
2005.10539,data,131,,,"The goal of this work is to generate music, based on Beethoven’s compositional model, obtaining the conductor’s score with all the orchestra instrument’s parts. Two approaches to the goal were established; ﬁrstly, the system was trained with each instrument individually, to generate all the different instrument’s parts, and then put them all together in a conductor’s score. Nevertheless training each part individually lead to a lack of coordination, so a new approach was addressed. This second approach consisted in training the system with information from different instruments at the same time, extracting the data vertically, to maintain the harmony (vertical) and time (horizontal) information."
2005.10539,data,135,,,"Introduction Romantic composer Ludwig van Beethoven wrote his Symphonies from 1799 to 1824, when he ﬁnished the No. 9 (Cooper 2000). Although there is no constancy of the existence of the 10th Symphony score, there exists some manuscripts found in Beethoven’s house after his death that are thought to be part of the upcoming Symphony. In 1988 Barry Cooper tried to ﬁnish it, building from 50 of those fragments the ﬁrst movement of the Symphony. Those manuscripts are kept in the museum dedicated to his life in his natal city, Bonn, although they can be seen online 1. The public manuscript is not easy to read and understand, so that existing data will not be used in this paper."
2005.10539,data,138,,,"Ludwig van Beethoven composed his symphonies between 1799 and 1825, when he was writing his Tenth symphony. As we dispose of a great amount of data belonging to his work, the purpose of this paper is to investigate the possibility of extracting patterns on his compositional model from symbolic data and generate what would have been his last symphony, the Tenth. A neural network model has been built based on the Long Short-Therm Memory (LSTM) neural networks. After training the model, the generated music has been analysed by comparing the input data with the results, and establishing differences between the generated outputs based on the training data used to obtain them. The structure of the outputs strongly depends on the symphonies used to train the network."
2005.10539,data,147,,,"Since the main goal of this paper is to obtain a score including every orchestra instrument’s part, the input ﬁles format were changed to symbolic data in mxl. This extension refers to a compressed music score, which Music21 easily processes. Mxl ﬁles are the compressed format of the so called MusicXML (Good 2001). In order to represent the output of the training, i.e. the weights of the different notes and durations, the model also returns a HDF5 ﬁle, i. e., Hierarchical Data Format version 5, commonly used to store big quantities of data. After the prediction process, given the obtained weights, Music21 allows us to generate the ﬁnal output in MIDI or MusicXML, formats accepted by Musescore (so the score can be visualised and played)."
2005.10539,data,153,,,"Our ﬁnal network is composed 3 different types of layers. The most relevant ones are the LSTM layers, which take the sequences and return new ones. Then, the Dropout layers prevent overﬁtting, ignoring randomly selected neurons during the training, setting those inputs to 0. The Dense (Density) layer serves as a full connection mechanism. This layer is the last one, so the system returns the same number of outputs as the different numbers of tuples (note name, note duration) the input data had. Finally, the activation function used for every layer is set, and it determines how each node’s output is represented. In this case, the softmax function (i. e., linear activation) is used, allowing the output to be interpreted as a probability between 0 and 1."
2005.10539,data,160,,,"Technical background Deep learning: LSTM Networks Included in the ﬁeld of Machine Learning, Deep Learning involves the use of artiﬁcial neural networks (Gulli and Pal 2017). There exists several types of neural networks, such as Deep Neural, Deep Brief and Recurrent Neural Networks (RNN). In this paper we work with the last ones, since we need to process sequential data, assuming that each event depends on previous ones. The most accurate RNN variant is the LSTM. As proved with Figure 1, we need the memory that this type of networks own. On it we ﬁnd the sequence F - F - F, a predictor without memory would return another F, although by learning from the notes before, it can extract that after three equal notes, it is probable that the upcoming note is two tones below the last one."
2005.10539,data,247,,,"Proposed in 1997, LSTM neural networks can learn longterm dependencies, improving the cells or neurons in the RNN graph. They have the ability to connect previous knowledge to a present task. Each cell has memory, and it decides to store or forget a data based on a given priority (i. e., represented as weights), assigned by the algorithm after the learning process. Figure 3 shows a LSTM cell or neuron. The top line represents the ﬂow of the cell state, which can be altered up to three times. The ﬁrst layer, sigmoid (σ), takes information from the previous state and determines if it is useful or not, returning a value between 0 and 1. As it is shown with the vertical arrow, it directly affects to the ﬂow of the cell state. The second layer is composed of the combination of the sigmoid (σ) and tanh functions, which chooses the data to be updated from the previous state, and creates a vector of candidate values to be added to the current cell state. The ﬁnal sigmoid (σ) layer determines the output, by deciding which parts of the state are more relevant. Those will be combined with a tanh function, converting the current state into values between 1 and -1 (Gulli and Pal 2017)."
2005.10539,data,328,,,"References [Biles 1994] Biles, J. 1994. Genjam: A genetic algorithm In Proc. of the International for generating jazz solos. Computer Music Conference, 131–137. Aarhus, Denmark: ICMC. [Cohen 1995] Cohen, H. 1995. The further exploits of aaron, painter. Stanford Humanities Review 4(2):141–158. [Colton 2012] Colton, S. 2012. The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04."
2005.10539,data,344,,,"The painting fool: Stories from building an automated painter. In Computers and creativity. Berlin, Heidelberg, Germany: Springer. 3–38. [Cooper 2000] Cooper, B. 2000. Beethoven. New York, NY, USA: Oxford University Press. [Cope and Mayer 1996] Cope, D., and Mayer, M. J. 1996. Experiments in musical intelligence, volume 12. Madison, WI, USA: AR editions Madison. [Cuthbert and Ariza 2010] Cuthbert, M. S., and Ariza, C. 2010. music21: A toolkit for computer-aided musicology In Proc. of International Sociand symbolic music data. ety for Music Information Retrieval Conference, 637–642. Utrecht, The Netherlands: ISMIR. [Ebcioglu 1990] Ebcioglu, K. 1990. An expert system for harmonizing chorales in the style of j.s. bach. The Journal of Logic Programming 8(1):145–185. [Gardner 2016] Gardner, L. 2016. Beyond the fence review computer-created show is sweetly bland. The Guardian. [Gervs and et al. 2005] Gervs, P., and et al., B. D.-A. 2005. Story plot generation based on cbr. Knowledge-Based Systems 18(4):235–242. [Good 2001] Good, M. 2001. Musicxml: An internetfriendly format for sheet music. In Proc. of the XML conference, 03–04. [Graves, Mohamed, and Hinton 2013] Graves, A.; Mohamed, A.; and Hinton, G. E. 2013. Speech recognition the with deep recurrent neural networks. International conference on acoustics, speech and signal processing, 6645–6649. Vancouver, Canada: IEEE. [Gulli and Pal 2017] Gulli, A., and Pal, S. 2017. Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow. Birmingham, UK: Packt Publishing Ltd."
2005.10539,data,347,,,"2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor? AI Magazine 30(3):71–71. [Schwanauer and Levitt 1993] Schwanauer, S., and Levitt, D. 1993. Musact: A connectionist model of musical harmony. In Machine Models of Music. Cambridge, MS, USA: The MIT Press. 497–510. [Tao Li 2011] Tao Li, Mitsunori Ogihara, G. T. 2011. Music Data Mining. Boca Ratn, FL, USA: CRC Press. [Thiemel 2001] Thiemel, M. 2001. Dynamics. New Grove online."
2005.10539,data,35,,,"Once the model is built and the input and output data are ready, it gets trained, generating an hdf5 ﬁle containing the weights (i. e., input notes’ priorities)."
2005.10539,data,35,,,"Results The system output differs from the information given to the training, although once with the same trained data, the system predicts the same score, which denotes a lack of variability."
2005.10539,data,350,,,"[Haynes and Cooke 2001] Haynes, B., and Cooke, P. 2001. Pitch. New Grove online. [Hiley 2001] Hiley, D. 2001. Clef (i). New Grove online. [Hiller and Isaacson 1958] Hiller, Jr., L. A., and Isaacson, L. M. 1958. Musical composition with a high-speed digital computer. J. Audio Eng. Soc 6(3):154–160. [Liang and Gotham 2017] Liang, F. T., and Gotham, M. e. a. 2017. Automatic stylistic composition of bach chorales with deep lstm. In Proc. of International Society for Music Information Retrieval Conference, 449–456. Suzhou, China: ISMIR. [Montfort, Baudoin, and et al. 2012] Montfort, N.; Baudoin, P.; and et al. 2012. 10 PRINT CHR (205.5+ RND (1));: GOTO 10. Cambridge, MA, USA: MIT Press. [Nayebi and Vitelli 2015] Nayebi, A., and Vitelli, M. 2015. Gruv: algorithmic music generation using recurrent neural networks. In Course CS224D: Deep Learning for Natural Language Processing. [New Groove Music Online 2001a] New Groove Music Online. 2001a. Key signature. [New Groove Music Online 2001b] New Groove Music Online. 2001b. Note (i). [Pachet 2003] Pachet, F. 2003. The continuator: Musical interaction with style. Journal of New Music Research 32(3):333–341. [Quintana and et al. 2013] Quintana, C. S., and et al., F. M. A. 2013. Melomics: A case-study of ai in spain. AI Magazine 34(3):99–103. [Rastall 2001] Rastall, R. 2001. Time signature. New Grove online. [Ritchie 2009] Ritchie, G. 2009. Can computers create humor?"
2005.10539,data,43,,,"Training Finally, we can generate the training data (i. e., sequence input and output). By establishing a certain sequence length, the output for each input sequence will be the ﬁrst note that comes after that sequence."
2005.10539,data,76,,,"For example, setting a sequence length equal to two, the ﬁrst stage of the system’s work ﬂow (i. e., data extraction) for the Figure 5 input would be the shown in Table 1. It is important to take into account that in case of establishing a big sequence length, the model may generalise, while setting a small sequence length may lead to an overlearning problem."
2005.10539,data,84,,,"Data representation Several ways of representing the Beethoven Symphonies’ scores have been studied for this paper. Firstly, we used MIDI ﬁles as an input for our system, as it is a data ﬁle which contains information about the sounds: what note is played, when, and how long or loud. Figure 4 shows the problem we boarded with the MIDI input. Music21 was unable to differentiate between the different string instrument’s MIDI channels."
2005.10539,"data, dataset",137,,,"This paper is structured as follows. Previous work on using Artiﬁcial Intelligence in computational creativity and concretely in music generation is exposed in the State of the art section. After that, musical deﬁnitions needed to understand requirements, limitations and characteristics found in the results are introduced. Later, the work developed for this paper is explained in detail, presenting the Deep Learning technique used, the needed toolkits, and how the data was represented. Then, the Music generation subsection is divided in: dataset creation, training, and prediction. The results section is focused on explaining the reason why the system returns a certain output when trained with a speciﬁc set of symphonies. The conclusions and future work are described in the last sections."
2005.10539,"data, dataset, python",314,,,"Dataset creation As previously mentioned, the Beethoven symphonies have been converted to an mxl ﬁle, which constitutes the dataset or corpus that we have used to obtain the desired results. Also, the instrument or instruments with which the system works has to be established, so the Python module music21 can divide the mxl score into all the present instruments, and take only the desired parts. This way, in the ﬁrst approach, where the goal is to obtain each instrument’s part individually, the note names and durations are stored in an independent ﬁle, being the different tuples of note names and durations the training data. Nevertheless, as the second approach trains the model with a set of chosen instruments at the same time, we need to store, besides the note name and duration, the offset (i. e., time data relating to the moment in which the note is being played regarding the score) and the name of the instrument that plays it. The offset information will be used to sort the data. After making sure that the events are sorted in a time-line as they are in the original score, it can be removed from the dataset, in order to reduce the data dimensionality and to avoid an overlearning problem in the model. This way, the training data will be composed of the different tuples of note names, note durations and the instrument’s name playing it (introduced in the second approach). At this point, a dictionary to encode each data tuple as a number is created, so the neural network can work with it. This dictionary will be also used for the decoding phase, after the prediction."
2005.10539,"data, python, open-source",129,,,"This project has been developed in Python. Data extracting and processing from the scores has been performed using the python’s library Music21 (Cuthbert and Ariza 2010), which allows parsing and generating scores in different formats. Furthermore, every musical action and representation that we needed to perform, was made possible using that library. For the Deep Learning engine we have used Keras 3 (Gulli and Pal 2017). Finally, in order to manage the score formats, Musescore 4 (i. e., open source program available for every platform) brought us the possibility to import and export the symphonies, so we could see the score and listen to it at the same time."
2005.10539,dataset,114,,,"Prediction For this task, the network input is generated again, as in the previous process (see Table 1). Since it needs to work over the same model, it is created again, with the same parameters, but now, instead of training the model, it loads the generated weights from the previous process (i. e., the hdf5 ﬁle). It is important at this point that the network input shapes and the loaded weights have the same dimensions. Once the model is ready, the encoding dictionary built during the dataset creation phase is inverted, for decoding the prediction results."
2005.10539,python,123,,,"In case of the input, reshaping into a 3 dimension matrix is needed so it is compatible with the LSTM layers, using Python’s numpy module. The ﬁrst dimension or shape of the network is the number of unique different sequences (i. e., sequence in in Table 1) obtained in the last step, the second one is the previously established sequence length and ﬁnally the last dimension is forced to be 1, so it has just one input information per sequence length. After that, the software normalises the input into sequential values, from 0 to 1. In case of the output, it is converted into a categorical model."
2010.02554,"code, github",45,,,The dependencies of parameters in our Pytorch implementation (https://github.com/pmorenoz/RecyclableGP) are clearly shown and evident from the code structure oriented to objects. It is also amenable for the introduction of new covariance functions and more structured variational approximations if needed.
2010.02554,"code, publicly available, github, code available",11,,,1The code is publicly available in the repository: github.com/pmorenoz/RecyclableGP/.
2010.02554,data,109,,,"Stationarity and expressiveness. We assume that the non-linear function f is stationary across subsets of data. If this assumption is relaxed, some form of adaptation or forgetting should be included to match the local GPs. Other types of models can be considered for the ensemble, as for instance, with several latent functions (Lázaro-Gredilla and Titsias, 2011) or sparse multi-output GPs (Álvarez and Lawrence, 2011). The model also accepts GPs with increased expressiveness. For example, to get multi-modal likelihoods, we can use mixture of GP experts (Rasmussen and Ghahramani, 2002)."
2010.02554,data,146,,,"The data D is assumed to be partitioned into an arbitrary number of K subsets that we aim to observe and process independently, that is, {D1, D2, . . . , DK}. There is not any restriction on the amount of subsets or learning nodes. The subsets {Dk}K k=1 do not need to have the same size, and we only restrict them to be Nk<N . However, since we treat with a huge number of observations, we still consider that Nk for all k ∈ {1, 2, . . . , K} is sufﬁciently large for not accepting exact GP inference due to temporal and computational demand. Notice that k is an index while k(·, ·) refers to the kernel."
2010.02554,data,151,,,"where, once again, φ∗ = {µ∗, S∗} are the global variational parameters that we aim to learn. One important detail of the sum of expectations in (16) is that it works as an average contrastive indicator that measures how well the global q(u∗) is being ﬁtted to the local experts qk(uk). Without the need of revisiting any distributed subset of data samples, the GP predictive qC(uk) is playing a different role in contrast with the usual one. Typically, we assume the approximate posterior ﬁxed and ﬁtted, and we evaluate its performance on some test data points. In this case, it goes in the opposite way, the approximate variational distribution is unﬁxed, and it is instead evaluated over each k-th local subset of inducing-inputs Zk."
2010.02554,data,161,,,"To obtain multiple independent approximations to the posterior distribution p(f |D) of the GP function, we introduce K variational distributions qk(f ), one per distributed partition Dk. In particular, each variational distribution factorises as qk(f ) = p(f(cid:54)=uk |uk)qk(uk), with qk(uk) = N (uk|µk, Sk) and p(f(cid:54)=uk |uk) being the standard conditional GP prior distribution given the hyperparameters ψk of each k-th kernel. To ﬁt the local variational distributions qk(uk), we build lower bounds Lk on the marginal log-likelihood (ELBO) of every data partition Dk. Then, we use optimisation methods, typically gradient-based, to maximise the K objective functions Lk, one per distributed task, separately. Each local ELBO is obtained as follows"
2010.02554,data,173,,,"Heterogeneous single-output GP. Extensions to GPs with heterogeneous likelihoods, that is, a mix of continuous and discrete variables yi, have been proposed for multi-output GPs (Moreno-Muñoz et al., 2018). However, there are no restrictions in our single-output model to accept different likelihoods p(yi|f (xi)) per data point {xi, yi}. An inconvenience of the bound in (1), is that, each i-th expectation term could be imbalanced with respect to the others. For example, if mixing Bernoulli and Gaussian variables, binary outputs could contribute more to the objective function than the rest, due to the dimensionality. To overcome this issue, we ﬁt a local GP model to each heterogeneous variable. We join all models together using the ensemble bound in (6) to propagate the uncertainty in a principled way. Although, data-types need to be known beforehand, perhaps as additional labels."
2010.02554,data,20,,,DATA ST. (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55)
2010.02554,data,214,,,"4.3 Heterogeneous tasks. We analysed how ensembles of recyclable GPs can be used if one of the local tasks is regression and the other a GP classiﬁer. (viii) London household data: We have two subsets of input-output variables: the binary contract of houses (leasehold vs. freehold) and the price per latitude-longitude coordinate in the London area. Three quadrants (Q) of the city {Q2, Q3, Q4} are trained with a GP classiﬁer and Q1 as regression. To clarify, Q1 is the right-upper corner given the central axes. Our purpose is to combine the local latent uk, learned with the binary data on {Q2, Q3, Q4} and the uk learned on Q1 via regression. Then, we search the global f to be predict with a Bernoulli likelihood in Q1. The ensemble shows a test NLPD of 7.94 ± 0.01 in classiﬁcation while the recyclable task predicts with an NLPD of 8.00 ± 0.01 in the Q1. We asses that the heterogeneous GP prediction is better in Q1 than the local GP classiﬁer. The mean GP of regression is passed through the sigmoid function to show the multimodality."
2010.02554,data,22,,,where yt and ft are the true output target and function values. Nt is the number of test data points.
2010.02554,data,27,,,"J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Uncertainty in Artiﬁcial Intelligence (UAI), pages"
2010.02554,data,290,,,"A priori, the ensemble GP bound is agnostic with respect to the likelihood model. There is a general derivation in Matthews et al. (2016) of how stochastic processes and their integral operators are affected by projection functions, that is, different linking mappings of the function f (·) to the parameters θ. In such cases, the local lower bounds Lk in (1) might include expectation terms that are intractable. Since we build the framework to accept any possible data-type, we propose to solve the integrals via Gaussian-Hermite quadratures as in Hensman et al. (2015); Saul et al. (2016) and if this is not possible, an alternative would be to apply Monte-Carlo methods. Computational cost and connections. The computational cost of the local models is O(NkM 2 k ), while the global GP reduces to O(((cid:80) k Mk)M 2) and O(M 2) in training and prediction, respectively. The methods in Table 1 typically need O((cid:80) k ) for global prediction. A last theoretical aspect is the link between the global bound in (6) and the underlying idea in Tresp (2000); Deisenroth and Ng (2015). Distributed GP models are based on the application of CI to factorise the likelihood terms of subsets. To approximate the posterior predictive, they combine local estimates, divided by the GP prior. It is analogous to (6), but in the logarithmic plane and the variational inference setup."
2010.02554,data,3,,,DATA SIZE →
2010.02554,data,339,,,"All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−5, ηL = 10−7, ηψ = 10−8, ηZ = 10−7} and we used M = 25 inducing-inputs."
2010.02554,data,34,,,"In our experiments with toy data, we used two versions of the same sinusoidal function, one of them with an incremental bias. The true expressions of f (·) are"
2010.02554,data,343,,,"viii) London household data: Based on the large scale experiments in Hensman et al. (2013), we obtained the register of properties sold in the Greater London county during the 2017 year (https://www.gov.uk/government/collections/ price-paid-data). All addresses of household registers were translated to latitude-longitude coordinates, that we used as the input data points. In our experiment, we selected two heterogeneous registers, one real-valued and the other binary. The real-valued output targets correspond to the log-price of the properties included in the registers. Moreover, the binary values make reference to the type of contract, yi = 1 if it was a leasehold and yi = 0 if freehold. Interestingly, we appreciated that both tasks share modes accross the input region, as they are correlated. That is, if there is more presence of some type of contract, it makes sense that the price increases or decreases accordingly. Therefore, after dividing the area of London in the four quadrants {Q1, Q2, Q3, Q4} shown in the last Figure of the manuscript, we trained the Q1 exclusively with the regression data. Our assumption is that there exists an underlying function f that is linked differently to the parameters depending if the problem is regression or classiﬁcation. With this in mind, we trained the ensemble bound on the entire area of the city with two local GPs, one coming from regression in Q1 and the other from classiﬁcation in {Q2, Q3, Q4}. To check if the error results showed an improvement in prediction, we compared the posterior GP prediction of the ensemble GP on Q1 with the local GP that did not observe any data on Q1. Results showed us, that even after not observing any binary data in Q1, the global GP performed better that the local GP with no regression information."
2010.02554,data,349,,,"In terms of distributed inference for scaling up computation, that is, the delivery of calculus operations across parallel nodes but not data or independent models, we are similar to Gal et al. (2014). Their approach can be understood as a speciﬁc case of our framework. Alternatively, if we look to the property of having nodes that contain usable GP models (Table 1), we are similar to Deisenroth and Ng (2015); Cao and Fleet (2014) and Tresp (2000), with the difference that we introduce variational approximation methods for non-Gaussian likelihoods. An important detail is that the idea of exploiting properties of full stochastic processes (Matthews et al., 2016) for substituting likelihood terms in a general bound has been previously considered in Bui et al. (2017) and Moreno-Muñoz et al. (2019). Whilst the work of Bui et al. (2017) ends in the derivation of expectation-propagation (EP) methods for streaming inference in GPs, the introduction of the reparameterisation of Gal et al. (2014) makes our inference and performance different from Moreno-Muñoz et al. (2019). There is also the inference framework of Bui et al. (2018) for both federated and continual learning, but focused on EP and the Bayesian approach of Nguyen et al. (2018). A short analysis of its application to GPs is included for continual learning settings but far from the large-scale scope of our paper. Moreover, the spirit of using inducing-points as pseudo-approximations of local subsets of data is shared with Bui and Turner (2014), that comments its potential application to distributed setups. More oriented to dynamical modular models, we ﬁnd the work by Velychko et al. (2018), whose factorisation across tasks is similar to Ng and Deisenroth (2014) but oriented to state-space models."
2010.02554,data,45,,,"We highlight several use cases for the proposed framework. The idea of recycling GP models opens the door to multiple extensions, with particular attention to the local-global modelling of heterogeneous data problems and the adaptation of model complexity in a data-driven manner."
2010.02554,data,48,,,"Then, if we have (10), which is the ﬁrst version of our ensemble lower bound LE , we can use the augmented likelihood term p(y|f∞) to introduce the local approximations to f instead of revisiting the data. This is,"
2010.02554,data,52,,,"We introduced a novel framework for building global approximations from already ﬁtted GP models. Our main contribution is the construction of ensemble bounds that accept parameters from regression, classiﬁcation and heterogeneous GPs with different complexity without revisiting any data. We analysed its performance on synthetic and real data with"
2010.02554,data,67,,,"Model recycling and use cases. The ability of recycling GPs in future global tasks have a signiﬁcant impact in behavioral applications, where ﬁtted private-owned models in smartphones can be shared for global predictions rather than data. Its application to medicine is also of high interest. If one has personalized GPs for patients, epidemiologic surveys can be built without centralising private data."
2010.02554,data,70,,,"In this paper, we investigate a general framework for recycling distributed variational sparse approximations to GPs, illustrated in Figure 1. Based on the properties of the Kullback-Leibler divergence between stochastic processes (Matthews et al., 2016) and Bayesian inference, our method ensembles an arbitrary amount of variational GP models with different complexity, likelihood and location of pseudo-inputs, without revisiting any data."
2010.02554,data,8,,,Appendix C. Combined Ensemble Bounds with Unseen Data
2010.02554,data,85,,,"A common theme in the previous approaches is the idea of model memorising and recycling, i.e. using the already ﬁtted parameters in another problem or joining it with others for an additional global task without revisiting any data. If we look to the functional view of this idea, uncertainty is still much harder to be repurposed than parameters. This is the point where Gaussian process (GP) models (Rasmussen and Williams, 2006) play their role."
2010.02554,data,89,,,"Data-driven complexity and recyclable ensembles. One of the main advantages of the recyclable GP framework is that it allows data-driven updates of the complexity. That is, if an ensemble ends in a new variational GP model, it also can be recycled. Hence, the number of global inducing-variables M can be iteratively increased conditioned to the amount of samples considered. A similar idea was already commented as an application of the sparse order-selection theorems by Burt et al. (2019)."
2010.02554,data,9,,,"Data Privacy and Conﬁdentiality at NeurIPS, 2019."
2010.02554,data,9,,,Figure 2: Recyclable GPs with synthetic data.
2010.02554,"data, code, data available",178,,,"In this section, we evaluate the performance of our framework for multiple recyclable GP models and data access settings. To illustrate its usability, we present results in three different learning scenarios: i) regression, ii) classiﬁcation and iii) heterogeneous data. All experiments are numbered from one to nine in roman characters. Performance metrics are given in terms of the negative log-predictive density (NLPD), root mean square error (RMSE) and mean-absolute error (MAE). We provide Pytorch code that allows to easily learn the GP ensembles.1 It also includes the baseline methods. The syntax follows the spirit of providing a list of recyclable_models = [GP1, GP2, GP3], where each GPk contains exclusively parameters of the local approximations. Further details about initialization, optimization and metrics are in the appendix. Importantly, we remark that data is never revisited and its presence in the ensemble plots is just for clarity in the comprehension of results."
2010.02554,"data, code, python, github",121,,,"The code for the experiments is written in Python 3.7 and uses the Pytorch syntax for the automatic differentiation of the probabilistic models. It can be found in the repository https://github.com/pmorenoz/RecyclableGP, where we also use the library GPy for some algebraic utilities. In this section, we provide a detailed description of the experiments and the data used, the initialization of both variational parameters and hyperparameters, the optimization algorithm for both the local and the global GP and the performance metrics included in the main manuscript, e.g. the negative log-predictive density (NLPD), the root mean square error (RMSE) and the mean absolute error (MAE)."
2010.02554,"data, data available",121,,,"Local likelihood reconstruction. The augmented likelihood distribution is perhaps, the most important point of the derivation. It allows us to apply conditional independence (CI) between the subsets of distributed output targets. This gives a factorized term that we will later use for introducing the local variational experts in the bound, that is, log p(y|f∞) = (cid:80)K k=1 log p(yk|f∞). To avoid revisiting local likelihood terms, and hence, evaluating distributed subsets of data that might not be available, we use the Bayes theorem but conditioned to the inﬁnite-dimensional augmentation. It indicates that the local variational distributions can be approximated as"
2010.02554,"data, dataset",108,,,"As we already mentioned in the manuscript, there might be scenarios where it could be not necessary to distribute the whole dataset D in K local tasks or, for instance, a new unseen subset k + 1 of observations might be available for processing. In such case, it is still possible to obtain a combined global solution that ﬁts both to the local GP approximations and the new data. For clarity on this point, we rewrite the principal steps of the ensemble bound derivation in section A but without substituting all the log-likelihood terms by its Bayesian approximation, that is"
2010.02554,"data, dataset",166,,,"One of the most desirable properties for any modern machine learning method is the handling of very large datasets. Since this goal has been progressively achieved in the literature with scalable models, much attention is now paid to the notion of efﬁciency. For instance, in the way of accessing data. The fundamental assumption used to be that samples can be revisited without restrictions a priori. In practice, we encounter cases where the massive storage or data centralisation is not possible anymore for preserving the privacy of individuals, e.g. health and behavioral data. The mere limitation of data availability forces learning algorithms to derive new capabilities, such as i) distributing the data for federated learning (Smith et al., 2017), ii) observe streaming samples for continual learning (Goodfellow et al., 2014) and iii) limiting data exchange for private-owned models (Peterson et al., 2019)."
2010.02554,"data, dataset",230,,,"v) Pixel-wise MNIST classiﬁcation: We took images of ones and zeros from the MNIST dataset. To simulate a pixel-wise unsupervised classiﬁcation problem, true labels of images were ignored. Instead, we threshold the pixels to be greater or smaller than 0.5, and labeled as yi = 0 or yi = 1. That is, we turned the grey-scaled values to a binary coding. Then, all pixels were described by a two-dimensional input in the range [−1.0, 1.0], that indicates the coordinate of each output datum. In the case of the zero image, we splitted the data in four areas, i.e. the four corners, as is shown in the subﬁgure (A) of Figure 4. Each one of the local tasks was initialized with an equally spaced grid of Mk = 16 inducing-inputs. The ensemble GP required M = 25 in the case of the number zero and M = 16 for the one. The plotted curves correspond to the test GP predictive posterior at the probit levels [0.2, 0.5, 0.8]. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,"data, dataset",311,,,"1.1 Background. The ﬂexible nature of GP models for deﬁnining prior distributions over non-linear function spaces has made them a suitable alternative in many probabilistic regression and classiﬁcation problems. However, GP models are not immune to settings where the model needs to adapt to irregular ways of accessing the data, e.g. asynchronous observations or missings input areas. Such settings, together with GP model’s well-known computational cost for the exact solutions, typically O(N 3) where N is the data size, has motivated plenty of aproaches focused on parallelising inference. Regarding the task of distributing the computational load between learning agents, GP models have been inspired by local experts (Jacobs et al., 1991; Hinton, 2002). Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al."
2010.02554,"data, dataset",320,,,"ii) Distributed GPs: In this second experiment, our goal is to compare the performance of the recyclable framework with the distributed GP methods in the literature (Tresp, 2000; Ng and Deisenroth, 2014; Cao and Fleet, 2014; Deisenroth and Ng, 2015). To do so, we begin by generating toy samples from the sinusoidal function f (x). The comparative experiment is divided in two parts, in one, we observe N = 103 and in the other, N = 104 input-output data points. In the ﬁrst case, we splitted the dataset D in K = 50 tasks with Nk = 200 and Mk = 3 per partition. Any of these distributed subsets were overlapping, and their corresponding input-spaces concatenated perfectly in the range x ∈ [0.0, 5.5]. For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm."
2010.02554,"data, dataset",322,,,"Two seminal works exploited this connection before the modern era of GP approximations. While the Bayesian committee machine (BCM) of Tresp (2000) focused on merging independently trained GP regression models on subsets of the same data, the inﬁnite mixture of GP experts (Rasmussen and Ghahramani, 2002) increased the model expresiveness by combining local GP experts. Our proposed method will be closer to the ﬁrst approach whilst the second one is also amenable but out of the spirit of this work. The emergence of large datasets, with size N >104, led to the introduction of approximate models, that in combination with variational inference (Titsias, 2009), succeed in scaling up GPs. Two more recent approaches that combine sparse GPs with ideas from distributed models or computations are Gal et al. (2014) and Deisenroth and Ng (2015). Based on the variational GP inference of Titsias (2009), Gal et al. (2014) presented a new re-parameterisation of the lower bounds that allows to distribute the computational load accross nodes, also applicable to GPs with stochastic variational inference (Hensman et al., 2013) and with non-Gaussian likelihoods (Hensman et al., 2015; Saul et al., 2016). Out of the sparse GP approach and more inspired in Tresp (2000) and product of experts (Bordley, 1982), the distributed GPs of Deisenroth and Ng (2015) scaled up the parallelisation mechanism of local experts to the range of N >106. Their approach is focused on exact GP regression, not considering classiﬁcation or other non-Gaussian likelihoods. Table 1 provides a description of these different methods and their main properties, also if each distributed node is a GP model itself."
2010.02554,"data, dataset",330,,,"Ideally, to obtain a global inference solution given the GP models included in the dictionary, the resulting posterior distribution should be valid for all the local subsets of data. This is only possible if we consider the entire data set D in a maximum likelihood criterion setting. Speciﬁcally, our goal now is to obtain an approximate posterior q(f ) ≈ p(f |D) by maximising a lower bound LE under the log-marginal likelihood log p(D) without revisiting the data already observed by the local models. We begin by considering the full posterior distribution of the stochastic process, similarly as Burt et al. (2019) does for obtaining an upper bound on the KL divergence. The idea is to use inﬁnite-dimensional integral operators that were introduced by Matthews et al. (2016) in the context of variational inference, and previously by Seeger (2002) for standard GP error bounds. The use of the inﬁnite-dimensional integrals is equivalent to an augment-and-reduce strategy (Ruiz et al., 2018). It consists of two steps: i) we augment the model to accept the conditioning on the inﬁnite-dimensional stochastic process and ii) we use properties of Gaussian marginals to reduce the inﬁnite-dimensional integral operators to a ﬁnite amount of GP function values of interest. Similar strategies have been used in continual learning for GPs (Bui et al., 2017; Moreno-Muñoz et al., 2019). Global objective. The construction considered is as follows. We ﬁrst denote y as all the output targets {yi}N i=1in the dataset D and f∞ as the augmented inﬁnite-dimensional GP. Notice that f∞ contains all the function values taken by f (·), including that ones at {xi}N k=1 for all partitions. The augmented log-marginal expression is therefore"
2010.02554,"data, dataset",350,,,"For the setting with N = 104 samples, we used K = 500 local tasks, that in this case, were overlapping. As we already commented in the main manuscript, the baseline methods underperform more than our framework in problems where partitions do not overlap in the input-space. Additionally, standard deviation (std.) values in Table 3 indicate that we are more robust to the ﬁtting crash of some task. This fact is understandable as our method searches a global solution q(u∗) that ﬁts to all the local GPs in average. In contrast, the baseline methods are based on a ﬁnal ensemble solution that is an analytical combination of all the distributed ones. Then, if one or more fails, the ﬁnal predictive performance might be catastrophic. Notice that the baseline methods only require to train the local GPs separately, thing that we did with the LBFGS optimization algorithm. The setup of the VEM algorithm during the ensemble ﬁtting was {VE = 30, VM = 10, ηm = 10−3, ηL = 10−6, ηψ = 10−8, ηZ = 10−8}. As in the previous experiment with toy data, we set M = 35 inducing-inputs. iii) Recyclable ensembles: For simulating potential scenarios with at least N = 106 input-output data points, we used the setting of the previous experiment, but with K = 5 · 103 tasks of Nk = 800 instead. However, as explained in the paper, its performance was hard to evaluate in the baseline methods, due to the problem of combining bad-ﬁtted GP models. Then, based on the experiments of Deisenroth and Ng (2015) and the idea of building ensembles of ensembles, we set a pyramidal way for joining the distributed local GPs. It was formed by two layers, that is, we joined ensembles twice as shown in the Figure 5 of this appendix."
2010.02554,"data, dataset, data https, data available",327,,,"The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g. K(cid:29)103, it is problematic for baseline methods, due to partitions must be revisited for building predictions and if one-of-many GP fails, performance decreases. Then, we repeated the experiment in a pyramidal way. That is, building ensembles of recyclable ensembles, inspired in Deisenroth and Ng (2015). Our method obtained {NLPD=4.15, RMSE=2.71, MAE=2.27}. The results in Table 3 indicate that our model is more robust under the concatenation of approximations rather than overlapping them in the input space. (iv) Solar physics dataset: We tested the framework on solar data (available at https://solarscience.msfc.nasa.gov/), which consists of more than N =103 monthly average estimates of the sunspot counting numbers from 1700 to 1995. We applied the mapping log(1 + yi) to the output targets for performing Gaussian regression. Metrics are provided in Table 2, where std. values were small, so we do not include them. The perfomance with 50 tasks is close to the baseline solutions, but without storing all distributed subsets of data."
2010.02554,"data, dataset, data https, data available",332,,,"4.1 Regression. In our ﬁrst experiments for variational GP regression with distributed models, we provide both qualitative and quantitative results about the performance of recyclable ensembles. (i) Toy concatenation: In Figure 2, we show three of ﬁve tasks united in a new GP model. Tasks are GPs ﬁtted independently with Nk=500 synthetic data points and Mk=15 inducing variables per distributed task. The ensemble ﬁts a global variational solution of dimension M =35. Notice that the global variational GP tends to match the uncertainty of the local approximations. (ii) Distributed GPs: We provide error metrics for the recyclable GP framework compared with the state-of-the-art models in Table 3. The training data is synthetic and generated as a combination of sin(·) functions (in the appendix). For the case with 10K observations, we used K=50 tasks with Nk=200 data-points and Mk=3 inducing variables in the sparse GP. The scenario for 100K is similar but divided into K=250 tasks with Nk=400. Our method obtains better results than the exact distributed solutions due to the ensemble bound searches the average solution among all recyclable GPs. The baseline methods are based on a combination of solutions, if one is bad-ﬁtted, it has a direct effect on the predictive performance. We also tested the data with the inference setup of Gal et al. (2014), obtaining an NLPD of 2.58 ± 0.11 with 250 nodes for 100K data. It is better than ours and the baseline methods, but without a GP reconstruction, only distributes the computation of matrix terms. (iii) Recyclable ensembles: For a large synthetic dataset (N =106), we tested the recyclable GPs with K=5 · 103 tasks as shown in Table 3. However, if we ensemble large amount of local GPs, e.g."
2010.02554,"data, dataset, data https, used dataset",206,,,"iv) Solar physics dataset: We used the solar physics dataset (https://solarscience.msfc.nasa.gov/) which consists of N = 3196 samples. Each input-output data point corresponds to the monthly average estimate of the sunspot counting numbers from 1700 to 1995. The output targets y were transformed to the real-valued domain via the mapping log(yi + 1) to use a normal likelihood distribution. We also scaled the input area to the range x ∈ [0, 100] and normalized the outputs to be zero-mean. The number of tasks was K = 50 and 20% of the data observations were reserved for test. The initial values of kernel and likelihood hyperparameters was {(cid:96) = 0.2, σ2 n is the initial likelihood variance, that we also learn. In this case, the setup of the VEM algorithm was {VE = 20, VM = 20, ηm = 10−5, ηL = 10−8, ηψ = 10−10, ηZ = 10−10}. The number of global inducing-inputs used for the ensemble was M = 90, whilst we used Mk = 6 for each distributed approximation."
2010.02554,"data, dataset, dataset provided, data available",109,,,"In practice, it might not be necessary to distribute the whole dataset D in parallel Recyclable GPs and new data. tasks, with some subsets Dk available at the global ensemble. It is possible to combine the samples in Dk with the dictionary of local GP variational distributions. In such cases, we would only approximate the likelihood terms in (3) related to the distributed subsets of samples. The resulting combined bound would be equivalent to (6) with an additional expectation term on the new data. We provide the derivation of this combined bound in the supplementary material."
2010.02554,dataset,156,,,"We consider a supervised learning problem, where we have an input-output training dataset D = {xi, yi}N i=1 with x ∈ Rp. We assume i.i.d. outputs yi, that can be either continuous or discrete variables. For convenience, we will refer to the likelihood term p(y|θ) as p(y|f ) where the generative parameters are linked via θ = f (x), being f (·) a non-linear function drawn from a zero-mean GP prior f ∼ GP(0, k(·, ·)), and k(·, ·) is the covariance function or kernel. Importantly, when non-Gaussian outputs are considered, the GP output function f (·) might need an extra deterministic mapping Φ(·) that transforms it to the appropriate parametric domain of θ."
2010.02554,dataset,20,,,Table 2: Performance metrics for distributed GP regression with the solar physics dataset. (std. ×102)
2010.02554,dataset,205,,,"vii) Banana dataset: The banana experiment is perhaps one of the most used datasets for testing GP classiﬁcation models. We followed a similar strategy as the one used in the MNIST experiment. After removing the 33% of samples for testing, we partitioned the input-area in four quadrants, i.e. as is shown in Figure 4. For each partition we set a grid of Mk = 9 inducing-inputs and later, the maximum complexity of the global sparse model was set to M = 25. The baseline GP classiﬁcation method also used M = 25 inducing-inputs and obtained an NLPD value of 7.29 ± 7.85 × 10−4 after ten trials with different initializations. Our method obtained a test NLPD of 7.21 ± 0.04. As we mentioned in the main manuscript, the difference is understandable as the recyclable GP framework used a total amount of 4 × 16 inducing-inputs, that capture more uncertainty than the 16 of the baseline method. The setup of the VEM algorithm was {VE = 20, VM = 10, ηm = 10−3, ηL = 10−5, ηψ = 10−6, ηZ = 10−5}."
2010.02554,dataset,97,,,"The construction of ensemble variational bounds from recyclable GP models is based on the idea of augmenting the marginal likelihood to be conditioned on the inﬁnite-dimensional GP function f∞. Notice that f∞ contains all the function values taken by f (·) over the input-space Rp, including the input targets {xi}N k=1 and the global ones Z∗. Having K partitions of the dataset D with their corresponding outputs y = {y1, y2, . . . , yK }, we begin by augmenting the marginal log-likelihood as"
2010.02554,"dataset, used dataset",218,,,"of Van der Wilk et al. (2017), we threshold images of zeros and ones to black and white pixels. Then, to simulate a pixel-wise learning scenario, we used each pixel as an input-output datum whose input xi contains the two coordinates (x1 and x2 axes). Plots in Figure 4 illustrate that a predictive ensemble can be built from smaller pieces of GP models, four corners in the case of the number zero and two for the number one. (v) Compositional number: As an illustration of potential applications of the recyclable GP approach, we build a number eight predictor using exclusively two subsets of the approximations learned in the previous experiment with the image of a zero. The trick is to shift Zk to place the local approximations in the desired position. (vi) Banana dataset: We used the popular dataset in sparse GP classiﬁcation for testing our method with M =25. We obtained a test NLPD= 7.21 ± 0.04, while the baseline variational GP test NLPD was 7.29 ± 7.85×10−4. The improvement is understandable as the total number of inducing points, including the local ones, is higher in the recyclable GP scenario."
2010.09647,code,123,,,"ing unconstrained. The base measure problem occurs, in a sense, but the remedy is both local and internal to the implementation of Stan. The latter Stan pushes on the user: if a user writes a Stan model with a parameter x and a transformed parameter f (x), and wishes to code a density on f (x) that corresponds to the pushforward of some known density p(x), then that user must think about base measures and partial derivatives themselves. Stan does not help; but then again, it also does not claim it would help, so Stan doesn’t compute anything visibly “incorrect”."
2010.09647,code,89,,,"We have named the Base Measure Problem and provided a solution to it. Implementing the solution in probabilistic programming systems should cause negligible loss of performance for cases that were already correctly handled, and expand the set of models in which the system can compute correct probability densities. Implementation does carry a code complexity cost, but that cost is minimized by using two-argument dispatch, or emulating it with a Visitor pattern. Despite correctly accounting for measures, no non-local information is required."
2010.09647,data,80,,,"The root of the base measure problem is that we didn’t want to compute with measures directly, but lost the base measure when representing probability distributions with densities. It’s not actually possible to infer the correct base measure from the data type representing the sample: a point on the unit circle in R2 is represented with two ﬂoating-point numbers, but using Lebesgue measure on R2 as the base is not helpful."
2010.09647,package,88,,,"The Jacobian-determinant correction 1/| det(Jfx)| accounts for the possibility that f changes the volume of an inﬁnitesimal volume element near x. The Jacobian determinant can be computed by forming the Jacobian of f , for example with automatic diﬀerentiation; but for many functions f , it’s available more eﬃciently. Thus a conventional choice is to package such f , together with their inverse f −1, in a Bijector class with a method for computing said Jacobian determinant."
2011.00242,code,100,,,"Therefore, in all applications, there are indications, in the form of source code, comments, and issues, that developers demand an effort to understand and improve cache keys, reasoning about alternative better ways to organize and identify content. We identiﬁed common content properties used to build cache keys, which are: (i) content ids, (ii) method signatures, and (iii) a tag-based identiﬁcation, in which the type of content, the class, the module or hierarchy are used (Evidence 19)."
2011.00242,code,11,,,Category (Acronym) Code Scattering and Tangling (CST)
2011.00242,code,12,,,Complex Naming Conventions (CNC) Additional Caching Code (ACC)
2011.00242,code,120,,,"Given that implementing cache is challenging, we noticed that in six applications developers made use of supporting libraries and frameworks. This was done to prevent adding much cache-related code to the base code, because such components raise the abstraction level of caching, providing some ready-to-use features (Evidence 22). Examples of such external components are distributed cache systems, e.g. Redis [27] and Memcached [26], and libraries that can act locally, e.g. Spring Caching [51], EhCache [52], Inﬁnispan [53], Rails low-level caching [54], Google Guava [55] and Caffeine [56]."
2011.00242,code,152,,,"First, we assigned concepts to pieces of extracted text (open coding), each representing application-level caching characteristics. Figure 2 exempliﬁes different codes identiﬁed during the analysis of an application. For instance, Code Tangling is created from the observation of cache-related implementation spread all over the application base code (underlined). Then, for each new concept, we veriﬁed whether they are connected somehow with existing ones, in order to generate categories (selective coding). Thus, the name assigned to a particular category aims at representing, at a higher abstraction level, all concepts related to it. Regarding the Code Tangling example, if Code Scattered were found afterwards, we could establish a relationship between the former and the latter to create a category, given that both are related to lack of separation of concerns."
2011.00242,code,161,,,"[7] M. P. Robillard, W. Coelho, and G. C. Murphy, “How effective developers investigate source code: An exploratory study,” IEEE Transactions on Software Engineering, vol. 30, no. 12, pp. 889–903, 2004. [Online]. Available: http://dx.doi.org/10.1109/TSE.2016. 2532873 J. Sillito, G. C. Murphy, “Asking and answering questions during a programming change task,” in IEEE Transactions on Software Engineering, vol. 34, [Online]. Available: no. 4. http://dx.doi.org/10.1109/TSE.2008.26 S. Nadi, T. Berger, C. K¨astner, and K. Czarnecki, “Where do conﬁguration constraints stem from? An extraction approach and an empirical study,” IEEE Transactions on Software Engineering, vol. 41, no. 8, pp. 820–841, aug 2015. [Online]. Available: http://dx.doi.org/10.1109/TSE.2015.2415793"
2011.00242,code,17,,,"to implementation issues of application-level caching, by providing solutions and guidance at the code level."
2011.00242,code,19,,,A code comment before an expensive operation: TODO cache the global properties to speed this up??
2011.00242,code,21,,,Labels of Evidence Sources: SC-Source Code (without comments); COM-Code Comments; IS-Issues; DOC-Documentation; DEV-Developers.
2011.00242,code,23,,,A code comment: Is hibernate taking care of caching and not hitting the db every time? (hopefully it is)
2011.00242,code,236,,,"As can be seen in Figure 3, caching implementation and associated design decisions are much more discussed and revised by developers than maintenance decisions. Caching implementation, which is spread in the code and involves the choice for appropriate locations to add and remove elements from the cache, is error-prone and can compromise code legibility. Consequently, many issues are associated with bug ﬁxes, technological details and code refactorings. Moreover, despite being less frequent, caching design is time-consuming and challenging, given that it requires understanding of the application behavior, as well as limitations, conditions and restrictions of content being cached. In applications analyzed, the mean (M) and standard deviation (SD) values of cache-related implementation issues are M = 47.45% and SD = 5.76%, while design issues achieve M = 37.92% and SD = 7.11%. Finally, because ﬁnegrained conﬁgurations require empirical analysis such as cache proﬁling, and there is little evidence that this was performed in investigated applications, maintenance decisions often result in the choice for default settings. Consequently, a lower number of issues is associated with such decisions, speciﬁcally M = 14.61% and SD = 3.44%. This issue analysis allowed us to understand the aspects of caching that require more effort from developers."
2011.00242,code,30,,,"Code comments. Given that caching is an orthogonal concern in the application, unrelated to the business logic, but interleaved with its code, code comments are often"
2011.00242,code,36,,,"reasoning, time, and modiﬁcations to the code (Evidence 14). We discussed our ﬁndings regarding cache design decisions and we next discuss observations made associated with how to implement design decisions."
2011.00242,code,38,,,"We performed an analysis of the issues available in issue platforms to investigate the primary sources of cacherelated problems, typically bugs, in the applications. Based on user messages, code reviews and commit messages that"
2011.00242,code,43,,,"A code snippet exposing a test of caching logic: it ”can set and get false values when return cache nil” do @store.set :test, false expect(@store.get(:test)).to be false end"
2011.00242,code,46,,,"Before addressing each of our research questions, we show an objective analysis of the impact of caching in the investigated applications by identifying all code and issues related to them. This gives a broad sense of how caching is implemented in target systems."
2011.00242,code,48,,,"Source code. Application source code is our core source of information. Since we focus on application-level caching, our analysis is concentrated in the core of the application (i.e. the business logic), which is where the caching logic is typically implemented."
2011.00242,code,49,,,A code snippet: cache id = ’objectmodel ’ . $entity defs[’classname’] . ’ ’ . (int)$id . ’ ’ . (int)$id shop . ’ ’ . (int)$id lang;
2011.00242,code,56,,,"Keep the cache API simple. (Evidence 15), (Evidence 17), (Evidence 20) and (Evidence 22) Caching logic tends to be spread all over the application, and a good solution should be employed to avoid writing messy code at the cost of high maintenance efforts."
2011.00242,code,59,,,"Furthermore, in cases where updates in the base code are not an option (due to time or technical restrictions), a transparent and automatic caching component can provide fast results. These solutions address layers before and after application boundaries, and require only a few adaptations to the application needs (Evidence 24)."
2011.00242,code,60,,,"All these caching design options may become complex and difﬁcult to understand. Indeed, identifying caching opportunities and ensuring consistency can add much code and may not be trivial to implement and understand. Due to the nature of application-level caching, such logic is spread all over the system. We noticed in 90% of the applications"
2011.00242,code,60,,,"Such supporting libraries and frameworks not only provide partial ready-to-use features but also reduce the amount of additional effort required to guarantee that the cache is working. We observed in all applications that the cache includes code dedicated to test, debug and conﬁgure cache components, which can be expensive in some scenarios (Evidence 23)."
2011.00242,code,63,,,"A developer quote: At ﬁrst glance, the cache code hinders the understanding of the business logic. Also, the cache logic itself it is not easy to get. A bug report on an issue platform: When you import categories with a parent category which does not exist, it prevents from duplicate it because of the cache."
2011.00242,code,87,,,"Although this problem is present in the code, there are indications that developers know about it and express they are willing to improve the provided solution. In order to reduce the impact of an infrastructure component to the system business logic, we identiﬁed cases where there are suggestions to design more extensible classes and modules, refactoring and reducing cache-related code, and reusing components (Evidence 17). This acknowledgment of technical debt was observed in 90% of the applications."
2011.00242,code,91,,,"such as design patterns, third-party libraries or aspects? 22 How is the caching logic mixed with the application code? 23 Is this extra cache logic tested? 24 What is the required format to cache? 25 How are objects translated to the cache? 26 How are names (keys) deﬁned for cached objects? 27 Do developers use another caching layer besides application-level? 28 Is any transparent or automatic caching component being used? 29 Do developers rely on automatic caching components? 30"
2011.00242,code,96,,,"Presence of complex constructs such as batch processing and asynchronous communication, which require extra effort and reasoning from developers to be implemented. Indication of use of third-party caching solutions to help the implementation, raising the level of abstraction of some caching aspects. Choice for complex keys of caching content, causing developers to spend time and effort to elaborate and understand such keys. the Code implemented to support caching logic, such as implemented caching tests, logic to monitor cache statistics and additional interfaces to support available caching providers."
2011.00242,"code, github",72,,,"Issues. An issue can represent a software bug, a project task, a help-desk ticket, a leave request form, or even user messages about the project in general. Usually, changes in the code are due to registered issues. Thus, implementation and design decisions are better explained by associated issues in issue platforms, such as GitHub Issue Tracker, JIRA, and Bugzilla."
2011.00242,"code, open-source",68,,,"To identify patterns of application-level caching adopted by developers and understand what kinds of caching implementations and decisions can be automatically inferred, characterize and evaluate application-level caching-related design and implementation from a perspective of the researcher as they are implemented in the source code and described in issues of web-based applications in the context of 10 software projects, obtained from open-source repositories and software companies."
2011.00242,data,102,,,"The high number of occurrences related to ensuring consistency refers to the expiration process of cached content, which requires extra reasoning from developers, because they should track which changes cause data content to become outdated, and be aware for how long the cache can provide stale data, in case the data source has been updated. In fact, consistency approaches have been widely investigated [4], [6], and the typical way of dealing with it is to analyze data dependency, from which conditions and constraints for consistency can be derived."
2011.00242,data,106,,,"Avoid caching per-user data. (Evidence 4) and (Evidence 26) It is recommended to avoid caching per-user data unless the user base is small and the total size of the cached data does not require an excessive amount of memory; otherwise, it can cause a memory bottleneck. However, if users tend to be active for a while and then go away again, caching per-user data for short-time periods may be an appropriate approach. For instance, a search engine that caches query results by each user, so that it can page through results efﬁciently."
2011.00242,data,107,,,"Regarding implementation choices, we observed common practices. The ﬁrst is associated with how to name cached data. In order to use in-memory caching solutions, there is no prescribed way to organize data. Typically, unique names are assigned to each cached content, thus leading to a key-value model—and this was the case in all investigated applications. Given that cache stores lots of data, the set of possible names must be large; otherwise, two names (keys) can conﬂict with each other and, thus stale (or even entirely wrong) data can be retrieved from"
2011.00242,data,11,,,RQ1. What and when is data cached at the application
2011.00242,data,110,,,"Despite choosing where and what to cache, cached values are valid only as long as the sources do not change, and when sources change, a consistency policy should be employed to ensure that the application is not serving stale data. Therefore, in nine analyzed applications, there are indications that developers demand an effort to design consistency approaches, reasoning about the lifetime of cached data, as well as eviction conditions and constraints. We identiﬁed common approaches to keep consistency, which are: (i) a less efﬁcient and straightforward approach is to invalidate cached values based on mapping actions that"
2011.00242,data,127,,,"The theoretical categories are described in Table 5, which presents their description, an original piece of content (example of evidence) and sources of data classiﬁed in each category. Moreover, categories are also shown in Figure 5a with the associated number of occurrences in the applications analyzed and classiﬁed according to each research question. Figure 5b shows the percentage of contribution of each application to the categories emerged from the study. These ﬁgures also present categories identiﬁed in RQ2 (described in Table 6) and RQ3 (described in Table 7), which are discussed in the following sections. Acronyms used in Figures 5a and 5b are introduced in Tables 5, 6, and 7."
2011.00242,data,13,,,Is there a relationship between the data cached and the application domain?
2011.00242,data,13,,,RQ1. What and when is data cached at the application level?
2011.00242,data,131,,,"Do not discard small improvements. (Evidence 3), (Evidence 8) and (Evidence 9) The user perceived latency is reduced by any caching solution employed. This means that even not obvious scenarios should be target of caching, i.e. it is not true that solely data that is frequently used and expensive to retrieve or create should considered for caching. Furthermore, data that is expensive to retrieve and is modiﬁed on a periodic basis can still improve performance and scalability when properly managed. Caching data even for a few seconds can make a large difference in high volume sites. If the data is handled more often than it is updated, it is also a candidate for caching."
2011.00242,data,136,,,"Following the introduced phases of data analysis, we performed mainly a subjective analysis of the data, collecting: (i) typical caching design, implementation and maintenance strategies; (ii) motivations, challenges and problems behind caching, and (iii) characteristics of caching decisions. These collected data were evaluated to conceptualize how the open codes were related to each other as a set of hypotheses in accounting for resolving the primary concern. Furthermore, we also made a broad analysis of the target systems in order to investigate how application-level caching was conceived in them. All the research phases were performed manually, as the collected data (most of them expressed in natural language) analysis is associated with the interpretation of caching approaches."
2011.00242,data,14,,,4.2 RQ1: What and when is data cached at the application level?
2011.00242,data,141,,,"7 DISCUSSION AND CONCLUSION Application-level caching has been increasingly used in the development of web applications, in order to improve their response time given that they are becoming more complex and dealing with larger amounts of data over time. Caching has been used in different locations, such as proxy servers, often as seamless components. Applicationlevel caching allows caching additional content taking into account application speciﬁcities not captured by off-theshelf components. However, there is limited guidance to design, implement and manage application-level caching, which is often implemented in an ad-hoc way. In this paper, we presented a qualitative study performed to understand how developers approach application-level caching. The study consisted of the selection ten web applications and investigation of caching-related aspects, namely design, implementation and maintenance practices."
2011.00242,data,141,,,"performance via adaptive content the Proceedings of Computing and Applications. [Online]. Available: http://dx.doi.org/10.1109/NCA.2011.55 [20] G. Soundararajan and C. Amza, “Using semantic information to improve transparent query caching for dynamic content web sites,” in Proceedings of the International Workshop on Data Engineering Issues in E-Commerce, vol. 2005. IEEE, 2005, pp. 132– 138. [Online]. Available: http://dx.doi.org/10.1109/DEEC.2005.25 [21] C. Amza, G. Soundararajan, and E. Cecchet, “Transparent caching with strong consistency in dynamic content web sites,” in Proceedings of international conference on Supercomputing. New York, New York, USA: ACM Press, jun 2005, p. 264. [Online]. Available: http://dx.doi.org/10.1145/ 1088149.1088185"
2011.00242,data,150,,,"From all analyzed applications, we observed that none of them use a proactive approach to cache content. Content is always cached after it requested (i.e. reactive approach) and, as a consequence, the ﬁrst request always results in a cache miss. Due to this, prefetching techniques can be used in order to populate the cache and prevent misses by predicting and caching data that will potentially be requested in the future. It can be based on heuristics, usage observations or even with the use of complex prediction algorithms [15], [32], [58], [59]. However, the design and implementation of a reactive cache component already requires signiﬁcant effort and reasoning to be properly done, and a proactive approach increases the complexity of the caching solution even more."
2011.00242,data,16,,,Deﬁne naming conventions Perform cache actions asynchronously Do not use cache as data storage Perform measurements
2011.00242,data,18,,,"Design of some kind of consistency approach such as expiration policies and invalidation, preventing stale data."
2011.00242,data,183,,,"the content being cached should be considered when using size-limited caches. In this case, an adequate trade-off between popularity (hits) and size of the items must be achieved. Keeping small popular items in the cache tends to optimize hit-ratio; however, a hit in a large item may be more beneﬁcial for an application than many hits on small items. At the same time, ﬁlling the cache with few large items may turn the cache performance dependent on a good replacement policy. Evaluate staleness and lifetime of cached data. (Evidence 2), (Evidence 3) and (Evidence 12) Every piece of cached data is already potentially stale, it is important to rethink the degree of integrity and potential staleness that the application can compromise for increased performance and scalability. Many cache implementations adopted an expiration policy to invalidate cached data based on a timeout since weak consistency is easier than deﬁning a hard-to-maintain, but more robust, invalidation process. In short, developers must"
2011.00242,data,186,,,"Even though these approaches focus on providing an adaptive behavior to application-level cache, none of them takes application speciﬁcities into account to autonomously manage their target. They attempt only to optimize cache decisions based on cache statistics like hit-ratio and access patterns at a higher level, thus, ignoring cache meta-data expressed by application-speciﬁc characteristics, which are closely related to application computations and could help to reach an optimal performance of the caching service on time. Addressing this issues, content-aware approaches have been proposed [45], [46] for admitting and replacing items in the cache by exploring descriptive caching hits in the application model or spatial locality (relationships among web objects). Though adaptive caching is in general new and innovative, it is far from being adopted as standard practice in software industry; many improvements must be achieved before this can happen, such as reducing the overhead in terms of resource consumption and processing time of the learning process, and providing easy ways to integrate application and caching method."
2011.00242,data,189,,,"The last issue is related to the maintenance of the cache system, which involves several aspects such as determining replacement policies and the size of the cache. According to Radhakrishnan [31], a traditional approach to maintain cache is to set up strategies based on initial assumptions or well-accepted characteristics of workload and access patterns. Thus, cache statistics such as hit ratio, the number of objects cached, and average object size can be collected by observing the application and cache at run-time. These data, compared in the context of desired values of these parameters, can be used to decide whether to change the ﬁrst choices. Usually, this process is repeated until an acceptable trade-off between conﬁguration and performance is reached, and then cache strategies are ﬁxed for the lifetime of the product or at least for several release cycles. However, its performance may decay over time due to changes in the workload characteristics and access patterns. Therefore, achieving acceptable application performance requires constantly tuning cache decisions, which implies extra time"
2011.00242,data,19,,,Fig. 6. Cacheability ﬂowchart: intuitive process to decide whether to cache or not particular data.
2011.00242,data,19,,,Guideline Evaluate different abstraction levels to cache Stack caching layers Separate dynamic static data Evaluate boundaries Specify selection criteria
2011.00242,data,198,,,"issues about whether to cache some speciﬁc data or not, showing that the connection between observed bottlenecks in the application and opportunities for caching is not straightforward and requires a deeper analysis (Evidence 1). Therefore, selection criteria based on common sense or past experiences are initial assumptions to ease such decisions. Despite these criteria are usually unjustiﬁed, they guide developers while selecting content. We observed in 90% of the applications the deﬁnition of selection criteria to make the distinction of cacheable from uncacheable content easier. These criteria were observed in explanations of cache design choices in comments, issues, and documentation. We identiﬁed common criteria used to determine whether to cache or not a speciﬁc content, which are: (i) content change frequency (Evidence 2), (ii) content usage frequency (Evidence 3), (iii) content shareability (Evidence 4), (iv) content retrieval complexity (Evidence 5), (v) content size (Evidence 6), and (vi) size of the cache (Evidence 7)."
2011.00242,data,2,,,Data Expiration
2011.00242,data,21,,,A sentence in documentation: We use Redis [a thirdparty solution] as a cache and for transient data.
2011.00242,data,221,,,"Abstract—Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users’ demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on speciﬁc details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications."
2011.00242,data,257,,,"Specify selection criteria. (Evidence 1), (Evidence 2), (Evidence 3), (Evidence 5), (Evidence 6) and (Evidence 7) Selecting the right data to cache involves a great reasoning effort given that data manipulated by web applications range in dynamicity, from being completely static to changing constantly. To optimize this selection process, there are four primary selection criteria used by developers while detecting cacheable content, which should be used in decisions regarding whether to cache. These criteria are described below, ordered according to their importance; i.e. the higher the inﬂuence level, the earlier it is presented. Data change frequency. Developers should seek for data that have some degree of stability, i.e. those that are more used than changed. Even if data are volatile and change in time intervals, caching still brings a beneﬁt. This is the ﬁrst factor to be considered since caching volatile data implies the implementation of consistency mechanisms, which is not trivial and requires an extra effort and reasoning from developers. In short, the cost of consistency approaches cannot be higher than the beneﬁt of caching. Besides, when stale data is not a critical issue, an approach of weak consistency can be employed, such as time-to-live (TTL) eviction, where data is expired after a time in cache, regardless of possible changes."
2011.00242,data,29,,,"Data retrieval complexity. Data that is expensive to retrieve, compute, or render, regardless of its dynamicity, is always considered a good caching opportunity."
2011.00242,data,35,,,"order to avoid this, we followed a systematic analysis of our data, and conclusions are all founded on these data. Moreover, cross-checks were performed using our different sources of evidence."
2011.00242,data,39,,,"ensure that the expiration policy matches the pattern of access to applications that use the data, which is based on determining how often the cached information is allowed to be outdated, and relaxing freshness when possible."
2011.00242,data,44,,,7 What kinds of bottlenecks are addressed by developers? 8 What motivated the need for explicit caching manipulation? 9 What is the granularity of the cached objects? 10 What is the importance of the cached data to the application? 11
2011.00242,data,44,,,"Separate dynamic from static data. (Evidence 3) and (Evidence 2) Content can be distinguished in static, dynamic, and user-speciﬁc. By partitioning the content, it is easier to select portions of the data to cache."
2011.00242,data,46,,,Evaluate staleness and lifetime of cached data Avoid caching per-user data (Evidence 4) and (Evidence 26) DG-07 Avoid caching volatile data (Evidence 2) and (Evidence 3) DG-08 DG-09 Do not discard small provements Keep the cache API simple
2011.00242,data,5,04/21/22,0,Data usage frequency. Frequent
2011.00242,data,55,,,"[4] D. R. K. Ports, A. T. Clements, I. Zhang, S. Madden, and B. Liskov, “Transactional Consistency and Automatic Management in an Application Data Cache,” Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation, pp. 279–292, oct 2010."
2011.00242,data,56,,,"Occurrences of multiple caching solutions, which can involve different third-party components or different application layers. As a consequence, the same content can be cached at different places in varying forms. Occurrences of caching content being added to the cache because they retrieve data from frameworks, libraries, or external applications."
2011.00242,data,57,,,"6 THREATS TO VALIDITY We now analyze the possible threats to the validity of this study, and how we mitigated them. Researcher bias is a typical threat to qualitative research because the results are subject to the researcher interpretation of the data. In our study, prior knowledge might have inﬂuenced results. In"
2011.00242,data,59,,,1 What are the motivations to employ cache? 2 What are the typical use scenarios? 3 Where and when data is cached? 4 What are the constraints related to data cached? 5 What are the selection criteria adopted to detect cacheable content? 6 Do developers adopt a pattern to decide which content should be
2011.00242,data,59,,,"[29] K. Nguyen and G. Xu, “Cachetor: detecting cacheable data the 9th Joint Meeting on to remove bloat,” in Proceedings of Foundations of Software Engineering. New York, New York, USA: ACM Press, aug 2013, p. 268. [Online]. Available: http://dx.doi.org/10.1145/2491411.2491416"
2011.00242,data,59,,,"of the application-level caching is not trivial and demands high effort, because it involves manual implementation by developers. Its design and maintenance involve four key challenging issues: determining how to cache the selected data, what data should be cached, when the selected data should be cached or evicted, and where the cached data"
2011.00242,data,6,,,Controller layer. Caching data at
2011.00242,data,64,,,"Due to the increasing demand and complexity involved with caching, developers try to decrease cache-related effort by adopting simple design solutions. We observed in seven of the ten analyzed applications design choices such as managing consistency based on expiration time, keeping default parameters, selecting data without any criteria, or even adopting external solutions, which do not claim extra"
2011.00242,data,68,,,"Finally, determining maintenance strategies to manage efﬁciently where data is placed, such as replacement policies or size of the cache, requires additional knowledge and reasoning from application developers. Nevertheless, there are no foundations to make this choice adequately. Therefore, RQ3 complements the analysis above, questioning how application speciﬁcities are leveraged to provide the desired performance to the cache system."
2011.00242,data,69,,,"Avoid caching volatile data. (Evidence 2) and (Evidence 3) Data should be cached when it is frequently used and is not continually changing. Developers should remember that caching is most effective for relatively stable data, or data that is frequently read. Caching volatile data, which is required to be accurate or updated in real time, should be avoided."
2011.00242,data,72,,,"time-consuming, error-prone and, consequently, a common source of bugs [25]. Gupta et al. [6] and Ports et al. [4] both address these implementation issues by providing high-level caching abstractions, in which developers can simply designate application functions as cacheable, and the proposed system automatically caches their results and invalidates the cached data when the underlying source changes."
2011.00242,data,75,,,"operations, requests, queries and shared content (accessed by multiple users) must be identiﬁed, focusing on recomputation avoidance. Even if some processing can be fast enough at a glance, it can potentially become a bottleneck when being invoked many times. Despite being frequently used, user-speciﬁc data cannot be shared and may not bring the beneﬁt of caching, being usually left out of the cache."
2011.00242,data,77,,,"Do not use cache as data storage. (Evidence 27) An application can modify data held in a cache, but the cache should be considered as a transient data store that can disappear at any time. Therefore, developers should not save valuable data only in the cache, but keep the information where it should be as well, minimizing the chances of losing data if the cache unexpectedly becomes unavailable."
2011.00242,data,8,,,Business or service layer. Caching data at
2011.00242,data,8,,,Size of the data. The size of
2011.00242,data,81,,,"[25] W. Wang, Z. Liu, Y. Jiang, X. Yuan, and J. Wei, “EasyCache: a transparent in-memory data caching approach for internetware,” in Proceedings of the 6th Asia-Paciﬁc Symposium on Internetware on Internetware. New York, New York, USA: ACM Press, nov 2014, pp. 35–44. [Online]. Available: http://dx.doi.org/10.1145/ 2677832.2677837 [26] “Memcached,” 2016."
2011.00242,data,84,,,"Our study was designed based on comparative and interactive principles of grounded theory [47]. The purpose of grounded theory is to construct theory grounded in data by identifying general concepts, develop theoretical explanations that reach beyond the known, and offer new insights into the area of study. The systematic procedures of grounded theory enable qualitative researchers to generate ideas. In turn, these ideas can be later studied and veriﬁed through traditional quantitative forms of research."
2011.00242,data,88,,,"Determining the cacheable content and the right moment of caching or clearing the cache content are a developer’s responsibility and might not be trivial in complex applications, motivating RQ1. In this research question, we aim to identify what data is selected to be cached, and the criteria used to detect such cacheable data. Furthermore, this question also explores when data should be evicted, as well as constraints, consistency conditions, and the rationale for all these choices."
2011.00242,data,91,,,"By using grounded theory, we aimed to construct a wellintegrated set of hypotheses that explain how the concepts operated. Thus, the selective coding involves identifying the core category that best explains how study data refers to a large portion of the variation in a pattern and is considered the primary concern or problem related to the study, integrating closely related concepts. Finally, theoretical codes conceptualize how the codes may relate to each other as hypotheses to be incorporated into the theory [47]."
2011.00242,data,99,,,"12 How much memory does the cached data consume? 13 What data is most frequently accessed? 14 How often is the cached data going to be used and changed? 15 What data is expensive? 16 What data depends on user sessions? 17 How up to date does the data need to be? 18 How is consistency assurance implemented? Why was it chosen? 19 Where and when is consistency assurance employed? 20 Which kind of operation/behavior affects cache consistency? 21 Do developers employ any technique to ease caching implementation,"
2011.00242,"data, code",107,,,"4 ANALYSIS AND RESULTS This section details the results of our study and their analysis, according to the research questions we aim to answer. Our collected data consist mainly of source code and issues (expressed in natural language) and, as these are qualitative data, we have undertaken a subjective analysis of the application-level caching (hereafter referred to simply as “caching”) aspects represented in the target systems. Note that we labeled some ﬁndings with “Evidence X,” so that we can later refer to them to support the guidelines we derived from this analysis."
2011.00242,"data, code",108,,,"We observed a higher number of categories, which are classes of observations made based on the analysis of these applications, associated with caching design and implementation than those associated with maintenance. Furthermore, the number of occurrences of each category, design and implementation categories also have higher numbers. This phenomenon was expected since the most representative portion of our qualitative data consists of source code and issues. Moreover, simple maintenance tasks and conﬁgurations are already executed and provided by external components, being commonly adopted. However, the number of occurrences do not reﬂect the importance of a category."
2011.00242,"data, code",135,,,"The second and third issues refer to deciding the right content to cache and the best moment of caching or clearing the cache in order to avoid cache thrashing and stale content. These decisions involve (i) choosing the granularity of cache objects, (ii) translating between raw data and cache objects, and (iii) maintaining cache consistency, which are all tasks to be accomplished by developers and might not be trivial in complex applications. Della Toffola et al. [5], Nguyen and Xu [29] and Infante [30] address part of these issues by identifying and suggesting caching opportunities. However, developers should still review the suggestions and refactor the code, integrating cache logic into the application."
2011.00242,"data, code",139,,,"To understand how developers integrate application-level caching logic in their applications, we analyzed the cache implementations from two perspectives. The ﬁrst consists of examining the explicit caching logic present in the application code, focusing on analyzing where the caching logic is placed, for what this logic is responsible, and when it is executed. The second evaluates the integration and communication between the application and an external caching component, which is usually used to relieve the burden on developers, easing cache implementation, raising abstraction levels, or even taking full control of caching. For this question, the most valuable data comes from source code and comments, which express implementation details. The theoretical categories referring to RQ2 are described in Table 6 and shown in Figure 5."
2011.00242,"data, code",158,,,"should be placed and maintained. Although applicationlevel caching is commonly being adopted, it is typically implemented in an ad hoc way, and there are no available practical guidelines for developers to appropriately design, implement and maintain caching in their applications. To ﬁnd caching best practices, developers can make use of conventional wisdom, consult development blogs, or simply search online for tips. Nevertheless, this empirical knowledge is unsupported by concrete data or theoretical foundations that demonstrate its effectiveness in practice. Despite there are existing tool-supported approaches that can help developers to implement caching with minimal impact on the application code [4], [5], [6], they do not consider all the aforementioned issues and do not take application speciﬁcities into account, letting most part of the reasoning, as well as the integration with the tool, to developers."
2011.00242,"data, code",165,,,"As previously described, this study is mainly based on development information of web applications. To investigate different caching constructs, we followed a set of steps to perform our qualitative study: (i) selection of a set of suitable web applications; (ii) speciﬁcation of a set of questions to guide us in the data analysis and (iii) analysis of each web application using the speciﬁed questions. The collected data consists of six different sources of information, explained as follows. Information about the application. Our goal is to identify caching patterns or decisions, which possibly depend on the application domain. Therefore, we collected general details of the applications to characterize them. The collected application data is (i) its description; (iii) programming languages and technologies involved; and (iii) size of the application in terms of the number of lines of code."
2011.00242,"data, code",243,,,"We followed the analytical process of coding in our analysis [47], which makes it easier to search the data and identify patterns. This process combines the data for themes, ideas and categories, labeling similar passages of text with a code label, allowing them to be easily retrieved at a later stage for further comparison and analysis. We used what we learned from the analysis to adapt our evaluation approach and observation protocols. Insights we had while coding the data and clustering the codes were captured in memos. There are three coding phases in classical grounded theory: open coding, selective coding, and theoretical coding. Open coding generates concepts from the data that will become the building blocks for the theory [47]. The process of doing grounded theory is based on a concept indicator model of constant comparisons of incidents or indicators to incidents [50]. Indicators are actual data, such as behavioral actions and events observed or described in documents and interviews. In this case, an indicator may be an architectural style or design pattern adopted to implement the cache, a data structure, a class, a control ﬂow logic, a comment, a discussion in the issues platform, a paragraph in the documentation, or any other evidence we can get from the data being analyzed."
2011.00242,"data, code",50,,,"We observed in eight from the ten analyzed applications that developers indicated that they had uncertainty in cache design, with respect to deciding what data should be cached and the right moment to do it, causing missed caching opportunities. There are comments in the source code and"
2011.00242,"data, code",80,,,"We observed in eight from the ten analyzed applications that they present code scattering and tangling, on caching logic, causing low cohesion and high coupling in the code. Caching control code, responsible for caching data in particular places, was spread all over the base code, being invoked when application requests were processed. Consequently, there is a lack of separation of concerns, leading to increased maintenance effort (Evidence 15)."
2011.00242,"data, code",82,,,"In addition to design issues, as shown in Figure 1, the cache system and the underlying source of data are not aware of each other, and the application must implement ways to interact with the cache. Therefore, our goal with RQ2 is to characterize patterns of how this implementation occurs in the application code; for example, ways to assigning names to cached values, performing lookups, and keeping the cache up to date."
2011.00242,"data, code",92,,,"In this question, we focus on going beyond the facts exposed by source code and analyze the reasoning behind caching decisions such as what and why data is cached or evicted, when and where caching is done, and what are the conditions and constraints involved with this. Therefore, issues, code comments, and documentation about the cache of applications were the primary source of information to ﬁnd answers to this question, because they convey (in natural language) the rationale behind implementation decisions."
2011.00242,"data, code",93,,,"Based on data presented in Table 4, we can observe a signiﬁcant amount of lines of code dedicated to implementing caching, ranging from 0.27% to 3.02%. It shows the importance of the caching in the project, considering that caching is a solution for a non-functional requirement (i.e. scalability and performance). Furthermore, caching logic is presented in a substantial amount of ﬁles, from 2.06% to 10.76%, which indicates the caching nature of being spread all over the application."
2011.00242,"data, code, database",210,,,"Model or database layer. At the model or database layer, a large amount of data can be cached, for lengthy periods. It is useful to cache data from a database when it demands a long time to process queries, avoiding unnecessary round-trips. Stack caching layers. (Evidence 9) and (Evidence 10) It is reasonable to say that the more data cached, the lower the chance of being hit without any content already loaded. Caching might be at the client, proxy server, inside the application in presentation, business, and model logics, or database. Despite the same data may be cached in multiple locations, when the cache expires in one of them, the application will not be hit with an entirely uncached content, avoiding processing and network round trips. However, it is important to keep in mind that caching layers imply a range of responsibilities, such as consistency conditions and constraints, and extra code and conﬁguration. Due to this, it is important to consider many caching layers but, at the same time, achieve a good trade-off between caching beneﬁts and implementation effort."
2011.00242,"data, code, database",237,,,"A code comment detailing the motivation for caching: Fetch all dashboard data. This can be an expensive request when the cached data has expired, and the server must collect the data again. A user announcement on issue platform: Caching has now landed: Fragment caching for each product; Fragment caching for the lists of products in home/index and products/index; Caching in the ProductsController, using expires in which caches for 15 minutes. A sentence in the documentation: Most stores spend much time serving up the same pages over and over again. [...] In such cases, a caching solution may be appropriate and can improve server performance by bypassing time-consuming operations such as database access. A user message on issue platform detailing the invalidation approach adopted: Ideally we cache until the topic changes (a new post is added, or a post changed) [...] less ideally we cache for N minutes A user message on issue platform: i can see that it may speed up the query responses, but is the saved time substantial enough to be worth the effort? A code snippet deﬁning a default setting: <defaultCache maxElementsInMemory=“1000” eternal=“false” timeToIdleSeconds=“60” timeToLiveSeconds=“0” overﬂowToDisk=“false” diskPersistent=“false”/>"
2011.00242,"data, code, database, data available",282,,,"As already discussed in the introduction, the development of application-level caching involves four key issues: determining how, what, when, and where to cache data. The main problem is that solutions for all of these issues usually depend on application-speciﬁc details, and are manually designed and implemented by developers, as shown in the example presented in Listing 1. In this example, we assume that an e-commerce application where a class named ProductsRepository is responsible for loading products from database. To reduce database workload, caching logic is inserted in this class within the methods getProducts, updateProduct, and deleteProduct, which retrieves, updates and deletes content from the database, respectively. The ﬁrst issue is related to the cache-aside implementation and the fact that the cache system and the underlying source of data are not aware of each other (as illustrated in Figure 1). The cache system itself is a passive component, and developers must implement, directly in the application code, ways to assign names to cached values, perform lookups, and keep the cache up to date. The extra logic also requires additional testing and debugging time, which can be expensive in some scenarios. The implementation and maintenance of application-level caching are a challenge because developers must refactor all the data access logic to encapsulate cache data into the proper application-level object, and its direct management leads to a concern spread all over the system—mixed with business code—which leads to increasing complexity and maintenance time. Thus, this on-demand and manual caching process can be very"
2011.00242,"data, data available",115,,,"Developers. In addition to the manual inspection of the above data, we asked developers to which we had access about caching-related implementation, decisions, challenges, and problems. To guide the extraction of the information needed from the above data for our qualitative analysis, we derived sub-questions for each research question, which convey characteristics that should be observed and evaluated while looking for answers to the main research questions, i.e. sub-questions were used to extract data. Results presented in Section 4 are derived from answers to those questions. Table 2 depicts the sub-questions derived, which serve as a checklist while analyzing our data."
2011.00242,"data, data available",120,,,"Moreover, we observed that all analyzed web applications did not adopt caching since their conception. As they increased in size, usage and performance analysis were performed and led to requests for improvements. Thus, developers had to refactor data access logic to encapsulate cache data into the proper application-level object, which is a task that can be very time-consuming, error-prone and, consequently, a common source of bugs. As result, we found a signiﬁcant number of issues speciﬁcally related to caching, achieving the maximum of 4.99% of the Pencilblue issues, which can express in numbers the impact of caching in the entire project (Evidence 15)."
2011.00242,"data, data https, data available",32,,,"caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 859–874, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2005.89"
2011.00242,"data, data https, data available",48,04/21/22,0,"[38] Q. Yang and H. H. Zhang, “Web-log mining for predictive web caching,” IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 1050–1053, [Online]. Available: http://dx.doi.org/10.1109/TKDE.2003.1209022"
2011.00242,"data, data https, database, data available",56,,,"[24] P. Larson, J. Goldstein, and J. Zhou, “MTCache: Transparent mid-tier database caching in SQL server,” Proceedings of the International Conference on Data Engineering, vol. 20, pp. 177–188, mar 2004. [Online]. Available: http://dx.doi.org/10.1109/ICDE. 2004.1319994"
2011.00242,"data, database",138,,,"Evaluate different abstraction levels to cache. (Evidence 9) and (Evidence 11) It is important to cache data where it reduces the most processing power and round trips, choosing locations that support the lifetime needed for the cached items, despite where it is located in the application. Different levels of caching provide different behavior, and possibilities must be analyzed. For instance, caching in the model or database level offers higher hit ratios, while caching in presentation layer can reduce the application processing overhead signiﬁcantly in the application in case of a hit. However, in the latter case, hit ratios are in general lower. It is possible to cache data at various layers of an application, according to the following layer-by-layer considerations."
2011.00242,"data, database",74,,,"the business layer should be considered if an application needs to process requests from the presentation layer or when the data cannot be efﬁciently retrieved from the database or another service. It can be implemented by using hash tables, library or framework. However, at this level, a large amount of data tends to be manipulated and caching it can consume more memory and leads to memory bottlenecks."
2011.00242,"data, database",84,,,"layer should be considered when data needs to be frequently displayed to the user and is not cached on a peruser basis. At this level, controllers usually work by serving parameterized content, which can be used as an identiﬁer in the cache. For example, if a list of states is presented to the user, the application can load these once from the database and then cache them, according to the parameters passed in the ﬁrst request."
2011.00242,"data, database",89,,,"Recently, latency and cost of Internet-based services are driving the proliferation of application-level caching, which is placed on the server-side and typically uses a key-value inmemory store system to cache frequently accessed or expensive to compute data that remain not cached in other caching levels, lowering the load on database or services that are difﬁcult to scale up [3], [4], [5]. Therefore, application-level caching has become a popular technique for enabling highly scalable web applications."
2011.00242,"data, database, data available",237,,,"Figure 1 illustrates an example where an applicationlevel cache is used to lower the application workload. First, the application receives an HTTP request (Step 1) from the web infrastructure. This HTTP request is eventually treated by an application component called M1, which in turn depends on M2. However, M2 can be an expensive operation (i.e. request the database, call a service or process a large amount of data). Therefore, M1 implements a caching layer, which veriﬁes whether the necessary processing is already in the cache before calling M2 (Step 2). Then, the cache component performs a look up for the requested data and returns either the cached result or a not found error (Step 3). If data is found, it means a cache hit and M1 can continue its execution without calling M2. If, however, a not found error is returned, it means a cache miss happened, then M2 needs to be invoked (Steps 4 and 5). The newly fetched result of M2 can be cached to serve future requests faster (Step 6) and eventually a response is sent to the user (Step 7). Furthermore, other caching layers can be deployed outside the application, at the web infrastructure."
2011.00242,"data, database, data https",67,,,"[16] K. S. Candan, W.-S. Li, Q. Luo, W.-P. Hsiung, and D. Agrawal, “Enabling dynamic content caching for database-driven web sites,” Proceedings of the ACM SIGMOD International Conference on Management of Data, vol. 30, no. 2, pp. 532–543, jun 2001. [Online]. Available: http://dx.doi.org/10.1145/376284.375736"
2011.00242,"data, database, dataset provided",156,,,"Regarding design choices, we observed common practices. The ﬁrst is associated with the lack of a speciﬁc approach to cache data. To process a client request, application components of distinct layers and other systems (databases, web services, and others) are invoked, and each interaction results in data transformation, which is likely cacheable. Due to this, nine analyzed applications present multiple caching solutions by not specifying cacheable layers, com ponents or data, and employing cache wherever can potentially provide performance and scalability beneﬁts (Evidence 8), no matter which is the application layer, component or data (Evidence 9). As a consequence, the same content can be cached at different places, from the database to controllers, in varying forms such as query results, page fragments or lists of objects (Evidence 10)."
2011.00242,"data, database, dataset provided",185,,,"2 BACKGROUND AND RELATED WORK Focused on server-side, caching of dynamic web content can be implemented at several locations across a web-based system architecture [15]. Depending on where caching is implemented, different types of content can be stored, such as the ﬁnal HTML page [16], intermediate HTML or XML fragments [17], [18], [19], database queries [20], [21], [22], [23], or database tables [24]. These alternatives are conceived out of application boundaries and can cache dynamic data automatically, which provide transparent caching components for developers. However, they do not take application speciﬁcities into account, providing good results in general but are less effective where complex logic and personalized web content are processed within the application [25]. Therefore, application-level caching is an appealing technique to improve performance, reduce workload and make the overall user experience more pleasurable by reducing communication delays of web-based systems."
2011.00242,"data, dataset provided",200,,,"Classiﬁcation: Design Intent: provide an intuitive process to decide whether to cache or not particular data. Problem: cache has limited size, so it is important to use the available space to cache data that maximizes the beneﬁts provided to the application. Otherwise, it can end up reducing application performance instead of improving it, consuming more cache memory and at the same time suffering from cache misses, where the data is not getting served from cache but is fetched from the source. Solution: even though there are many criteria that contribute for identifying the level of data cacheability, there is a subset that would conﬁrm this decision regardless of the values of the other criteria. Changeability is the ﬁrst criterion that should be analyzed while selecting cacheable data, then usage frequency, shareability, retrieval complexity, and cache properties should be considered. Figure 6 expresses a ﬂowchart of the reasoning process to decide whether to cache data, based on the observation of data and cache properties. All criteria are tightly related to the application speciﬁcities and should be speciﬁed by the developer."
2011.00242,"data, dataset provided",37,,,"asynchronously with caching. Provide an intuitive process decide to whether to cache or not particular data. Given cacheable content, provide an intuitive process to choose a consistency management approach based on data speciﬁcities."
2011.00242,"data, dataset provided",85,,,"Deﬁne naming conventions. (Evidence 19) and (Evidence 18) To deﬁne appropriate names for cached data, it is important to assign a name that is related to its context, the data itself, and the caching location. It can provide two direct beneﬁts: (a) prevention of key conﬂicts, and (b) guidance of cache actions such as updates and deletes of stale data in case of changes in the source of information."
2011.00242,"data, dataset, dataset provided",314,,,"Rules of thumb: (a) Despite being frequently used, user-speciﬁc data are not shareable and may not bring the beneﬁt of caching, being usually avoided by developers. In this case, a speciﬁc session component is used to keep and retrieve user sessions. (b) If the data changes frequently, it should not be immediately discarded from cache. An evaluation of the performance beneﬁts of caching against the cost of building the cache should be done. Caching frequently changing data can provide beneﬁts if slightly stale data is allowed. (c) Expensive spots (when much processing is required to retrieve or create data) are bottlenecks that directly affect application performance and should be cached, even though it can increase complexity and responsibilities to deal with. Methods with high latency or that consists of a large call stack are some examples of this situation and opportunities for caching. In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects."
2011.00242,"data, dataset, dataset provided",338,,,"In addition, we list content properties that should be avoided, which do not convey the inﬂuence factors in a good way and lead to problems such as cache trashing. (a) User-speciﬁc data. Avoid caching content that varies depending on the particularities of the request, unless weak consistency is acceptable. Otherwise, the cache can end up being fulﬁlled with small and less beneﬁcial objects. As result, the caching component achieves its maximum capacity earlier and is ﬂushed or replaced many times in a brief period, which is cache thrashing. (b) Highly time-sensitive data. Content that changes more than is used should not be cached given that it will not take advantage from caching. The cost of implementing and designing an efﬁcient consistency policy may not be compensate. (c) Large-sized objects. Unless the size of the cache is large enough, do not cache large objects, it will probably result in a cache trashing problem, where the caching component is ﬂushed or replaced many times in a short period. Example: we list some typical scenarios where data should be cached and also give explanations based on the criteria presented. (a) Headlines. In most cases, headlines are shared by multiple users and updated infrequently. (b) Dashboards. Usually, much data need to be gathered across several application modules and manipulated to build a summarized information about the application. (c) Catalogs. Catalogs need to be updated at speciﬁc intervals, are shared across the application, and manipulated before sending the content to the client. (d) Metadata/conﬁguration. Settings that do not frequently change, such as country/state lists, external resource addresses, logic/branching settings and tax deﬁnitions. (e) Historical datasets for reports. Costly to retrieve or create and does not need to change frequently."
2011.00242,database,150,,,"caching at a granularity best suited to the application, providing a way to cache entire HTML pages, page fragments, database queries or even computed results; rather than laying in front of the application servers (e.g., a proxy-level cache) or between the application and the database (e.g., a query or database cache). For example, many websites have highly-personalized content, thus rendering wholepage web caches is mostly useless; application-level caches can be used to separate shared content from customized content, and then the shared content can be cached and reused among users [4]. Memcached [26] and Redis [27] are popular solutions and are a critical web infrastructure component for some big players such as Amazon, Facebook, LinkedIn, Twitter, Wikipedia, and YouTube [28]."
2011.00242,database,36,,,"components is a common bottleneck and, consequently, an opportunity for caching. Consider caching for database queries, remote server calls and requests to web services, which are made across a network."
2011.00242,database,76,,,"The second design choice is associated with the concern of reducing the communication latency between the application and other systems, which increases the overall response time of a user request. Therefore, we noticed caching in application boundaries in nine of the analyzed applications, addressing remote server calls and requests to web services, database queries, and loads dependent on ﬁle systems, which are common bottlenecks (Evidence 11)."
2011.00242,"database, code, github",162,,,"GitHub, the widely known common code hosting service. We selected GitHub projects that match the following criteria: (i) projects with some popularity (at least 350 stars); (ii) projects containing application-level caching implementation and issues (at least 50 occurrences of cache-related aspects); (iii) projects written in different programming languages; and (iv) projects of different domains. The ﬁrst criterion indicates that projects are interesting and were possibly evolved by people other than the initial developers. The second ensures that selected projects would present caching-related implementation and issues, which would contribute more to the study. The inclusion of applications was not restricted to any particular technology or programming language given that the study is focused on identifying language-independent caching patterns and guidelines expressed in the source code. Furthermore, we found applications that use cache in other architectural (e.g. database"
2011.00242,github,19,,,"[55] “Google Guava,” 2016. [Online]. Available: https://github.com/"
2011.00242,github,9,,,[Online]. Available: https://github.com/
2011.00242,open-source,113,,,"Therefore, we analyzed systems with a broad range of characteristics. Aiming at reducing the inﬂuence of particular software features on the results, we selected systems of different sizes (from 21 KLOC to 250 KLOC, without comments and blank lines), written in different programming languages, adopting different frameworks and architectural styles, and spanning different domains. We studied ten systems in total, of which nine are open-source software projects, and one is from the industry. Due to intellectual property constraints, we will refer to the commercial system as S1. Table 3 summarizes the general characteristics of each target system."
2011.00242,open-source,119,,,"are described in the issues, we classiﬁed them into three different categories, which follow the main topic of our research questions: Design (e.g. changes in the selection of content to be put in the cache), Implementation (e.g. bugs in the implementation) and Maintenance (e.g. performance tests, adjustments in replacement algorithms or size limits of the cache). Results of the performed analysis are presented in Figure 3, in which applications are ordered ascending by the number of cache-related issues, which is shown next to each application name. Only open source projects with issues related to caching are detailed in this ﬁgure."
2011.00242,open-source,134,,,"is implemented, the number of LOC associated speciﬁcally with caching (without comments and blank lines), and the number of issues related to it. This analysis is shown in Table 4, in the columns #Cache Files, Cache LOC and #Cache Issues, respectively. This table also gives an overview of further information of each application to which we had access. They are some form of documentation and access to developers. It is important to note that available documentation about caching, in most cases, was limited to an abstract or general description of how caching was adopted. This documentation was available in some open source systems, and we only had access to developers of our system from the industry."
2011.00242,open-source,89,,,"In order to investigate different aspects of caching, it was important to select representative systems that make extensive use of application-level caching. To obtain applications that employ application-level caching, we searched through open-source repositories, from which information can be easily retrieved for our study. Based on text search mechanisms, we assessed how many occurrences of cache implementation and issues were present in applications to ensure they would contribute to the study. The more caching-related implementation and issues, the better, so"
2011.00242,"open-source code, open-source",6,,,The open-source applications were selected from
2011.00242,python,2,04/21/22,0,PHP Python
2011.05411,data,10,,,"Original training data End-users Service Provider Service Provider, Third-parties"
2011.05411,data,101,,,"Along with the privacy-preserving techniques such as Secure Aggregation, diﬀerential privacy, and Homomorphic Encryption designated for protecting local ML parameters, the FL client application installed at end-users’ devices must be secure to prevent from unauthorised access, cyber-attack, or data breach directly from the devices or from the communications between the users’ devices and a coordination server. This precondition is same as any other systems in which a variety of security and privacy techniques are readily integrated into FL applications, as well as secure communications protocols such as IPSec, SSL/TLS and HTTPS"
2011.05411,data,102,,,"[124] Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F., 2015. Is feature selection secure against training data poisoning?, in: International Conference on Machine Learning, pp. 1689–1698. [125] Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., Beaufays, F., 2018. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903 ."
2011.05411,data,103,,,"Furthermore, the requirements of purpose limitation and data minimisation are not always feasibly carried out in MLbased systems. The majority of ML algorithms heavily rely on data quality and quantity, thus researchers tend to collect as much related data as possible. Therefore, determining 1) the purposes of data collection as well as 2) what data is adequate, limited, and relevant only to the claimed purposes before executing such ML algorithms are problematic challenges. These requirements overly restrict the natural operations of ML-based services and applications to a smaller range than ever before."
2011.05411,data,105,,,"The purpose of this principle is to ensure that a Data Controller should keep personal data correctly, updated, and not misleading any matter of fact. In centralised FL settings, a coordination server does not store any individual locally trained ML model parameters except the aggregated results from a batch of participants, and the anonymised global ML model. This information is stored and processed (i.e., for updating global model) in its original form without any changes, and updated for every training round. For these reasons, FL systems automatically satisfy the GDPR accuracy principle."
2011.05411,data,107,,,"Dr. Kai Sun is the Operation Manager of the Data Science Institute at Imperial College London. She received the MSc degree and the Ph.D degree in Computing from Imperial College London, in 2010 and 2014, respectively. From 2014 to 2017, she was a Research Associate at the Data Science Institute at Imperial College London, working on EU IMI projects including U-BIOPRED and eTRIKS, responsible for translational data management and analysis. She was the manager of the HNA Centre of Future Data Ecosystem in 20172018. Her research interests include translational research management, network analysis and decentralised systems."
2011.05411,data,11,,,to protect data in transit between clients and the server.
2011.05411,data,112,,,"ing set without having access to the original data. For instance, Hitaj et al. based on GANs have developed an attack at user-level which allows an insider to infer information from a victim just by analysing the shared model parameters in some consecutive training rounds [57]. This attack can be accomplished at client-side without interfering the whole FL procedure, even when the local model parameters are obfuscated using DP technique. A malicious coordination server can also recover partial personal data by inspecting the proportionality between locally trained model parameters sent to the server and the original data samples [4, 122]."
2011.05411,data,116,,,"In most of the real-world scenarios, data, particularly personal data, is generated and stored in data silos, either end-users’ devices or service providers’ data centres. Most of conventional ML algorithms are operated in a centralised fashion, requiring training data to be fused in a data server. Essentially, collecting, aggregating and integrating heterogeneous data dispersed over various data sources as well as securely managing and processing the data are non-trivial tasks. The challenges are not only due to transporting highvolume, high-velocity, high-veracity, and heterogeneous data across organisations but also the industry competition, the complicated administrative procedures, and essentially, the"
2011.05411,data,116,,,"We are now living in a data-driven world where most of applications and services such as health-care and medical services, autonomous cars, and ﬁnance applications are based on artiﬁcial intelligence (AI) technology with complex data-hungry machine learning (ML) algorithms. AI has been showing advances in every aspect of lives and expected to ""change the world more than anything in the history of mankind. More than electricity.” 1. However, the AI technology is yet to reach its full potential, also the realisation of such AI/ML-based applications has been still facing longstanding challenges wherein centralised storage and computation is one of the critical reasons."
2011.05411,data,119,,,"Dr. Nguyen B.Truong is currently a Research Associate at Data Science Institute, Imperial College London, United Kingdom. He received his Ph.D, MSc, and BSc degrees from Liverpool John Moores University, United Kingdom, Pohang University of Science and Technology, Korea, and Hanoi University of Science and Technology, Vietnam in 2018, 2013, and 2008, respectively. He was a Software Engineer at DASAN Networks, a leading company on Networking Products and Services in South Korea in 2012-2015. His research interest is including, but not limited to, Data Privacy, Security, and Trust, Personal Data Management, Distributed Systems, and Blockchain."
2011.05411,data,119,,,"[78] Lindell, Y., Pinkas, B., 2000. Privacy preserving data mining, in: Annual International Cryptology Conference, Springer. pp. 36–54. [79] Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, l-diversity: Privacy beyond k-anonymity. ACM Trans M., 2007. actions on Knowledge Discovery from Data (TKDD) 1, 3–es. [80] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017a. Communication-eﬃcient learning of deep networks from decentralized data, in: Artiﬁcial Intelligence and Statistics, pp. 1273– 1282."
2011.05411,data,120,,,"Large-scale data collection, aggregation and processing at a central server in such ML-based systems not only entail the risks of severe data breaches due to single-point-offailure but also intensify the lack of transparency, data misuse and data abuse because the service providers are in full control of the whole data lifecycle [118]. In addition, as ML algorithms operate in a black-box manner, it is also challenging to provide insightful interpretation of how the algorithms execute and how certain decisions are made [83, 91]. Consequently, most of the ML-based systems ﬁnd it diﬃcult to satisfy the requirements of transparency, fairness, and automated decision-making in the GDPR."
2011.05411,data,127,,,"ML is a disruptive technology for designing and building intelligent systems that can automatically learn and improve from experience to accomplish a task without being explicitly programmed. For this purpose, an ML-based system builds up a mathematical model (i.e., model training process) based on a sample set (i.e., training data) whose parameters are to be optimised during this training process. As a result, the system can perform better predictions or decisions on a new, unseen task. Typically, an ML task can be formulated as a mathematical optimisation problem whose goal is to ﬁnd the extremum of an objective function. Thus, an optimisation method is of paramount importance in any ML-based systems."
2011.05411,data,13,,,Keywords: Federated Learning Data Protection Regulation GDPR Personal Data Privacy Privacy Preservation
2011.05411,data,133,,,"Depending on encryption schemes and classes of computational operations that can be performed on an encrypted form, homomorphic encryption techniques are divided into diﬀerent categories such as partial, somewhat (SWHE), and fully homomorphic encryption (FHE)[2]. Some classic encryption techniques, including Rivest–Shamir–Adleman (RSA), is SWHE wherein simple addition and multiplication oper ations can be executed [2]. FHE, ﬁrstly proposed by Graig et al. in [45, 46], enables any arbitrary operations (thus, enables any desirable functionality) over cipher-text, yielding results in encrypted forms. In FHE, computation on the original data or the cipher-text can be mathematically transferred using a decryption function without any conﬂicts."
2011.05411,data,135,04/21/22,0,"3. An assessment of the data security and privacy risks that might be induced by each operation, along with the technical measures implemented to mitigate and manage the risks. For instance, in an FL system, the operation of sending local ML model parameters to a coordination server for global ML model update might be the target of inference attacks, thus, inducing privacy leakage. The measures called Secure Aggregation and Homomorphic Encryption mechanisms are implemented along with the technical report. Even though such privacy-preserving methods are implemented to strengthen FL systems, there exist some risks that can be exploited for illegitimate purposes such as model poisoning with back-door sub-tasks. These possible attacks, which lead to non-compliance with the GDPR, should be addressed."
2011.05411,data,136,,,"As depicted in Table 2, in FL settings, personal data is regarded as local model parameters, not the original data samples as in traditional cloud-based ML systems. A service provider, who implements an FL system, is Data Controller and Data Processor combined as the service provider (i) dictates end-users (i.e., Data Subject) to train an ML model using their local training data and to share such locally trained model, (ii) processes the local model parameters sent from end-users (i.e., aggregates and updates the global model), and (ii) disseminates the global models to all end-users and requests the end-users to update their local models. Furthermore, in centralised FL settings, a service provider can only"
2011.05411,data,136,,,"This purpose limitation principle can be interpreted that an FL service provider needs to clearly inform clients about the purpose of a global ML model training as well as how clients’ local personal data and devices’ computation are used to locally train a requested ML model provided by the service provider. The principle also states that the service provider can further process the data for other compatible purposes. In this respect, FL systems fully satisfy with the principles if suﬃcient privacy-preserving mechanisms such as Secure Aggregation and diﬀerential privacy are readily implemented into the systems. This is because locally trained ML models from clients are aggregated only for the global model updates and cannot be individually extracted and exploited (by the coordination server) for other purposes."
2011.05411,data,138,,,"clarify the whole data management processes along with the necessity and proportionality of these processes. Such assessments are important tools for accountability and essential to eﬃciently manage the data security and privacy risks, to demonstrate the compliance, as well as to determine the measure have been taken to address the risks. However, carrying out a DPIA or PIA is not mandatory for every data processing operation. It is only required when the operation is ""likely to result in a high risk to the rights and freedoms of natural persons"" (Article 35(1)). The guideline for the criteria on the DPIA/PIA obligatory is described under Article 35(3), 35(4) which are adopted by DPAs to carry out such assessments."
2011.05411,data,139,,,"Attackers might carry out model inversion (MI) attack to extract sensitive information contained in training data samples, for instance, by reconstructing representatives of classes which characterising features in classiﬁcation ML models [38]. MI attacks do not require the attacker to actively participate in the training process (i.e., black-box or passive attacks). For example, it is possible to recover images from a facial recognition model for a particular person (i.e., all class members depict this person) using MI by deriving a correct weighted probability estimation for the target feature vectors [112, 43]. In this scenario, the experiment results show that this MI attack can reconstruct images that are visually similar to the victim’s photos [38]."
2011.05411,data,142,,,"One of the fundamentals of FL is eﬃcient optimisation algorithms for federated settings wherein training data is nonIID, massively and unevenly distributed across local nodes, ﬁrst introduced by Konečn`y et al. in 2016 [70]. The distributed settings for the federated optimisation is formulated as follows. Let 𝐾 be the number of local nodes, ℙ𝑘 be the set of data samples stored on node 𝑘 ∈ {1, 2, .., 𝐾}, and be the number of data samples stored on node 𝑛𝑘 = |ℙ𝑘| 𝑘. As personal data in each local node is diﬀerent, we can assume that ℙ𝑘 ∩ ℙ𝑙 = ∅ if 𝑘 ≠ 𝑙 and ∑𝐾 𝑘=1 𝑛𝑘 = 𝑛. The distributed problem formulation for the minimisation objective is deﬁned as:"
2011.05411,data,142,04/21/22,0,"sonal data, any previous infringement, and the nature, gravity, and duration of the current infringement. For instance, Facebook and Google were hit with a collective $8.8 billion lawsuit (Facebook, 3.9 billion euro; Google, 3.7 billion euro) by Austrian privacy campaigner, Max Schrems, alleging violations of GDPR as it pertains to the opt-in/opt-out clauses. Speciﬁcally, the complaint alleges that the way these companies obtain user consent for privacy policies is an ""all-ornothing"" choice, asking users to check a small box allowing them to access services. It is a clear violation of the GDPR’s provisions per privacy experts and the EU/UK. A list of ﬁnes and notices (with non-compliance reasons) issued under the GDPR can be found on Wikipedia12"
2011.05411,data,146,,,"rameters updates and aggregation between local nodes and a central coordination server are strengthened by privacypreserving and cryptography techniques, which enhance data security and privacy [48, 123, 14, 15, 96]. The FL capability could potentially inaugurate new opportunities for service providers to implement some sorts of ML algorithms for their applications and services without acquiring clients’ personal data, hence naturally complying with data protection regulations like the GDPR. Unfortunately, despite the distributed collaborative learning model of FL empowered by privacy-preserving measures, personal information can be stealthily extracted from local training parameters [4, 96, 130, 57, 86]. As a consequence, FL-based service providers still stay within the regulatory personal data protection framework and are still liable for implementing GDPR-compliant mechanisms when dealing with EU/UK citizens."
2011.05411,data,148,,,"for the aggregation of independently trained neural networks in [95]. Since then, this technique has been improved to return statistically indistinguishable results among participants while ensuring that such noise-added model parameters do not aﬀect much on the accuracy of the global model in FL settings [111, 4, 48, 1, 114, 82]. As a consequence, adversaries cannot distinguish individual records in the FL training process and do not know whether or not a targeted client participating in the training; thus, preserving data privacy and protecting against the inference attacks. Generally, there are two types of employing diﬀerential privacy techniques for local nodes in FL settings: batch-level and userlevel where random noise is added by measuring parameters’ sensitivity from data points in a mini-batch and users themselves, respectively."
2011.05411,data,15,,,"Data Poisoning [11, 84, 124, 68, 23, 63]"
2011.05411,data,150,,,"Furthermore, local nodes not only passively contribute local training results but also get updated about intermediate stages of a global training model from a coordination server. This practice enables the opportunity for malicious participants to manipulate the training process by providing arbitrary updates in order to poison the global model [41, 9], which calls for an investigation on security models along with insightful analysis of privacy guarantees for a centralised FL framework. Accordingly, the FL framework then needs to be strengthened by employing further privacy and security mechanisms to protect personal data eﬀectively and to comply with intricate data protection legislation like the GDPR. A summary of related articles in terms of attack models with associated privacy preservation methods in centralised FL is depicted in Table 1. Detailed descriptions along with analysis are carried out in the following subsections."
2011.05411,data,151,,,"According to the ﬁrst principle, a service provider providing an FL application, as a Data Controller, must specify its legal basis in order to request end-users to participate in the FL training. There are total six legal bases required by the GDPR namely (1) Consent, (2) Contract, (3) Legal Obligation, (4) Vital Interest, (5) Public Task, and (6) Legitimate Interest (deﬁned in Article 6 of the GDPR in detail). These lawful bases might need to come along with other separate conditions for lawfully processing some special cate gory data including healthcare data, biometric data, racial and ethnic origin. Depending on speciﬁc purposes and context of the processing, the most appropriate one should be determined and documented before starting to process personal data."
2011.05411,data,154,,,"One of the privacy-preserving objectives of centralised FL is that a coordination server is unable to inspect the data or administer the training process at a local node. This, however, prohibits the transparency of the training process; thus, imposes a new vulnerability of a new type of attack called model poisoning [12, 87, 22, 41, 9, 6]. Generally, model poisoning attacks aim at manipulating training process by feeding poisoned local model updates to a coordination server. This type of attack is diﬀerent from data poisoning [11, 84, 124, 68, 23, 63], which is less eﬀective in FL settings [9, 6] because the original training data is never shared with a server. Thus, this section is mainly dedicated to analysing the model poisoning attacks in FL."
2011.05411,data,16,,,"[44] Gentry, C., 2010. Computing arbitrary functions of encrypted data."
2011.05411,data,166,,,"In this paper, we examine the centralised FL in which there exists a centralised server (i.e., service provider) requests to coordinate the whole training process. Speciﬁcally, this coordination server (i) determines a global model to be trained, (ii) selects participants (i.e., local nodes) for each training round, (iii) aggregates local training results sent by the participants, (iv) updates the global model based on the aggregated results, (v) disseminates the updated model to the participants, and (vi) terminates the training when the global model satisﬁes some requirements (e.g., accurate enough). Local nodes passively train the model over their local data as requested, and send the training results back to the server whenever possible. The workﬂow cycle in a centralised FL framework consists of four steps (illustrated in Fig. 2) as follows:"
2011.05411,data,169,,,"Nevertheless, both centralised and decentralised architectures are required to acquire model consistency, particularly when data parallelism is employed. There are numerous strategies to update parameters in order to maintain the consistency of a global model, respected to a synchronisaIn this regard, Asyntion model among compute nodes. chronous Parallel (ASP) [99, 29], Bulk Synchronous Parallel (BSP) [47], and Stale Synchronous Parallel (SSP) [58] are the most common approaches to update parameters in a distributed learning system. The BSP and the ASP update parameters once receiving all gradients from a bulk of compute nodes (barrier synchronisation) and from just any node (no synchronisation), respectively. Generally, the BSP is relatively slow due to the stall time of waiting whereas ASP is faster as it does not perform any synchronisation; as a tradeoﬀ, the convergence in BSP is guaranteed but uncertain in"
2011.05411,data,171,,,"can be generated by injecting a hidden backdoor model intentionally, as illustrated in Fig. 4. Compromised participants analyse the targeted global model; the poisoned model is then trained on backdoor data samples using dedicated techniques such as constrain-and-scale accordingly, and feed the parameters to a coordination server as other honest participants. The objective of this attack is that the global model is replaced by a joint model consisting of both original task and the injected backdoor sub-task while retaining high accuracy on the two. The backdoor training at the adversary can be empowered by modifying minimisation strategies such as constrain-and-scale, which optimises both gradients of the loss and the backdoor objective [6]. A parameter estimation mechanism is then used for generating parameters submitted to the coordination server for honest participants’ updates. As secure aggregation is used for preventing the server from inspecting individual models, this poisoning model is unable to detect [9, 6]."
2011.05411,data,177,,,"To ensure privacy, an FL system is designed in a way that does not let the service provider (i.e., the coordination server) to directly access and obtain either original training data or locally trained ML models at end-users’ devices. Instead, end-users, as participants in the FL system, will only send the results back to the coordination server when they are ready. An FL client-side application should oﬀer several options for clients to participate in the training process proactively that allows a client to fully control the local training as well as of the sending/receiving ML model updates to/from a coordination server. Furthermore, FL systems only process data (i.e., local ML model parameters) for an explicit purpose (i.e., aggregates results and updates a global model), which is in ways that clients would reasonably expect whilst having minimal privacy impact. For these reasons, either Consent or Legitimate Interest legal basis can be appropriate for an FL application10."
2011.05411,data,180,,,"In this federated setting, minimising the number of iterations in the optimisation algorithms is paramount of importance as there is limited communication capability of the local nodes. In the same paper, Konečn`y et al. proposed a novel distributed gradient descent by combining the Stochastic Variance Reduced Gradient (SVRG) algorithm [64, 72] with the Distributed Approximate Newton algorithm (DANE) [110] for distributed optimisation called Federated SVRG (FSVRG) [70]. The FSVRG computes gradients based on data on each local node 𝑘, obtains a weighted average of ℙ𝑘 the parameters from all the 𝐾 local nodes, and updates new parameters for each node after round. This algorithm is then experimented based on public Google+ posts, clustered by about 10, 000 users as local nodes, for predicting whether a post will receive any comments. The results show that the FSVRG outperforms the native gradient descent algorithm as it converges to the optimum within only 30 iterations."
2011.05411,data,181,04/21/22,0,"4. Privacy-Preservation in Centralised Federated Learning Framework As an ML model can be cooperatively trained while retaining training data and computation on-device, FL naturally oﬀers privacy-guarantee advantages compared to the traditional ML approaches. Unfortunately, although personal data is not directly sent to a coordination server in its original form, the local ML model parameters still contain sensitive information because some features of the training data samples are inherently encoded into such models [5, 81, 4, 96, 86]. For example, authors in [5] have shown that during the training process, correlations implied in the training data are concealed inside the trained models, and personal information can be subsequently extracted. Melis et al. have also pointed out that modern deep-learning models conceal internal representations of all kinds of features, and some of them are not related to the task being learned. Such unintended features can be exploited to infer some information about the training data samples. FL systems, consequently,"
2011.05411,data,186,,,"Model poisoning attacks are always inherent in collaborative learning including FL. As shown by Bagdasaryan et in [6], just by controlling less than 1% Byzantine paral. ticipants, an adversary can successfully insert a backdoor functionality into a global model without reducing much accuracy, preventing the coordination server from detecting the attack. Solutions to mitigate model poisoning attack at server-side have to detect and ﬁlter out poisoned model updates from malicious clients (i.e., model anomaly detection) [41, 63]. For this purpose, the server needs to access either participants’ training data or parameter model updates, which breaks the privacy-preservation catalyst of FL. Besides, Secure Aggregation protocol is assumed to be implemented at both client- and server-side, which prevents the server from inspecting individual model updates; consequently, ruling out any solutions for model poisoning attacks [41]. Indeed, no resolutions have been proposed that eﬀectively tackle model poisoning attacks at server-side, which imposes as a critical research topic for FL."
2011.05411,data,194,,,"The data minimisation principle in the GDPR necessitates a Data Controller (e.g., a service provider) to collect and process personal data that is adequate, limited, and relIn traditional centralised evant only to claimed purposes. ML algorithms, this data minimisation requirement is a challenge as it is not always possible to envision what data and the minimal amount of data are necessary for training an ML model. In this regard, FL appears as a game-changer as an FL system does not need to collect and process original training data; instead, a service provider only needs to gather local ML models from participants for assembling the global model. Generally, with privacy-preserving techniques introduced in Section 4, an FL system can assure that the coordination server obtains aggregated local model parameters from participants for global model updates only (i.e., the claimed purposes) while acquiring nothing about individual’s contribution. The aggregation mechanism also assures that the global model itself contains no individual sensitive features that can be exploited by adversaries to extract or infer any personal information."
2011.05411,data,196,,,"The challenge to provide this right to Data Subjects is that the GDPR demands the Data Controller to concisely, intelligibly, and speciﬁcally specify what and how the local ML model is used in the FL training, along with expected outputs of the mechanism11. Same as many complex ML mechanisms, FL is as a black-box model; thus, it cannot be precisely interpreted of how it works and predicting the outcomes. The GDPR supervisory board recognises the challenges and relaxes the requirement for AI/ML mechanisms by accepting a general explanation as an indication of how and what personal data is going to be processed. As a result, for an FL system, the right to be informed is achieved as privacy information including purposes for processing local ML model (i.e., to build a global ML model), retention periods (i.e., no longer in use after each training round), and who it will be shared with (only the coordination server) can be determined as in Terms and Conditions when a client accepts to participate in an FL system."
2011.05411,data,20,,,"[24] Chen, X.W., Lin, X., 2014. Big data deep learning: challenges and"
2011.05411,data,204,,,"Federated Learning Systems FL emerges a new approach to tackle data privacy challenges in ML-based applications by decoupling of data storage and processing (i.e., local model training) at end-users’ devices (i.e., local nodes) and the aggregation of a global ML model at a service provider’s server (i.e., a coordination server). The privacy-preservation advantage of FL compared to the traditional centralised ML approaches is undeniable: It enables to train an ML model whilst retaining personal training data on end-users’ devices. Only locally trained model parameters, which contain the essential amount of information required to update the global model, are shared with a coordination server. Nevertheless, such model parameters still enclose some sensitive features that can be exploited to reconstruct or to infer related personal information as depicted in Section 4. Subsequently, an FL system still retains within the GDPR and is liable for complying with obligatory requirements. This section closely examines whether a GDPR requirement should be complied or inapplicable and should be waived in FL settings. Unsolved challenges on fully complying with the GDPR are also determined and discussed."
2011.05411,data,205,,,"In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with diﬃculties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not suﬃcient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR."
2011.05411,data,210,04/21/22,0,"number of data samples and data distributions among personal mobile devices. Training over non-IID data has been shown to be much less accurate as well as slower convergence than IID data in federated settings [128]. Konečn`y with his colleagues at Google went further on improving the eﬃciency of the FSVRG algorithms in distributed settings by minimising the information in parameter update to be sent to an orchestration server [71]. Two types of updates are considered called structured updates and sketched updates in which the number of variables used in an ML model is minimised as many as possible, along with the compression of the information in the full model updates. Another ambitious federated optimisation approach is that local nodes are independently trained diﬀerent ML models as a task in a multi-learning objective simultaneously [113]. Generally, local nodes generate data under diﬀerent distributions which naturally ﬁt separate learning models; however, these models are structurally similar resulting in the ability to model the similarity using a multi-tasking learning (MTL) framework. Therefore, this approach improves performance when dealing with non-IID data as well as guarantees the learning convergence [113]."
2011.05411,data,22,,,"[59] Horvitz, E., Mulligan, D., 2015. Data, privacy, and the greater good."
2011.05411,data,234,04/21/22,0,"A Data Subject is assumed to have the right ""not to be subject to a decision based solely on automated processing, including proﬁling"" - Article 22(1), the GDPR. Therefore, an FL client, as a Data Subject, has the right to receive meaningful information and explanation about whether the result of the processing (i.e., a global ML model) used in an automated decision-making system will produce legal eﬀects concerning the client or similarly signiﬁcantly aﬀects the client. Unfortunately, due to the black-box operation model and the limitation of the transparency in ML, including FL, training process, results (e.g., a global ML model in FL) are generally generated without any proper explanation [119]. Thus, it is infeasible to predict whether outcomes of an ML model might aﬀect the legal status or legal rights of the Data Subject, or negatively impact on its circumstances, behaviour or choices. Consequently, any FL system fails to comply with the GDPR requirements of the data subject’s right in control of automated decision making. Fortunately, this requirement can be neglected if a Data Controller explicitly mentions the lack of automated decision making and proﬁling right when asking for Data Subject’s consent to process personal data."
2011.05411,data,242,,,"data; instead, inferring attributes or membership of the original trained data from local model parameters can also induce serious privacy leakage [42, 85, 86, 93, 94] (e.g., an attacker can ﬁgure out whether a speciﬁc data sample (of a patient) is used to train a model of a disease). This is the baseline for the membership attack. Authors in [85, 86, 94] have investigated membership attacks in FL and demonstrated the capability of these attacks in both passive and active approaches. For instance, the gender of a victim can be inferred with a very high accuracy of 90% when conducting this attack in a binary gender classiﬁer on the FaceScrub dataset7. Other features, which are uncorrelated with the main task, can also be inferred such as race and facial appearance (e.g., whether a face photo is wearing glasses) [86]. Nasr et al. proposed an active attack approach called gradient ascent by exploiting the privacy vulnerabilities of SGD optimisation algorithms. This attack based on the correlation between the local gradients of the loss and the direction and the amount of changes of model parameters when minimising the loss to ﬁt a model to train data samples in the SGD algorithms. This active membership attack was conducted on the CIFAR100"
2011.05411,data,279,,,"The nature of decoupling between data storage and processing at client-side and global ML model aggregation at server-side in centralised FL leads to the unnecessity of providing the (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure, (5) Right to restrict processing, (6) Right to data portability, and (7) Right to object. For instance, regarding the ""Right to erasure"", if a user requests to delete its data (i.e., local ML model parameters sent to an FL server), literally, the only way to fulﬁl the user’s request is to thoroughly re-train the global model without using user’s data from the round that the user ﬁrst participates [50]. This is unnecessary and impractical in FL settings as only local ML model parameters (possibly privacy guarantee-strengthened with diﬀerential privacy) in aggregated encrypted forms (by using Secure Aggregation and other advanced cryptography techniques) are shared with a coordination server. Consequently, it is worthless for a Data Subject to have control over its local ML model as (i) the model parameters are protected by privacy-preserving techniques from inference attacks; (ii) the server is unable to separate the user’s data from the others, the server also does not store the model once it is aggregated to update the global model; and (iii) the global model is wholly anonymised and cannot be exploited to extract or infer any individual information."
2011.05411,data,283,,,"Standing on these federated optimisation research works, McMahan et al. proposed a variation of the SGD called FederatedSGD along with the Federated Averaging algorithm that can train a deep network at 100 times fewer communications compared to the naive FSVRG [81, 80]. The catalyst of such algorithms is to leverage the increasingly powerful processors in modern personal mobile devices to perform high-quality updates than simply calculating gradient steps. Speciﬁcally, each client not only calculates the gradients but also computes the local model for multiple times; the coordination server only performs aggregation of the local models from the clients. This results in fewer training rounds iterations (thus fewer communications) while producing a decent global model. These proposed algorithms well suited for scenarios that are highly limited communication bandwidth with high jitter and latency. In these scenarios, the naive FSVRG algorithms proposed in [70, 71] are not eﬃcient enough. Indeed, the algorithms are utilised for a realworld application for text prediction in Google keyboard in Android smartphones (i.e., G-board)5 [125]. In this system setting, the FederatedSGD is executed locally on the smartphone to compute gradient descent using local data. The gradient is then sent to an aggregation server. This server performs the FederatedAveraging algorithm which randomly selects a fraction of smartphones for each training round, and takes the average of all gradients sent from the selected participants to update the global model. This updated global model is distributed to all participants; the local nodes will then update their local models accordingly."
2011.05411,data,293,,,"As FL is in the early stage, a fruitful area of multi-disciplinary research is commenced in order to ﬂourish the technology and to comply with the GDPR fully. Firstly, eﬃcient cryptographic and privacy primitives for decentralised collaborative learning must be further developed, particularly for counteracting model poisoning and inference attacks. Furthermore, as these privacy-preserving techniques such as SMC impose non-trivial performance overheads, further eﬀort on how to eﬃciently utilise such techniques on FL applications are required. Secondly, research on transparency, interpret-ability and algorithm fairness in FL systems should be profoundly carried out. Even though a sub stantial amount of research has been conducted in centralised AI/ML settings, there is still an open question whether these approaches could be employed and how to sensibly adapt them to the decentralised settings where training data is highly skewed non-IID and unevenly distributed across sources. The sampling constraints should be investigated to see how much extend they aﬀect and how to mitigate the bias of the global training model. For instance, the agnostic FL framework introduced in [89] naturally yields good-intent fairness as it modelled the target distribution as an unknown mixture of the distributions instead of the uniform distribution in typical FL training algorithms. This agnostic FL framework, as a result, can control for bias in the training objective. Thirdly, it requires more research on interpretable and unbiased ML models and algorithms that can be employed over encrypted settings to well consolidate with advanced encryption schemes in FL systems. Besides, the trade-oﬀs between privacy utility, accuracy, interpretability, and fairness in an FL framework need to be thoroughly explored."
2011.05411,data,3,04/21/22,0,11https://ico.org.uk/for-organisations/guide-to-data-protection/guideto-the-general-data-protection-regulation-gdpr/individual-rights/right-tobe-informed
2011.05411,data,31,,,This research was supported by the HNA Research Centre for Future Data Ecosystems at Imperial College London and the Innovative Medicines Initiative 2 IDEA-FAST project under grant agreement No 853981.
2011.05411,data,316,,,"This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]. We present a systematic summary of the threat models, possible attacks, and the privacy-preserving techniques in FL systems, along with the analysis of how these techniques can mitigate the risk of privacy leakages. Furthermore, insightful analysis of how an FL-system complies with the GDPR is also provided. Obligations and appropriate measures for a service provider to implement a GDPR-compliant FL system are examined in details following the rational guidelines of the GDPR six principles."
2011.05411,data,32,,,"member of the Academia Europaea. His research interests are in the areas of data mining for largescale scientiﬁc applications including distributed data mining methods, machine learning and informatics systems."
2011.05411,data,330,,,"AI/ML-based applications and services are high on the agenda in most sectors. However, the unregulated use or misuse of personal data is dramatically spreading, resulting in severe concerns of data privacy. A series of severe personal data breaches such as Facebook’s Cambridge Analytica scandal, along with urgent mobile applications during the SARS-CoV2 pandemic for large-scale contact tracing and movement tracking [61] trigger worldwide attention respecting to a variety of privacy-related aspects including algorithm bias, ethics, implications of politic settings, and legal responsibility. This leads to a critical demand for eﬀective privacy-preserving techniques, particularly for ""datahungry"" AI/ML-based systems, wherein FL is a prospective solution. The decoupling between local storage and processing at end-users devices and the aggregation of processing results at server-side in FL undoubtedly mitigate the risk of massive data breaches in a traditional centralised system while giving full control of personal data back to the users. Although FL is in its infancy, we strongly believe that the collaborative computation with decentralised data storage as in FL systems has tremendous advantages to facilitate a variety of AI/ML-based applications without directly accessing end-users’ data. Thus, FL systems are presumed to naturally comply with strict data protection legislation such as the GDPR. However, such FL systems still stay within the GDPR regulatory data protection framework as the local processing results sent to a server from end-users (e.g., locally trained ML model parameters) conceal some sensitive features that can be exploited to infer personal information of the end-users. Accordingly, FL systems are the target of some types of attack such as inference attacks and model poisoning, which could lead to infringements of the GDPR. Therefore, the systems must be strengthened by applicable privacy mechanisms such as SMC, diﬀerential privacy, and encrypted transfer learning methods [106]."
2011.05411,data,35,,,"[128] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V., arXiv preprint Federated learning with non-iid data."
2011.05411,data,37,,,"It is worth noting that standard distributed ML algorithms are generally designed to train independent identically-distributed (IID) data, and this assumption does not hold in federated settings due to the signiﬁcant diﬀerences of the"
2011.05411,data,37,,,"[108] Sarwate, A.D., Chaudhuri, K., 2013. Signal processing and machine learning with diﬀerential privacy: Algorithms and challenges for continuous data. IEEE signal processing magazine 30, 86–94."
2011.05411,data,38,,,• Provide insightful examination on pros and cons of the existing privacy-preserving techniques as well as prospective solution approaches in order for a FL-based service to comply with the EU/UK General Data Protection Regulation (GDPR).
2011.05411,data,4,04/21/22,0,10https://ico.org.uk/for-organisations/guide-to-data-protection/guide to-the-general-data-protection-regulation-gdpr/lawful-basis-forprocessing/
2011.05411,data,4,04/21/22,0,2.2.1. Data Anonymisation
2011.05411,data,4,04/21/22,0,5.2.3. Data Minimisation
2011.05411,data,40,,,"[23] Chen, X., Liu, C., Li, B., Lu, K., Song, D., 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 ."
2011.05411,data,42,,,"[36] Dwork, C., Smith, A., Steinke, T., Ullman, J., 2017. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application 4, 61–84."
2011.05411,data,45,,,"[50] Ginart, A., Guan, M., Valiant, G., Zou, J.Y., 2019. Making ai forget you: Data deletion in machine learning, in: Advances in Neural Information Processing Systems, pp. 3518–3531."
2011.05411,data,46,,,"[31] Du, W., Han, Y.S., Chen, S., 2004. Privacy-preserving multivariate statistical analysis: Linear regression and classiﬁcation, in: Proceedings of the 2004 SIAM international conference on data mining, SIAM. pp. 222–233."
2011.05411,data,53,,,"The natural advantage of FL compared to the traditional cloud-centric ML approaches is the ability to reassure data privacy and (presumably) comply with the GDPR because personal data is stored and processed locally, and only model parameters are exchanged. In addition, the processes of pa 2https://gdpr-info.eu/"
2011.05411,data,54,04/21/22,0,"1. A systematic description of data processing operations, associated purposes, along with clariﬁcation and justiﬁcation of the operations. For instance, the operation of asking Data Subject’s consent for local ML training and sending the ML model parameters to a coordination server should be documented in detail."
2011.05411,data,54,,,"Even though homomorphic encryption oﬀers rigorous privacy-guarantee to individuals as the original data in plaintext has never been disclosed, there is a practical limitation in performing computation over cipher-text due to the tremendous computational overhead. As a consequence, employing homomorphic encryption in large-scale data training remains impractical [49]."
2011.05411,data,55,,,"[14] Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K., 2016. Practical secure aggregation for federated learning on user-held data. arXiv preprint arXiv:1611.04482 ."
2011.05411,data,55,,,"[49] Gilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M., Wernsing, J., 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy, in: International Conference on Machine Learning, pp. 201–210."
2011.05411,data,56,,,"[34] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M., 2006. Our data, ourselves: Privacy via distributed noise generation, in: Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer. pp. 486–503."
2011.05411,data,56,,,"[5] Ateniese, G., Mancini, L.V., Spognardi, A., Villani, A., Vitali, D., Felici, G., 2015. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classiﬁers. International Journal of Security and Networks 10, 137–150."
2011.05411,data,62,,,"Diﬀerential privacy technique has been widely employed in various ML algorithms such as linear and logistic regression [19], Support Vector Machine (SVM) [102] and deep learning [20, 1], as well as in ML-based applications such as data mining [39] and signal processing with continuous data [108]."
2011.05411,data,62,,,"This principle obligates Data Controllers to implement appropriate measures in place to eﬀectively protect personal data. Thus, in order to comply with this principle, a centralised FL system requires to implement security and privacy mechanisms not only at a coordination server but also at end-users’ devices as the FL system itself does not guarantee security and privacy."
2011.05411,data,66,04/21/22,0,The GDPR deﬁnes 6-core principles as rational guidelines for service providers to manage personal data as illustrated in Fig. 7 (The GDPR Articles 5-11). These principles are broadly similar to the principles in the Data Protection Act 1998 with the accountability that obligates Data Controllers to take responsibility for complying with the principles and implementing appropriate measures to demonstrate the compliance.
2011.05411,data,68,,,"Basically, this principle ensures that a Data Controller does not keep personal data for longer if the data is no longer needed for the claimed purposes. In this case, data should be erased or anonymised. There is an exception for data retention only if the Data Controller keeps the data for public interest archiving, scientiﬁc or historical research, or statistical purposes."
2011.05411,data,75,,,"Mr. Siyao Wang is a PhD student of the Data Science Institute at Imperial College London. He received the BSc degree in Computer Science and Technology from the University of Chinese Academy of Sciences in 2018. He received the MRes degree in Medical Robotics and ImageGuided Intervention from Imperial College London in 2019. His research interests include machine learning, deep learning, computer vision and artiﬁcial intelligence applications in healthcare."
2011.05411,data,75,,,"[61] Ienca, M., Vayena, E., 2020. On the responsible use of digital data to tackle the covid-19 pandemic. Nature medicine 26, 463–464. [62] Jagannathan, G., Wright, R.N., 2005. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 593–599."
2011.05411,data,77,04/21/22,0,"3. Federated Learning: A Distributed Collaborative Learning Approach In many scenarios, the traditional cloud-centric ML approaches are no longer suitable due to the challenges of complying with strict data protection regulations on vast aggregation and processing personal data. By nature, most personal data is generated at the edge by end-users’ devices (e.g., smart phones, tablets, and wearable devices) which are equipped with increasingly powerful computing capability"
2011.05411,data,8,,,Personal Data Data Subject Data Controller Data Processor
2011.05411,data,81,,,"[39] Friedman, A., Schuster, A., 2010. Data mining with diﬀerential privacy, in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 493–502. [40] Fung, B.C., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: A survey of recent developments. ACM Computing Surveys (Csur) 42, 1–53."
2011.05411,data,85,,,"[118] Truong, N.B., Sun, K., Lee, G.M., Guo, Y., 2019. Gdpr-compliant personal data management: A blockchain-based solution. IEEE Transactions on Information Forensics and Security 15, 1746–1761. [119] Wachter, S., Mittelstadt, B., Floridi, L., 2017. Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7, 76– 99."
2011.05411,data,87,,,"As illustrated in Fig. 8, the investigation of non-compliance and decision of punishment are carried out by DPAs once there is a suspicion or a claim ﬁled by a customer. The compliance inspection will conduct some analysis to see whether a suspicious organisation follows the legal requirement of Privacy&Security-by-design approach and satisﬁes some standard assessments such as Data Protection Impact Assessment (DPIA) and Privacy Impact Assessment (PIA), which are essential parts of the GDPR accountability obligations."
2011.05411,data,87,,,"data protection regulations and restrictions such as the EU General Data Protection Regulation (GDPR)2 [59]. In traditional ML algorithms, large-scale data collection and processing at a powerful cloud-based server entails the singlepoint-of-failure and the risks of severe data breaches. Foremost, centralised data processing and management impose limited transparency and provenance on the system, which could lead to the lack of trust from end-users as well as the diﬃculty in complying with the GDPR [118]."
2011.05411,data,94,,,"The GDPR requires Data Controllers to provide the following rights for Data Subjects if capable (The GDPR Articles 12-23): (1) Right to be informed, (2) Right of access, (3) Right to rectiﬁcation, (4) Right to erasure (Right to be forgotten), (5) Right to restrict processing, (6) Right to data portability, (7) Right to object, and (8) Rights in relation to automated decision making and proﬁling."
2011.05411,data,94,,,"methods. Frontiers in Applied Mathematics and Statistics 3, 9. [73] Li, N., Li, T., Venkatasubramanian, S., 2007. t-closeness: Privacy beyond k-anonymity and l-diversity, in: 2007 IEEE 23rd International Conference on Data Engineering, IEEE. pp. 106–115. [74] Li, O., Liu, H., Chen, C., Rudin, C., 2017. Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions. arXiv preprint arXiv:1710.04806 ."
2011.05411,data,95,,,"Dr. Yike Guo (FREng, MAE) is the director of the Data Science Institute at Imperial College London and the Vice-President (Research and Development) of Hong Kong Baptist University. He received the BSc degree in Computing Science from Tsinghua University, China, in 1985 and received the Ph.D in Computational Logic from Imperial College London in 1993. He is a Professor of Computing Science in the Department of Computing at Imperial College London since 2002. He is a fellow of the Royal Academy of Engineering and a"
2011.05411,data,95,,,"Mr. Florian Guitton received a BSc in Software Engineering from Epitech (France) in 2011 and a MSc in Advanced Computing from the University of Kent (United Kingdom) in 2012. In 2012 he joined the Discovery Sciences Group at Imperial College London where he became Research Assistant working on iHealth, eTRIKS and IDEA-FAST EU programs. He is currently a PhD candidate at Data Science Institute, Imperial College London working on distributed data collection and analysis pipeline in mixed-security environments with the angle of optimising user facing experiences."
2011.05411,data,98,,,"share a global ML model, which can be considered as anonymous information, with third-parties as it does not possess any other personal data (e.g., original training data as in traditional ML systems). Therefore, Data Processors in FL settings are also the service providers, but not other players (i.e., third-parties). The processing mechanisms in FL are also uncomplicated compared to the traditional ones as they are only related to the aggregation of the local ML models as well as the update of the global ML model."
2011.05411,"data, data available",150,,,"Normally, DPAs might require a variety of information with a detailed explanation from Data Controller to perform the analysis including documents of organisational and technical measures related to the implementation the GDPR requirements as well as independent DPIA and PIA reports frequently conducted by the Data Controller. DPAs may also require to be given access to data server infrastructure and management system including personal data that is being processed. In this respect, besides the legal basis such as consents from end-users, an FL service provider can only provide documentation of how FL-related mechanisms are implemented along with privacy-preserving technical measures such as secure aggregation, diﬀerential privacy, and homomorphic encryption. Other inquiries from DPAs such as direct access to the FL model training operations and inspection of individual local model parameters from a particular end-user are technically infeasible for any FL systems."
2011.05411,"data, data available",85,,,"FL settings. Training data in FL is unbalanced and non-IID, which is scattered across millions of personal mobile devices with signiﬁcant higher-latency, lower-throughput connections compared to the traditional techniques working on a cloud-centric data server. In addition, the data and computing resources in personal devices are only intermittently available for training. Therefore, to actualise FL, optimisation algorithms must be well adapted and eﬃciently performed for federated settings (i.e., federated optimisation [70])."
2011.05411,"data, dataset",114,,,"Another approach to preserve data privacy and security in ML is to utilise homomorphic encryption techniques, particularly in centralised systems, e.g., cloud servers, wherein data is collected and trained at a server without disclosing the original information. Homomorphic encryption enables the ability to perform computation on an encrypted form of data without the need for the secret key to decrypt the ciphertext [44]. Results of the computation are in encrypted form and can only be decrypted by the requester of the computation. In addition, homomorphic encryption ensures that the decrypted output is the same as the one computed on the original unencrypted dataset."
2011.05411,"data, dataset",120,,,"As aforementioned, a trained ML model contains unintended features that can be utilised to extract personal information. Thus, local ML model parameters from a federated optimisation algorithm can be exploited by an adversary to infer personal information, particularly when combining with related information such as model data structure and meta-data. This information can be either original training data samples (i.e., reconstruction attack) [38, 111, 81, 4, 57, 96, 112, 6, 93, 130, 43] or membership tracing (i.e., to check if a given data point belongs to a training dataset) [15, 112, 86]."
2011.05411,"data, dataset",136,,,"The authors in [1] have proposed an SGD algorithm integrated with diﬀerential privacy performing over some batches (a group) of data samples. This algorithm estimates the gradient of the group by taking the average of the gradient loss of these batches and adds noise (generated by Gaussian mechanism) to the group to protect the privacy. This algorithm is implemented to train on the MNIST and CIFAR-10 datasets showing sensible results as it achieves only 1.3% and 7% less accurate compared to the non-diﬀerentially private conventional baseline algorithms on the same datasets, respectively. Similar to the mechanism proposed by Shokri and Shmatikov in [111], the authors have proposed a mechanism to monitor the total privacy budget (i.e., privacy accounting)"
2011.05411,"data, dataset",161,,,"Generally, there are three gradient descent methods that are categorised based on the amount of training data used in the gradient calculation of the objective function 𝑓 (𝜃) [103]. The ﬁrst category is batch gradient descent, in which the gradients are computed over the entire training dataset  for one update. The second category is stochastic gradient descent (SGD), that, in contrast to batch gradient descent, randomly selects a sample (or a subset) from  and performs the parameters update based on the gradient of this sample only (one sample per step, the whole process sweeps through the entire dataset). The third one is mini-batch gradient descent in which the dataset is subdivided into mini-batches of 𝑛 training samples (𝑛 is the batch-size); the parameters update is then performed on every mini-batch (single minibatch per step)."
2011.05411,"data, dataset",172,,,"Furthermore, the local nodes can leverage the perturbation method to prevent a coordination server and other adversaries from disclosing model parameters updates and original training dataset. The idea of employing perturbation technique to FL is that a local node adds random noise to its local model parameters in order to obscure certain sensitive attributes of the model before sharing. As a result, adversaries, in case it can successfully derive such model parameters, is unable to accurately reconstruct the original training data or infer some related information. In other words, the perturbation method could prevent adversaries from carrying out inference attacks on a local model trained by a particular client. This privacy-preservation method typically adopts diﬀerential privacy technique that adds random noises to either training dataset or model parameters, oﬀering statistical privacy guarantees for individual data [35, 33, 7]. Indeed, before the proposal of FL, diﬀerential privacy with SMC has been suggested as a privacy-preserving technique"
2011.05411,"data, dataset",178,,,"Data anonymisation or de-identiﬁcation is a technique to hide (e.g., hashing) or remove sensitive attributes, such as personally identiﬁable information (PII), so that a data subject cannot be identiﬁed within the modiﬁed dataset (i.e., the anonymous dataset) [92]. As a consequence, data anonymisation has to balance well between privacy-guarantee and utility because hiding or removing information may reduce the utility of the dataset. Furthermore, when combined with auxiliary information from other anonymous datasets, a data subject might be re-identiﬁed, subjected to a privacy attack called linkage attack [40]. To prevent from linkage attack, numerous techniques have been proposed such as k-anonymity [116], l-diversity [79], a k-anonymity-based method, and tcloseness - a technique built on both k-anonymity and l-diversity that preserves the distribution of sensitive attributes in a dataset so that it reduces the risk of re-identifying a data subject in a same quasi-identiﬁer group [73]."
2011.05411,"data, dataset",186,,,"To overcome such challenges, Federated Learning (FL), proposed by Google researchers in 2016, has appeared as a promising solution and attracted attention from both industry and academia [70, 71, 81, 80]. Generally, FL is a technique to implement an ML algorithm in decentralised collaborative learning settings wherein the algorithm is executed on multiple local datasets stored at isolated data sources (i.e., local nodes) such as smart phones, tablet, PCs, and wearable devices without the need for collecting and processing the training data at a centralised data server. FL allows local nodes to collaboratively train a shared ML model while retaining both training dataset and computation at internal sites [70]. Only results of the training (i.e., parameters) are exchanged at a certain frequency, which requires a central server to coordinate the training process (centralised FL) or utilises a peer-to-peer underlying network infrastructure (i.e., decentralised FL) to aggregate the training results and calculate the global model."
2011.05411,"data, dataset",199,,,"In this regard, FL is an alternative for the cloud-centric ML technique that facilitates an ML model to be trained collaboratively while retaining original personal data on their devices, thus potentially mitigates data privacy-related vulnerabilities. It is a cross-disciplinary technique covering multiple computer science aspects including ML, distributed computing, data privacy and security that enables end-users’ devices (i.e., local nodes) to locally train a shared ML model on local data. Only parameters in the training process are exchanged for the model aggregation and updates. The diﬀerence between FL and the standard distributed learning is that in distributed learning, local training datasets in compute nodes are assumed to be independent and identically distributed data (IID) whose their sizes are roughly the same. FL is, thus, as an advancement of distributed learning as it is designed to work with unbalanced and non-independent identically-distributed data (non-IID) whose sizes may span several orders of magnitude. Such heterogeneous datasets are resided at a massive number of scattering mobile devices under unstable connectivity and limited communication bandwidth [81, 80, 65]."
2011.05411,"data, dataset",200,,,"SMC, also known as multi-party computation (MPC) or privacy-preserving computation, was ﬁrstly introduced by Yao in 1986 [126] and further developed by numerous researchers. Its catalyst is that a function can be collectively computed over a dataset owned by multiple parties using their own inputs (i.e., a subset of the dataset) so that any party learns nothing about others’ data except the outputs [51, 18, 27]. Speciﬁcally, 𝑛 parties 𝑃1, 𝑃2, .., 𝑃𝑛 own 𝑛 pieces of pri, respectively to collectively comvate data 𝑋1, 𝑋2, ..., 𝑋𝑛 pute a public function 𝑓 (𝑋1, 𝑋2, .., 𝑋𝑛) = (𝑌1, 𝑌2, .., 𝑌𝑛). The only information each party can obtain from the computation is the result (𝑌1, 𝑌2, .., 𝑌𝑛) and its own inputs 𝑋𝑖 . Classical secret sharing such as Shamir’s secret sharing [109, 17] and veriﬁable secret sharing (VSS) schemes [26] are the groundwork for most of the SMC protocols."
2011.05411,"data, dataset",217,,,"Regarding the Fairness and Transparency requirements, as AI/ML algorithms like deep learning are normally operated in a black-box fashion, it is limited of transparency of how certain decisions are made, as well as limited understanding of the bias in data samples and training process [30, 83, 3, 91]. An FL system is not an exception. Generally, if the training data is poorly collected or intentionally prejudicial and fed to an ML, including FL, system, the results apparently turn out to be biased. If the trained model is then utilised for an automated decision-making system, then it probably leads to discrimination and injustice. Furthermore, the nature of preventing service providers from accessing original training dataset as well as the inability to inspect individuals’ locally trained ML model due to Secure Aggregation mechanism ampliﬁes the lack of transparency and fairness in FL systems. As a result, an FL system ﬁnds it problematic to transparently execute the training operations as well as to ensure any automated decisions from the system are impartially performed. This, consequently, induces the impracticality for any FL systems and fails to fully comply with the GDPR requirements of fairness and transparency."
2011.05411,"data, dataset",226,,,"Geyer et al. in [48] have developed another method to implement diﬀerential privacy for federated optimisation in FL settings that conceals the participation of a user in a training task; as a result, the whole local training dataset of the user is protected against diﬀerential attacks. This approach is diﬀerent from the batch-level one, which aims at protecting a single data point in a training task. The proposed method utilises a similar concept of privacy accounting from [1] that allows a coordination server to monitor the accumulated privacy budget by observing the moment accountant and privacy loss proposed in [1]. The training process is halted once the accumulated privacy budget reaches a pre-deﬁned threshold, implying that the privacy guarantee is no further tolerated. The Gaussian mechanism is also used to generate random noise which is then added to distort the sum of gradients updates to protect the whole training data. The proposed method has been experimented on MNIST dataset, and the results show that with a suﬃciently large number of participants (e.g., about 10,000 clients), the accuracy of the FL trained model almost achieves as high as the nondiﬀerential-privacy baseline while a certain level of privacy guarantee over the local training data still holds."
2011.05411,"data, dataset",67,,,"In traditional ML approaches, this sort of algorithms performs a vast number of fast iterations over a large dataset homogeneously partitioned in data servers. Such algorithms require super low-latency and high-throughput connections to the training data [80]. Therefore, solving this optimisation problem in the context of FL is diﬀerent from the traditional ML approaches as such conditions do not hold in"
2011.05411,"data, dataset",71,,,"Reconstruction attacks using MI and GANs are only feasible if and only if all class members in an ML model are analogous which entails a similarity between the MI/GANreconstructed outputs and the training data (e.g., facial recognition of a speciﬁc person, or MNIST dataset for handwritten digits6 used in [4]). Fortunately, this precondition is less practical in most of the FL scenarios."
2011.05411,"data, dataset",90,,,"learning system target two main objectives: (i) privacy of the training dataset and (ii) privacy of the local model parameters (from an optimisation algorithm such as a gradient descent variant) which are exchanged with other nodes and/or a centralised server [111]. In this respect, prominent privacy-preserving techniques in ML include data anonymisation [92], diﬀerential privacy [34], secure multi-party computation (SMC) [126], and homomorphic encryption [44]."
2011.05411,"data, dataset",96,,,"SMC is beneﬁcial to data privacy preservation in distributed learning wherein compute nodes collaboratively perform model training on their local dataset without revealing such dataset to others. Indeed, SMC has been employed in numerous ML algorithms such as secure two-party computation (S2C) in linear regression [31], Iterative Dichotomiser3 (ID3) decision tree learning algorithm [78], and k-means clustering algorithm for distributed data mining [62]. However,most of SMC protocols impose non-trivial overheads which require further eﬃciency improvements with practical deployment."
2011.05411,"data, dataset provided",139,,,"The GDPR clearly diﬀerentiates three participant roles, namely: Data Subject, Data Controller and Data Processor, along with associated requirements and obligations under the EU/UK data protection law. While serving as a better privacy and security framework, the GDPR also aims at protecting data ownership by obligating Data Controllers to provide fundamental rights for Data Subjects to control over their data (""How?"" in Fig. 1). For these purposes, the GDPR introduces and sets high-standard for the consent lawful basis in which Data Controller shall obtain consent from Data Subject in order to process data. Data Controller takes full responsibility to regulate the purposes for which and the methods in which, personal data is processed under the Terms and Conditions deﬁned in the consent."
2011.05411,"data, dataset provided",142,,,"In this article, we conduct a survey on existing FL studies with an emphasis on privacy-preserving techniques from the GDPR-compliance perspective. Firstly, we brieﬂy review the challenges on data privacy preservation in conventional centralised ML approaches (Section 2) and introduce FL as a potential approach to address the challenges (Section 3). Secondly, the state-of-the-art privacy-preserving techniques for centralised FL are described with the analysis of how these solutions can mitigate data security and privacy risks (Section 4). Thirdly, we provide an insightful deliberation with potential solution approaches of how an FL system can be implemented in order to comply with the EU/UK GDPR (Section 5). Unsolved challenges hindering an FL system from complying with the GDPR are also speciﬁed along with the future research directions."
2011.05411,"data, dataset provided",178,,,"The GDPR diﬀerentiates three participant roles, namely Data Subject, Data Controller and Data Processor, and designates associated obligations for these roles under the EU data protection law. Data Controllers are subject to comply with the GDPR by determining the purposes for which, and the method in which, personal data is processed by Data Processors - who will be responsible for processing the data on behalf of Data Controllers. Furthermore, Data Controllers should take appropriate measures to provide Data Subjects with information related not only to how data is shared but also to how data is processed in the manner ensuring security and privacy of personal data. The GDPR also clearly speciﬁes rights of Data Subjects, giving data owners the rights to inspect information about how the personal data is being processed (e.g., Right to be informed and Right of access) as well as to fully control the data (e.g., Right of rectiﬁcation and erasure, and Right to restriction of processing)."
2011.05411,"data, dataset provided",6,04/21/22,0,5.3. Rights of Data Subject
2011.05411,"data, dataset provided",81,,,"To meet stringent requirements of the GDPR, conventional ML-based applications and services are required to implement measures that eﬀectively protect and manage personal data adhering to the six data protection principles in the GDPR, as well as to provide mechanisms for data subjects to fully control their data. Although ML-based systems are strengthened by several privacy-preserving methods, implementing these obligations in a centralised MLbased system is non-trivial, sometimes technologically impractical [119, 53]."
2011.05411,"data, dataset, data available",222,,,"called compute nodes and grouped into clusters. For efﬁciency, the calculations in the training process should be parallelised using concurrency methods such as model parallelism and data parallelism [24]. Model parallelism distributes an ML model into diﬀerent computing blocks; available computing nodes are then be assigned to compute some speciﬁc blocks only. Model parallelism requires mini-batch data is replicated at computing nodes in a cluster, as well as regular communication and synchronisation among such nodes [29]. Data parallelism, instead, keeps the completeness of the model on each computing node but partitions the training dataset into smaller equal size shards (also known as sharding), which are then distributed to computing nodes in each cluster [8]. The computing nodes then train the model on their subset as a mini-batch, which is especially eﬀective for SGD variants because most operations over mini-batches are independent in these algorithms. Data parallelism can be found in numerous modern ML frameworks including TensorFlow3 and Pytorch4. The two parallelism techniques can also be combined (so-called Hybrid parallelism) to intensify the advantages while mitigating the drawbacks of each one; as a result, a hybrid system can achieve better eﬃciency and scalability [25]."
2011.05411,"data, publicly available",151,,,"The new GDPR legislation has come into force from May 2018 in all European Union (EU) countries which is a major update to the EU Data Protection Directive (95/46/EC) (DPD-95) introduced in the year 1995. The GDPR aims to protect personal data (more comprehensive range depicted in ""Which?"" - Fig. 1) with the impetus that ""personal data can only be gathered legally, under strict conditions, for a legitimate purpose"". The full regulation is described in detail across 99 articles covering principles, and both technical and admin requirements around how organisations need to process personal data. The GDPR creates a legal data protection framework throughout the EU/UK member states which has impacted commercial and public organisations worldwide processing EU/UK residents’ data (""Global"" in Fig. 1)."
2011.05411,"data, publicly available",77,,,"The GDPR establishes supervisory authorities in each member state which are independent public authorities called Data Protection Authorities (DPAs). DPAs are responsible for supervising and inspecting whether a Data Controller is compliant with the data protection regulations whilst the Data Controller is responsible for demonstrating the compliance. The questions are judiciously raised: How can an FL system be investigated and validated by DPAs, and how can it demonstrate the compliance?"
2011.05411,dataset,126,,,"Proposed by Dwork et al. in 2006, diﬀerential privacy [34] is an advanced solution of the perturbation privacy-preserving technique in which random noise is added to true outputs using rigorous mathematical measures [40]. As a result, it is statistically indistinguishable between an original aggregate dataset and a diﬀerentially additive-noise one. Thus, a single individual cannot be identiﬁed as any (statistical) query results to the original dataset is practically the same regardless of the existence of the individual [34, 33, 35]. However, there is a trade-oﬀ between privacy-guarantee and utility as adding too much noise and improper random Nguyen Truong et al.: Preprint submitted to Elsevier"
2011.05411,dataset,141,,,"where the training dataset is in form of a set of input-output pairs (𝑥𝑖, 𝑦𝑖), 𝑥𝑖 ∈ ℝ𝑑 and 𝑦𝑖 ∈ ℝ, ∀𝑖 ∈ {1, 2, .., 𝑛}. In Equation 2, 𝑛 is the number of samples in the dataset, 𝑤 ∈ ℝ𝑑 is the parameter vector, and 𝑓𝑖(𝑤) is a loss function. This formulation covers both linear and logistic regressions, support vector machines, as well as complicated non-convex problems in Artiﬁcial Neural Networks (ANN) including Deep Learning [70]. This problem requires an optimisation process that can be eﬃciently computed by using a gradient descent algorithm with back-propagation technique [105, 101] for minimising the overall loss with respect to each model parameters."
2011.05411,dataset,156,,,"It is worth to emphasise that the separation of the four steps in the cycle is not a strict requirement in every training round. For instance, an asynchronous SGD algorithm can be used in which results of the local training can be immediately applied to update the local model before obtaining updates from other participants [21]. This asynchronous approach is typically utilised in distributed training for deep learning models on a large-scale dataset as it maximises the rate of updates [29, 25]. However, in FL settings, the synchronous approach, which requires the coordination from a centralised server, has substantial advantages over the asynchronous ones in terms of both communication eﬃciency and security because it allows advanced technologies to be integrated such as aggregation compression, secure aggregation with SMC, and diﬀerential privacy [80, 71, 55, 120]."
2011.05411,dataset,18,,,"ness will signiﬁcantly depreciate reliability and usability of the dataset [33, 35, 40]."
2011.05411,dataset,247,,,"Shokri and Shmatikov in [111] have proposed a communication eﬃcient privacy-preserving SGD algorithm for deep learning in distributed settings in which local gradient parameters are asynchronously shared among participants with an option of adding noise to such updates for the differentially private protection of the individual model parameters. In this algorithm, participants can choose a fraction of parameters (randomly selected or following a strategy) to be updated at each round so that their local optimal can converge faster while being more accurate. In order to integrate diﬀerential privacy technique into the algorithm, the 𝜀 total privacy budget parameter and the sensitivity of gradient are taken into account to control Δ𝑓𝑖 the trade-oﬀ between the diﬀerential privacy protection and the model accuracy. Laplacian mechanism is used to generate noise during both parameter selection and exchange processes based on the estimation of the Δ𝑓𝑖 sensitivity and the allocated 𝜀 privacy budget. The proposed algorithm has experimented on MNIST and SVHN datasets showing the trade-oﬀ between strong diﬀerential privacy guarantees and high accuracy of the training model. However, with a large number of participants sharing a large fraction of gradients, the accuracy of the proposed algorithm with diﬀerential privacy is better than the standalone baseline. It is worth noting that in this algorithm, local gradients can be exchanged directly or via a central server, which can feasibly be implemented in the FL settings."
2011.05411,dataset,38,,,"as accumulated privacy loss by observing privacy loss random variables. Based on the experiment, the authors also indicate that privacy loss is minimal for large group size (with a large number of datasets)."
2011.05411,dataset,40,,,"[92] Narayanan, A., Shmatikov, V., 2008. Robust de-anonymization of large sparse datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008), IEEE. pp. 111–125."
2011.05411,dataset,47,,,"FL is well-suited for sorts of ML models that are formulated as minimisation of some objective functions (loss functions) on a training dataset for parameter estimation, particularly for gradient-based optimisation algorithms [70]. The minimisation objective can be formulated as follows:"
2011.05411,dataset,71,,,"Although gradient descent-based optimisation methods were successfully engaged in various ML algorithms, they have recently re-gained much attention since the emergence of large-scale distributed learning, including FL [16, 29]. In these scenarios, a complex model, e.g., a deep neural network (DNN) with millions of parameters, is trained on a very large dataset across multiple nodes. These nodes are"
2011.05411,dataset,89,04/21/22,0,"2. Local Computation: Once receiving the global ML model from the server, the participants updates its current local ML model and then trains the updated model using the local dataset resided in the device. This step is operated at local nodes, and it requires end-users’ devices to install an FL client program to perform training algorithms such as FederatedSGD and Federated Averaging, as well as to receive the global model updates and send the local ML model parameters from/to the server."
2011.0684,data,106,,,"(cid:12) (cid:12) 2, 𝑥𝑛 is the 𝑛th data symbol (cid:205)𝑈 −1 where 𝐵1,𝑛 = (cid:12)ℎB,𝑛+𝑖 𝑁 (cid:12) 𝑖=0 (cid:12) (cid:12) (cid:205)𝑈 −1 (cid:12) is the 𝑛th noise symbol (cid:12)𝑣B,𝑛+𝑖 𝑁 at Bob, and 𝐵2,𝑛 = 1√ 𝑖=0 𝑈 component at Bob and where it is observed that 𝐵1,𝑛 ⊥⊥ 𝑥𝑛 ⊥⊥ 𝐵2,𝑛. As detailed in A-A and A-B, the components can respectively be derived as:"
2011.0684,data,106,,,"Prior to the secure data transmission between Alice and Bob, a handshake protocol must take place. Depending on it, Eve may obtain different degrees of information regarding the channels, which leads to different decoding capabilities and so, different security performance. PLS performance highly depends on the availability of CSI at the communication parties. It is assumed that Alice knows Bob CSI but does not know Eve CSI who is assumed to be an external passive node of the network that tries to eavesdrop the data. Furthermore, Bob and Eve CSI’s are considered spatially independent."
2011.0684,data,109,,,"In this paper, a new scheme is introduced in order to establish a secure communication at the physical layer between a base station, Alice, and a legitimate user, Bob, in the presence of a passive eavesdropper, Eve. Alice uses a time reversal precoder, implemented in the frequency domain with OFDM, to add to the transmitted data an artiﬁcial noise that lies in the null-space of Bob but degrade Eve’s channel. The proposed technique only requires a single transmit antenna and is therefore well suited for devices with limited capabilities, such as in IoT for instance."
2011.0684,data,11,,,1) Data term: E (cid:2)|E𝑆𝐷𝑆 1
2011.0684,data,113,,,"Equation (9) shows that the addition of AN in the FD TR SISO OFDM communication can secure the data transmission. The degree of security depends on G and the amount of data energy, 𝛼, that is injected into the communication, with respect to the amount of AN energy injected (via 1 − 𝛼), as explained in Section III. It is to be noted that, since w is generated from an inﬁnite set of possibilities, even if Eve knows its equivalent channel HEH∗ B and the spreading sequence, she cannot estimate the AN signal to try retrieving the data."
2011.0684,data,124,,,"It has to be pointed out that lower bounds of the SINR at Bob and Eve were determined for the three investigated scenarios. From simulations, the closed form approximated SINR lower bounds, derived in (15), (22), (28), and (34), are observed to be very tight and are therefore used in the remaining as an approximation. By doing so, an analytical expression of the SR can be determined using (11) as a function of 𝛼. It is therefore straightforward to determine the amount of data energy to inject in the communication, with respect to AN, in order to maximize the ergodic SR."
2011.0684,data,131,,,"and covariance matrix E (cid:2)(S𝐻 vB)(S𝐻 vB) 𝐻 (cid:3) = 𝜎2 V,BI𝑁 . In (6), each transmitted data symbol is affected by a real gain √ 𝛼 2 at the position of the legitimate receiver. 𝑈 This frequency diversity gain consequently increases the received useful signal power at Bob in fading environments and increases with the BOR value. Considering a ﬁxed bandwidth, the TR focusing effect is enhanced for higher BOR’s at the expense of the data rate. It is also observed that no AN contribution is present in (6) since (3) is respected. A ZF equalization is performed at the receiver leading to:"
2011.0684,data,137,,,"where G is a 𝑁 × 𝑄 decoding matrix performed by Eve and vE is a complex AWGN. The nature of the decoding matrix is determined by the considered scenarios, which are presented in the next Section II-B. The noise variance is E (cid:2)|𝑣E,n|2(cid:3) = 𝜎2 V,E. The gain of the data component in (8) depends on G and does not necessarily provide a SNR enhancement due to a TR effect. Similarly, the AN component does not necessarily cancel out, depending on G. After ZF equalization, the estimated symbols are: BS(cid:1) −1 (cid:16)√ ˆxE = (cid:0)GHEH∗ √ √ 1 − 𝛼 (cid:0)GHEH∗ 𝛼x +"
2011.0684,data,14,,,Eve can decode the data thanks to G = S𝐻 H∗ sequence is:
2011.0684,data,14,,,"Therefore, the optimal amount of data energy to inject is given by:"
2011.0684,data,141,,,"The scheme consists in data transmission onto OFDM blocks with 𝑄 subcarriers. Without loss of generality, it is considered that only one data block x is sent and is composed of 𝑁 symbols 𝑥𝑛 (for 𝑛 = 0, ..., 𝑁−1, with 𝑁 ≤ 𝑄). The symbol 𝑥𝑛 is a zero-mean random variable (RV) with variance E (cid:2)|𝑥𝑛|2(cid:3) = 𝜎2 𝑥 = 1, i.e., a normalized constellation is considered. The block is then spread in the FD by a factor 𝑈 = 𝑄/𝑁, called back-off rate (BOR), thanks to the spreading matrix S of size 𝑄 × 𝑁. S is designed in such a way not to increase the PAPR, as suggested in [46]."
2011.0684,data,142,,,"Fig. 8 presents the SR enhancement thanks to the waterﬁlling optimization. The SR gain is deﬁned as the difference between the maximal SR obtained after and before optimization. As a reminder, before and after optimization, the mean energy radiated dedicated to the useful data remains unchanged, and the AN signal always remains in Bob’s null space. The optimal amount of data energy to inject is computed thanks to (36), (38), and (40) in order to ensure a maximal ergodic SR. The SR is then further increased via the waterﬁlling optimization procedure, as described in section III-E. As it can be observed, there is an increase of the SR gain for all three models and all BOR values thanks to the waterﬁlling."
2011.0684,data,16,,,"From (7), a perfect data recovery is possible in high SNR scenarios."
2011.0684,data,164,,,"Channel-based adaptation secrecy schemes were ﬁrst introduced in [17]–[19]. In these works, it was proven that positive secrecy rate (SR) can be obtained even if, on average, the channel between Alice and Bob is a degraded version of the one between Alice and Eve, by optimizing or adapting at the transmitter side the communication parameters. In doing so, the precoded signal is optimal for Bob’s channel but not for Eve’s one since they experience different fading. The concept of AN addition was ﬁrst established in [20]–[22]. The idea is to make Eve’s channel condition artiﬁcially degraded by intentionnaly adding an AN signal to the transmitter data. This AN signal is designed in such a way not to degrade Bob’s channel, therefore leading to a PLS enhancement, [1]."
2011.0684,data,177,,,"AN is added either on all the channel taps or on a set of selected taps. While the condition for AN generation is given, its derivation is however not detailed. In [9], [41], [42], FD precoders using OFDM and AN injection are presented. In [9], the AN is injected in the null space of Bob but only limited decoding capacilities were attributed to Eve. In [41], [42], the idea is to use several OFDM subcarriers for dummy data transmission, i.e., several subcarriers are used for data obfuscation. However, the encryption information must be shared between the transmitter and the legitimate receiver, leading to more processing needed at the receiver. In addition, the security is enhanced when more subcarriers are used for data obfuscation, at the expense of the data rate. Furthermore, it is assumed that Eve has no knowledge about the legitimate link."
2011.0684,data,190,,,"the optimal amount of transmitted data energy is derived thanks to eq.(36), (38), and (40). It leads to the coefﬁcient 𝛼𝐺 that maximizes the ergodic opt SR of the communication depending on G. It is a unique power coefﬁcient weighting the 𝑄 components of the useful transmitted data. Since the channel capacity is proportionnal to the subcarrier energy, and since Alice has access only to the instantaneous channel capacity at Bob, she can tune the amount of transmitted data energy at each subcarrier, i.e., she can apply a different weight at each subcarrier, to enhance the instantaneous capacity at Bob. In doing so, at each channel realization, she determines a new set of coefﬁcients, denoted w = [𝛼𝐺 𝜶𝐺 ]𝑇 , that enhances the instantaneous w,0 capacity at Bob. Because Bob and Eve channels are independent, enhancing the channel capacity at Bob does not change the ergodic capacity at Eve. This power allocation strategy is described below. 𝑤 = [𝛼𝐺 𝑤 ,0"
2011.0684,data,202,,,"The left part of Fig.6 illustrates the values of 𝛼opt given by (36), (38), and (40) that maximize the ergodic SR determined from the closed-form approximations (35), (37), and (39), as well as obtained from the numerical simulations, as a function of the BOR. There is a slight discrepency between the analytical estimations of the optimal amount of data energy to inject using (36), (38), and (40), and its numerical estimation. However, the resulting analytical SR does not differ much from the maximal SR obtained in simulation, as it can be observed on the right part of Fig.6. Indeed, as observed in Fig.5, the SR is a function that varies slowly about its maximum, for all models. So, for a given BOR value, Alice can make a rough determination of 𝛼𝐺 opt depending on Eve decoding structure, and therefore the available SR, if 𝛿𝐵 and 𝛿𝐸 are known. One can also note that much more AN power should be injected to"
2011.0684,data,217,,,"Abstract—A frequency domain (FD) time-reversal (TR) precoder is proposed to perform physical layer security (PLS) in single-input single-output (SISO) systems using orthogonal frequency-division multiplexing (OFDM) and artiﬁcial noise (AN) signal the data transmission to the legitimate receiver but degrades the decoding performance of the eavesdropper. This scheme guarantees the secrecy of a communication towards a legitimate user when the transmitter knows the instantaneous channel state information (CSI) of the legitimate link thanks to the channel reciprocity in time division duplex (TDD) systems, but does not know the instantaneous CSI of a potential eavesdropper. Three optimal decoding structures at the eavesdropper are considered in a fast fading (FF) environment depending on the handshake procedure between Alice and Bob. Closed-form approximations of the AN energy to inject the communication are derived. In addition, the required conditions at the legitimate receiver’s end to guarantee a given SR are determined when Eve’s signal-to-noise ratio (SNR) is inﬁnite. Furthermore, a waterﬁlling power allocation strategy is presented to further enhance the secrecy of the scheme. Simulation results are presented to demonstrate the security performance of the proposed secure system."
2011.0684,data,24,,,"• The data and noise are independent of each other. • ℎ𝐵,𝑖 ⊥⊥ ℎ𝐵, 𝑗 , ∀𝑖 ≠ 𝑗,"
2011.0684,data,280,,,"SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains. In [33], the information-theoretic secrecy capacity of an Offset-QAMbased ﬁlterbank multicarrier (FBMC-OQAM) communication over a wiretap frequency selective channel is studied. The authors compare the secrecy capacity of the FMBC-OQAM modulation with a cyclic preﬁx-orthogonal frequency-division multiplexing (CP-OFDM) modulation."
2011.0684,data,285,,,"First, it can be seen that the analytical models given by (35), (37), and (39) well approximate the simulation curves and remain tight upper bounds for all scenarios. In addition, one can notice the importance of the AN addition on the SR. In fact, one can observe a SR enhancement with the addition of AN except for very high percentages of AN sent, i.e., when 1 − 𝛼 → 1, or for very low percentages of AN sent, i.e., 1 − 𝛼 → 0. Furthermore, for all three models, SR→ 0 when 1 − 𝛼 → 1 since the SINR at Bob and Eve drops to zero. As anticipated from sections III-B2a and III-B2c, high SR values are obtained, i.e., low decoding performance at Eve, when she has the same capabilities as Bob, and when she only knows her own channel. It is also observed that these two scenarios exhibit very similar behaviours except when 1 − 𝛼 → 0, as explained in section III-B2c. Finally, one can observe lower SR values when Eve implements a matched ﬁltering decoding structure. This can be understood from (23) where it is noticed that each transmitted data symbol is afffected by a real gain at Eve such that it beneﬁts from a frequency diversity gain, leading to higher decoding performances at Eve, and so, lower SR values. In fact, Eve SINR is about U times larger with the MF decoder compared to the SDS and the OC decoders."
2011.0684,data,31,,,"In order to transmit secure data between Alice and Bob, the useful data is precoded and an AN signal w is added before transmission, as depicted in Fig.1."
2011.0684,data,335,,,"While many works implement these schemes with multiple antennas at the transmitter, using for instance frequency diverse array beamforming [23], [24], directional modulation (DM) [25], antenna subset modulation (ASM) [26], nearﬁeld direct antenna modulation (NFDAM) [27], [28], spatial diversity [29]–[32], or waveform design [33], only few works perfom PLS using single-input single-output (SISO) systems [9], [34]–[42]. SISO systems are indeed more suitable to resource-limited devices such as in IoT-type applications. In [34], a symbol waveform optimization technique in timedomain (TD) is proposed to reach a desired SINR at Bob with AN injection, under power constraint, when eavesdropper’s CSI is not known. Another approach to increase the SINR in SISO systems is time reversal (TR) pre-ﬁltering. This has the advantage to be implemented with a simple precoder at the transmitter. TR achieves a focusing gain at the intended receiver position only, thereby naturally offering intrinsic antieavesdropping capabilities, [35], [43]. TR is achieved by up/downsampling the signal in the TD. While the impact of the back-off rate (BOR), deﬁned as the up/downsampling rate [44], was studied in [9], [35], limited non-optimal decoding capabilities were attributed to Eve. Another approach to provide security at the physical layer is the use of orthogonal frequency-division multiplexing (OFDM) scheme which can be implemented in time or frequency domain (FD). In [36], [37] FD OFDM schemes are presented consisting of subcarriers index selection. Only several subcarriers are used for data transmission depending on their channel gains."
2011.0684,data,4,,,A. Data term
2011.0684,data,42,,,"V,E + (𝑈 + 1) −𝑈𝜎2 V,B, and 𝑇4 = (𝑈 + 1)(𝑈 + 3)𝜎2 V,B, the optimal amount of data energy to transmit is:"
2011.0684,data,45,,,"precoded data without pilot to Bob. From the FF assumption, Eve cannot learn the precoding performed by the transmitter. In this conﬁguration, Eve implements a decoding structure that takes beneﬁt of her own channel knowledge, denoted by OC."
2011.0684,data,48,,,"However, if Alice sends a precoded pilot in addition to the precoded data to Bob, Eve is then able to evaluate her equivalent channel H∗ BHE, and therefore to implement a matched ﬁltering decoding structure, denoted by MF. This is depicted in Fig.3"
2011.0684,data,5,,,1) Data term:
2011.0684,data,52,,,"In doing so, each data symbol is transmitted onto 𝑈 different subcarriers with a spacing of 𝑁 subcarriers, introducing frequency diversity. The spread sequence is then precoded with the complex conjugate of Bob’s channel H∗ B, before addition of the AN signal w and transmission."
2011.0684,data,55,,,"1) At the intended position: At Bob, a simple despreading operation is performed. Thanks to the precoding at the transmitter side, every received data symbol is affected by a real gain, as expressed in (6). The ergodic SINR for transmitted symbol 𝑛 is given by:"
2011.0684,data,57,,,"The AN should not have any impact at Bob’s position but should corrupt the data everywhere else since Alice does not have any information about Eve’s instantaneous CSI, i.e., Eve is a passive node. Furthermore, this signal should not be guessed at the unintended positions to ensure the secure"
2011.0684,data,69,,,"If Alice only transmits precoded data to Bob, Eve is not able to know anything but H𝐵𝐸 , the channel between Bob and Eve. In that situation, she cannot do better but to implement the same decoding structure as Bob, denoted by the abbreviation SDS. In that scenario, Eve only despreads the received sequence. This situation is presented in Fig.2."
2011.0684,data,72,,,"where 𝐸 𝐺 3,𝑛 are respectively the data, noise and AN 𝑛th symbol components of the received signal at Eve’s position, for a particular decoding structure G. The expression of the SINR at Eve depends on the receiving structure G whose design depends on the amount of knowledge Eve can obtain. The expression (16) is therefore derived for the three considered scenarios."
2011.0684,data,75,,,"Equations (45), and (46) are convex expressions that can be minimized as a function of 𝛼. Let’s denote 𝛼𝑆𝐷𝑆 ∞ , 𝛼𝑀 𝐹 ∞ , as the amount of data energy to inject, with respect to AN, in order to guarantee a desired communication SR when 𝛿𝐸 = ∞, respectively for the ﬁrst, second and third scenario. It can be shown that:"
2011.0684,data,76,,,"PLS can be achieved by increasing the signal-plusinterference to noise ratio (SINR) at Bob and decreasing the SINR at Eve. This can be done by designing a suitable channel-based adaptive transmission scheme, and/or by injecting an artiﬁcial noise (AN) signal to the data. These time and/or techniques can be implemented in the space, frequency domains, [1], [15], [16]."
2011.0684,data,77,,,"III. PERFORMANCE ASSESSMENTS The classical metric used to evaluate the degree of secrecy in a communication in the PLS ﬁeld is the secrecy channel capacity (SC). The SC is deﬁned as the maximum transmission rate that can be supported by the legitimate receiver’s channel while ensuring the impossibility for the eavesdropper to retrieve the data, [47]. In the ergodic sense, it can be expressed as:"
2011.0684,data,78,,,"life. Wireless communication has become the dominant access for most of these services but it is intrisically unsecure due to its unbounded nature. Therefore, several issues have emerged and need to be urgently adressed such as data conﬁdentiality and integrity. The amount of leaked information is also an important feature that needs to be considered and minimized in order to guarantee secrecy of wireless transmissions, [1]–[3]."
2011.0684,data,8,,,B. MF Decoder 1) Data term:
2011.0684,data,8,,,C. Optimal amount of data energy to inject
2011.0684,data,87,,,"This paper shows, consequently, with analytical and simulation results, that a scheme exploiting only frequency degrees of freedom can achieve a positive ergodic secrecy rate to considerably jeopardize any attempt of an eavesdropper to retrieve the data. This approach can be easily integrated into existing standards based on OFDM and does not necessitate extra hardware. However, a perspective of this work is to extend it to multiple antenna systems to assess the beneﬁt of the extra spatial degree of freedom."
2011.0684,data,89,,,"the unintended position, the received signal before ZF equalization is given by √ (8). Let’s introduce E𝐺 𝛼GHEH∗ 2 = GvE and E𝐺 1 − 𝛼GHEw being respectively the data component, 3 = the noise component, and the AN component of the received signal at Eve for a particular decoding structure G. Using the Jensen’s inequality, an approximation of a lower-bound of the averaged SINR of the symbols 𝑛 at the unintended position can be derived as2:"
2011.0684,data,99,,,"With the closed-form approximations of the SR (35), (37), and (39), it is possible to determine the SNR at Bob and the amount of data energy 𝛼 that guarantees a given SR, as a function of the communication parameters. Let’s introduce Δ being the targetted SR in bit per channel use, 𝛿𝑆𝐷𝑆 , 𝛿𝑀 𝐹 𝐵 , and 𝛿𝑂𝐶 being respectively Bob’s required SNR for the ﬁrst, 𝐵 second and third investigated scenario. Remembering that 𝜎2 V,B = 1"
2011.0684,"data, data available",86,,,"[36] Y. Lee, H. Jo, Y. Ko, and J. Choi, “Secure index and data symbol modulation for ofdm-im,” IEEE Access, vol. 5, pp. 24 959–24 974, 2017. [37] J. M. Hamamreh, E. Basar, and H. Arslan, “Ofdm-subcarrier index selection for enhancing security and reliability of 5g urllc services,” IEEE Access, vol. 5, pp. 25 863–25 875, 2017."
2011.07981,code,35,,,"[33] “Distribution code,” ESB Networks,” Grid code, April 2016. [34] W. H. Kersting, “Radial distribution test feeders,” IEEE Transactions on"
2011.07981,data,126,,,"To examine the performance of the proposed data recovery approach, the interruption of the communication channels from the substation or one of the DERs is simulated, resulting in simultaneous loss of four signals in the former case or three signals in the latter cases. Without a recovery plan, considering the mean values in the training set for the missing signals seems like the most logical choice. Fig. 5 presents the performance of DA in this scenario. As noted, DA can no longer predict the correct conﬁguration, especially for the cases in which the communication from DER4, DER5, or DER6 is interrupted. This establishes the vulnerability of DA under the interruption of communication channels."
2011.07981,data,130,,,"To simulate anomalous measurements, at each step, the measurements from one of the DERs are multiplied by 0.9, to examine a negative bias, and then 1.1, to examine a positive bias. α is calculated with the manipulated values, as well as the original values. Fig. 9 presents the percentage of the data points placed in each speciﬁed range of α. As noted, with the original values, for the majority of the data points α is below 60 (more than 92% of the data points, in all the scenarios), while with the manipulated values, α takes larger values (above 60 for over 95% of the data points, in all the"
2011.07981,data,133,,,"While deploying information technologies streamlines the system management, it can cause more dependencies between the physical and cyber components, which makes the system more vulnerable to natural disasters and cyber-attacks [15]. In fact, proneness to loss of online data has undermined the popularity of the approaches that rely on online measurements in practice, especially when the system may face the loss of multiple signals, because of, e.g., interruption of communication channels, or cyber-attacks. Securing the power system management against cyber-attacks has been the subject of many recent studies, like [16] and [17]. This highlights the importance of resiliency in the development of a TI function. None of the reviewed TI approaches is resilience-oriented."
2011.07981,data,170,,,"This paper is dedicated to developing a topology identiﬁcation function for distribution networks that relies only on the measurements available to DERMS. These measurements include the operating condition of the grid and DERs [1]. Considering the applicability of discriminant analysis (DA) as a widely-used statistical classiﬁer, DA is employed for this purpose. While originally developed for DERMS, this method is applicable for DMS too since DMS and DERMS share the DERs measurements [1]. To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,18,,,It is worth emphasizing that the quadratic programming data recovery approach is applicable only when some mea 5
2011.07981,data,186,,,"In addition to the loss of multiple signals, the TI approach should be resilient against malicious data. In this subsection, a benchmark is introduced to detect if one or a group of measurements contain anomalous values. Suppose the measurements from a speciﬁc meter, i.e., X s = [xs l ], are suspected to contain anomalous values. Considering (10), the information of the other measurements can be exploited to estimate the normal value of X s, i.e., X r = [xr l ]. To discern if X s is anomalous, the idea is to compare the likelihood of X r associating with any of the network topologies, i.e., ∪K i=1(ki) denotes the union of all the possible topologies. For this purpose, the likelihood ratio, denoted by α = Λ(X r : X s| ∪K i=1 (ki)), is employed as a benchmark to detect if X s is anomalous. The likelihood ratio is deﬁned as:"
2011.07981,data,188,,,"Abstract—Network topology identiﬁcation (TI) is an essential function for distributed energy resources management systems (DERMS) to organize and operate widespread distributed energy resources (DERs). In this paper, discriminant analysis (DA) is deployed to develop a network TI function that relies only on the measurements available to DERMS. The propounded method is able to identify the network switching conﬁguration, as well as the status of protective devices. Following, to improve the TI resiliency against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. By deploying the propounded data recovery approach and Bayes’ theorem together, a benchmark is developed afterward to identify anomalous measurements. This benchmark can make the TI function resilient against cyberattacks. Having a low computational burden, this approach is fast-track and can be applied in real-time applications. Sensitivity analysis is performed to assess the contribution of different measurements and the impact of the system load type and loading level on the performance of the proposed approach."
2011.07981,data,51,,,"[30] J. Hallinan, “Chapter 2 - data mining for microbiologists,” in Systems Biology of Bacteria, ser. Methods in Microbiology, C. Harwood and A. Wipat, Eds. Academic Press, 2012, vol. 39, pp. 27 – 79."
2011.07981,data,58,,,"Fig. 9. The percentage of the data points in the test set placed in each speciﬁed range of α, with the normal values and manipulated values of the measurements from: a) DER1, b) DER2, c) DER3, d) DER4, e) DER5, f) DER6."
2011.07981,data,66,,,"As discussed, the loss of multiple signals and malicious data are two phenomena that jeopardize the applicability of approaches relying on online measurements in practice. In this section, an approach is proposed to recover the original values of the measurements that are missing or suspected to contain anomalous values. From now on, they are referred to as the missing signals."
2011.07981,data,78,,,"It should be emphasized that in this approach, we do not consider the objective of network reconﬁguration operations, but instead, to form the training and test data sets, all the possible conﬁgurations are considered and in each conﬁguration, all the probable variations of loads consumption and DERs generation are simulated. In this order, whatever the objective of network conﬁguration is, it will include a subset of this data set."
2011.07981,data,82,,,"Afterward, a quadratic programming optimization approach, based on the recovery of the lost signals, was presented to make the proposed TI approach resilient against the interruption of communication channels. The speciﬁc optimization problem can be settled efﬁciently and a global extremum is guaranteed. Furthermore, by exploiting this data recovery approach, a benchmark was introduced to detect anomalous measurements. This benchmark can be employed to enhance the resiliency of the proposed TI against cyber-attacks."
2011.07981,data,85,,,"To enhance the resiliency of the proposed TI function against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. Following, by deploying this data recovery approach and Bayes’ theorem, a benchmark is introduced to detect if some measurements contain anomalous values. This benchmark makes the proposed TI function resilient against cyber-attacks. This approach requires a low processing time, which makes it suitable to be employed in real-time applications."
2011.07981,data,864,,,"This approach requires a low processing time, which makes it suitable to be employed in real-time applications. This approach is able to consider weakly-meshed conﬁgurations, works with any type of mea (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)(cid:12)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:11)(cid:22)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)(cid:19)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:23)(cid:23)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:11)(cid:24)(cid:13)(cid:14)(cid:14)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)(cid:14)(cid:21)(cid:5)(cid:7)(cid:8)(cid:9)(cid:1)(cid:11)(cid:12)(cid:10)(cid:13)(cid:11)(cid:20)(cid:10)(cid:13)(cid:1)(cid:19)(cid:25)(cid:26)(cid:27)(cid:17)(cid:18)(cid:25)(cid:26)(cid:27)(cid:17)(cid:28)(cid:25)(cid:26)(cid:27)(cid:17)(cid:12)(cid:25)(cid:26)(cid:27)(cid:17)(cid:22)(cid:25)(cid:26)(cid:27)(cid:17)(cid:24)(cid:25)(cid:26)(cid:27)(cid:17)(cid:20)(cid:25)(cid:26)(cid:27)(cid:17)(cid:29)surements, can treat unbalanced networks, and can be applied to identify the switching conﬁguration and the operation of protective devices."
2011.07981,data,9,,,of the proposed data recovery approach are presented.
2011.10563,code,23,,,The code of HINDSIGHT++ is exclusively written in R and a proof of concept implementation is available in [8]. The
2011.10563,code,80,,,"backbone is the CRAN interface to Keras, a high-level, userfriendly Application Programming Interface (API) which enables quick and dynamic experimentation with DL algorithms, while it further allows for easy neural network architectural prototyping. Keras is designed to run on top of multiple back-end environments, including Theano, CNTK, and Tensorflow (default). Furthermore, it offers the option for executing the code either on top of CPUs or GPUs."
2011.10563,data,108,,,"[11] V. Raida, P. Svoboda, M. Kruschke, and M. Rupp, “Constant rate ultra short probing (crusp): Measurements in live lte networks,” in ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1–6, IEEE, 2019. [12] C. Maier, P. Dorﬁnger, J. L. Du, S. Gschweitl, and J. Lusak, “Reducing consumed data volume in bandwidth measurements via a machine learning approach,” in 2019 Network Trafﬁc Measurement and Analysis Conference (TMA), pp. 215–220, IEEE, 2019."
2011.10563,data,109,,,"For the bandwidth prediction task under study, therefore, we ﬁnd that hyperparameter optimization is especially vital in 5G networks, since they introduce higher data rates and larger variations over time. Coupled with the mobility aspect, a suboptimal hyperparameter set will fail to capture the intrinsic network characteristics. Among the two available optimization solutions under study, we observe that BOA provides slightly better results, since it utilizes probability theory concepts for minimizing the error. We further observe that 4G networks offer signiﬁcantly lower data rates with a lower variation factor which signiﬁcantly diminishes the need for advanced hyperparameter optimization solutions."
2011.10563,data,160,,,"• Random Search (RS). Instead of evaluating all possible hyperparameter combinations, RS iterates over a smaller sample [3]. As the number of iterations increase, the probability of converging to a better solution increases as well. The number of iterations required to reach to a good solution depends on several factors including the size of the hyperparameter space, search range, data complexity, and so forth. RS has a fairly simple implementation and does not require any scientiﬁc knowledge for the hyperparameters under study. In addition, it provides signiﬁcant gains in terms of computational complexity, since it only tests a limited number of combinations, which is a safe approach under the assumption that non all hyperparameters are equally important. As a rule of thumb, there is a 95% chance that RS will reach to a good solution with only 60 iterations."
2011.10563,data,18,,,Multivariate data analysis often involves features with variable scales. Since LSTM weight allocation is prone to such
2011.10563,data,260,,,"The topic of bandwidth forecasting in MBB networks has been in the spotlight for a long time. However, it has lately become more prominent due to the fast-expanding next generation network infrastructures, which in turn, yield to a larger exchange of network data, hence, increasing the need for higher and more accurate predictions. Furthermore, 5G introduces new frequency bands, such as the mmWave, which drives data rates to groundbreaking limits, thus, posing new challenges to the forecasting process. Research studies focus on integrating robust and effective algorithms, while maintaining the implementation and computational complexity in acceptable levels. Over the years, numerous solutions have been proposed ragning from traditional statistical methods, such as the Naive, Autoregressive Integrated Moving Average (ARIMA), and Vector Autoregression (VAR), to considerably higher complexity algorithms, namely dynamic linear models, TBATS [1] (i.e., based on exponential smoothing), and Prophet [2] (i.e., widely used by Facebook). Within the context of Artiﬁcial Intelligence (AI), both Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM) networks have been projected as high efﬁcient algorithms for time series modeling. Between the two, LSTM networks appears to be the most ﬁtting solution for the bandwidth forecasting study, considering the complex long term dependencies and the unforeseeable behavior that mobile traces display due to mobility."
2011.10563,data,270,,,"A supervised Machine Learning (ML) solution for downlink throughput prediction in MBB networks is proposed in [9]. The authors argue that ML can be used as a tool for signiﬁcantly reducing the data volume consumption over the network, while maintaining the predictive error in acceptable levels. Furthermore, in [10], Raida et. al leverage constant rate probing packets to estimate available bandwidth in a controlled Long-Term Evolution (LTE) environment, while in [11], they extend their work by further testing and validating the developed framework in live LTE networks. In [12], Maier et al. introduce a novel AI model with feed-forward neural networks for both downlink and uplink bandwidth forecasting. To ﬁnd a ﬁtting speed test duration, the authors investigate two scenarios. First, they train a model with a duration ﬁxed to a value lower than the default, while second, they dynamically determine a duration by using the results of a pretrained neural network model. Experimental approaches studying the problem of bandwidth prediction have also been addressed in [13, 14, 15, 16, 17, 18, 19]. Beyond empirical-based studies, theoretical models have also been published. In [20], Gao et al. introduce a theoretical learning based throughput prediction system for reactive ﬂows, while authors in [21] propose a novel stochastic model for user throughput prediction in MBB networks that considers fast fading and user location."
2011.10563,data,3,,,3.1.2 Data Import
2011.10563,data,3,,,3.2 Data Preprocessing
2011.10563,data,30,,,"The objective of this ﬁrst phase is two-fold. First, to provide support with all required software components, and second, to initiate the data import process."
2011.10563,data,32,,,"data, it is critical that we consider a transformation function leveraging one of the following state-of-the-art normalization methods, known as min-max, z-score, and tanh [37]."
2011.10563,data,39,,,"Fig. 2: A high-level representation of the HINDSIGHT++ control ﬂow diagram. The operations for each block are described in Section 3, highlighted with a matching title. Dashed lines delimit the data preprocessing stage."
2011.10563,data,42,,,"[13] Y. Liu and J. Y. Lee, “An empirical study of throughput prediction in mobile data networks,” in 2015 IEEE Global Communications Conference (GLOBECOM), pp. 1– 6, IEEE, 2015."
2011.10563,data,44,,,"Finally, data undergoes transformation and reshaping functions to comply with the LSTM input requirements. The outcome of this process are the 2-D and 3-D matrices summarized in Table 1. X and Y represent the LSTM input and output, respectively."
2011.10563,data,53,,,"[34] R. Yu, Y. Li, C. Shahabi, U. Demiryurek, and Y. Liu, “Deep learning: A generic approach for extreme condition trafﬁc forecasting,” in Proceedings of the 2017 SIAM international Conference on Data Mining, pp. 777–785, SIAM, 2017."
2011.10563,data,54,,,"[33] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, “Long short-term memory neural network for trafﬁc speed prediction using remote microwave sensor data,” Transportation Research Part C: Emerging Technologies, vol. 54, pp. 187–197, 2015."
2011.10563,data,55,,,Experimental Design: All experiments are carried out on an x86-64 architecture with a 16GB RAM while an NVidia Titan X graphics card is used to accelerate data preprocessing. The operating system is based on a Linux Ubuntu 16.04.6 LTS distribution. All reported RT values could signiﬁcantly vary under different architectural designs.
2011.10563,data,60,,,"[32] J. Wang, J. Tang, Z. Xu, Y. Wang, G. Xue, X. Zhang, and D. Yang, “Spatiotemporal modeling and prediction in cellular networks: A big data enabled deep learning approach,” in IEEE INFOCOM 2017-IEEE Conference on Computer Communications, pp. 1–9, IEEE, 2017."
2011.10563,data,61,,,"4. In addition, HINDSIGHT++ supports one-hot-encoding for converting categorical variables to binary features and a number of options for dealing with missing data (e.g., interpolation, Moving Average (MA), Kalman ﬁltering, etc.). We do not cover these additional aspects in detail, since they are not part of our data."
2011.10563,data,77,,,"The process of data preparation in LSTM networks is rather perplexed time consuming. The complexity factor signiﬁcantly increases considering the different data format combinations (e.g., multivariate, parallel) and parameters (e.g., lags, prediction steps). Figure 2 (dashed frame) encloses all steps followed for providing compatibility with the LSTM models. In line with the AutoML concept, data preprocessing is treated as a black box4."
2011.10563,data,80,,,"Michael Alexander Riegler, Chief Research Scientist, SimulaMet, holds degrees in computer science from the University of Oslo (UiO) and Klagenfurt experienced in University. He is medical multimedia data analysis and understanding, image processing, image retrieval, parallel processing, crowdsourcing, social computing and user intent. He is a recognized expert in his ﬁeld, serving as a member of the Norwegian Council of Technology on Machine Learning for Healthcare."
2011.10563,data,93,,,"Takeaways: The hyperparameter optimization paradigm is critical for improving several performance aspects in LSTM networks. However, its signiﬁcance is highly dependant on the underlying data attributes. For example, sequences that display large signs of variance over time are notably harder to predict, thus requiring hyperparameter conﬁgurations of high precision. On the contrary, sequences that either meet the stationarity test requirements or show consistent trends across time have limited gains, since most combinations of hyperaparameters within the predeﬁned search range will provide comparable performance."
2011.10563,data,96,,,"Training of neural networks is an iterative process that involves identifying unique data properties and learning the model parameters. At each iteration, we track both training and validation error, which we use as bias and variance measures, or else, indicators of underﬁtting or overﬁtting. The validation set always reﬂects the last portion of the initial training set. We test the ﬁnal model on an never-seenbefore testing set and use the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as our error metrics."
2011.10563,"data available, code, open-source, code available, data, open-source code",70,,,"3.1.1 Load Prerequisites HINDSIGHT++ relies on a number of open-source packages, or else, a collection of functions, compiled code, and data, available in CRAN. To guarantee software stability and robustness, it is critical that all dependencies are pre-installed in the system. Therefore, we enable a veriﬁcation process that automatically locates, installs, and loads any missing packages and libraries."
2011.10563,"data, code, open-source",186,,,"In this work, we present HINDSIGHT++, a novel, lightweight, versatile, and open-source R-based framework for bandwidth forecasting experimentation in MBB networks with LSTM networks2. The main source of motivation behind the design of HINDSIGHT++ is two-fold. First, to limit the implementation complexity of LSTM networks, and second, to minimize the predictive error. Toward this goal, we adopt the concept of Automated Machine Learning (AutoML), an acronym used to describe the process of data pipeline automation. In particular, AutoML covers the complete learning routine, from raw data preprocessing to the ﬁnal model production. Moreover, HINDSIGHT++ is compliant with parallel and multivariate data and supports a wide forecasting horizon. Last, it offers different hyperparameter optimization options and LSTM variants. The source code of HINDSIGHT++ is structured in a dynamic fashion, so it can accommodate new features and algorithms. Users are encouraged to integrate more libraries and software components to satisfy their cause, but also to enrich the capabilities of the framework."
2011.10563,"data, dataset",112,,,"Next is the data import process where users select a number of f ∈ [1, z] .csv ﬁles via an interactive window. Due to its universal design, HINDSIGHT++ operates under the assumption that the selected datasets are parallel3. In addition, equidimensional datasets are required, since LSTM models expect input arrays with equal sizes. The last prerequisite is that the dependent variable locates in the ﬁrst array position followed by any number of additional of regressors. We annotate the matrix dimensions for each dataset as df ∈ Rn×m, where n and m represent the number of samples and features, respectively."
2011.10563,"data, dataset",138,,,"Furthermore, we observe that LSTM models operate on a low MAE region which is the result of the low-variance data rates experienced in 4G under mobility, thus restricting the room for vast performance improvement. Across the two dataset portions, we ﬁnd that, in average, validation error is lower than testing error, which is the common scenario, since model tuning takes place in the former set. Last, Figure 5 illustrates the bandwidth time series graph of three NYU-METS traces10. Color is used for mapping the forecasting lines ( (RSV L)) with the associated part of the trace, i.e., training, validation, and testing set. As evident, LSTM models follow the data trends along the traces with minimal error deviation."
2011.10563,"data, dataset",172,,,"5Gophers: Exploring potential patterns across the 5G bandwidth measurements, we observe that in some scenarios the optimized LSTM models feature a third HL. Again, this is a rather expected result considering the higher data rates and variation in 5G in addition with the challenging bandwidth trends over time and under mobility. Likewise, the average number of neurons per HL across all layers equals to 197, 79, and 32 for HL-1, HL-2 and HL-3, respectively. We observe that the number of neurons per HL for both datasets follow a decreasing trend across the network, which also coincides with the neuron selection from authors in [30] (i.e., HL-1=256 and HL-2=128). Additionally, the average LR and BS across all traces equal to 0.004 and 28, respectively, while the number of epochs averages 76, which is an ≈ 17% increase comparing to the 4G counterpart result hinting a slower gradient descent convergence."
2011.10563,"data, dataset",62,04/21/22,0,"At a glance, we observe that 5Gophers error show an ≈ 100-fold increase when compared to NUY-METS.This observation is in line with our expectations, considering the higher data rates that users experience in a 5G network environment (i.e., up to 1500Mbps, see Figure 6). Next, we follow up the results discussion per dataset."
2011.10563,"data, dataset",93,,,"Takeaways: The designated results reveal that hyperparameter optimization is crucial for achieving superior performance in 5G networks, while trading computational resources. The above ﬁnding is further reinforced by the reported model hyperparameters, which signiﬁcantly vary across technology standards, mobility scenarios, and to some extent network operators. Across the two datasets, we ﬁnd that 5G data require LSTM conﬁgurations with an increased number of HLs and epochs, while featuring signiﬁcantly lower BS values, which further reveals the challenging network conditions in such scenarios."
2011.10563,"data, dataset, open-source",240,,,"6 CONCLUSIONS AND FUTURE WORK In this paper, we studied the challenging task of bandwidth forecasting in next-generation MBB networks under mobility. To ease our goal, we designed HINDSIGHT++, an open-source R-based framework that allows for LSTM experimentation in time series data. We speciﬁcally focused on the hyperparameter optimization aspect which is critical for achieving state-of the performance. We analysed and performed a comparative analysis between two opensource datasets of bandwidth measurements in operational 4G and 5G networks, respectively, aiming to quantify the LSTM performance improvement under different conﬁguration setups. Results show that hyperparameter optimization provides signiﬁcant beneﬁts under 5G settings compared to 4G settings and the optimal parameters for 4G cannot be directly applied considering the substantially higher data rates and variation over time that users experience in 5G network conditions. As for the future, we plan on integrating feature selection algorithms, additional LSTM variants (e.g., Convolutional Neural Networks (CNN) LSTM and multiplicative LSTM [51]), and provide support for supervised classiﬁcation tasks. Furthermore, we will carry out a dedicated measurement campaign to collect additional network features (e.g., signal strength, latency, etc.). Using the new data, we aim to explore potential correlation or causation relationships and show whether or not they bring any signiﬁcant gains to the LSTM performance."
2011.10563,"data, dataset, open-source data, open-source",223,,,"Abstract—Bandwidth forecasting in Mobile Broadband (MBB) networks is a challenging task, particularly when coupled with a degree of mobility. In this work, we introduce HINDSIGHT++, an open-source R-based framework for bandwidth forecasting experimentation in MBB networks with Long Short Term Memory (LSTM) networks. We instrument HINDSIGHT++ following an Automated Machine Learning (AutoML) paradigm to ﬁrst, alleviate the burden of data preprocessing, and second, enhance performance related aspects. We primarily focus on bandwidth forecasting for Fifth Generation (5G) networks. In particular, we leverage 5Gophers, the ﬁrst open-source attempt to measure network performance on operational 5G networks in the US. We further explore the LSTM performance boundaries on Fourth Generation (4G) commercial settings using NYU-METS, an open-source dataset comprising of hundreds of bandwidth traces spanning different mobility scenarios. Our study aims to investigate the impact of hyperparameter optimization on achieving state-of-the-art performance and beyond. Results highlight its signiﬁcance under 5G scenarios showing an average Mean Absolute Error (MAE) decrease of near 30% when compared to prior state-of-the-art values. Due to its universal design, we argue that HINDSIGHT++ can serve as a handy software tool for a multitude of applications in other scientiﬁc ﬁelds."
2011.10563,"data, open-source",218,,,"Over the past years LSTM networks have been successfully applied for network performance related tasks. Cui et al. proposed a stacked bidirectional LSTM architecture for network-wide trafﬁc estimation [29], while authors in [30] studied the applicability of LSTM for a real-time bandwidth prediction problem. Furthermore, Zhao et. al. focused on short-term trafﬁc estimation by considering temporal–spatial correlation [31], whereas, authors in [32] proposed a hybrid framework that combines LSTM networks and an auto-encoder based model for network prediction from a spatio-temporal angle. Additional work on mobile trafﬁc forecasting has been further proposed in [33, 34, 35]. Distinct from the preceding studies, this paper stands out and distinguishes itself by putting focus on the hyperparameter optimization aspect applied on data from different countries, network operators, technology standards, and mobility scenarios. In addition, to the best of our knowledge, this is the ﬁrst study that attempts to perform a bandwidth forecasting comparative analysis between 4G and 5G networks under mobility with LSTM networks. Last, we make available to the community an open-source framework which follows an end-to-end AutoML paradigm and allows for ample experimentation with LSTM networks."
2011.10563,dataset,107,,,"We study the topic of bandwidth forecasting in commercial 5G networks under mobility, while we revisit Fourth Generation (4G) scenarios and draw additional insights by performing a comparative analysis between the two technology standards. Thereupon, we leverage two opensource datasets, 5Gophers, which features network measurements from operational 5G networks in the US, and NYU Metropolitan Mobile Bandwidth Trace (NYU-METS), a counterpart dataset that comprises of hundreds of 4G bandwidth measurements in the wild. We adopt systematic investigation to showcase the signiﬁcance of hyperparameter optimization at each use case when compared to existing state-of-the-art approaches."
2011.10563,dataset,122,,,"made after closely observing the fast saturation rate of the gradient descent error. However, we argue that future 5G applications may require a higher number of iterations to converge to a good solution, although, this would result to a substantial increase of the RT complexity. Second, the hyperparameter search range can be stretched to accom modate a higher pool of values, which could potentially add to the performance. The selected values were decided as a compromise to the performance versus RT complexity tradeoff. Last, both datasets are composed of traces with a limited number of samples which can have a negative effect on the LSTM performance since they are known to perform"
2011.10563,dataset,123,,,"At present time, 4G networks are considered the norm of wireless telecommunication systems sustaining a big portion of society’s global network functions. However, the recent advances in technology indicate that in the upcoming decade 5G will become the next big thing with several network operators already moving toward its adoption. Therefore, it is dire that we identify the principal network characteristics and trends between the two technology standards and show to what extent they dictate the performance of LSTM networks. In addition, our experiments aim to identify the potential gains of hyperparameter optimization in delivering more efﬁcient and robust models. Next, we provide a description of the two datasets under study."
2011.10563,dataset,174,,,"VALTE012301230123MAERMSE7TrainA7TrainBBus57Bus62ABus62BNTrainLIMSVLRSVLRSBDBOAVLVALTE050100150200050100150200050100150200MAERMSESprintASprintBVerizonAVerizonBWalkingMSVLRSVLRSBDBOAVL010002000300040005000051525t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)0500150025000246812t (s)Bandwidth (Mbps)050010001500051015t (s)Bandwidth (Mbps)to the bandwidth forecasting problem. Likewise, we compare RSV L and RSBD to remove the hyperparameter optimization bias. On the one hand, we ﬁnd that Bidirectional LSTM networks outperform the Vanilla counterpart in two out of the four driving traces (i.e., SprintA and VerizonA) with an average testing MAE improvement of ≈ 15%. On the other hand, however, they seem to be performing worse in the other two driving traces (i.e., SprintB and VerizonB) by an average testing MAE percentage of ≈ 15%. This ﬁnding veriﬁes the fact that bidirectional LSTM networks are very dependant on the respective dataset underlying characteristics. Therefore, the question of whether or not they are ﬁtting for a given application can only be answered after systematic empirical analysis."
2011.10563,dataset,193,,,"Tables 4 and 5 provide additional insights with respect to the RT complexity. In particular, each row reports the RT [min] per trace and across the available conﬁgurations. In average, we observe that RSBD present a higher RT of ≈ 54% when compared to the RSV L. The above result is not balanced across the two datasets due to the randomness bias during the hyperparameter selection process. For example, instances of the algorithm where higher values for HLs or neurons are selected will require more computational power to accomplish. On the other hand, the average increase in RT between BOAV L and RSV L reaches to a percentage of ≈ 154%, which can be attributed to the following. First, since BOA leverages the Bayes Theorem for minimizing the gradient descent error, it requires more time to reach to a good solution, opposed to RS which randomly scans the search space. And second, the R BOA implementation is not optimized to run on top of a GPU, which adds to the RT overhead."
2011.10563,dataset,215,,,"Setting the Baseline: For the baseline experiments (M SV L), we adopt a stacked Vanilla architecture with two HLs and the model hyperparameters7 reported in [30] and illustrated in Table 2. Note that these hyperparameters was prior optimized (e.g., by means of sensitivity analysis or by using indicative values based on previous experience with datasets of similar nature) on the same dataset [30]. LSTM experimentation takes place in a per-trace fashion, i.e., the ﬁrst x% part of the sequence is used for training and validation, while the remaining y% is used for testing. In addition, we adopt a walk-forward validation, or else, rolling forecast approach, where each prediction value is used as new input to the model for forecasting the next step. Last, we overcome the random LSTM weights initialization bias by repeating each baseline experiment 25 times and reporting the median values for both of our error metrics8. Toward Performance Optimization: We repeat the experimental campaign by enabling hyperparameter optimization while testing the available LSTM variants. Our goal is to test whether the hypothesis stating that a set of ﬁxed model hyperparameters is sub-optimal for modelling"
2011.10563,dataset,39,,,"To the best of our knowledge, little research has been conducted to provide guidelines on the LSTM hyperparameter selection, since most studies focus on particular architectures and datasets [28]. Therefore, we are moving"
2011.10563,dataset,46,,,0204060801001200200400t (s)Bandwidth (Mbps)0204060801001200200400600t (s)Bandwidth (Mbps)02040608010012005001500t (s)Bandwidth (Mbps)02040608010012004008001200t (s)Bandwidth (Mbps)01002003004005006000400800t (s)Bandwidth (Mbps)1024102410240.011001283units1units2units3lrepochsbslayers7TrainA1024102410240.011001283units1units2units3lrepochsbslayersBus62A1024102410240.011001283units1units2units3lrepochsbslayersLIMSVLRSVLRSBDBOAVL1024102410240.011001283units1units2units3lrepochsbslayersSprintA1024102410240.011001283units1units2units3lrepochsbslayersSprintB1024102410240.011001283units1units2units3lrepochsbslayersVerizonA1024102410240.011001283units1units2units3lrepochsbslayersVerizonB1024102410240.011001283units1units2units3lrepochsbslayersWalkingMSVLRSVLRSBDBOAVLbest with larger datasets.
2011.10563,dataset,54,,,"across the available traces and datasets. Figures 7 and 8 illustrate the selected hyperparameters along the selected NYU-METS traces and 5Gophers datasets, respectively. The edges of each radar plot map to the maximum search range values across the seven hyperparameters, while color is used to discriminate between each conﬁguration."
2011.10563,dataset,66,,,"LSTM performance heavily relies on the selection of the model hyperparameters and the underlying dataset characteristics (i.e., periodic patterns and trends). Since these two aspects are tightly tied with each other, knowing how to efﬁciently tune an LSTM model becomes an important but rather challenging task, thus, urging for a hyperparameter optimization solution. Among the most popular approaches"
2011.10563,dataset,72,,,"The next phase involves splitting each dataset into a training (80%) and a testing (20%) set. Both of these percentages can be conﬁgured as appropriate. We deﬁne the dimensions for f ∈ Rnte×m, where ntr and each set as dtr nte are the number of samples for the training and testing set, respectively, while m represents the number of features."
2011.10563,dataset,73,,,"Time series visualization can be used to highlight certain phenomena at glance, including periodic patterns or trends, extreme outliers, odd observations, abrupt changes over time, and so forth. Moreover, it allows for visual inspection of the developed models to assess how well they ﬁt the datasets under study. A different color scheme is used to distinguish the training, validation, and testing set."
2011.10563,"dataset, open-source",24,,,"• Last, we open-source HINDSIGHT++ to the community allowing for further experimentation with a variety of datasets and applications [8]."
2011.10563,"dataset, open-source data, open-source",188,,,"5.1 Datasets We exploit 5Gophers5 [48, 49], the ﬁrst open-source dataset that attempts to study the performance of 5G in commercial settings, featuring three operational operators (two mmWave and one mid-band carrier) in the US. 5Gophers allows for a wide range of analyses, including impact of mobility, handoffs, network operator performance, and many more. In this study, we focus on a single walking, and two driving traces (i.e., medium mobility between 20 to 50Km), since we are primarily interested in the mobility aspect. The walking trace comprises from both 4G and 5G readings, followed by additional features, such as cell and handover identiﬁers6. The experimental setup under medium mobility features three SGS10 devices mounted on a vehicle’s front windshield and measuring the iPerf performance for three major operators in Atlanta, i.e., Sprint, T-Mobile, and Verizon. A bandwidth estimation measure is recorded every 120sec. All traces alongside the number of available samples are reported in Table 4."
2011.10563,"dataset, open-source data, open-source",96,,,"We complement our study with NYU-METS [50], an open-source dataset that comprises of 4G LTE bandwidth measurements carried out in the New York University (NYU) metropolitan area. NYU-METS covers several transportation modes including bus, subway, and ferry. The experimental setup features an LTE-enabled mobile device and a remote server located at the NYU lab. Transmission Control Protocol (TCP) measurements are carried out using the iPerf cross-platform tool with a sampling rate of one second. Likewise, Table 5 lists the available bandwidth traces"
2011.10563,"dataset, used dataset",63,,,"Overall, it is evident that the selected LSTM models adopt disparate strategies for minimizing the gradient descent error function, a result that veriﬁes the high degree of complexity in the neural network ecosystem. Next, we isolate a number of use cases per dataset in an effort to discover any hidden patterns or trends alongside the pool of hyperparameters."
2011.10563,github,37,,,"[50] https://github.com/NYU-METS/Main. [51] B. Krause, L. Lu, I. Murray, and S. Renals, “Multiplicative lstm for sequence modelling,” arXiv preprint arXiv:1609.07959, 2016."
2011.10563,provide implementation,61,,,"4 FRAMEWORK VALIDATION We organize the following content into three main parts. First, we provide an overview of HINDSIGHT++ technical and implementation details. Next, we discuss the training, validation, and testing process, while we outline the key LSTM parameters. Last, we present the available benchmarking options alongside a brief visualization description."
2011.10916,data,10,,,"5. handling larger dimension-ed data efﬁciently across the modalities,"
2011.10916,data,122,,,"Our observation is that aligning the entire data doesn’t improve the accuracy by a signiﬁcant margin, while we also agree that adding more depth to the DCCA in section 4.2 would certainly give us better results as the parameter count would go up. Generally, with multimodal fusion, we need a lot of parameters to capture all the nuances and complexity attached with cross-modal context and capturing these long-range minute dependencies might be improved as the parameter count go up. Thus, currently, the best way to improve accuracy seems to be adding more parameters on the upper sections of the model. We also suggest the following other ideas for the future work:"
2011.10916,data,137,,,"Now we brieﬂy discussion the hyper-parameter selection. Most of the attention literature uses Adam optimizer and we found that for this model, it gave stable decrease in the loss as well. Due to the large parameter count, model time complexity and limited computational resources; we only tried handful of hyper-parameter setups. We split train:validation:test data into approxiamtely 18.5k:2.25k:2.25k sample ratio, which is a 80:10:10 percentage ratio. We used the validation set for hyper-parameter tuning, upon ﬁnding the satisfactory setup, we merged validation and train set into one and re-learned the model with it; testing the generalize-ability of the model only on the test set. The selected hyper-parameters are given in Table 1. The model is shown in Figure 2c."
2011.10916,data,147,,,"We ﬁrst use our classiﬁcation head on all 3 unimodal delta self-attention module individually, to compare the gains of the proposed multimodal fusion technique later. Here we expect that multimodal fusion will add more context to the data and thus, the gains should be comparably higher. Third experiment is between aligned and unaligned data, with aligned data, we have aligned the sequence intervals of all 3 modalities according to the base text modality, parts of any modality not in the interval have been discard; with the unaligned data, we are not discarding any parts regardless of their inclusion in any interval, nor we have any “base” or reference modality. Our expectation is that aligned data of course gives us an indicator of which range to look into for a spoken word’s effect"
2011.10916,data,184,,,"Here we evaluate the results in the table 2 quantitatively. We couldn’t compare out per class binary classiﬁcation results with the MulT[22] model as they hadn’t reported it, so it’s unfair to claim that this model can outperform MulT on per class basis. We can still observe a couple of things from the all class classiﬁcation accuracy Acc6 and the F1 score: 1) Our model is giving very competative results to the MulT model. We had only used 3 base attention modules in place of 6 in the MulT model. This would signiﬁcantly reduce the computation requirement. 2) We can see that this multi-modal fusion has been effective in capturing the context of the situation because of its superior performance over uni-modal models. 3) Given that the model is performing on-par for unaligned data, we can prove our hypothesis that attention would indeed naturally provide the alignment and thus we do not explicitly need to align the sequences as a pre-processing step."
2011.10916,data,4,,,6.1 Pre-processing the Data
2011.10916,data,4,,,Aligned Data Unaligned Data
2011.10916,data,51,,,"If we ﬁnd out that we do not need to align the data, it will be a great time-saver for realtime inference. Our hypotheses is that due to the natural alignment provided by cross-attention, we wouldn’t need our data to be aligned for this architecture."
2011.10916,data,54,,,"Table 2: Table showing results of experiments on the test date for unimodal performance of the delta self-attended modules, fused DCCA modules, and then multi-modal hierarchical attention modules for both aligned and unaligned data. The results are compared with MulT, G-MFN, SotA1, SotA2, and SotA3."
2011.10916,data,91,,,"For all the approaches taken so far, the below mentioned missing gaps will be addressed in this project. In reccurent networks, looking at the entire sequence of data and outputting the last, single hidden state as its representation, often turns into the model forgetting information way past the current timestamp. They do not preserve the long-range dependencies [22] due to this bottleneck. This domino effect of updating a single hidden state each timestep can also result in exploding or vanishing gradients."
2011.10916,data,98,,,"Second, data can be unaligned across modalities. A frowning face may relate to a pessimistic word spoken in the past. It’s not always between current word and current expression. And thus, third related point is, neutral expressions are quite idiosyncratic [8]. Some people may always look angry given their facial conﬁguration. This raises the need for delta attention [28], we need to take cross-modal context plus the temporal context within each individual modality to negate the effect of “monotonous-across-the-time” features."
2011.10916,"data, data available",30,,,"[17] S. Nemati, R. Rohani, M. E. Basiri, M. Abdar, N. Y. Yen, and V. Makarenkov. A hybrid latent space data"
2011.10916,dataset,123,,,"To have the same dimensionality for the fusion at later stage, we take word-level granularity and for the words spoken at each interval (note that there is interval information alongwith features for all 3 modalities in the dataset), we pad and stack the visual (Facet 4.2) and acoustic (COVAREP) sequences, making all three sequences for a certain time interval [t, t + (cid:15)] having the same length. To make sure that the attention modules don’t interpret padded values (“[PAD]” for words and 0s for the other two), we use attention masks which will mask away the padded values."
2011.10916,dataset,2,,,5 Dataset
2011.10916,dataset,41,,,"[13] P. P. Liang and R. Salakhutdinov. Computational modeling of human multimodal language: The mosei dataset and interpretable dyanamic fusion. In First Workshop and Grand Challenge on Computational Modeling of Human Multimodal Language, 2018."
2011.10916,dataset,54,,,"Due to the novelty of the concept and components, we mainly focused the efforts on building the model architecture and trying out various layers more than trying the same architecture on various datasets. Thus, these experiments are only on the dataset described in section 5, CMU-MOSEI [2]."
2011.10916,dataset,60,,,"The dataset comes with high-level features in form of glove embeddings having 300 dimensions. But for the purpose of using a transformer, we used raw text to get the advantage of dynamic context alignment. The CMU-MOSEI SDK provides the facility to align the visual and vocal computational sequences with the verbal as the base modality reference."
2011.10916,dataset,73,,,"[2] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236–2246, Melbourne, Australia, July 2018. Association for Computational Linguistics."
2011.10916,dataset,75,,,"We trained the uni-modal self-attention modules for all 3 modalities L, V, A separately ﬁrst, this can be done in parallel as these 3 modules are independent of each other after the initial interval alignment performed in the pre-processing step. The selected hypermeter is given in Table 1. The hypermeter selection method of splitting the dataset into train:valid:test sets is given in the section 4.4."
2011.10916,"dataset, used dataset",122,,,"Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) 1 [2] is the largest and the latest dataset of sentence level sentiment analysis and emotion recognition. It contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics. It has around 23k samples, each with around 1000 text features, 300 visual frames at 15Hz sampling rate, and 150 acoustic features at 20Hz sampling frequency. Most of the transformer based papers have been benchmarked on this dataset, because of its size and the variety in “in-the-wild” emotions. Thus, this project will also use this dataset, to be able to compare the results fairly."
2011.10916,github,3,,,1https://github.com/A2Zadeh/CMU-MultimodalSDK
2012.01288,data,13,,,Research supported by BRD — Groupe Societe Generale Data Science Research Fellowships.
2012.01288,data,31,,,"1 Faculty of Mathematics and Computer Science 2 Human Language Technologies Research Center 3 Data Science Center University of Bucharest ana.uban@gmail.com, alina.ciobanu@my.fmi.unibuc.ro, liviu.p.dinu@gmail.com"
2012.01288,data,50,04/21/22,0,"17. Vulic, I., Moens, M.: Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014. (2014) 349–362"
2012.01288,dataset,42,,,"6. Ciobanu, A.M., Dinu, L.P.: Building a Dataset of Multilingual Cognates for the Romanian Lexicon. In: Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014. (2014) 1038–1043"
2012.01288,dataset,57,,,"The Romance Languages We compute the cosine similarity between cognates for each pair of modern languages, and between modern languages and Latin as well. We compute an overall score of similarity for a pair of languages as the average similarity for the entire dataset of cognates. The results are reported in Table 5."
2012.01288,"dataset, used dataset",48,,,"The Romance Languages vs English Further, we introduce English into the mix as well. We run this experiment on a subset of the used dataset, comprising the words that have a cognate in English as well4. The subset has 305 complete cognate sets."
2012.01288,publicly available,152,,,"1. Obtain word embeddings for each of the two languages. 2. Obtain a shared embedding space, common to the two languages. This is accomplished using an alignment algorithm, which consists of ﬁnding a linear transformation between the two spaces, that on average optimally transforms each vector in one embedding space into a vector in the second embedding space, minimizing the distance between a few seed word pairs (for which it is known that they have the same meaning), based on a small bilingual dictionary. For our purposes, we use the publicly available multilingual alignment matrices that were published in [12]. 3. Compute semantic distances for each pair of cognates words in the two languages, using a vectorial distance (we chose cosine distance) on their corresponding vectors in the shared embedding space."
2012.11723,data,131,,,The paper is organized as follows. In section II we start by describing a representative network that we use to illustrate the methodology to calculate latency bounds. Section III explains the analytical results behind these calculations. Section IV presents the analysis of the representative network. Section V discusses a free-rider principle which states that a fast network designed to transport the signals from cameras and other high data rate sensors can transport the ﬂows from CAN buses and other slow sources for free. Section VI explains a simple method to derive bounds on the latency and storage of fast ﬂows. Section VIII summarizes the main points of the paper and gives hints on which future in vehicle network architectures may beneﬁt from this work.
2012.11723,data,166,,,"From a more academic point of view, one could also assume links to be asymmetric in line rate. We examine the latency and memory utilization when links are designed to have a line rate equal to 125% of the peak bandwidth of data required, independent from the standardized Ethernet modes of operation. That is, we design the links so that their maximum utilization is 80%. We consider the case when the processor only sends slow trafﬁc to fast devices (e.g., cameras), as it it the situation that corresponds to the slowest links which might result in larger latency and memory occupancy. In particular, we assume that the processor can send at most a burst of 4 packets of 512 bits to a given camera. The delay of these four packets on the 10Mb/s link attached to the camera is then at least 4 × 512/(10Mb/s) = 200µs."
2012.11723,data,169,,,"the aggregation of camera data requires high line rate links. Again the speciﬁc execution of this model could vary greatly between manufacturers and few have actually come to the market so far. It could be argued that a shift away from privately owned individual vehicles towards automated robotaxi ﬂeets may drive this transition from functional clusters to geometrical zones, as the aspects of functional variance between customer chosen options, generational carry over and nameplate spread become less important for automated robotaxi ﬂeets while integration of systems and sensors becomes more important to solve the task of perception and control for driver-less operation in such vehicles. A further building block to allow this data convergence, which can be observed in the telecommunications business for quite some years already, is the only recent availability of multi-Gb/s Ethernet physical layer links for application inside a vehicle. All these aspects together create the environment wherein the concepts of this paper can be successfully deployed."
2012.11723,data,204,,,"Roy Myers received his Bachelor’s and Master of Science in Electrical Engineering from Georgia Institute of Technology in 1990 and 1991, respectively. He then joined National Semiconductor LAN Division designing mixed signal Ethernet IC products. In 1996, he joined Enable Semiconductor, as a founding member, developing Ethernet 100Base-TX transceivers. Enable Semiconductor was acquired by Lucent Technologies in March 1999. After leaving Lucent, he co-founded Terablaze Inc in 2000 where he was Director of Engineering developing highly scalable network fabrics and layer 2/3 Gigabit Ethernet switching solutions. TeraBlaze Inc was acquired by Agere Systems in 2004. Roy continued to lead the Gigabit Ethernet switch development at Agere and then at LSI after acquisition of Agere Systems in 2007. He left LSI in 2007 to join Aquantia Corp (AQ:NYSE) where he was Chief Architect developing a series of 10GBase-T and Multi-Gig MAC/PHY products to serve client, enterprise, and data center markets. Since April 2018, Roy is a co-founder and SVP of Engineering of Ethernovia Inc developing network solutions for the next generation of automobiles. Roy holds 14 granted patents in communication technology."
2012.11723,data,219,,,"Due to the very different approaches of OEMs towards their product strategy and resulting network architectures, we need to classify them based on more abstract concepts in order to compare them. The most common starting point is and was a functional clustering. Here electronic control units (ECUs) which are generally related in their functionality and thereby often designed within the same organizational branch of the OEM [29] are connected together directly, offering only very limited interfaces to systems of other functional clusters. Such clusters may for example be the engine, the drive train, or the infotainment system. The theoretical extreme of this is often described as a domain-based system. How these different functional clusters or domains interact is very different for different OEMs. As the need to exchange data between domains has increased, e.g., to avoid duplication of expensive sensors in constrained packaging spaces, the so-called zonal model has gained much attention. In the zonal architecture the focus lies on integrating different functionalities onto a smaller set of ECUs, which share access to sensors and actuators. This also leads to shorter cables and more importantly for this paper, a reduced number of hops in the network where particularly"
2012.11723,data,219,,,"Each source, such as a camera, a radar, an audio/video server, a 5G antenna, a CAN electronic control unit, a button, and so on, is attached to a bridge that polices and shapes the trafﬁc. The policing veriﬁes that the source is not misbehaving, such as a faulty switch that thinks it is being pushed every millisecond. The shaping separates the packets by the maximum gap consistent with the deadline to deliver a group of packets or with the rate of a stream or ﬁle transfer. For instance, say that a camera produces 2400 packets of 1500 Bytes every 16 milliseconds and that one wishes to deliver these 2400 packets in about 12 milliseconds. Then, the bridge transmits one packet every 12ms/2400 = 5µs. As another example, a bridge for a 5G antenna could send one 1500 Byte sized packet every 10µs to carry a data rate of 1.2Gb/s. Thus, every source of trafﬁc is shaped, including best effort sources. We assume that three of the four fast devices attached to a core switch send 12, 000b (or 1, 500 Bytes) packets every 5µs and one sends 12, 000b packets every 15µs."
2012.11723,data,42,,,"For instance, a network designed to transport the data from cameras, radars, lidars, audio/video servers to processors and user interfaces can also transport the slow ﬂows between CAN buses, push-buttons, relays, and so on."
2012.11723,data,52,,,"The network devices are classiﬁed as core, fast, or slow. Using Rene Cruz’s results, one shows that a network fast enough to transport signals from cameras and other fast sensors can also transport the data from slow sources and guarantee them their desired bounded latency."
2012.11723,"data, data available",85,,,"required to control the camera. With the introduction of Energy Efﬁcient Ethernet (EEE)[30] this issue has partially been addressed. In EEE the power consumption of the lower bandwidth direction is reduced by turning off the transmitter while no data is available. Thus, the line rates at which the frames are transmitted remain symmetrical. As the Small Flow Approximation only depends on the line rate of a frame, it is completely untouched by EEE."
2012.11723,download,152,,,"Thus, this architecture uses two central components of the IEEE time-sensitive networks standards: policing and shaping. It locates these components in the source bridges, instead of the network switches. The main beneﬁts of this approach are that the network uses a single priority class and that no further trafﬁc reshaping or scheduling is required in the switches. As we explain in the paper, control signals are delivered with guaranteed sub-millisecond latency even though they share queues with bursty best effort trafﬁc and audio/video streams. These latency guarantees are deterministic, not probabilistic: they are worst-case guarantees derived from analyzing the worst case behavior of the network, not by simulations. The architecture ensures that the web download of a new movie cannot interfere with the break signal from the self-driving control system or from the break pedal. Such guarantees are"
2012.12578,data,111,,,"Web Services are collection of standards that combined to offer services one node to another [3]. The Web Service uses Internet technology like the Hypertext Transfer Protocol (HTTP) as a protocol to transport data [4]. Although, other protocols such as the Java Messaging System (JMS) protocol, File Transfer Protocol (FTP) and (SMTP) also exist for the exchange of services provided by Internet devices, HTTP has been the standard for such exchange [5]. Hence, the SOAP as an XML-based protocol can be exchanged through various data formats and transport protocols."
2012.12578,data,127,,,"The client response time is higher than the server response time. This claim is evident from client response time trend line that indicates upward transition with the payload. The client starts the communication by obtaining a Java Naming and Directory Interface (JNDI) connection to the server which provides the access to the connection factory and the connection queue. The client always reads the data from queue in sequence and this process is resource demanding. On the other hand, the server produced and put the payload on the queue in any transaction. Since JMS supports different states and connects once, the server uses less resources to push the request, hence the payloads are queued with little effort."
2012.12578,data,149,,,"Abstract - Although serialization improves the transmission of data through utilization of bandwidth, but its impact at the communication systems is not fully accounted. This research used Simple Object Access Protocol (SOAP) Web services to exchange serialized and normal messages via Hypertext Transfer Protocol (HTTP) and Java Messaging System (JMS). We implemented two web services as server and client endpoints and transmitted SOAP messages as payload. We analyzed the effect of unserialized and serialized messages on the computing resources based on the response time and overhead at both server and client endpoints. The analysis identified the reasons for high response time and causes for overhead. We provided some insights on the resources utilization and trade-offs when choosing messaging format or transmission protocol. This study is vital in resource management in edge computing and data centers."
2012.12578,data,22,,,when the GC assumes the data object to be large hence forcing it reclaim unused memory space for a new object.
2012.12578,data,43,,,"[5] B. Billet and V. Issarny, “Dioptase: a distributed data streaming middleware for the future web of things”, Journal of Internet Services and Applications, vol.5, issue 1, pp. 76-81, 2014."
2012.12578,data,65,,,"These studies investigated the effect of serialization and/or improved on the performance of SOAP. To complement the previous studies, our research identified the causal effects of the overheads and the response time of exchanging serialized high-volume messages via different transport protocols. The outcome can provide a new dimension of how improving SOAP performance can enhance the data exchange on distributed systems."
2012.12578,data,90,,,"A study conducted by [9] made a comparison between binary and XML serialization on .net and Java platforms. The authors used object types to compare the two data formats. The study presented a breakdown of serialization effect by analyzing duration taken by an object to be serialized in the memory on each platform. In the findings, Java proved to be better in handling binary serialization than the .net. On the other hand, .net is better in supporting deserialization of any object."
2012.12578,data,90,,,"Message serialization is one of the formats of information exchange among the applications in distributed systems [6]. The serialization process translates object into stream of bytes and transmits over the network [7]. Deserialization on the other hand, converts back the streamed bytes into its original form [8]. Serialization eliminates the need for procedure calls by creating object bytes of the data and works well for processing transitory data. These attributes made serialization to improve response time in communication."
2101.03069,data,112,,,"A manual search identified research output on children's health and school reopening within the three countries. The focus on the scientific output in the three countries varies. In Spain, significant attention has been given to hospitalization and to severe cases of children with COVID-19. Tagarro et al. (2020) reported on the early screening and severity of coronavirus in Madrid by investigating data from 365 tested children in the first two weeks of March 2020. Attention has also been given in Spain to the psychological effects and well-being of children (Orgilés et al., 2020; Idoiaga et al., 2020)."
2101.03069,data,142,,,"The data collected on news outlets and tweets were used to investigate the activity and topics covered in the (social) media when reporting scientific outputs, for each country. Overlay maps were used to show those topics. We include overlay maps for tweets in this chapter. We mention that single noun phrases ""covid"" and ""children"" were removed from the maps since they are redundant for our analysis. The overlay maps for news outlets are uploaded on figshare. For illustrative purposes, we selected examples to study differences in the reporting of scientific literature in the news media, by individually reading and analyzing the contents of selected scientific and news articles, and the tweets text. The specific findings will be reported for each country in the following section."
2101.03069,data,192,,,"Twitter data on mentions of publications were also collected from Altmetric.com in October 2020. This included any tweet identified by Altmetric.com that refers to a DOI in our set of publications. More detailed Twitter data was rehydrated directly from Twitter (using the Twitter API) on December 2nd, 2020. This resulted in a total number of 540,615 tweets, covering 66.7% (3,811) of the publications. The first identified tweet was on January 14, 2019, and the last recorded one on October 24, 2020. From our tweet data, 65.8% (182,548) of the 277,419 distinct Twitter users provided geolocation information. This allowed us to link tweets to the three countries selected in our study. We identified 16,548 tweets with a Spanish geolocation, which referenced 932 distinct DOIs. Much less Twitter activity was captured in the cases of the Netherlands and South Africa. In the Netherlands, 1,478 tweets were collected, linking to 229 distinct DOIs, whereas in South Africa, 1,062 tweets could be linked to 290 distinct DOIs."
2101.03069,data,3,,,Data and methods
2101.03069,data,51,,,"Peeples, L. (2020). Face masks: what the data say. Nature, 586(7828), 186-189. Pollán, M., Pérez-Gómez, B., Pastor-Barriuso, R., Oteo, J., Hernán, M. A., Pérez-Olmeda, M., ... &"
2101.03069,data,58,,,"Lastly, data on policy interventions regarding the closure and reopening of schools was retrieved from the UNESCO Institute for Statistics, which includes daily global information on the state of schools since the outbreak of the pandemic. With regard to the announcements and specificities of the measures, we manually searched national news media platforms."
2101.03069,data,58,,,"The authors would like to thank Altmetric for providing access to data on news and Twitter mentions. Rodrigo Costas is partially funded by the South African DST‐NRF Center of Excellence in Scientometrics and Science, Technology, and Innovation Policy (SciSTIP). Jonathan Dudek is partially funded by TU Delft COVID-19 Response Fund.   References"
2101.03069,"data, data available",134,04/21/22,0,"dominant Twitter accounts to be found in the data. One would expect to find highly active Twitter users in cases where the platform is used to amplify messaging. In the data, only two accounts (both from Spain) were found to have tweeted more than 100 times during the eight-month period. One of the accounts belongs to a paediatrician while the other to the Spanish Society for Paediatric Infectious Diseases. In the absence of an ideologically-motivated group, movement or collective, and some evidence that scientific rather than political activity is the driver of social media activity, Twitter is not in this case being used as a communication platform to amplify messaging about the risks or benefits of children attending school during the COVID-19 pandemic."
2101.03069,"data, data available",255,,,"The scientific output results in mixed evidence of infection and transmission as they pertain to children. The limitations of the scientific studies and the consequent levels of uncertainty were conveyed when reporting findings. This is, however, not a unanimous approach. For example, a viewpoint in the Archives of Disease in Childhood (Munro and Faust, 2020), is entitled “Children are not COVID-19 super spreaders: time to go back to school”. The title appears to be inflated by the urgent need for policy decisions. The authors write “At the current time, children do not appear to be super spreaders. Serosurveillance data will not be available to confirm or refute these findings prior to the urgent policy decisions that need to be taken in the next few weeks such as how and when to reopen schools.” They continue “Governments worldwide should allow all children back to school regardless of comorbidities. Detailed surveillance will be needed to confirm the safety of this approach, despite recent analysis demonstrating the ineffectiveness of school closures in the recent past (Viner et al. 2020b). The media highlight of a possible rare new Kawasaki-like vasculitis that may or may not be due to SARS-CoV2 does not change the fact that severe COVID-19 is as rare as many other serious infection syndromes in children that do not cause schools to be closed”. The title suggests no uncertainty"
2101.03069,"data, database",149,,,"The two databases (WHO and CORD-19) do not represent distinctive sets of publications, having quite  substantial overlap. In order to avoid duplicates, the two databases were merged and cleaned. For a reliable merging of the two databases, as well as for the further tracing of the (social) media reception of the publications, it was necessary to count with unique document identifiers (e.g., PubMed Identifiers, Digital Object Identifiers, etc.). Particularly Digital Object Identifiers (DOI) are commonly assigned to scientific publications to univocally identify scientific documents across databases and the web-at-large. The main inconvenience of using DOIs is that we can only identify and combine publication data for half of the papers included in the CORD-19 database and a third of those included in the WHO database (Figure 1)."
2101.03069,"data, dataset, data available",125,,,"We also investigated the profiles of tweeters whose tweets were collected in our dataset. In particular, we looked into the share of tweeters who had tweeted about science (i.e., tweeted an academic publication) before the pandemic, that is, tweeters from our sample present in altmetric data from 2019. We found that 59.3% in Spain, 60.6% in the Netherlands and 65.1% in South Africa had mentioned other scientific articles in their tweets prior to the pandemic. From the total of 8,597 distinct tweeters identified in all three countries, 5,141 had already referenced scientific output before the COVID-19 pandemic. Moreover, we attempted to determine the professions of the 8,597 tweeters."
2101.03069,"data, dataset, database",131,,,"Data collection The data collected for this study was extracted from a variety of sources: scientific publications, news outlets, and social media discussions and policy interventions. Since the outbreak of the pandemic, different community- and organization-led initiatives have been conducted to make scientific publications on COVID-19 openly accessible. In this study, we made use of the COVID-19 Open Research Dataset (CORD-19) and the World Health Organization (WHO) COVID-19 Global literature on coronavirus disease database. These two databases are of special interest due to the combination of sources they include, containing not only studies published in scientific journals but also preprints from the main global repositories (e.g., BioRxiv, MedRxiv, SSRN, etc.)."
2101.03069,"data, dataset, database",216,,,"A final number of 5,713 publications along with their DOIs have been collected in our final dataset of scientific output. We proceeded to identify news outlets and social media discussions around the scientific publications in our dataset. News media items mentioning a DOI in our set were identified with data from Altmetric.com, retrieved in October 2020. From a total of 19,922 news items found globally for the set of DOIs in our database, 424 news articles could be identified as originating from the Netherlands, Spain, or South Africa. This was done by matching the URLs of the news outlet coming from Altmetric.com with the URLs of Dutch, Spanish, and South African national newspapers and broadcasting services, as extracted from Wikipedia and other websites listing news outlets. The final list of news outlets from each country was verified and curated manually. We identified 200 news items from Spain, which referenced 81 distinct DOIs. In South Africa, 79 news pieces referenced 72 distinct DOIs and in the Netherlands, 145 news items referenced 83 distinct DOIs. The titles and short abstracts of the news articles (where available in the data from Altmetric.com) were analyzed manually  for our study."
2101.03069,database,1,,,database
2101.03069,database,145,,,"The available scientific output about the role of children and schools in the COVID-19 pandemic has not been picked up in the social media in the three countries in our study to the same degree. We found that only 17.9% of the publications in our database have been tweeted about in the three countries; this is much less than the coverage of about 63.0% of all attention for CORD-19 publications as overall captured by Altmetric (Colavizza et al. 2020). A total of 932 DOIs (16.3% of the scientific output) has been mentioned in the Spanish tweets on the topic. In the Dutch tweets, only 4% of the scientific output (229 articles) has been mentioned, whereas in South Africa 289 articles (5%) have been mentioned."
2101.03069,database,222,,,"Spain While in the Netherlands and South Africa schools reopened after around two months of closure, in Spain school reopening was delayed until after the summer holidays. Figure 3 depicts the announced and implemented measures, in chronological order, both at the national, as well as the regional levels. The policy measures registered no difference between primary and secondary schools. The figure also includes the timeline distribution of the news outlets and tweets in our database, which have been identified as originating from Spain. A total of 188 news articles and 15,603 tweets were identified between the beginning of February and the end of September 2020. News articles on the topic registered brief appearances before the school closure in March, as well as more consistent appearances around the reopening of schools in September. As for tweets, we can observe small peaks around the time of the announcements in March, as well as shortly before and after the schools reopening in September. Further activity has been registered during the school closure, with peaks around end of April, when the government announced a plan for easing lockdown restrictions, as well as in July and August, when no other policy intervention has been announced nor occurred."
2101.03069,database,67,,,"Note: Number of total publications by database [Pubs], publications in 2020 [Pubs in 2020], share of publications with Document Object Identifier (DOI) [%DOI in 20202], number of publications related to children and schools [Pubs children] and share of publications with a DOI related to children and schools [%DOI children]"
2101.03069,dataset,90,,,"An overview of the topics covered by tweets from Spain is depicted by a VOSviewer map in Figure 4. The nodes in the map present the co-occurrence of the most relevant keywords identified from the titles of the 5,713 articles in our dataset. The color coding of the map reflects the prevalence of mentions of those articles in tweets from Spain relative to the worldwide collected tweets: The darker the color, the more focus on the keywords relative to the worldwide tweets on the topic."
2101.03069,dataset,92,,,"We identified 740 researchers, 741 health professionals, and 296 journalists based on terms found in the user descriptions of the tweeters. We note a possible overlap between the groups, as someone can be both a health professional and, e.g., hold a PhD (one of the indicators for being a researcher). Given the limited available information on Twitter and the limitation of our search algorithms, we expect that these results are underestimating the true presence of those professions in our dataset.  Discussion"
2101.03069,"dataset, database",88,,,"We downloaded the two complete databases on October 15, 2020. Table 1 shows some descriptive values of the size of the database at the time. We searched within the title and abstract fields for documents containing the words ‘children’ and ‘schools’. After merging the ‘Pubs children’ documents of both datasets, a total of 5,713 publications were retrieved. This is our final set of scientific publications from which we trace their (social) media reception."
2101.10245,data,102,,,"The average true positive rate per gesture per user of the different configurations is shown in Figure 2. Many configurations result in similar performance, but the best configuration was found to be: window size of 4096 points, 50% overlap, and 16 bins above and below 𝑓0. More details about the machine learning and cross validation techniques are discussed later. We save the STFT for three seconds of time data (discarding the initial startup windows). An example of the STFT with the best found configuration can be seen in Figure 3."
2101.10245,data,111,,,"In the first phase, 8 participants were recruited from university classes (age range: 19-30, Male: 60%). During a session, participants were introduced to the AirWare data collection mobile application and a demonstration of all the gestures were given via the video recording. Participants were then instructed to show the researcher each gesture. They weren’t told about the sensor locations on the phone. The ambient environment was relatively quiet and without many acoustic disturbances. Participants were instructed to hold the smart-phone in one hand and perform gestures “above” the phone with the other hand."
2101.10245,data,134,,,"Figure 10 shows the performance of ‘Model 3’ and ‘Random Forest’ as we gradually increase the percentage of calibration data from 10% to 50%. We increase the training size by 10% and evaluated the models using the remaining data from the user not used in calibration. As we can see in Figure 10, both ‘Model 3’ and the ‘Random Forest’ model gradually increase performance as more user-specific calibration data is added. Moreover, both models begin to saturate between 30% and 50% of training data used from the user. If we assume saturation is achieved at 50%, this corresponds to the system needing 2–3 examples of each gesture from the user during calibration."
2101.10245,data,146,,,"Different users participated in each phase to protect against crossover effects. That is, no user participated in both phases of the data collection. We show later on that requiring the IR sensor to be activated greatly increases the ability of the machine learning algorithm to correctly identify the gesture. Practically, this means that the AirWare system will almost certainly require an “instructional application” that trains users to perform the gestures, and then verifies that the user understands how to perform the gesture such that the IR sensor is activated. While this is an additional limitation of the system because it imposes constraints on the gestures, such an instructional application would likely be required no matter what, as learning to perform 21 gestures for any user without some instruction can be considered a daunting task."
2101.10245,data,15,,,Fig. 1. Progression of the AirWare interface used in our data collection.
2101.10245,data,151,,,"When training the network, we apply random perturbations to the input spectrogram and IR sequences to help avoid over fitting and increase generalization performance (i.e., data expansion). We randomly shift the data temporally up to 10%. That is, we shift the entire spectrogram sequence forward or backward in time randomly by up to 10%. The sequences are 2.5 seconds in duration, so this means that the the spectrogram and/or the IR stream might shift by 250 ms. This is applied to the IR data and the spectrogram separately (resulting in different random time shifts). This helps with generalization performance because the Samsung Gesture API is somewhat inconsistent in the timings for when it provides the push notification that the IR sensor has been activated. Therefore this data expansion mirrors the actual use case well."
2101.10245,data,161,,,"In the second phase, 13 participants were recruited (age range: 19-30, Male: 66%). Participants were similarly introduced to the data collection application but were also instructed about the location of IR proximity sensor on the phone (as described). The user interface showed the participant whenever the IR sensor detected a movement through an animated label on the application. The gesture data was registered only when the IR sensor detected a movement; otherwise, the interface prompted the user to repeat the gesture. On average, users had some initial trouble learning how to manipulate the sensor for some gestures such as “tap” but were quickly able to alter their strategy to tap towards the top of the phone (where the IR sensor was located). All users were able to successfully activate the IR sensor after two or three trials per gesture."
2101.10245,data,161,,,"of. Each phase differed in what the data collection application judged to be a properly performed gesture. In the first phase, we collected gesture data from the participants for every gesture in our vocabulary regardless of whether the IR sensor was activated. That is, the user performed a gesture based upon their memory of how the gesture was performed from the instructional videos. In the second phase, we only informed the participant that a gesture was performed successfully when the IR proximity sensor was activated. That is, they were asked to repeat the gesture until they learned how to perform the gesture while also activating the sensor at the top of the phone screen. In this way, users needed to manipulate the way they performed the gesture such that they understood where the proximity sensors was physically located on the phone and how to activate it with each gesture."
2101.10245,data,165,,,"7.3.3 User Calibrated Model. In this cross validation strategy, we combine knowledge from the previous two strategies to test the performance of the model. We first split the data based on the user; 𝑁 − 1 users’ data in the training set. From the 𝑁 𝑡ℎ user data, we perform a 5-fold 60%-40% stratified shuffle split, as done for the personalized model. We then combine the training data from the 𝑁 − 1 users with the 60% split of training data from the 𝑁 𝑡ℎ user and use the remaining 40% of data from the 𝑁 𝑡ℎ user as a testing set, as shown in Figure 7. The model performance for each user improves significantly for all users with this training strategy. Thus, the model learns from other users as well as the test user to classify the gestures of the test user. Note that this"
2101.10245,data,187,,,"Participants performed 21 different gestures as instructed on the screen of the phone via a custom data collection app. A gesture name would appear on the screen and the user would perform the in-air gesture (Figure 4). All sensor data was saved locally on the phone for later processing. Users went through each gesture one time as practice (practice data was not used in analysis) and then were presented with a random permutation of the gestures. For participants in the second phase, the practice session lasted as long as was needed for the subject to learn how to activate the IR sensor. In all, each participant performed between 5 and 10 iterations of each gesture. The different number of gestures per participant is an artifact of the way the gestures were randomly presented. We let participants perform gestures for 45 minutes and then ended the session. On average, each participant performed about 250 gestures. Note that the smartphone was used for data collection only. Subsequent analysis was performed offline."
2101.10245,data,191,,,"7.3.1 Leave One User Out. We explore the performance of our classifiers using ‘leave one subject out’ cross validation strategy. The strategy used in this case is to train the model on data from 𝑁 − 1 users and test it on the 𝑁 th user, as described in Figure 7. This approach analyzes whether we can classify the gestures successfully without requiring the system to be calibrated. This implies that for practical implementations, we can directly use a pre-trained, out-of-the-box classifier to classify the gestures. Through this strategy, we try to generalize the learning of our classifier across different types of users. This is the ideal scenario for a gesture system, requiring no user input or calibration before use. As can been seen from Figure 8, average true positive rate per gesture per user ranges from approximately 19% to 46% for different classifiers. Our best performing model in this case is ’Model 3’ which is a deeper network in terms of the number of convolutional layers and dense layers."
2101.10245,data,241,,,"We introduce AirWare, an in-air hand-gesture recognition system that uses the already embedded speaker and microphone in most electronic devices, together with embedded infrared proximity sensors. Gestures identified by AirWare are performed in the air above a touchscreen or a mobile phone. AirWare utilizes convolutional neural networks to classify a large vocabulary of hand gestures using multi-modal audio Doppler signatures and infrared (IR) sensor information. As opposed to other systems which use high frequency Doppler radars or depth cameras to uniquely identify in-air gestures, AirWare does not require any external sensors. In our analysis, we use openly available APIs to interface with the Samsung Galaxy S5 audio and proximity sensors for data collection. We find that AirWare is not reliable enough for a deployable interaction system when trying to classify a gesture set of 21 gestures, with an average true positive rate of only 50.5% per gesture. To improve performance, we train AirWare to identify subsets of the 21 gestures vocabulary based on possible usage scenarios. We find that AirWare can identify three gesture sets with average true positive rate greater than 80% using 4–7 gestures per set, which comprises a vocabulary of 16 unique in-air gestures. CCS Concepts: • Human-centered computing → Gestural input; Ubiquitous and mobile devices; • Computing methodologies → Machine learning; Neural networks;"
2101.10245,data,251,,,"In our pilot tests, we asked participants to perform each of the 21 gestures in the way “that made the most sense to them.” In this way, we sought to collect more realistic data where participants could be trained simply from a textual prompt of what the gesture was, without explicit training or demonstration. Therefore, we thought the gesture would be more intuitive to the user (since they exhibit their internalization of the gesture, rather than mimicking a gesture they were shown). However, this data was never classifiable at a rate more than chance. We abandoned the idea that a large gesture vocabulary could be collected without explicitly demonstrating the gestures to participants. Based on our experience in the pilot study, we decided to update our methodology to include showing videos of the gestures being performed. Participants were then asked to perform the gesture to demonstrate their understanding. Therefore, all participants were shown how to perform the gestures and participants demonstrated their understanding to the researchers before data collection started. Practically, this also means that new users of AirWare would also need to go through the same instructional videos to learn how to perform the gestures in the vocabulary. We see this as a necessary limitation of the AirWare system: without an instructional phase, there is too much variability among the gestures performed to detect them reliably."
2101.10245,data,257,,,"When a user performs a gesture, it may or may not activate the IR sensor. We performed two rounds of data collection. The first did not require that users activate the IR sensor with the gesture and the second did require that the IR sensor be activated. The segmentation procedure differs slightly between these two scenarios. When we required the user to activate the IR sensor, segmentation was straightforward: we buffer the audio signal 1.25 seconds before and after the IR activation. When we did not require the IR sensor to activate, we buffered 1.25 seconds before and after any “event of interest.” We define this event to be when either the IR sensor is activated or when the magnitude of frequency bins directly greater than and less than 𝑓0 increase by 10 dB. Intuitively, this occurs when there is enough motion to cause reflections of the Doppler audio signal. We also note that, when not requiring the IR sensor to be activated, we expect an increased number of false positives because any motion might trigger the segmentation algorithm. Positively, requiring the IR sensor to be activated by the gesture can be considered an effective means of reducing false positives. Negatively, it also requires users to manipulate their gestures in a way that they always trigger the sensor at the top of the phone. This limitation is discussed in more depth in the next section."
2101.10245,data,325,,,"this research question, our network is modified to use only the IR activation information or only the Doppler signature information. From Figure 5 this corresponds to only using one of the input branches in the network. The performance of the model trained using individual sensor information is compared with the performance of the model trained using the combined sensor information. The cross validation strategy used in all cases is leave-one-subject-out, wherein we train the model on 𝑁 − 1 users’ data and test the model on the 𝑁 th user data. If we only look at the best performing models from Figure 6, we see that we are able to achieve an average true positive rate of 35% per class with a standard error of 0.02% when only the IR sensor information is used. Note that model 2 and 3 are identical when only using the IR signal branch. In comparison, using audio Doppler only sensor information results in a best performing model with average true positive rate of 13% with a standard error of 0.03%. From Figure 6, we can see that the performance with only IR information is better than performance of using Doppler only regardless of the machine learning model employed. However, when we combine the information from the two modalities, the performance improves for all convolutional neural network models and the random forest model. Thus, we conclude that combining the two sensing modalities is advantageous for in-air gesture recognition, resulting in a performance increase of about 10% average true positive rate per gesture. The improvements are statistically significant based upon a two-tailed T-test (𝑝 < 0.01). In all analyses in the remainder of the paper, we use the combined Doppler and IR sensor modalities as features for the machine learning models."
2101.10245,data,329,,,"Recall that these data sets are collected using separate experiments and different users. In each scenario, we train the models using leave-one-subject-out cross validation. That is, no subject’s data is simultaneously used for training and testing. For our evaluation metric, we choose the average true positive rate per gesture. Because class imbalance exists, accuracy is not a good indicator of performance as classes that occur less often will receive less weight in the evaluation. Moreover, binary scores like recall and precision are harder to interpret when micro or macro averaged. Per-class true positive rate, alternatively, captures how well we perform for each gesture. For this analysis, we choose to use the random forest baseline model, as it is the best performing baseline model (discussed later). Table 1 describes the per-class true positive rate for requiring versus not requiring the IR sensor to be activated for the random forest model. The main conclusion we draw from Table 1 is that requiring the IR sensor to activate does increase the performance of the AirWare algorithm. Moreover, there are other advantages for requiring that the sensor be activated such as reducing false positives and reducing needless computation. This is because the audio Doppler signal is likely to result in a number of false positives from movement by the user and near the user; whereas the IR sensor is relatively robust to these types of noise. However, requiring that the IR sensor be activated also requires users to manipulate the way they perform in-air gestures to activate the sensor. In this way, the AirWare system will likely require some instruction to the users for how to reliably perform different gestures. Thus, in the remainder of our analysis we only use the gesture set that requires IR activation to segment gestures."
2101.10245,data,332,,,"7.3.2 Personalized Model for Each User. In this analysis, we analyze the performance of our classifier by calibrating the model to each user. For each user, we perform a 5-iteration 60% training and 40% testing stratified, shuffled split, as described in Figure 7. The test size is chosen to make sure there are at least 2 instances of each class are present in the test data [Pedregosa et al. 2011]. In effect, we train 13 independent models using only data collected from a specific individual for training and testing. We would like to see if the variation in gesture performance within the user is able to predict the classes successfully. This cross validation mirrors a use case where users would need to provide example gestures to the system as a calibration phase. This is less ideal in terms of practical usage but may be necessary to increase performance. From Figure 8, we see that, on average, the performance deteriorates for all the models as compared to leave one subject out. In this case, Random Forest performs equally well compared to ’Model 3’ at approximately 24% average true positive rate per gesture per user. From Figure 9, we can see that none of the users benefit from a fully personalized model when compared with ‘leave one subject out’ performance. We are able to achieve an average true positive rate of 23.68% per class with a standard error of 0.03% across users for the 21-class gesture set. It is unclear, however, if the personalized models do not perform consistently because the training data is limited. Convolutional networks tend to require large amounts of training data to perform well, so it is possible that the decrease in performance is due to a significantly smaller training set."
2101.10245,data,334,,,"For each user, we perform a 5-iteration 60% training and 40% testing stratified, shuffled split, as described in Figure 7. The test size is chosen to make sure there are at least 2 instances of each class are present in the test data [Pedregosa et al. 2011]. In effect, we train 13 independent models using only data collected from a specific individual for training and testing. We would like to see if the variation in gesture performance within the user is able to predict the classes successfully. This cross validation mirrors a use case where users would need to provide example gestures to the system as a calibration phase. This is less ideal in terms of practical usage but may be necessary to increase performance. From Figure 8, we see that, on average, the performance deteriorates for all the models as compared to leave one subject out. In this case, Random Forest performs equally well compared to ’Model 3’ at approximately 24% average true positive rate per gesture per user. From Figure 9, we can see that none of the users benefit from a fully personalized model when compared with ‘leave one subject out’ performance. We are able to achieve an average true positive rate of 23.68% per class with a standard error of 0.03% across users for the 21-class gesture set. It is unclear, however, if the personalized models do not perform consistently because the training data is limited. Convolutional networks tend to require large amounts of training data to perform well, so it is possible that the decrease in performance is due to a significantly smaller training set. This motivates us to combine our two cross validation strategies in order to increase the amount of training data, but also employ a personalized calibration procedure."
2101.10245,data,338,,,"In this section, we compare the predictive ability of gestures collected requiring that the IR sensor be activated versus not requiring the IR sensor to activate to segment gestures. Recall that these data sets are collected using separate experiments and different users. In each scenario, we train the models using leave-one-subject-out cross validation. That is, no subject’s data is simultaneously used for training and testing. For our evaluation metric, we choose the average true positive rate per gesture. Because class imbalance exists, accuracy is not a good indicator of performance as classes that occur less often will receive less weight in the evaluation. Moreover, binary scores like recall and precision are harder to interpret when micro or macro averaged. Per-class true positive rate, alternatively, captures how well we perform for each gesture. For this analysis, we choose to use the random forest baseline model, as it is the best performing baseline model (discussed later). Table 1 describes the per-class true positive rate for requiring versus not requiring the IR sensor to be activated for the random forest model. The main conclusion we draw from Table 1 is that requiring the IR sensor to activate does increase the performance of the AirWare algorithm. Moreover, there are other advantages for requiring that the sensor be activated such as reducing false positives and reducing needless computation. This is because the audio Doppler signal is likely to result in a number of false positives from movement by the user and near the user; whereas the IR sensor is relatively robust to these types of noise. However, requiring that the IR sensor be activated also requires users to manipulate the way they perform in-air gestures to activate the sensor. In this way, the AirWare system will likely require some instruction to the users for how to reliably perform different gestures."
2101.10245,data,350,,,"We now seek to understand if the performance of the classifier can be improved by reducing the simultaneous number of gestures that a classifier must distinguish for a given application. In this scenario, we wish to divide the gestures into smaller subsets based upon what combinations of gestures are most appropriate for different categories of applications. We assume that the application in question somehow instructs the user of what gestures are currently supported. If a user were to perform an unsupported gesture, the system would misinterpret that gesture. Sub-setting the gesture set, we enumerate 4 different categories: Generic, Mapping, and Gaming. Each gesture set comprises 4 to 7 gestures. Together, these categories include 16 distinct gestures. Thus, the vocabulary is large, but managed by never having more than 7 gestures available at a time. We test the performance of the model for these reduced gesture sets using ‘user calibrated’ strategy discussed above. We also employ the most accurate architecture as selected through previous analysis and parameters remain the same as from our previous hyper-parameter tuning. Confusion matrices are generated in the same manner as previously discussed. A summary of the different reduced sets overall and per user appears in Figure 11. As shown, there are a number of users for which the system works well for and a number of individuals that it does not always achieve high true positive rate. In particular, users 1, 3, and 4 have reduced recognition rates compared to other users. Upon review of the data, these also corresponded to users that did not perform many practice trials while learning the gestures. These users only practiced the gestures one time compared to other participants performing gestures multiple times before they reported that they were ready to start the experiment. As such, these participants may have rushed through the learning of the gestures or not taken the experiment as seriously as others."
2101.10245,data,42,,,"In this section, we analyze the performance of using IR sensor data only, Doppler data only, and the combined sensors. With this analysis we seek to understand how advantageous it is to combine the sensors. To investigate"
2101.10245,data,42,,,"Once all data is collected for all users, we employ normalization of each of the IR features (angle and velocity) and of the entire spectrogram magnitudes such that the all features are zero mean and unit standard deviation."
2101.10245,data,67,,,"(4) We investigate personalized calibration to boost the recognition true positive rate of the classifier, as well as providing an out-of-the-box gesture recognition system, showing that user calibration improves the performance of AirWare. We also investigate the amount of training data required to calibrate the AirWare system, concluding that 2-3 examples per gesture are needed to properly calibrate the system."
2101.10245,data,68,,,"Finally, we also wish to understand about how much training data is required before the performance of the different models begins to saturate. That is, about how many calibration examples are required before the performance plateaus? To investigate this question we look at the training curves for ‘Model 3’ and ‘Random Forest’ since these are the best performing models."
2101.10245,data,94,,,"(2) A human subjects evaluation: we validate the technology in a user study with 13 participants. (3) We compare two different methods for collecting gesture data. The first requires the IR sensor to be activated and the second is a free-form system. We conclude that the free-form system creates variability in the way gestures are performed such that the machine learning algorithms cannot readily identify the gestures. Therefore, the AirWare system requires users to be instructed on how to perform each gesture."
2101.10245,"data, dataset",149,,,"We would also like to point out the problems with the Samsung S5 gesture sensing API. Samsung has deprecated support for the device and access to the sensor output is limited. Moreover, there is no way to access the raw sensor values without rooting the phone. This limits the impact of our current approach to pervade the current market, but doesn’t limit the research contribution. This deprecation did affect our user study. Because the sensor API was deprecated, many of the angle and velocity measures were flagged “unknown.” We removed those incomplete records from our dataset but the reliability of the sensor reading is called into question. As such, our results might represent a lower bound of performance and may be further increased with more reliable sensor readings or more expressive IR sensor data."
2101.10245,github,38,,,"François Chollet et al. 2015. Keras. https://github.com/fchollet/keras. Bruno Dumas, Denis Lalanne, and Sharon Oviatt. 2009. Multimodal interfaces: A survey of principles, models and frameworks. Human"
2101.10245,open-source,41,,,"To normalize and control dynamic range, we take the decibel magnitude of the STFT. The implementation of the STFT grid search and feature extraction techniques have been made open source and are available at [Mundada 2017]."
2101.10245,python,105,,,"To create, train and validate machine learning algorithms we use a combination of packages in Python. Specifically, we use the “scikit-learn” library [Pedregosa et al. 2011] and Keras [Chollet et al. 2015] with the TensorFlow [Abadi et al. 2015] back-end. We chose to investigate several different machine learning baselines and also several different convolutional neural network architectures. It was unclear what neural network architecture and parameters of the architecture would be optimal, so we chose to train several variants and perform hyper parameter tuning for each architecture."
2101.10245,"python, github, open-source",70,,,"Raunak Mundada. 2017. AirWare Open Source Repository. https://github.com/raunakm90/AirWare. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, Oct (2011), 2825–2830."
2101.12447,data,39,,,"[22] Laurens van der Maaten and Geoffrey Hinton, “Visualizing data using t-SNE,” Journal of machine learning research, vol. 9, no. 86, pp. 2579–2605, 2008. 2"
2101.12447,data,55,,,"[20] Andreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo M¨uller, “Causability and explainability of artiﬁcial intelligence in medicine,” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 9, no. 4, pp. e1312, 2019. 2"
2102.0082,data,13,,,176                                                                   Int’l Conf. Data Mining | DMIN’11 |
2102.0082,data,13,,,177                                                                   Int’l Conf. Data Mining | DMIN’11 |
2102.0082,data,13,,,178                                                                   Int’l Conf. Data Mining | DMIN’11 |
2102.0082,data,13,,,179                                                                   Int’l Conf. Data Mining | DMIN’11 |
2102.0082,data,13,,,180                                                                   Int’l Conf. Data Mining | DMIN’11 |
2102.0082,data,154,,,"Abstract—Data mining techniques can be used to discover useful patterns by exploring and analyzing data andit’s feasible to synergistically combine machine learning tools to discover fuzzy classification rules.In this paper, an adaptive neuro fuzzy network with TSK fuzzy type and an improved quantum subtractive clustering has been developed. Quantum clustering (QC) is an intuition from quantum mechanics which uses Schrödinger potential and time-consuming gradient descent method. The principle advantage and shortcoming of QC is analyzedandbased on its shortcomings, an improved algorithm through a subtractive clustering method is proposed. Cluster centers represent a general model with essential characteristics of data which can be use as premise part of fuzzy rules.The experimental results revealed that proposed Anfis based on quantum subtractive clustering yielded good approximation and generalization capabilities and impressive decrease in the number of fuzzy rules and network output accuracy in comparison with traditional methods."
2102.0082,data,16,,,1- Sort samples in vector  by ascending. 2- Choose first  thassociated data points as cluster
2102.0082,data,163,,,"A. Quantum Clustering Clustering is the procedure that classifies the set of physicalabstract objects to several clusters composed of similar objects.Methods of data clustering are usually based on geometric or probabilistic considerations [6]. We can consider intuition based on other fields of study to formulating new clustering procedures. Quantum clustering introduced by Horn [3] is a novel method from quantum mechanics that research an operator in Hilbert space expressed by Schrödinger equation, the solution is wavefunction. When the wave-function is given, they can work out the potential function by Schrödinger equation, which will determine the distribution of particle, and the cluster centers are the k minimum in the potential. According to the quantum theory, particle with lowerpotential vibrate less, and relatively stable, so we can treat this kind of particle as the cluster centers, and then distribute data points to the associated clusters [5]."
2102.0082,data,196,,,"ANFIS is a neuro-fuzzy system developed byJang[2]. It has a feed-forward neural networkstructure where each layer is a neuro-fuzzy system .In this section, we describe the basic architecture, and learning rules of the Anfis. The Anfis structure identification involves two phases: 1) structure identification and 2) parameter identification. The former is related to determining the number of fuzzy if–then rules and a proper partition of the input space. The latter is concerned with the learning of model parameters, such as membership functions andlinear coefficients. As shown in Fig. 1, this network is composed of five layers that introduced by Jang et al [2]. Nevertheless the Anfis suffers from curse of dimensionality that the number of extracted fuzzy rules increases exponentially due to grid partitioning of input data. In this paper we use scatter one instead to find adequate number of rules through new proposed clustering method. For a zero and first order Sugeno Fuzzy model common rules are as following respectively. Consider the Anfis model has n inputs[1, 4]."
2102.0082,data,27,,,(b) Fig. 3.Comparison of RMSE in the training and test data in proposed method (a) and previous method (b).
2102.0082,data,36,,,The SC algorithm assumes each data point is a potential cluster center. Consider a collection of  data pointin  dimensional space. Then calculate a density measure for data points as follows [1]:
2102.0082,data,39,,,"Methods of data clustering are usually based on geometric or probabilistic considerations. The problem of unsupervised learning of clusters based on locations of points in data *Ali Mousavi is with Department of Artificial Intelligence, Faculty of"
2102.0082,data,46,,,"he last decade has seen an explosive growth in the generation and collection of data, advances in data mining and automation of predictor systems. Hence a lot of new intelligently and automatically assist in transforming this data into useful knowledge have been proposed."
2102.0082,data,47,,,"Where  is the th data point. In the QC algorithm, we use the k minima of Schrödinger potential to determine the location of the cluster centers. This potential is part of the Schrödinger equation (7), for which   is a solution."
2102.0082,data,49,,,"First, calculate the vector of potential   for all data samples according to (8) equation. Second, use subtractive clustering explained in section C to obtain optimal number of cluster centers as parameter, . Third, the cluster centers are calculated through following stages:"
2102.0082,data,51,,,"Where      and  is center of selected cluster. The data points around the first cluster center will have reduced density measures. Thus, these data points never selected as the next cluster center. The process continues until a sufficient number of cluster centers were obtained [5]."
2102.0082,data,57,,,"The eigenvalue  of Schrödinger’s equation is the lowest eigenfunctions of the operator  representing the ground state. Moreover, when the minima of   are defined as the cluster centers, the assignments of data points to clusters are obtained by a gradient descent algorithm allowing auxiliary point variables                   , to follow dynamics as follows:"
2102.0082,data,76,,,"Where  is a constant and             is th data point. A data point with many neighboring data points will have a high density value. Thus, data points with a high value of potential are more suited to be the potential cluster centers. A data point with highest density value is selected as the first cluster center. Then the density measure for all of data points will be update by following equation:"
2102.0082,data,77,,,"space is in general ill-defined [3, 6]. Hence intuition based on other fields of study may be useful in formulating new heuristic procedures. There are a lot of clustering methods which leads to generate adequate numbers of fuzzy rules. The cluster centers represent a general model with essential characteristics of data. Each cluster center can be use as premise part of a fuzzy rule [2, 6]."
2102.0082,"data, dataset",112,,,"An Adaptive Neuro Fuzzy Network with a TSK fuzzy type combined with an improved quantum subtractive clustering method to obtain appropriate number of fuzzy rules is proposed. The subtractive clustering, a density based algorithm, is used to determine number of cluster centers. Moreover a modified quantum clustering, an idea from quantum mechanics is applied to obtain cluster centers. Cluster centers represent a general model with essential characteristics of data which can be use as premise part of fuzzy rules. It caused impressive decrease in number of fuzzy rules and network accuracy. Finally we construct our model to predict fuel consumption in MPG dataset."
2102.0082,"data, dataset",181,,,"Generally effective partitioning of input space can reduce number of fuzzy rules and increase learning speed of Anfis. In this paper we use a quantum subtractive clustering to determine fuzzy rules, and then a modified Anfis is applied to data set to predict the output. The experiments used the well-known automobile mile-per-gallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile. At first records with missing values has been eliminated, then we normalize data to [0,1] interval. Data set divided into two partition; train data and test data, according to even record and odd records from original dataset, respectively. Here we use training data set to construct and learn the model while test data is used to validate the model."
2102.0082,"data, dataset",203,,,"We have compared our new Quantum Subtractive Clustering method with traditional QC applied in Anfis [1].  Fig. 2(a) shows comparison results between the desired and traditional QC model outputs for test data. Here horizontal axis shows number of test data and vertical axis determines desired and model fuel consumption. Also Fig .2(b) shows comparison diagrams between the desired and our model outputs for test data set.  i s  A s  d i f f e r e n c e  o b v i o u s  a n d  b e t w e e n  p r o p o s e d  m o d e l  o u t p u t  i s  l e s s  t h e  t h a n  p r e v i o u s  m e t h o d .  A s  h a s  r e s u l t ,  a n d  b e t t e r  g e n e r a l i z a t i o n  c a p a b i l i t y ."
2102.0082,dataset,138,,,"Therefore we propose a new method to construct an adaptive neuro fuzzy network with a TSK fuzyy type. Also we use a modified QC to determine the premise part of fuzzy rules. Moreover subtractive clustering method is applied to determine the optimal number of clusters. We performed a learning method by a hybrid learning scheme using back propagation (BP) and a least-square estimator (LSE). The experiments used the well-known automobile mile-pergallon (MPG) prediction dataset which consists of 392 records. In this example, six input variables are composed of a car’s cylinder number, displacement, horsepower, weight, acceleration, and model year. The output variable to be predicted by the six input variables is the fuel consumption of the automobile."
2102.03237,data,11,,,"Knowledge Discovery from Data, 3(3). doi:10.1145/1552303.1552304"
2102.03237,data,11,,,Table 3: Summary of Three Labeled Data Used for Comparison
2102.03237,data,11,,,at the 2011 IEEE 11th International Conference on Data Mining.
2102.03237,data,12,,,Table 1: Summary of Labeled Data in Selected Studies Evaluating Author-ity2009
2102.03237,data,122,,,"To promote the use of ORCID as a labeling source for author name disambiguation, however, several issues need to be addressed. First, our discussion of representativeness shows that name instances labeled through ORCID linkage may not generalize to the population of scientists because it over-represents early and mid-career researchers and underrepresents Asian names. This implies that ORCID cannot eliminate the need for author name disambiguation in bibliographic data until it becomes a universal author identification system. The same issue occurred to other labeled data in this study. To mitigate the problem, stratified sampling of name instances may be considered to create a set of labeled name instances that represent better population data."
2102.03237,data,13,,,Figure 1: An Overview of Data Linkage and Outcome Data for Analysis
2102.03237,data,13,,,Figure 5: Comparison of Block Size Distributions in Labeled Data against Author-ity2009
2102.03237,data,13,,,[Title] ORCID-linked labeled data for evaluating author name disambiguation at scale
2102.03237,data,14,,,Figure 3: Distribution of Gender Associated with Labeled Instances in Three Labeled Data
2102.03237,data,14,,,Figure 4: Distribution of Ethnicity Associated with Labeled Instances in Three Labeled Data
2102.03237,data,14,,,Figure 6: Evaluation of Disambiguation Performances of Author-ity2009 Evaluated on Two Labeled Data
2102.03237,data,14,,,Table 2: An Illustration of Data Instance in AUT-ORC Linking Author-ity2009 and ORCID
2102.03237,data,141,,,"How can we describe the disambiguation performance of Author-ity2009 evaluated on each labeled data? Figure 6 reports B-cubed (B3) recall, precision, and F1 scores calculated on AUT-ORC and AUT-NIH in comparison with two baseline performances: AINI – all forename initials based disambiguation and FINI – first forename initial based disambiguation). According to Figure 6a, Author-ity2009 performs better than AINI but worse than FINI in finding all name instances associated with distinct authors (Recall). To be specific, the high recall by FINI is expected because only name instances within the same block are disambiguated and all name instances of the same author belong to the same block. As a block consists of name instances sharing the same full surname and first forename initial, matching author identities of"
2102.03237,data,147,,,"This study showed that the ORCID-linked labeled data can be used to evaluate the performance of a disambiguation method implemented on large-scale bibliographic data. As a use case, this study evaluated the disambiguation performance of Author-ity2009 using 3 million name instances linked to ORCID researcher profiles (AUT-ORC). For comparison, two other popular data sources - NIH-funded PI information (AUT-NIH) and self-citation information (AUT-SCT) – were also used to label name instances in Author-ity2009. Results showed that ORCID-linked labeled data better represented the gender, ethnicity and block size distributions of Author-ity2009 t (AUT-ORC > AUT-SCT > AUT-NIH), but did worse in terms of publication-year distributions (AUT-SCT > AUT-NIH > AUT-ORC), suggesting that ORCID, which skews toward younger researchers, may be more effectively used for recent disambiguation tasks."
2102.03237,data,15,,,Figure 2: Distribution of Publication Year Associated with Labeled Instances in Three Labeled Data
2102.03237,data,15,,,Keywords: author name disambiguation; labeled data; ORCID; record linkage; Author-ity2009
2102.03237,data,165,,,"The performance of Author-ity2009 evaluated on AUT-NIH exhibits the same patterns as those reported for AUT-ORC. One difference is that Author-ity2009 achieves almost perfect precision, recall, and F1 (> 0.99) in AUT-NIH. Another difference, however, is that the performance gaps between Author-ity2009 and baselines (Figure 6b) are smaller for AUT-NIH than they were for AUT-ORC (Figure 6a). Taken together, these two observations imply that name instances in AUT-NIH tend to be less ambiguous than those in AUT-ORC: their author identities can be matched based on initialized forenames more frequently than those in AUT-ORC (better performing baselines), while Author-ity2009 can also produce better disambiguation results for them than it does for AUT-ORC name instances. This indicates that depending on the ambiguity levels of labeled data and performance of baselines, the same performance of a disambiguation method can result in evaluation results providing slightly different impressions."
2102.03237,data,18,,,(2) How do ORCID-linked labeled data compare to other labeled data generated by different methods?
2102.03237,data,194,,,"We conducted a sensitivity analysis on the distribution of ethnicity to see how it would change if ethnicity prediction results are different. For this, we randomly changed ethnicity tags of name instances that constitute 10% of each ethnic groups and re-run the evaluation procedures. The 10% random selection is based on the performance differences between Ethnea and NamePrism reported in Ye et al., (2017). The results show that the ethnicity tagging errors indeed changed the distributions (5~12% differences in ratios depending on ethnicities). However, performance evaluation results for Author-ity2009 in comparison with baselines (AINI and FINI) over different linked data (AUT-ORC, AUT-NIH, and AUTSCT) did not change much. Interestingly, induced tagging errors, the performance gaps between Authority2009 and baselines were shown to get widened for many ethnicities. We conjecture that highly ambiguous names such as Chinese, English, and Korean were wrongly tagged as other less ambiguous ethnicities, which increased the name ambiguity level of the ethnicity groups and decreased the performances of baseline disambiguation on them."
2102.03237,data,2,,,Labeled Data
2102.03237,data,2,,,Linked Data
2102.03237,data,202,,,"One concern regarding the evaluation procedure in this study is that the ethnicity and gender tagging may be inaccurate. The tools – Ethnea and Genni – used for ethnicity and gender predictions in this study are reported to produce more accurate and less missing prediction results than other existing tools at the time of their publication (Mishra, Fegley, Diesner, & Torvik, 2018; Torvik & Agarwal, 2016). However, several tools have recently showed that they outperform previous techniques including Ethnea and Genni (e.g., Santamaría & Mihaljević, 2018; Ye et al., 2017). However, we believe Ethnea and Genni are adequate tools for gender and ethnicity predictions to group name instances in MEDLINE for evaluating author name disambiguation by Author-ity2009 because their prediction models were built and validated based on the MEDLINE data. For example, the high-performing tool for ethnicity prediction in Ye et al. (2017) shows very promising prediction results on Wikipedia and Email/Twitter data but it is unknown how it would perform on author name instances in MEDLINE which are the target of ethnicity prediction for this study."
2102.03237,data,209,,,"Ethnicity Distribution: Several studies have investigated how name ethnicities are distributed as a means to characterize labeled data (e.g., J. Kim et al., 2019; Lerchenmueller & Sorenson, 2016). According to Figure 4, the largest ethnic group in Author-ity2009 (black bar) is English (24.36%), followed by Japanese (10.15%), German (8.39%), Chinese (7.60%), and Hispanic (6.78%). The largest ethnic group in AUT-ORC (gray bar) is also English (24.66%) whose ratio is very close to that in Author-ity2009. Unlike Author-ity2009, however, the second largest group in AUT-ORC is Hispanic (14.16%), followed by Italian (12.05%). This disparity can be attributed to the fact that researcher profiles in ORCID are disproportionally associated with European countries, especially Italy and Spain (Youtie et al., 2017). In contrast, Asian names (e.g., Japanese, Chinese, and Korean) are under-represented in AUT-ORC as they are in ORCID (J. Kim et al., 2019; Youtie et al., 2017)."
2102.03237,data,218,,,"To address the problems, a few studies have recently begun to use the Open Researcher & Contributor ID (ORCID)4 data as an authority source to label name instances for disambiguation evaluation (e.g., J. Kim, 2018; J. Kim, 2019b; J. Kim et al., 2019). Similarly, several studies have discussed the potential of using ORCID for authority control within and across digital libraries (e.g., Francis, 2013; Mallery, 2016; Thomas, Chen, & Clement, 2015). ORCID is an open platform of more than 5 million researcher profiles curated by individual researchers for education history, authorship, and employment information (Haak, Fenner, Paglione, Pentz, & Ratner, 2012). Like other authority sources mentioned above, linking ORCID to bibliographic data can produce large-scale labeled data of up to one million instances (J. Kim, 2019b). Unlike other sources, however, author profiles in ORCID are not limited to specific disciplines, geographic regions, organizations, or high-visibility scholars. This implies that ORCID has a potential to label names of researchers from diverse backgrounds and thereby overcome the limited coverage of other authority sources."
2102.03237,data,228,,,"Many author name disambiguation studies have evaluated the performances of their proposed methods on truth data labeled by human coders (e.g., Han, Giles, Zha, Li, & Tsioutsiouliklis, 2004; Qian, Zheng, Sakai, Ye, & Liu, 2015; Santana, Gonçalves, Laender, & Ferreira, 2017; X. Wang, Tang, Cheng, & Yu, 2011). Generating manually labeled data is, however, a daunting challenge. Given the same queues of name instances, for example, human coders can disagree up to 25% of cases (e.g., Liu et al., 2014; Smalheiser & Torvik, 2009). In addition, labeling decisions agreed upon by human coders can be wrong (J. Kim, 2018; Shin, Kim, Choi, & Kim, 2014). Mostly importantly, manual labeling is not scalable. Labeling a few thousand name instances can take several months (Kang, Kim, Lee, Jung, & You, 2011) or require multiple verification steps (Song, Kim, & Kim, 2015), which is labor-intensive and timeconsuming. So, manual labeling is often unsuitable for evaluating a disambiguation task handling a large number of name instances."
2102.03237,data,243,,,"In an effort to avoid the limitations of manual labelling, several studies have created labeled data without human coders. For example, Torvik and Smalheiser (2009) labeled name instances sharing the same email addresses as representing the same author. To decide whether name instances refer to the same author or not, other studies used different features of publication data such as shared coauthors (e.g., Cota, Ferreira, Nascimento, Gonçalves, & Laender, 2010) or self-citation (e.g., Levin, Krawczyk, Bethard, & Jurafsky, 2012). These labeling methods produce labels at large scale (up to millions of labeled instances) but their labeling results have rarely been verified for accuracy.1 As they are designed to produce positive (i.e., label match) sets of name instance pairs, they often require negative (i.e., label nonmatch) sets generated by heuristic rules (e.g., name instances with different name string and no shared coauthors are assumed to refer to different authors). To correct this problem, an iterative clustering method that triangulates multiple matching features such as coauthors, email addresses, and self-citation has been proposed. But its effectiveness can be constrained if those discriminating features are poorly recorded for a given set of name instances (J. Kim, Kim, & Owen-Smith, 2019)."
2102.03237,data,30,04/21/22,0,"Kim, J., & Owen-Smith, J. (In print). ORCID-linked labeled data for evaluating author name disambiguation at scale. Scientometrics. Doi: 10.1007/s11192-020-03826-6"
2102.03237,data,39,,,"do not affect disambiguation evaluation, the accuracy of ORCID records may be tested on various samples or sensitivity analyses may be conducted to find how many errors in ORCID-linked labeled data are acceptable for robust evaluation results19."
2102.03237,data,40,,,"Kim, J., Kim, J., & Owen-Smith, J. (2019). Generating automatically labeled data for author name disambiguation: an iterative clustering method. Scientometrics, 118(1), 253-280. doi:10.1007/s11192-018-2968-3"
2102.03237,data,43,,,"authors, of any gender. PLOS ONE, 13(9), e0195773. doi:10.1371/journal.pone.0195773 Müller, M. C., Reitz, F., & Roy, N. (2017). Data sets for author name disambiguation: An empirical"
2102.03237,data,44,,,"Schulz, C., Mazloumian, A., Petersen, A. M., Penner, O., & Helbing, D. (2014). Exploiting Citation Networks for Large-Scale Author Name Disambiguation. Epj Data Science, 3(1). doi:10.1140/epjds/s13688-014-0011-3"
2102.03237,data,44,,,"bibliographic data. Scientometrics, 112(2), 747-766. doi:10.1007/s11192-017-2410-2 Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify"
2102.03237,data,65,,,"AUT-SCT consists of self-citing name instance pairs that are assumed to refer to the same authors. Because only positive matching instances can identified in these data, labeled pairs can be only used for evaluating how many pairs are correctly classified Author-ity2009 (≈ recall). Table 4 reports the accuracy of Author-ity2009 and baseline methods in classifying instance pairs in AUT-SCT."
2102.03237,data,71,,,"conducted by Torvik and Smalheiser (2009). The resulting data, AUT-ORC, contain 3,076,501 author name instances, which we used to assess the disambiguation performance of Author-ity2009. Table 2 shows an example of a data instance in AUT-ORC in which an author name in a MEDLINE paper is associated with a PMID, byline position, name string, Author-ity2009 ID, and ORCID ID."
2102.03237,data,74,,,"worldwide. In AUT-ORC, researcher’s geo-locations were unevenly distributed (e.g., researchers in Italy and Spain are over-represented) but such an imbalance was more pronounced in AUT-NIH in which almost 55% of name instances were English origins. AUT-SCT exceeded other two labeled data by extracting more than 4 million instance pairs in self-citation relation, although its use for disambiguation evaluation is confined to measuring recall."
2102.03237,data,8,,,AUT-NIH: Linking Author-ity2009 with NIH PI Data
2102.03237,data,80,,,"Files containing disambiguated names in Author-ity2009 (Torvik & Smalheiser, 2018) are downloaded from Illinois Data Bank6. A unique author in the Author-ity2009 file is represented by an author ID with a list of name instances of the author. A name instance is represented by an instance ID which is a numeric combination (e.g., 1234567_2) of (1) PMID (7~8 digit numbers) of a paper in which the instance appears"
2102.03237,"data, database",259,,,"Another group of studies has relied on third-party data sources that control the accuracy of researcher information. For example, Kawashima and Tomizawa (2015) evaluated the disambiguation performance of SCOPUS on a list of 75,405 Japanese author names in 573,338 papers. For this, they used the Database of Grants-in-Aid for Scientific Research (KAKEN) that maintains a unique ID number of a funded researcher in Japan with a list of her/his verified publications. An author name instance in a SCOPUSindexed paper was compared to each KAKEN researcher profile by comparing name strings, publication records, and affiliations. If a match was found, the KAKEN researcher ID was assigned to the author name instance. Such a record linking technique has been used in other studies to label name instances of Italian researchers (D'Angelo, Giuffrida, & Abramo, 2011) and Dutch researchers (Reijnhoudt, Costas, Noyons, Borner, & Scharnhorst, 2014) using each nation’s administrative scholarly databases. Other sources for labeling include NIH-funded researcher profiles2 (e.g., K. Kim, Sefid, Weinberg, & Giles, 2018; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009) and Highly Cited Researchers data3 (e.g., Liu et al., 2014; Torvik & Smalheiser, 2009). While these record linkage procedures produce large-scale, accurate labeling results, it also provides biased results (Lerchenmueller"
2102.03237,"data, database",40,,,"Kim, J. (2017). The impact of author name disambiguation on knowledge discovery from large-scale scholarly data. (Ph.D.), University of Illinois at Urbana-Champaign, Retrieved from http://hdl.handle.net/2142/98269 IDEALS database."
2102.03237,"data, dataset",101,,,"Furthermore, ORCID-linkage can help researchers label the name instances of authors who work in diverse research fields for which labeled data are scarce. Most existing labeled datasets for author name disambiguation were created to disambiguate author names in a few scientific domains, especially Computer Science and Biomedical Sciences (A. A. Ferreira, Gonçalves, & Laender, 2012; Müller et al., 2017). For those who need to mine ambiguous bibliographic data that represent diverse fields, ORCIDlinkage can be an effective way to generate labeled data for their ad-hoc disambiguation tasks."
2102.03237,"data, dataset",137,,,"Second, ORCID-linked labeled data can complement other types of linkage-based labeled data. Our comparisons across three different types of linked labeled data showed that , ORCID-linked labeled data could captured the aspects of Author-ity2009’s performance that were also identified in the other two datasets. This means ORCID linkage can be used as an alternative to other labeling methods if they are unavailable. In addition, ORCID-linkage can be used to help researchers evaluate the labeling quality of other labeled data. Out of 312,951 instances in AUT-NIH, for example, a total of 32,131 instances were also linked to 3,578 ORCID ids. Among them, 99 name instances were assigned to different authors by the ORCID-linkage and the NIH-ExPORTER linkage used in Lerchenmueller and Sorenson (2016)."
2102.03237,"data, dataset",142,,,"Different levels of name ambiguity might arise from the different sizes of labeled data in our study: AUTORC contains more than 3 million instances, while AUT-NIH consists of 312K instances. As name ambiguity in bibliographic data tends to increase with data size (Fegley & Torvik, 2013; J. Kim, 2017), AUT-ORC might be naturally more ambiguous than AUT-NIH. Other differences between these datasets may result from the data sources from which they were drawn. AUT-NIH relied on funded PI information. So, the name instances that could be labelled were restricted to those of researchers who have ever received funds from NIH, a group likely to be more prominent and more homogenous than science itself. In contrast, AUT-ORC utilized ORCID profile data for more than 5 million researchers"
2102.03237,"data, dataset",196,,,"English name instances constitute the majority in AUT-NIH (diagonal-line bar, 54.53%), while other ethnicities are heavily underrepresented compared to their ratios in Author-ity2009. This might be because AUT-NIH is created based on information of PIs who have ever received funds from NIH in the U.S. Non-US investigators are generally ineligible to apply for NIH funds, so it makes sense that the name instance distribution in this dataset would skew toward English names. This English-skewed distribution is also confirmed in Lerchenmueller and Sorenson (2016) who found that 84% of all ethnicity-identified instances in the whole Author-ity2009 linked to NIH ExPORTER are ‘Caucasian’ (including many European names as well as English). Meanwhile, many instances in self-citation relation are also English (horizontal-line bar; 34.65%) but the ratio differences of other ethnicities against Authority2009 are smaller compared to those in AUT-ORC and AUT-NIH. As such, three labeled data are common in that English name instances are prevalent but none of them represents well the ethnicity distribution in Author-ity2009 because some ethnicities are over-represented while others underrepresented."
2102.03237,"data, dataset",21,,,(1) How well do ORCID-linked labeled data represent the population of name instances in a large-scale bibliographic dataset?
2102.03237,"data, dataset",217,,,"This study also creates a benchmark labeled dataset by linking Author-ty2009 with Principal Investigator (PI) information recorded in the National Institutes of Health (NIH) funded research data (ExPORTER). This NIH-linkage has been used in several studies to evaluate author name disambiguation for MEDLINE because ExPORTER provides the PMIDs of research papers in MEDLINE that result from NIH funds (e.g., K. Kim, Sefid, & Giles, 2017; K. Kim et al., 2018; Liu et al., 2014; Torvik & Smalheiser, 2009). After an Author-ity2009 paper’s PMID is found to be associated with a specific NIH grant, the author names in the paper are compared to the names of the PI who received the funding. If a PI’s name is found to match an author name, her/his unique NIH PI ID is assigned to the author name as a label. This study reuses the list of NIH PI IDs linked to the Author-ity2009 in Lerchenmueller and Sorenson (2016)13. To make this NIH-linked labeled data (AUT-NIH) comparable to AUT-ORC, each name instance in AUTNIH is assigned an ethnicity and a gender using Ethnea and Genni each."
2102.03237,"data, dataset",227,,,"Block Size Distribution: Another way to discover how three labeled datasets represent Author-ity2009 is to compare the distributions of block sizes in each dataset. A common practice in author name disambiguation research is to collect author name instances into a block if they match on the full surname and first forename initial. Comparisons that support disambiguation are then performed within blocks (K. Kim et al., 2018). Many studies have used the block size distribution to characterize labeled data (e.g., J. Kim et al., 2019; Levin et al., 2012; Müller, Reitz, & Roy, 2017; Torvik & Smalheiser, 2009). Block sizes can become huge because labeled data contain a few hundreds of thousands (AUT-NIH) or millions (AUT-ORC) of name instances. So, block size distributions are plotted using a cumulative density function on log-log axes. Figure 5 visualizes the block size distributions in AUT-ORC and AUT-NIH. Note that AUT-SCT cannot produce a block size distribution because self-citation pairs only contain match information at the pair level and, thus, the matching status of name instance pairs that are not in self-citation relation but that may nevertheless fall within a block defined by surname and first initial are still unknown."
2102.03237,"data, dataset",253,,,"Gender Distribution: To provide another indicator of how well each labelled dataset represents Authority2009, we turn to comparisons of the gender composition of author name instances. Figure 3 shows that the majority of name instances in all datasets are male (black bar; 57%) while female instances (22.32%) and NULL (i.e., gender unidentifiable) instances (20.28%) make up the rest with similar percentages. Such an imbalanced gender distribution is broadly characteristic of scientific authorship in general (Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013) as of biomedical science (Jagsi et al., 2006). The gender imbalance is also observed in AUT-ORC (gray bar) in which male names constitute 67.46 percent of all name instances while the percentage of female instances (22.70%) is quite similar to that in Autority2009. The higher ratio of male instances in AUT-ORC than in Author-ity2009 seems to be a trade-off with the reduced ratio of Null name instances. The same pattern is observed in AUT-NIH and AUT-SCT in which the dominance of male names are more prevalent (i.e., 73.95% and 65.44% each) than in Autority2009 and AUT-ORC but with lower ratios of Null names and similar ratios of female names. These observations indicate that despite the minute differences in gender ratios, three labeled data shared similar patterns of gender distribution."
2102.03237,"data, dataset",272,,,"The answers to these questions can help disambiguation researchers to make informed choices of labeled data and to create evaluation and ground-truth datasets at scale. Several studies have attempted to answer similar questions by discussing how ORCID profiles represent the author population in Web of Science (Youtie, Carley, Porter, & Shapira, 2017), what issues need to be addressed before ORCID can be used as a gold standard for author disambiguation (Albusac, de Campos, Fernández-Luna, & Huete, 2018; Eichenlaub & Morgan, 2017), and how record-linkage-based labeling may or may not work in author disambiguation under certain conditions (Anderson A Ferreira, Gonçalves, & Laender, 2020; Reijnhoudt et al., 2014). This study contributes to that growing literature by demonstrating the use of ORCID-linked labeling against another large-scale disambiguated dataset constructed using different linkage-based labeling methods. Specifically, this study labels name instances in MEDLINE by linking them with ORCID researcher profiles. Then, the performances of Author-ity2009 which disambiguates MEDLINE author names, is evaluated using the labeled data. For comparison, two labeled datasets are created using two widely-used sources - NIH-funded researcher information and self-citation information. The three labeled datasets are compared for their representativeness of Author-ity2009 as well as to evaluate results of the Author-ity2009’s disambiguation performances. After that, a discussion follows about the implications and challenges of using ORCID for labeling. In the following section, labeling procedures via record-linkage for Author-ity2009 are described in detail."
2102.03237,"data, dataset",319,,,"In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b. Both these distances may be due to the relatively larger European focus of ORCID and the US focus of NIH data. The largest name blocks in Author-ity2009 tend to be created by highly ambiguous Asian name instances. If the distributions of both labeled datasets were representative of Author-ity2009, we would expect to see their distributions track closely with those of the random subset of Author-ity2009 name instances."
2102.03237,"data, dataset",336,,,"In Figure 5, the x-axis shows block sizes ranging from 1 (i.e., a single instance block) to 25,917 in Author-ity2009 (Figure 1a). They-axis represents the ratio of blocks with a specific size or larger (cumulative) over all blocks is shown. For example, in Author-ity2009, blocks with 2 or more name instances (blue circles) constitute 63.47% of all blocks, which in reverse means that 36.53% of blocks contain only one instance. In Figure 5a, the block size distributions of Author-ity2009 (blue circles) and AUT-ORC (green x-markers) are compared. Both distributions are highly skewed: most blocks are small while a few are huge. In Author-ity2009, for example, blocks with 12 or fewer instances make up 80% of all blocks, while in AUT-ORC, blocks with 20 or fewer do so. But they begin to differ as the block size increases over those 80% thresholds. The curvature of AUT-ORC turns downward more than that of Author-ity2009. This means that in AUT-ORC, large blocks make up a smaller proportion of AUT-ORC than they do inAuthor-ity2009. To see if this difference naturally occurs due to different data sizes (AUTORC ≈ 3M vs Author-ity2009 ≈ 40M), we randomly select a set of Athor-ity2009 name instances of the same general size as AUT-ORC (i.e., 3M). Their block size distribution is depicted on the figure in red. As shown in Figure 5a, the random data’s block size distribution (red triangles) has a different shape from AUT-ORC’s, while it has a similar curvature as that of Author-ity2009. This implies that the block size distribution in AUT-ORC is biased toward small sizes when compared to its population data, Authority2009. The same pattern is also observed for AUT-NIH in Figure 5b."
2102.03237,"data, dataset",98,,,"Distribution of Publication Years: As shown in Table 3, the three labeled datasets – AUT-ORC, AUTNIH, and AUT-SCT –contain different numbers of labeled name instances (pairs). How do those instances differ and which is most representative of the overall Author-ity2009 dataset? To characterize the composition of labeled data, publication years of papers in which a labeled instance appears are counted. Figure 2 compares the publication year distributions of three labeled datasets. Note that for AUT-SCT, years associated with each self-citing name instance pair are counted."
2102.03237,"data, dataset provided",167,,,"Second, the accuracy of ORCID records still needs to be verified. As acknowledged by ORCID, some records may contain errors due to “benign” (unintentional) mistakes by profile creators (e.g., claiming other researcher’s work as their own)18. Note that other labeled data may have the same verification problems. Human experts can produce inaccurate labels and often disagree on labeling decisions even given the same information (Shin et al., 2014; Song et al., 2015). Although NIH PI data are curated with special care by NIH, the linkage process for labeling may entail erroneous matching between PI names and author names in NIH-funded papers. As shown above regarding the labeling quality of AUT-NIH (see 3rd paragraph in Conclusion and Discussion), ORCID-linked data provided more accurate labeling results than the other method but still contained erroneous labels. To ensure that errors in ORCID records"
2102.03237,"data, dataset provided",299,,,"Third, ORCID-linked labeled data can provide more enriched evaluation results. They can be used together with other labeled data for triangulating a disambiguation method’s performance. Unlike selfcitation-based labeled data, ORCID-linked labeled data can be used to measure both clustering and classification performances. Unlike NIH-linked labeled data, ORCID-linked labeled data contain a greater range of ambiguous names across ethnicities, which can enable a disambiguation method to be evaluated on name instances with different ambiguity levels. This in turn allows for more focused analysis to address difficult disambiguation tasks such as those presented by synonyms and homonyms. Moreover, ORCID-linkage can produce labeled instances that are challenging to disambiguate but are not easily collectable by other labeling methods. For example, FINI could not reach perfect recall in AUT-ORC (Figure 6a and Figure 7a). As detailed above (see Clustering Performance: AUT-ORC and AUT-NIH), 273,782 name instances of 12,646 authors (= unique ORCID ids) are recorded in a way that their ‘surname + first forename initial’ strings of the same author are different. This means ORCID-linkage could produce labeled name instances that refer to the same authors but do not belong to the same blocks. Such synonymous name variants existing across blocks have been insufficiently studied in disambiguation research (Backes, 2018; Gomide et al., 2017) because many studies have created labeled data by collecting (= blocking) ambiguous name instances sharing at least the full surname and first forename initial (J. Kim, 2018; Müller et al., 2017). Using ORCID-linked labeled data, scholars can develop disambiguation models that address synonyms as well as homonyms."
2102.03237,"data, dataset, data https",148,,,"9 https://databank.illinois.edu/datasets/IDB-9087546 10 26 ethnicities include: African, Arab, Baltic, Caribbean, Chinese, Dutch, English, French, German, Greek, Hispanic, Hungarian, Indian, Indonesian, Israeli, Italian, Japanese, Korean, Mongolian, Nordic, Polynesian, Romanian, Slav, Thai, Turkish, and Vietnamese. Some name instances are assigned compound ethnicities (e.g., “Jane Kim” → Korean-English) if the surname and forename of an author name are associated frequently with different ethnicities.  11 Genni + Ethnea for the Author-ity 2009 dataset. (2018). Retrieved from: https://doi.org/10.13012/B2IDB9087546_V1 12 https://en.wikipedia.org/wiki/Andrea 13 https://dx.doi.org/10.6084/m9.figshare.3407461.v1. Instead of 355K instances in the original linked data, this study filters 313K instances recorded in papers published between 1991 and 2009."
2102.03237,"data, dataset, publicly available, data available",130,,,"This study suggests several implications for researchers and practitioners of author name disambiguation. First, ORCID can be an effective source of authority for creating labeled data. This study illustrated that ORCID-linkage can generate millions of labeled name instances in a bibliographic data, which is not easily achievable by manual or other record-linkage-based labeling. In addition, ORCID-linkage can be repeated without much additional cost once technical procedures for record-linkage are implemented. Moreover, ORCID data continue to be expanded, publicly available, and released annually. This means labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated dataset and updated on a regular basis, enabling sustained evaluation of author name disambiguation in ever-growing digital libraries."
2102.03237,"data, publicly available",169,,,"Author-ity2009 is chosen as an evaluation target for three reasons. First, Author-ity2009 conducts author name disambiguation on a digital library scale: 61.7M name instances in 18.6M papers published between 1966~2009 as indexed in MEDLINE. Evaluating disambiguation results for such a large bibliographic corpus can be a daunting challenge. So, Author-ity2009 can be a good use case to illustrate how ORCIDlinkage can contribute to the performance and evaluation of an important, large-scale disambiguation task. Second, the performance of Author-ity2009 have been evaluated on different types of labeled data in several studies (e.g., J. Kim, 2019b; Lerchenmueller & Sorenson, 2016; Liu et al., 2014; Torvik & Smalheiser, 2009), as summarized in Table 1. This provides a context for comparing ORCID with other labeling sources to better understand its strengths and weaknesses. Third, Author-ity2009 is publicly available for research, enabling scholars to replicate and validate this study."
2102.03237,"data, publicly available, dataset provided",132,,,"In evaluating the clustering results of Author-ity2009, ORCID-linked labeled data effectively captured the ‘high precision over high recall’ strategy of Author-ity2009. Although comparative labeled data also produced the same evaluation results, ORCID-linked labeled data could provide more nuanced details about the Author-ity2009’s performance when name instances were evaluated across ethnic name groups. As such, ORCID-linkage can be used as a labeling method to produce large-scale truth data to evaluate the performance of a disambiguation method from various aspects. Three large-scale labeled data – AUTORC, AUT-NIH, and AUT-SCT – used in this study are publicly available17. The data sharing is expected to assist researchers to develop, compare, and validate disambiguation models using diverse, large-scale labeled data."
2102.03237,"data, publicly available, dataset provided, data available",221,,,"How can we evaluate the performance of a disambiguation method implemented on big bibliographic data? This study suggests that the open researcher profile system, ORCID, can be used as an authority source to label name instances at scale. This study demonstrates the potential by evaluating the disambiguation performances of Author-ity2009 (which algorithmically disambiguates author names in MEDLINE) using 3 million name instances that are automatically labeled through linkage to 5 million ORCID researcher profiles. Results show that although ORCID-linked labeled data do not effectively represent the population of name instances in Author-ity2009, they do effectively capture the ‘high precision over high recall’ performances of Author-ity2009. In addition, ORCID-linked labeled data can provide nuanced details about the Author-ity2009’s performance when name instances are evaluated within and across ethnicity categories. As ORCID continues to be expanded to include more researchers, labeled data via ORCID-linkage can be improved in representing the population of a whole disambiguated data and updated on a regular basis. This can benefit author name disambiguation researchers and practitioners who need large-scale labeled data but lack resources for manual labeling or access to other authority sources for linkage-based labeling. The ORCID-linked labeled data for Authortiy2009 are publicly available for validation and reuse."
2102.03237,database,14,,,"funding database in Japan. Scientometrics, 103(3), 1061-1071. doi:10.1007/s11192-015-1580-z"
2102.03237,database,140,,,"This study shows the potential of ORCID-linkage-based labeling for evaluating author name disambiguation by assessing the disambiguation performance of Author-ity2009 (Torvik & Smalheiser, 2009; Torvik, Weeber, Swanson, & Smalheiser, 2005). Author-ity2009 is a bibliographic database that contains disambiguated author names in MEDLINE5, the world’s largest digital library of biomedical research, maintained by the U.S. National Library of Medicine (NLM). In Author-ity2009, author names are disambiguated in two steps. First, name pairs are compared for similarity over various features such as middle name initial, coauthor name, affiliation, and Medical Subject Headings. Next, the instance pairs are grouped into clusters by a maximum-likelihood-based, hierarchical agglomerative clustering algorithm using the pairwise similarity calculated in the first step."
2102.03237,database,27,,,"author names in a large-scale bibliographic database. Paper presented at the Library of Congress International Symposium on Science of Science, Washington D.C. http://hdl.handle.net/2142/88927"
2102.03237,dataset,158,,,"Other differences between labeled datasets also help illuminate particular strengths and weaknesses in Author-ity2019. AUT-ORC and AUT-NIH have different levels of name ambiguity. When the Authority2009’s performance was compared with two commonly-used baseline methods, it was less impressive on AUT-NIH where simpler the baseline methods accomplished equivalently high precision and recall to the more sophisticated Author-ity2009. In contrast, in AUT-ORC, the performance gaps between Authority2009 and baseline methods widened substantially. Considering that the baseline methods are deterministic (matching name instances on full surname and initialized forename), their strong performances mean that (1) while many name instances in AUT-ORC and AUT-NIH are not ambiguous, (2) AUT-ORC contains more ambiguous names than AUT-NIH. We observed the same patterns in comparisons of performance across groups of ethnic name instances known to vary in their ambiguity (Figure 7 and Figure 8)."
2102.03237,dataset,161,,,"Clustering Measure: To assess the performances of Author-ity2009 on three labeled datasets, author name instances referring to the same author are grouped into a cluster. Specifically, a truth cluster is the collection of author name instances that share the same ORCID ID (AUT-ORC) or the same NIH PI ID (AUT-NIH). Meanwhile, a predicted cluster is the collection of author name instances that share the same Author-ity2009 ID (AUT-ORC and AUT-NIH). Then, the predicted cluster is compared to the truth cluster to quantify how well it contains only and all instances that belong to the truth cluster. This study uses B-Cubed (B3), one of most frequently used clustering metrics in author name disambiguation (J. Kim, 2019a). This measure is comprised of three metrics: B3 Recall, B3 Precision, and B3 F1, which are defined as follows:"
2102.03237,dataset,255,,,"As reported above, three labeled datasets together highlight different aspects of Author-ity2009’s disambiguation performance. They all showed that Author-ity2009 is highly accurate in disambiguating author name instances. It demonstrated special strength in distinguishing author name instances that belong to different authors and in producing almost perfect clustering precision (AUT-ORC and AUTNIH). In addition, Author-ity2009 performed well in finding name instances of unique authors, producing very high clustering recall (> 0.96; AUT-ORC and AUT-NIH) and classification accuracy (= 98.06%; AUT-SCT) scores. Note that the Author-ty2009 is by design aimed to disambiguate with high precision because incorrectly matched name instances (merged author identities created by false positives) are more harmful than wrongly mismatched ones (split author identities created by false negatives) for bibliometric analyses (Fegley & Torvik, 2013; Liu et al., 2014; Torvik & Smalheiser, 2009). The evaluation results described so far strongly suggest that Author-ity2009 achieved its stated precision-over-recall goals. Using the name instances stratified into different ethnic groups, the three labeled datasets discussed here provide a deeper understanding of Author-ity2009’s disambiguation performance. Author-ity2009 achieved high precision regardless of ethnic name types (AUT-ORC and AUT-NIH). But its recall was relatively weak in disambiguating some ethnic names, when compared with baseline performances (AUTORC, AUT-NIH, and AUT-SCT), suggesting possibilities to improve the algorithm."
2102.03237,dataset,303,,,"Another benchmark labeled dataset is a list of name instance pairs that represent self-citation relations. This self-citation information has been used in several studies to develop and test automatic labeling methods (e.g., J. Kim, 2018; J. Kim et al., 2019; Liu et al., 2014; Schulz, Mazloumian, Petersen, Penner, & Helbing, 2014; Torvik & Smalheiser, 2009). This labeling method is based on the assumption that if a paper cites another and they have the same author names, those names refer to the same author. To generate a list of citing references for a paper, reference lists of papers in MELDINE are connected to their cited papers via matching PMIDs. Then, author names in a cited paper are compared to those in citing papers. Following the common practice using this labeling method, if two name instances in cited and citing papers each match on the full surname and the first forename initial, we treat them as instances of the same author. More than 6.2M self-citation pairs are detected in Author-ity2009. To be comparable to AUT-ORC and AUT-NIH, each name instance in a self-citation pair is assigned an ethnicity and a gender, too. Table 3 characterizes the sources of record linkage and labeling methods of the three labeled datasets – AUT-ORC, AUT-NIH, and AUT-SCT – and presents the numbers of labeled instances and unique authors in each dataset. Note that the number of unique authors is unavailable for AUT-SCT because only name instances that have self-citation relationships can be labelled. It is thus impossible to know from this dataset alone whether name instances without self-citation refer to the same author.14."
2102.03237,dataset,327,,,"To evaluate the disambiguation performances of Author-ity2009, author name instances disambiguated by Author-ity2009 need to be labeled. This study attempts to link ORCID ids to 40M author name instances that appear in about 9M papers published between 1991 and 2009 in Author-ity2009. Author-ity2009 disambiguates author name instances in MEDLINE but does not provide their raw name strings. So, this study proceeds from the whole MEDLINE corpus (2016 baseline version) retrieved from the National Library of Medicine repository7. We select MEDLINE records for papers published between 1991 and 2009 (MEDLINE2009) to align with the publication year range of Author-ity2009. Next, name instances in MEDLINE2009 are compared to the author profiles in ORICD. For this MEDLINE2009-ORCID linkage, a 2018 ORCID release version is used8. To find author name instances recorded in both MEDLINE2009 and ORCID, paper titles with five or more words in MEDLINE2009 are encoded into ASCII format, deprived of non-alphabetical characters, and lowercased. Any duplicate titles after the preprocessing are removed. Then, each title (which is associated with a unique PMID) is compared to the publication lists in ORCID researcher profiles. If a match is found between bibliographic records in MEDLINE2009 and ORCID, author name strings that appear in the matched MEDLINE2009 paper are compared with the name string of the ORCID researcher whose list of publications contains the matched title. If two name strings in MEDLINE2009 and ORCID are matched on the full surname plus the first forename initial, they are assumed to refer to the same author and the ORCID ID of the matched researcher profile is assigned to the name instance in MEDLINE2009. As shown in Figure 1, this matching process produces a labeled dataset, MED-ORC, in which an author name instance in a MEDLINE paper is associated with an ORCID ID."
2102.03237,dataset,39,,,indicates that AUT-NIH most closely matches Author-ity2009 in terms of the publication year distribution of name instances. The other two labeled datasets over-represent recent years heavily (AUTORC) and slightly (AUT-SCT) relative to Author-ity2009..
2102.03237,dataset,8,,,5 https://www.nlm.nih.gov/bsd/medline.html 6 https://databank.illinois.edu/datasets/IDB-4222651
2102.03237,dataset,88,,,"Although three labeled datasets produced similar evaluation results, they had different characteristics. First, AUT-ORC and AUT-NIH were used to evaluate both the precision and the recall of Authority2009’s clustering of name instances that refer to the same unique authors. But AUT-SCT could be used only to evaluate how well Author-ity2009 decided that self-citing name instance pairs refer to the same authors (≈ recall). This means that AUT-SCT could only provide partial evaluation of Author-ity2009’s disambiguation performance."
2102.03237,dataset,9,,,17 Datasets can be downloaded at https://doi.org/10.6084/m9.figshare.13404986.v1
2102.03237,"dataset, data https",23,,,"Torvik, V. I., & Smalheiser, N. R. (2018). Author-ity 2009 - PubMed author name disambiguated dataset."
2102.03681,code,124,,,"Sometimes, Stan is able to produce vectorized code such as in matrix multiplication. This is consistent with our benchmark results since Stan came closest to FastAD for this operation (see Section 5.3). It is also consistent with how it is implemented, since they allocate extra memory for double values for each matrix and the multiplication is carried out with these matrices of primitive types. However, this vectorization does come at a cost of at least 4 times extra memory allocation than what FastAD allocates. Moreover, the backward-evaluation requires heap-allocating a matrix on-the-ﬂy every time. FastAD incurs no such cost, only allocates what is needed, and never heap-allocates during AD evaluation."
2102.03681,code,145,,,"N values. For a complicated model as such, there are many opportunities for FastAD to cache certain evaluations for constants as mentioned in Section 4.3 and 5.4. In particular, the exponential function eh reuses its forward-evaluated result, and many log-pdfs cache the log of its parameters such as log(σ) in the Normal log-pdfs and log(γ) in the Cauchy log-pdfs (σ, γ are the second parameters of their respective distributions, which are constant in this model). Note that this caching is automatically done in FastAD, which would be tedious to manually code for the baseline. Hence, this shows that due to automatic caching, FastAD forward and backward-evaluation combined can be faster than a manually-written forward evaluation only, which puts FastAD at an optimal performance."
2102.03681,code,167,,,"One example is choosing the correct specialization of an operation depending on the shapes of the input. As seen in Section 4.1, all nodes are given a shape trait. Depending on the input shapes, one may need to invoke diﬀerent routines for the same node. For example, the normal log-pdf node behaves quite diﬀerently depending on whether the variance parameter is a scalar σ2 or a (covariance) matrix Σ. Namely, if the variance has a matrix shape, we must perform a matrix inverse to compute the log-pdf, which requires a diﬀerent code from the scalar case. Using a C++ design pattern called Substitution-Failure-Is-Not-An-Error (SFINAE), we can choose the correct routine at compile-time. The beneﬁt is that there is no time spent during run-time in choosing the routine anymore, whereas in libraries like CppAD, they choose the routines at run-time for every evaluation of the node [2]."
2102.03681,code,41,,,"Fig. 4: Sum and product benchmarks of other libraries against FastAD plotted relative to FastAD average time. Fig. 4a,4c use built-in functions whenever available. Fig. 4b,4d use the naive iterative-based code for all libraries."
2102.03681,code,47,,,"comments on the ﬁrst draft and for taking the time to optimize the benchmark code for their respective libraries. We also thank Art Owen and our colleagues Kevin Guo, Dean Deng, and John Cherian for useful feedback and corrections on the ﬁrst draft."
2102.03681,code,60,,,"This example really highlights the beneﬁts of vectorization. As noted in Section 4.1, this was the one benchmark example where Stan was able to produce vectorized code, which is consistent with Figure 6 that Stan is the only library that has the same order of magnitude as FastAD. Other libraries did not produce vectorized code."
2102.03681,code,71,,,"If we assume that the most expensive operation is the matrix multiplication, AD evaluation approximately takes two matrix multiplications between a matrix and a vector. We can then approximate a lower bound for the manually-written gradient computation time to be two times that of the baseline. The relative time of FastAD to this approximated time is 1.1, implying about 10% overhead from a manually-written code."
2102.03681,code,73,,,"Hence, in total, one AD evaluation requires three matrix multiplications between two K ×K matrices. If we approximate a manually-written gradient computation to take three times as long as the baseline (one multiplication), FastAD time relative to this approximated time is 3.27 3 = 1.09. This shows then that FastAD only has about 9% overhead from a manually-written code, which is extremely optimal."
2102.03681,data,112,,,"In this section, we cover a few key ideas of our implementation1. In Section 4.1, we ﬁrst discuss the beneﬁts of vectorization and the diﬃculties of integrating it into an AD system. We then explain how FastAD fully utilizes vectorization and demonstrate that other libraries do not fully take advantage of it. In Section 4.2, we discuss some memory management and performance issues stemming from the use of the “tape” data structure. We then explain how FastAD overcomes these challenges using expression templates and a lazy allocation strategy. Finally, Section 4.3 covers other compile-time optimizations that can further maximize performance."
2102.03681,data,145,,,"Most AD systems manage a data structure in memory often referred to as the “tape” to store the sequence of operations via function pointers as well as the node values and adjoints. This tape is modiﬁed dynamically and requires sophisticated memory management to eﬃciently reuse memory whenever possible. Stan even writes their own custom memory allocator to alleviate memory fragmentation, promote data locality, and amortize the cost of memory allocations [3]. However, the memory is not fully contiguous and may still over-allocate. For some libraries, on top of memory management of these operations, a run-time check must be performed at every evaluation to determine the correct operation [2]. Others like Stan rely on dynamic polymorphism to look up the vtable to call the correct operation [3]."
2102.03681,"data, code",208,,,"Vectorization refers to the parallelization of operations on multiple data at the hardware level. On a modern Intel 64-bit processor supporting AVX, four doubleprecision ﬂoating point numbers can be processed simultaneously, roughly improving performance by a factor of four. While the compiler optimization is able to vectorize a user’s code sometimes, it is not guaranteed because vectorization requirements are quite stringent. For example, vectorization is not guaranteed if memory access is not done in a contiguous fashion and is impossible if there is any dependency between loop iterations. This makes it quite challenging to design an AD system that can always predict compiler optimization to create vectorized code. However, vectorization can make AD extremely fast, powerful, and practical even in complex problems. In practice, we come across many examples where operations can be vectorized during gradient computation. For example, matrix multiplication, any reduction from a multi-dimensional variable to a scalar such as summation or product of all elements, and any unary and binary function that is applied element-wise such as exponential, logarithm, power, sin, cos, tan, and the usual arithmetic operators."
2102.03681,github,7,,,1 github page: https://github.com/JamesYang007/FastAD
2102.03681,github,7,,,2 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,github,7,,,3 github page: https://github.com/JamesYang007/ADBenchmark
2102.03681,package,18,,,"2. Bell, B.: Cppad: a package for c++ algorithmic diﬀerentiation http://www.coin-or."
2102.03681,package,56,,,"6. Griewank, A., Juedes, D., Utke, J.: Algorithm 755: Adol-c: A package for the automatic diﬀerentiation of algorithms written in c/c++. ACM Trans. Math. Softw. 22(2), 131–167 (Jun 1996). https://doi.org/10.1145/229473.229474, https: //doi.org/10.1145/229473.229474"
2103.01553,data,102,,,"We present MoCA, a dynamic veriﬁer to analyze C11 program traces valid under MCA model for assertion violations na data races. The technique is shown to be sound and precise. The empirical results demonstrate the utility of MoCA over existing techniques for C11 . Future Work: In future we would like to explore the extensions of our work to reactive systems and include richer program constructs such as locks and memory barriers. Another area of possible investigation would be to combine current work with symbolic trace veriﬁcation so as to avoid re-runs of the input program."
2103.01553,data,11,,,"τ (cid:48) ), er takes its data from"
2103.01553,data,249,,,"Notice that shadow-write events are a particular constuct of our technique; naturally, deﬁnitions of C11 relations do not contain them. In §V, we shall present one of our main contributions of redeﬁning the above-mentioned relations (keeping shadow-write events in consideration) which admit only the MCA behaviors of a C11 program. Reordering restrictions. For any two events e(cid:48), e ∈ Eτ s.t. thr(e) = thr(e(cid:48)) ∧ e(cid:48)<τ e, we have the following: if e ∈ E W((cid:119)rel), then it restricts events such as e(cid:48) from reordering after it. We denote this downward reordering restriction on e(cid:48) by e as (cid:56)(e, e(cid:48)). Similarly, if e(cid:48) ∈ E R((cid:119)acq), it restricts a later event e from reordering before it. We denote this upward reordering restriction on e by e(cid:48) as (cid:57)(e(cid:48), e). Furthermore, C11 also disallows reordering of events e(cid:48), e that share program dependence (such as data, address and control), which we represent by dep(e(cid:48), e)."
2103.01553,data,30,,,"Intuitively, a read event takes the data from the last write whose shadow-write updated the shared memory, unless there is a later write from the same thread."
2103.01553,data,329,,,"Implementation details. We present a prototype implementation to experimentally validate MoCA technique. The implementation is built on rInspect [7]. MoCA-tool takes a C11 program as input. The input program P is statically transformed to (cid:98)P by the early-write transformation (§VI). Further, (cid:98)P is instrumented using LLVM to recognize newly enabled events dynamically during execution of a sequence. The events are communicated to MoCA scheduler, which orchestrates the order of execution of events using source-DPOR algorithm. MoCA-tool re-runs (cid:98)P for every maximal sequence explored. After analyzing the input program P , MoCA reports assert violations if any. According to C11 standard if the order of occurrence of a pair of na ordered events can potentially be reversed in a trace, then the behavior of the trace is undeﬁned. Such a behavior can defy the coherence speciﬁcation (shco), (co) and produce invalid values. Thus, MoCA also reports data races on non-atomic memory accesses. Experiment details. We performed tests to validate correctness wrt MCA using diy7 family of litmus tests [17] (sample listed in Table III). To test C11 coherence, we synthesized 56 litmus tests relevant to the C11 coherence rules (eg. row 1-4, Table IV) and borrowed multi-threaded benchmarks from the SV-Comp benchmark suite [18] (eg. row 5-8, Table IV). We remodeled them for C11 with the use of atomic data types and C11 memory orders. We recorded time (column ‘Time’) and the number of maximal sequences explored (column ‘#Seq’), which includes at least one execution corresponding to each trace and (possibly) few redundant executions owing to the non-optimal nature of the underlying source-DPOR algorithm."
2103.01553,data,343,,,"MoCA-tool takes a C11 program as input. The input program P is statically transformed to (cid:98)P by the early-write transformation (§VI). Further, (cid:98)P is instrumented using LLVM to recognize newly enabled events dynamically during execution of a sequence. The events are communicated to MoCA scheduler, which orchestrates the order of execution of events using source-DPOR algorithm. MoCA-tool re-runs (cid:98)P for every maximal sequence explored. After analyzing the input program P , MoCA reports assert violations if any. According to C11 standard if the order of occurrence of a pair of na ordered events can potentially be reversed in a trace, then the behavior of the trace is undeﬁned. Such a behavior can defy the coherence speciﬁcation (shco), (co) and produce invalid values. Thus, MoCA also reports data races on non-atomic memory accesses. Experiment details. We performed tests to validate correctness wrt MCA using diy7 family of litmus tests [17] (sample listed in Table III). To test C11 coherence, we synthesized 56 litmus tests relevant to the C11 coherence rules (eg. row 1-4, Table IV) and borrowed multi-threaded benchmarks from the SV-Comp benchmark suite [18] (eg. row 5-8, Table IV). We remodeled them for C11 with the use of atomic data types and C11 memory orders. We recorded time (column ‘Time’) and the number of maximal sequences explored (column ‘#Seq’), which includes at least one execution corresponding to each trace and (possibly) few redundant executions owing to the non-optimal nature of the underlying source-DPOR algorithm. Further, if a test contains na races (column ‘race?’) then we report the number of maximal sequences that contain na race(s) (column ‘#Rseq’)."
2103.01553,data,43,,,"[10] B. Norris and B. Demsky, “Cdschecker: checking concurrent data structures written with c/c++ atomics,” in ACM SIGPLAN Notices, vol. 48, no. 10. ACM, 2013, pp. 131–150."
2103.01553,data,48,,,"[23] M. Chalupa, K. Chatterjee, A. Pavlogiannis, N. Sinha, and K. Vaidya, “Data-centric dynamic partial order reduction,” Proc. ACM Program. Lang., vol. 2, no. POPL, pp. 31:1–31:30, 2017."
2103.01553,data,73,,,"MoCA explores all relevant program behaviors for detecting safety assertion violations as well as non-atomic (na) data races. Central to MoCA is source-DPOR (Algorithm 1 of [4]), which is a near-optimal improvement over DPOR [5]. It is noteworthy that source-DPOR algorithm used in MoCA is as is, i.e., without any modiﬁcation. This was feasible because of several reasons:"
2103.01553,data,92,,,• (mo4): a read hb-ordered after a write either reads-from the write or from a write mo-ordered after that write; • (to): all sc ordered events must form a total-order and →[c]::hb τ • (co): a read must take its data from a write event occurring in the trace. The (co) rule ensures that a read does not take a value from thin-air i.e. a value not generated in the program execution.
2103.02044,code,9,,,Code availability (software application or custom code)
2103.02044,data,11,,,Commercial softwares in preclinical imaging poorly conform to standards regarding data
2103.02044,data,11,,,"repositories. Sharing data from independent research investigations, beyond saving"
2103.02044,data,12,,,appropriate data storage to ensure adequate management and organization to the research
2103.02044,data,12,,,coherent semantics to guarantee consistent sharing of image data across several third-party
2103.02044,data,12,,,"community to efficiently store, process and share biomedical imaging data."
2103.02044,data,12,,,"including data gathered from CEST-MRI experiments, in particular GlucoCEST imaging for"
2103.02044,data,12,,,introduced in preclinical imaging. The Small Animal Big-data warehouse Environment for
2103.02044,data,12,,,of data and ensuring higher standards of reproducibility of the experiments.
2103.02044,data,13,,,"For human population studies on biomedical imaging, large data repositories are routinely"
2103.02044,data,13,,,"Imaging research generates large amounts of data and information, mostly produced by"
2103.02044,data,13,,,build their own data management and process infrastructure upon the XNAT system with
2103.02044,data,13,,,"complexity of data management, since the same patient can typically undergo several"
2103.02044,data,13,,,"exchange of image data within the preclinical imaging community, thereby enhancing the"
2103.02044,data,13,,,guidance for stakeholders willing to strengthen their data reusability. Unlike similar initiatives
2103.02044,data,13,,,"image data. Recently, other commercial data management systems have been also"
2103.02044,data,13,,,"imaging data structures has then been extended, including longitudinal studies (several"
2103.02044,data,13,,,impelling need of guidelines for handling image data. Although several successful solutions
2103.02044,data,13,,,of the Shanoir platform that was developed for data management dedicated to human
2103.02044,data,13,,,preclinical investigation protocols and easily process preclinical image data. We believe that
2103.02044,data,13,,,"scholarly data motivated the establishment of the Findable, Accessible, Interoperable and"
2103.02044,data,14,,,Class Provider that can send data directly to the XNAT server and the Representational
2103.02044,data,14,,,"Research (SABER) supports preclinical workflow and promotes data sharing, although this"
2103.02044,data,14,,,Uploader has been designed to overcome an XNAT limitation regarding data upload. There
2103.02044,data,14,,,XNAT webpage containing two projects related to the data presented in this work.
2103.02044,data,14,,,"access to the most advanced imaging technologies, training and data services in biological"
2103.02044,data,14,,,"addition, machine-readable metadata that are necessary for data discovery are still needed."
2103.02044,data,14,,,an instance and broaden its basic features to support their data and project management
2103.02044,data,14,,,believe that XNAT-PIC will allow a streamlined exchange and reuse of image data among
2103.02044,data,14,,,"committed to human scholar, the FAIR Principles are intended to make the data"
2103.02044,data,14,,,dissemination [10]. Other examples of extensible data management platforms are The
2103.02044,data,14,,,engine works with simple data flows on a step-by-step basis and can perform computational
2103.02044,data,14,,,is a flexible online system for data management devoted to multicenter studies that covers
2103.02044,data,14,,,"multiple formats, archiving, and distributing data, and setting data protection and"
2103.02044,data,14,,,platform is unavailable in internet. Flywheel is a commercial data management system to
2103.02044,data,14,,,"store research data of different imaging modalities in a centralized archive, thus improving"
2103.02044,data,14,,,them yielding large amounts of data usually stored in several workstations. Storing and
2103.02044,data,15,,,"FAIRification in terms of Accessibility and Reusability. Generally, the data stored in an"
2103.02044,data,15,,,a waste of time and resources. Data sharing among research groups is another critical
2103.02044,data,15,,,"all the aspects from data acquisition from multiple sources to storage, processing, and"
2103.02044,data,15,,,and processing research data supported by FAIR data principles” as stated in The Vienna
2103.02044,data,15,,,"image data are inadequate. Therefore, standard tools for the analysis of large image"
2103.02044,data,15,,,studies was developed by Doran et al. along with in-house applications for data selection
2103.02044,data,16,,,"MRI2DICOM to convert the binary data to DICOM standard or, given the data already in"
2103.02044,data,16,,,"data, allowing to extract and quantify the information. The FAIR revolution also involves the"
2103.02044,data,16,,,"executable for processing the data. The pipeline is executed by the XNAT Pipeline Engine,"
2103.02044,data,16,,,images. This make them are hard to handle by users with limited experience in data
2103.02044,data,16,,,"questions. Therefore, the data organization is usually tailored to the study of interest and"
2103.02044,data,16,,,"researchers with tools to easily extract, import, and archive biomedical image data and with"
2103.02044,data,16,,,"sharing preclinical data in a safe, fast, and reliable manner is therefore imperative."
2103.02044,data,16,,,"the ParaVision® data structure, deciphers the binary file (2dseq) containing the image and"
2103.02044,data,16,,,variety of machinery yielding tons of raw data but the current practices to store and distribute
2103.02044,data,17,,,"and downloading data, while the XNAT Compressed Uploader is a tool that runs in the XNAT"
2103.02044,data,17,,,"from the lack of standard tools to store, process, and share imaging data produced by"
2103.02044,data,17,,,"labs. Data loss not only implies losing processed images and the know-how gained, but also"
2103.02044,data,17,,,needs to be matched to the XNAT data hierarchy. The capability of XNAT to manage different
2103.02044,data,17,,,tasks on project data. The workflow is defined in an eXtensible Markup Language (XML)
2103.02044,data,17,,,"trusted users have rights to access, manipulate and work with images. Data, tools and"
2103.02044,data,17,,,"web platform, to process medical imaging data derived from brain imaging studies [14]."
2103.02044,data,18,,,"[57] L. Persoon et al., “A novel data management platform to improve image-guided precision"
2103.02044,data,18,,,"is a broad collection of cancer image data  [7], the Alzheimer’s Disease Neuroimaging"
2103.02044,data,18,,,"neuroimaging repositories [26], [27]. SAS offers a cloud-based solution for exchanging data"
2103.02044,data,18,,,"stewardship,” Sci. Data, vol. 3, p. 160018, Mar. 2016."
2103.02044,data,18,,,storage and each vendor uses a proprietary format for its data. In order to deal with emerging
2103.02044,data,18,,,"that attribute, while the Value Representation (VR) describes the data type and format of each"
2103.02044,data,18,,,"that relate to scholarly data must be turned into FAIR. This includes the raw material, such"
2103.02044,data,19,,,"Connectome Project [6] is a compilation of neural data, The Cancer Imaging Archive (TCIA)"
2103.02044,data,19,,,Reusable (FAIR) Data Principle [59]. These rules are necessary to govern the scientific data
2103.02044,data,19,,,"[59] M. D. Wilkinson et al., “The FAIR Guiding Principles for scientific data management and"
2103.02044,data,19,,,"and biomedical imaging [61]. In this scenario, XNAT-PIC acts as a first step towards data"
2103.02044,data,20,,,"[18] T. Doel et al., “GIFT-Cloud: A data sharing and collaboration platform for medical imaging"
2103.02044,data,20,,,"as MRI, CT, PET and US. Future work will be needed to include data types related to"
2103.02044,data,20,,,"authorized users [15], [16]. It was originally conceived to deal with data management of"
2103.02044,data,20,,,"neuroimaging data,” Neuroinformatics, vol. 5, no. 1, pp. 11–33, 2007."
2103.02044,data,21,,,Initiative (ADNI) is a shared catalogue of image data related to the Alzheimer’s Disease [8]
2103.02044,data,22,,,"Data Flows in Medical Image Analysis,” Front. ICT, vol. 3, p. 15, Aug. 2016."
2103.02044,data,22,,,"for healthcare professionals,” in Fundamentals of Clinical Data Science, Springer International Publishing, 2018, pp. 37–53."
2103.02044,data,23,,,"[20] M. Beier et al., “Multicenter data sharing for collaboration in sleep medicine,” Futur. Gener."
2103.02044,data,24,,,Small Animal Big-data warehouse Environment for Research Small Animal Shanoir  Service Class Provider  Single Photon Emission Computed Tomography  The Cancer Imaging Archive Value Multiplicity
2103.02044,data,24,,,"[60] P. Jansen, L. van den Berg, P. van Overveld, and J. W. Boiten, “Research data stewardship"
2103.02044,data,30,,,"In Figure 7, the workflow for processing DWI acquisitions is schematically presented. Post processed image data and other files are uploaded back to XNAT in the resource folder"
2103.02044,data,39,,,"E. Williams et al., “Image Data Resource: A bioimage data integration and publication platform,” Nat. Methods, vol. 14, no. 8, pp. 775–781, Jul. 2017."
2103.02044,data,49,,,"[17] S. J. Doran et al., “Informatics in Radiology: Development of a Research PACS for Analysis of Functional Imaging Data in Clinical Research and Clinical Trials,” RadioGraphics, vol. 32, no. 7, pp. 2135–2150, 2012."
2103.02044,data,5,,,Availability of data and material
2103.02044,data,5,,,of biomedical image data.
2103.02044,data,53,,,"[10] S. Das, A. P. Zijdenbos, J. Harlap, D. Vins, and A. C. Evans, “LORIS: a web-based data management system for multi-center studies,” Front. Neuroinform., vol. 5, no. JANUARY 2012, p. 37, Jan. 2012."
2103.02044,data,58,,,"Figure 2: Snapshots of the XNAT-PIC application. A) Upon launch, users can choose between converting raw image data to DICOM standard or upload pre-existing DICOM images to XNAT. B) If the converter has been selected, MRI2DICOM allows the user to browse to the directory of the project in ParaVision® format."
2103.02044,data,7,,,data itself and any complementary material.
2103.02044,data,72,,,"D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and R. L. Buckner, “Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults,” J. Cogn. Neurosci., vol. 19, no. 9, pp. 1498–1507, Sep. 2007."
2103.02044,data,89,,,"Figure 3: Schematic representation of the custom variables implemented in XNAT to match a typical data organization in preclinical longitudinal studies: “group” refers to treated and untreated mice; “timepoints” is related to the timing of the acquisition: t0 = pre, before treatment; t1 = post1w, 1 week after; t2 = post2w, 2 weeks after; “dose” refers to amount of drug (dose1 and dose2, respectively) used in this specific experiment."
2103.02044,data,9,,,"data [24], [25]."
2103.02044,data,92,,,"Figure 5: Snapshots of the project (panel A) and subject (panel B) webpage in XNAT with multiple image sessions (MR, CT, and PET, respectively). The custom variable “group” describes the patient status (treated or untreated) and is shown at each level of the XNAT data hierarchy, while the custom variable “timepoint” referring to the timing of the treatment administration is displayed at the subject level (panel B) in the Label field."
2103.02044,"data, data available",16,,,Our future plan is to deploy a federated XNAT portal to collect preclinical imaging data from
2103.02044,"data, data available",17,,,"Identifier is missing, preventing the data to be found by both humans and computers. In"
2103.02044,"data, database",18,,,"images from several sources, to save data in a safe database, and to share data among"
2103.02044,"data, database",37,,,"I. B. Ozyurt et al., “Federated web-accessible clinical data management within an extensible neuroimaging database,” Neuroinformatics, vol. 8, no. 4, pp. 231–249, Dec. 2010."
2103.02044,"data, dataset",11,,,possibility to export proprietary raw image datasets to internationally recognized data
2103.02044,"data, dataset",17,,,"process imaging datasets. It offers a platform to store and distribute the data, manage the"
2103.02044,"data, dataset provided",11,,,data. Commercial softwares distributed by imaging device manufacturers provide the
2103.02044,"data, dataset provided",18,,,"(Figure 3). Notably, XNAT-PIC users can adjust the custom variables accordingly to their data"
2103.02044,"data, open-source",17,,,"data, all within an XNAT environment. All the developments are free and open-source. We"
2103.02044,"data, publicly available",15,,,studies and the relative costs are prompting imaging scientists to share image data in public
2103.02044,"data, publicly available",38,,,"J. Ellenberg et al., “A call for public archives for biological image data,” Nature Methods, vol. 15, no. 11. Nature Publishing Group, pp. 849–854, 01-Nov-2018."
2103.02044,"data, publicly available, data repository",14,,,principles whose goal is to provide a public data repository compliant to open science
2103.02044,database,10,,,eventually uploaded to the database as project level resources.
2103.02044,database,11,,,"studies, the Medical Imaging Research Management and Associated Information Database"
2103.02044,database,112,,,"Abbreviations ADNI AIM API CEST CIM COINS CT DICOM DICOM SEG DICOM Segmentation Digital Object Identifier DOI Decimal String DS Diffusion Weighted Imaging DWI European Open Science Cloud  EOSC European Population Imaging Infrastructure  EPI2 Findable, Accessible, Interoperable and Reusable  FAIR 18-fluorodeoxyglucose  FDG France Life Imaging FLI Human Imaging Database  HID Hypertext Transfer Protocol  HTTP Long String LO Longitudinal Online Research and Imaging System  LORIS Medical Database Multi-Modal Molecular Imaging Italian Node  Magnetic Resonance Imaging  Neuroimaging Informatics Technology Initiative Open Access Series of Imaging Studies  Optical Coherence Tomography  Open Health Imaging Foundation  Optical Imaging  Other Picture Archiving and Communication System  Photoacoustic Imaging  Positron Emission Tomography Representational State Transfer  Region of Interest"
2103.02044,database,13,,,Human Imaging Database (HID) and the Collaborative Informatics and Neuroimaging Suite
2103.02044,database,13,,,"based web application, exploiting the PostgreSQL database system. Users can personalize"
2103.02044,database,19,,,the ROI and uploads the results back to the database as XNAT resources (Figure 8). A
2103.02044,database,20,,,"facilitates scripting interactions with the XNAT database [41], [42], iii) Requests 2.23.0 that"
2103.02044,database,3,04/21/22,0,"Science, Database"
2103.02044,dataset,12,,,The processing of large volumes of biomedical image datasets requires dedicated platforms
2103.02044,dataset,12,,,processing preclinical image datasets. MRI2DICOM is a MR image converter from
2103.02044,dataset,13,,,"discover image datasets normally not accessible, promoting the free exchange and reuse"
2103.02044,dataset,13,,,"neither tools for importing large, multimodal preclinical image datasets nor pipelines for"
2103.02044,dataset,13,,,processing large image datasets. This workflow is based on the steps schematically
2103.02044,dataset,15,,,"2. XNAT-PIC Uploader to upload large, multimodal image datasets in DICOM standard to"
2103.02044,dataset,15,,,"The datasets analyzed in the current study are openly available in the CIM-XNAT repository,"
2103.02044,dataset,15,,,complexity and the variety of preclinical trial datasets. The time needed to perform these
2103.02044,dataset,15,,,dynamically added to the ‘standard’ DICOM dictionary to describe CEST-MRI datasets. The
2103.02044,dataset,15,,,"heterogeneous datasets,” Front. Neuroinform., vol. 5, Dec. 2011."
2103.02044,dataset,15,,,once the dataset is successfully imported to XNAT. The original raw images can be
2103.02044,dataset,16,,,"Upon conversion to DICOM format, the image dataset can be uploaded to XNAT. XNAT-PIC"
2103.02044,dataset,16,,,associate the dataset stored in XNAT with a Digital Object Identifier (DOI) or Persistent
2103.02044,dataset,16,,,needs to process large scale image datasets. The urgency was therefore to scale up this
2103.02044,dataset,16,,,"to our CIM-XNAT instance, upload their image datasets and add the pipelines to their own"
2103.02044,dataset,17,,,"datasets need to be established. In this paper, we present an extension of XNAT for"
2103.02044,dataset,18,,,"as imaging dataset, as well as the tools, workflows, and pipelines needed to process the"
2103.02044,dataset,19,,,"image datasets of different modalities to XNAT such as MRI, PET, CT, and US, allowing"
2103.02044,dataset,26,,,"several image repositories have emerged enabling the discovery of datasets from peer reviewed publications or research studies in the life science domain, from biological imaging"
2103.02044,dataset,30,,,Figure 1: Schematic workflow of image archiving and processing. XNAT-PIC is a suite of tools aimed at facilitating the management and the analysis of preclinical image datasets.
2103.02044,"dataset, publicly available",17,,,"[21] P. Kalendralis et al., “Multicenter CT phantoms public dataset for radiomics reproducibility"
2103.02044,download,14,,,"in the resource folder, download the morphological image in DICOM standard and the"
2103.02044,download,14,,,"parametric image in NIfTI format corresponding to the user selection, download the ROI"
2103.02044,download,16,,,"2. For each subject, create a local folder and then download the corresponding DICOM"
2103.02044,download,16,,,"contains XML instructions to take this user input, create the working directory, download the"
2103.02044,download,23,,,projects (See: Adding Pipelines To Your Project: https://wiki.xnat.org/documentation/how to-use-xnat/adding-pipelines-to-your-project); ii) XNAT Admins can download XNAT-PIC
2103.02044,download,4,,,available for download.
2103.02044,github,10,,,"pipelines from https://github.com/szullino/XNAT-PIC-Pipelines, install and register the"
2103.02044,github,14,,,https://github.com/szullino/XNAT-PIC [31]. The application uses the numpy 1.15.4 and
2103.02044,github,3,,,https://github.com/szullino/XNAT-PIC-Pipelines
2103.02044,"github, download",25,,,Table 2: Processing pipelines currently installed on our CIM-XNAT instance (http://cim-xnat.unito.it) and available for download at https://github.com/szullino/XNAT-PIC-Pipelines.
2103.02044,"github, download",51,,,The latest releases of the source codes of XNAT-PIC are available to download from the and GitHub https://github.com/szullino/XNAT-PIC. XNAT-PIC is a free software and is distributed under the terms of the GNU General Public License v3 or any later version as stated by the Free Software Foundation.
2103.02044,open-source,13,,,"available micro-services in SAS comprise Dicomifier, a generic and open-source Bruker to"
2103.02044,open-source,14,,,"In this work we have developed XNAT-PIC, a free and open-source application consisting"
2103.02044,open-source,15,,,"Preclinical Imaging Centers (XNAT-PIC). XNAT is a worldwide used, open-source platform"
2103.02044,open-source,15,,,preclinical imaging available at http://cim-xnat.unito.it. XNAT is a free and open-source Java
2103.02044,open-source,22,,,"[34] D. Mason, “SU‐E‐T‐33: Pydicom: An Open Source DICOM Library,” in Medical Physics,"
2103.02044,open-source,43,,,"[54] T. Urban et al., “LesionTracker: Extensible open-source zero-footprint web viewer for cancer imaging research and clinical trials,” Cancer Res., vol. 77, no. 21, pp. e119–e122, Nov. 2017."
2103.02044,package,14,,,"package to run MRI2DICOM and XNAT-PIC Uploader as a stand-alone executable, in both"
2103.02044,"publicly available, github",16,,,GNU General Public License v3 or any later version and available on GitHub. Some work
2103.02044,python,14,,,XNAT-PIC Uploader is built in Python 3.7.6. The communication with XNAT is possible
2103.02044,python,14,,,descriptor invokes a bash script running a Python wrapper consisting of a sequence of
2103.02044,python,14,,,script passes the resulted mask to a Python script that computes statistical calculations in
2103.02044,python,15,,,to process multiple subjects within the same project. A Python 2.7 virtual environment has
2103.02044,python,15,,,“Python Software Foundation.” [Online]. Available: https://www.python.org/.
2103.02044,python,16,,,3.5 has been used to bundle the Python applications and all its dependencies into a single
2103.02044,python,16,,,pipeline runs on Python 3.8.3 and uses the following libraries: numpy 1.18.5 [44]
2103.02044,python,18,,,"for Python to run MATLAB scripts within a Python session [40], ii) pyxnat-1.2.1.0.post3 that"
2103.02044,python,19,,,"acqp) into Python dictionaries [36]. Lastly, it saves all the relevant information into the"
2103.02044,python,19,,,"mask [45], the image processing library opencv-python 4.4.0.40 [46], and nibabel 3.1.1 for"
2103.02044,python,20,,,"Python library to encrypt the files containing the XNAT login credentials [37], [38]. PyInstaller"
2103.02044,python,21,,,“opencv-python · PyPI.” [Online]. Available: https://pypi.org/project/opencv-python/4.4.0.40/. [Accessed: 18-Feb-2021].
2103.02044,python,30,,,"[41] Y. Schwartz et al., “PyXNAT: XNAT in Python,” Front. Neuroinform., vol. 6, p. 12, May 2012."
2103.02044,python,30,,,“Get Started with MATLAB Engine API for Python - MATLAB & Simulink.” [Online]. Available: https://www.mathworks.com/help/matlab/matlab_external/get-started-with-matlabengine-for-python.html. [Accessed: 18-Feb-2021].
2103.02044,python,6,,,Python scripts in order to:
2103.02044,"python, code",28,,,resource descriptor. The resource descriptor invokes a bash script passing both the T2 weighted DICOM image and the DICOM RT-STRUCT directories to a Python code to be
2103.02044,"python, github",24,,,"[36] M. Caffini, “Project-Beat--Pyhton.” [Online]. Available: https://github.com/mcaffini/Project Beat---Python."
2103.02044,"python, open-source",13,,,MRI2DICOM is a free and open-source tool built in Python 3.7.6 downloadable at
2103.02044,"python, open-source",15,,,"through xnatpy 0.3.22, a new and open-source XNAT Python client, and pyAesCrypt 0.4.3"
2103.02044,"python, package",17,,,converted into a NIfTI mask by the Python package dcmrtstruct2nii [55]. The same bash
2103.03806,code,154,,,"In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to ﬁnd automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the ﬁeld of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers’ architecture to automatically detect malicious software. We propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection."
2103.03806,code,56,,,"2. We model Android malware detection as a binary and a multi-label text classiﬁcation problem and propose a novel feature representation by considering the software applications’ source code as a set of features. We apply text preprocessing on these features to keep the important information like permissions, intents, and activities."
2103.03806,code,59,,,"While in our work, we propose a noval approach, we used BERT to better detect malware, we reﬁned the pre-trained model to efﬁciently learn representations of source code language syntax and semantics. Our Context-Aware network learns contextual characteristics from a natural language sentences perspective thanks to the attention mechanism layers in the Transformer-based architecture."
2103.03806,code,89,,,"The wildly used type is static analysis [3]. It is a known way of identifying malicious applications among benign applications, and this analysis focuses on the source code of software components that may be affected by malware. It is less expensive in terms of resources and time since no need to activate the malware by executing the code to capture the features, it can identify the maliciousness at the code level. For static analysis, there are mainly three practices to detect"
2103.03806,data,154,,,"Due to the exponential growth of digitalization usage and easy access to internet technology, cyber threats on information systems such as computers and smartphones increased. Malicious software (malware) is the primary tool used by attackers to perform cyber attacks. Different malware categories such as Trojans, Adwares, and Risktools are expeditiously developed and updated with recent encryption and deformation technologies to become a more severe threat to cyberspace. This rapid development incident often with harmful consequences to different data users at the level of both persons and businesses. For example, IBM reported that the average cost of a data breach is $3.86 million as of 2020, however, the average time to identify a breach was 207 days [1]. As result, IT security professionals and researchers need to update the tools available to automatically detect new malware attacks."
2103.03806,data,157,,,"RNN or also called sequence modeling like LSTM [10], is a family of neural networks for processing sequential data. A recurrent network that maps an input sequence of x values to a corresponding sequence of output o values [10]. A loss L measures how far each O is from the corresponding training target y [10]. The loss L internally computes y = sof tmax(O) and compares this to the target y [10]. In terms of limitations, RNN based architectures are hard to parallelize because the forward propagation graph is inherently sequential each time step may be computed only after the previous one, where both the runtime and the memory cost are O(t) and cannot be reduced since states computed in the forward pass must be sorted until they are reused during the backward pass."
2103.03806,data,3,,,4.1 Data Collection
2103.03806,data,53,,,"[26] Hiromu Yakura, Shinnosuke Shinozaki, Reon Nishimura, Yoshihiro Oyama, and Jun Sakuma. Malware analysis of imaged binary samples by convolutional neural network with attention mechanism. In Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy, pages 127–134, 2018."
2103.03806,data,67,,,"[1] IBM. Cost of a data breach report 2020. 2020. [2] Anusha Damodaran, Fabio Di Troia, Corrado Aaron Visaggio, Thomas H Austin, and Mark Stamp. A comparison of static, dynamic, and hybrid analysis for malware detection. Journal of Computer Virology and Hacking Techniques, 13(1):1–12, 2017."
2103.03806,data,78,,,"and classify malware: Permission-based (verify if the requested permissions are needed for the app to ensure the normal behavior on the app access and usage of the user data), Signature-based (identify if the app signature matches one of the malware signatures among the library collected in advance), and speciﬁcation-based (verify if the app violates the rules set by experts to decide the maliciousness of a program under inspection)."
2103.03806,data,84,,,"Recent research in deep learning, focus mainly on Transformer-based models such as BERT [4] and XLNet [5]. These approaches clearly showed impressive results in various state-of-the-art natural language processing (NLP) [6] and computer vision [7] tasks. Thanks to the attention mechanisms layers [8] added to the encoder-decoder architecture, Transformers can focus on the most important patterns in the data, leading to a remarkable boost in performance."
2103.03806,"data, dataset",108,,,"Also, RoBERTa [13] an extension of BERT with some modiﬁcations in the pre-training procedure of the architecture, uses the same architecture. The modiﬁcations include training the model longer, with larger batches, and on a larger amount of data. Also, the predictive objective of the following sentence was deleted. And in order to on longer sequences, the authors applied dynamic modiﬁcation on the masking scheme [12] applied to the training data. They also collect a new dataset of comparable size to other privately used datasets to better control the effects of training set size."
2103.03806,"data, dataset",161,,,"This paper studies the challenge of malware classiﬁcation using our novel approach for Transformer-based malware detection. We detailed the malware text classiﬁcation methodology and used it for feature representation. The BERT based model achieved high accuracy results for both binary and cross-category classiﬁcation, compared to the other baseline pre-trained language models. The results from the experiments show that the best binary accuracy is 0.9761 and for the multi-classiﬁcation, it is 0.9102. We can conclude that the proposed approach’s results of feature representation as text input for a Transformer-based model, are very good. So, the implementation of pre-trained linguistic models based on Transformer architectures in cybersecurity tasks can outperform standard RNN models like LSTM, when applied on a state-of-the-art dataset like Androzoo. Future work will therefore consist of testing other models, testing other types of data, and creating an API to detect malware in new applications."
2103.03806,"data, dataset",170,,,"Once the data is created and annotated in the right format, we split the data into train and test. We conduct all our experiments using BERT, we ﬁne-tuned it on our train dataset. We ﬁxed the hyperparameters based on each classiﬁcation type. We train BERT to predict Malware/Benign (i.e., binary classiﬁcation) for each sample, then, to predict the categories of malware (i.e, multi-classiﬁcation). The Transformer architecture has speciﬁc input formatting steps including the creation of special tokens and ids. We use the Transformers implementation of the hugging face library [34] for the binary classiﬁcation of Android applications. Only the Transformer architecture, layers, and weights are implemented, while all data formatting must be done beforehand to be compatible with the Transformer. While most pre-trained Transformers have essentially the same steps. Here we test this approach with BERT. Figure 4 gives a detailed overview of our approach."
2103.03806,"data, dataset, code",145,,,"In this paper, we propose a malware detection approach using a Transformer-based algorithm. We experiment with different Transformer model architectures on our data. The dataset includes 11 different malware categories namely adware, spyware, ransomware, clicker, dropper, downloader, riskware, SMS-sender, horse-trojan, backdoor, and banker [9]. Our methodology focuses on the static analysis level on the source code of Android applications, to identify different categories of malware. Indeed, we did not limit the features to permission-based only but considered the whole software code as an important set of feature representation for the analysis. We started with training the model with the features after preprocessing, then a binary classiﬁcation of the apps to malicious and benign, and ﬁnally a cross-category classiﬁcation at the malware level."
2103.03806,"data, dataset, code, download",270,,,"Once the list of APKs is deﬁned, we write a script to download the ﬁles. Then, we decompiled the downloaded APKs using Jdax [32], which creates folders of the apps’ ﬁles [33]. We extracted the AndroidManifest.xml ﬁle from each sample. The manifest presents essential information about the application to the Android system, information the system must have before it can run any of the application’s code, including the list of permissions, the activities, services, broadcast receivers, content providers, the version, and the meta-data. These ﬁles are then treated as text ﬁles and passed through the preprocessing phase, in this step and to conserve the important information about the features, we apply speciﬁc cleaning of the not important, mostly repeated words, in the code. We manually analyzed different examples and created a list of words and expressions that do not provide additional info, so the cleaning included lexicon removal, punctuation removal and we conserved the digits and the cases of the characters. The purpose of the preprocessing is to reduce the size of the input. The ﬁnal dataset format has 4 columns, the ID column, represented by the APK hash name, the Text column representing the Manifest ﬁles after preprocessing, the Label column, a binary format equal to 1 if the app is malware and 0 if not, and ﬁnally the Category column representing the malware type name (exp: adware)."
2103.03806,"data, dataset, data https, download",310,,,"Based on state-of-the-art taxonomies for Android malware categories [29] & [30] We selected 11 categories 3 namely; adware (displays advertising and entice a user to install it on their device), spyware (installs itself on the user device with the aim of collecting and transferring information without the user is aware of it), ransomware (takes personal data hostage), clicker (a type of trojan that performs a form of ad fraud. These “clickers” continuously make connections to websites, consequently awarding threat actors with revenue on pay-per-click bases), dropper (a syringe program or dropper virus, is a computer program created to install malicious software on a target system), downloader (a type of Trojan horse that downloads and installs malicious ﬁles), riskware (a software whose installation can represent a risk for the security of the computer, but however, not inevitably), SMS-sender (presents itself as a regular SMS messaging application and uses its basic permissions to send/receive short messages), horse-trojan (is designed to damage, disrupt, steal, or in general inﬂict some other harmful action on your data or network), backdoor (when introduced into the device, usually without the user’s knowledge, turns the software into a Trojan horse, and banker (is designed to steal data from users’ online bank accounts as well as data from online payment systems and plastic card systems). We select the list of APKs to download based on the recent creation and analysis date, then re-analyze this list with VirusTotal [31], to ﬁnally create our dataset list including 12,000 benign apps and 10,000 malware apps."
2103.03806,"data, dataset, publicly available",112,,,"We collected the Android applications from the Androzoo public dataset. Androzoo, one of the stae of the art android malware dataset [27], is a growing collection of Android applications from several sources, including the ofﬁcial Google Play app market. It currently contains 13,320,014 different APKs, each of which has been analyzed by dozens of different antivirus products to ﬁnd out which applications are detected as malware. This public data is up to date with weekly analysis on the samples [28]. The data is labeled based on these analyses into malware and benign, and different malware categories and families."
2103.03806,dataset,145,,,"We conducted the experiments on the preprocessed dataset. Fine-tuning the pre-trained models, clearly gave the highest accuracy results for this classiﬁcation task compared to the LSTM baseline. The best classiﬁcation model is BERT. The test metrics results of Table 1 show that each Transformer learns differently depending on each architecture. The results in Table 1 and Table 2 prove that BERT outperformed the other baseline models in both binary and multi-classiﬁcation malware detection. For BERT, the best learning rate shows that only two epochs are required before the loss starts to increase. Our ﬁne-tuning with the training set included changing the hyperparameters to boost the results. To evaluate the ﬁnal results, we used different evaluation metrics. The pretrained models achieved good results overall, but BERT obtained the best performance in both tasks."
2103.03806,dataset,22,,,Table 1: Detection results using the feature representation approach across difference networks on the test dataset for both binary classiﬁcation.
2103.03806,dataset,22,,,Table 2: Detection results using the feature representation approach across difference networks on the test dataset for cross-category malware classiﬁcation.
2103.03806,dataset,44,,,"applies a CNN with an attention mechanism to images converted from binary datasets, by calculating an attention map to extract characteristic byte sequence. The distinction of regions in the attention map shows regions having higher importance for classiﬁcation in the image."
2103.03806,dataset,55,,,2. MCC: The Matthews Correlation Coefﬁcient (MCC) is bast used for binary classiﬁcation with an unbalanced dataset. It has a range of -1 to +1. We chose MCC over F1-score for binary classiﬁcation as recommended in this study [36]. MCC equation is deﬁned as fellow :
2103.03806,dataset,55,,,"To test the proposed approach, we evaluate it in terms of three main aspects: (1) the proﬁtability on large and recent categorical datasets, (2) the feature representation ability for information context extraction from android apps, and (3) the performance compared to the state-of-the-art approaches."
2103.03806,"dataset, code",118,,,"This section explains the overall process of malware detection. The core idea of this work is to create, a malware detection framework using a Transformer-based approach. To reach this goal, we conducted a static analysis on the collected corpora from a natural language sentences perspective. So, we need a dataset including source code ﬁles and different categories of malware types. Figure 2 explains the logical ﬂow of our Android malware detection. This Process is mainly divided into 4 phases. First, the Android ﬁles collection, then the Decompilation phase of the APK ﬁles, Feature Mining, and ﬁnally Deep Learning (DL) models training experiments."
2103.03806,"dataset, code, package",277,,,"Among these approaches this study [16] builds AMalNet, a DN framework to learn multiple integration representations and family assignment with Graph CNN (GCNs) to model high-level graph semantics and use an Independent RNN (IndRNN) to decode deep semantic information. SeqMobile [17], is a behavior-based sequence approach. it uses different recurrent neural networks (RNN). It extracts the semantic feature sequence, which can provide information of certain malicious behaviors, from binary ﬁles under a certain time constraint. This paper [18], presents a new approach based on OpCode-level FCG. The FCG is obtained through static analysis of Operation Code (OpCode) using a Long Short-Term Memory (LSTM). the authors conduct experiments on a dataset on 1,796 Android malware samples classiﬁed into two categories and 1,000 benign Android apps. The authors of [19] focused on step size as an important factor in relation to input size using RNN. They tested the model with three different feature vectors (hot-coding feature vector, random feature vector and Word2Vec feature vector) using hyper parameters. [20] transform the android package kit (APK) ﬁle into a lightweight RGB image using a predeﬁned dictionary and intelligent mapping, then apply a CNN on the obtained images for malware family classiﬁcation. Multiple other examples of DNN based approach [21] and [22], have been developed, with varying the feature extraction, selection, and representation methods in the aim of boosting the detection results."
2103.03806,"dataset, publicly available",32,,,3. We conduct extensive experiments on our preprocessed Android dataset collected from public resources with different category-annotated labels. This preprocessed dataset will be released publicly for the research community.
2103.03806,github,68,,,"[31] Virus Total. Virustotal-free online virus, malware and url scanner. Online: https://www. virustotal. com/en, 2012. [32] jadx. jadx - dex to java decompiler. Online: https://github.com/skylot/jadx, 2012. [33] Nicolas Harrand, César Soto-Valero, Martin Monperrus, and Benoit Baudry. Java decompiler diversity and its"
2103.03806,open-source,49,,,"[28] Pei Liu, Li Li, Yanjie Zhao, Xiaoyu Sun, and John Grundy. Androzooopen: Collecting large-scale open source android apps for the research community. In Proceedings of the 17th International Conference on Mining Software Repositories, pages 548–552, 2020."
2103.03968,data,178,,,"Our experiments show that the proposed sinogram interpolation algorithm can lead to large improvements in image quality. Due to the additional denoising eﬀect of both regularization functions used by the proposed algorithm, this improvement is more signiﬁcant when applied on low-dose scans. This means that the proposed algorithm is especially well-suited for sinogram restoration in lowdose CT. Our other experiments, not reported in this paper because of space limitations, show that the proposed algorithm can also be used to eﬀectively interpolate the sinogram in more general cases than the case considered in this paper, for example when the angular spacing of the missing projection views is non-uniform or when some detector measurements are corrupted. The experimental results suggest that our proposed method can reduce the number of projection measurements that are needed to reconstruct an image with diagnostic quality. The reduction in the number of projection measurements also means that less data will need to be transmitted in situations where the image reconstruction is performed in a diﬀerent site."
2103.03968,data,214,,,"Abstract. As the medical usage of computed tomography (CT) continues to grow, the radiation dose should remain at a low level to reduce the health risks. Therefore, there is an increasing need for algorithms that can reconstruct high-quality images from low-dose scans. In this regard, most of the recent studies have focused on iterative reconstruction algorithms, and little attention has been paid to restoration of the projection measurements, i.e., the sinogram. In this paper, we propose a novel sinogram interpolation algorithm. The proposed algorithm exploits the self-similarity and smoothness of the sinogram. Sinogram selfsimilarity is modeled in terms of the similarity of small blocks extracted from stacked projections. The smoothness is modeled via second-order total variation. Experiments with simulated and real CT data show that sinogram interpolation with the proposed algorithm leads to a substantial improvement in the quality of the reconstructed image, especially on low-dose scans. The proposed method can result in a signiﬁcant reduction in the number of projection measurements. This will reduce the radiation dose and also the amount of data that need to be stored or transmitted, if the reconstruction is to be performed in a remote site."
2103.03968,data,5,,,3.1 Experiment with simulated data
2103.03968,data,6,,,3.2 Experiment with real CT data
2103.03968,data,64,,,"22. Wang, J., Lu, H., Liang, Z., Eremina, D., Zhang, G., Wang, S., Chen, J., Manzione, J.: An experimental study on the noise properties of x-ray CT sinogram data in Radon space. Physics in Medicine and Biology 53(12), 3327 (2008)"
2103.03968,database,210,,,"We ﬁrst applied our algorithm on scans simulated from a brain phantom, which we obtained from the BrainWeb database [3]. We simulated nθ projections from this phantom, for two values of nθ = 1440 and 960. For each nθ, we ﬁrst reconstructed the image of the phantom from the full set of nθ projections and from nθ/2 projections; we denote these images with xnθ and xnθ/2, respectively. We then applied the proposed algorithm and the dictionary-based interpolation algorithm to interpolate the subset of nθ/2 projections to generate nθ projections and reconstructed the image of the phantom from the interpolated projections. We will denote these images with xproposed nθ/2. We simulated two levels of noise in the projections with diﬀerent number of incident photons: N0 = 106 and N0 = 5 × 104. We will refer to these simulations as low-noise and high-noise, respectively. For both simulations, we assumed the detector electronic noise to be additive Gaussian with a standard deviation of 40. As the reference scan that we need for block matching for computation of Rs, we used the simulated scan of a diﬀerent brain phantom from the same database."
2103.03968,database,62,,,"3. Cocosco, C.A., Kollokian, V., Kwan, R.K.S., Pike, G.B., Evans, A.C.: Brainweb: Online interface to a 3d mri simulated brain database. In: NeuroImage (1997) 4. Feldkamp, L.A., Davis, L.C., Kress, J.W.: Practical cone-beam algorithm. J. Opt."
2103.06238,data,1,,,Data
2103.06238,data,1,,,data
2103.06238,data,114,,,"In gathering data, the researchers used the Survey Method using questionnaires to be answered by the students. The questionnaire consists of two questions that are related to the study. The overall response of the students is used as the basis of the project to pursue allowing the project to gather relevant data to determine its validity and success. Data analysis is the next step after data gathering. In this part, the project analyzed all the data that were gathered, meaning the answers from the survey were organized and then interpreted. The results then served as a guide for the validity and pursuit of the project."
2103.06238,data,5,,,Data Gathering and Data Analysis
2103.06238,data,5,,,analysis. http://w3.unisa.edu.au/researchstudents/milestones/data.asp
2103.06238,data,85,,,"The gathering of information is the next step which is vital in interpreting or analyzing structure responses that increase the core knowledge on the project to develop. The data can be gathered in a diverse way, for example, interviews, observations, gathering of receipts and forms, surveys, experiments, and other related activities. The data or information that has been gathered must be summarized, interpreted, and analyzed before having conclusions (UniSA, 2016)."
2103.06238,"data, code",165,,,"The study uses a Modified Iterative Development as a guide for the design and development of the VR 3D Model and its environment. Iterative development is a procedure on which there is a breaking down of the computer program advancement of a huge application into little portions. In the iterative development process, the highlighted code is planned, created, and tried in rehashed cycles. In each repetition, there is re-designing of additional features, developed and verified until it is ready to be deployed or installed. Since this project used Modified Iterative Development, there are some changes of the iteration on the stages to ensure that the program follows the desired outcomes set by the developers. Figure 1 illustrates the Modified Iterative Development to produce an increment project that is being developed. The process starts with the Planning, Data Gathering and Data Analysis, System Requirements, Designing, Testing, and Evaluation."
2103.12883,data,198,,,"Abstract— Since the application of Deep Q-Learning to the continuous action domain in Atari-like games, Deep Reinforcement Learning (Deep-RL) techniques for motion control have been qualitatively enhanced. Nowadays, modern Deep-RL can be successfully applied to solve a wide range of complex decision-making tasks for many types of vehicles. Based on this context, in this paper, we propose the use of Deep-RL to perform autonomous mapless navigation for Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs), robots that can operate in both, air or water media. We developed two approaches, one deterministic and the other stochastic. Our system uses the relative localization of the vehicle and simple sparse range data to train the network. We compared our approaches with a traditional geometric tracking controller for mapless navigation. Based on experimental results, we can conclude that Deep-RLbased approaches can be successfully used to perform mapless navigation and obstacle avoidance for HUAUVs. Our vehicle accomplished the navigation in two scenarios, being capable to achieve the desired target through both environments, and even outperforming the geometric-based tracking controller on the obstacle-avoidance capability."
2103.12883,data,44,,,"This work contains the following contributions: • We propose two new approaches based on state-ofart Deep-RL algorithms for ground robots that can successfully perform goal-oriented mapless navigation for HUAUVs, using only range data readings and the vehicles’ relative localization data."
2103.12883,data,46,,,"both air and underwater environments can be obtained using sensors like a lidar and a Sonar, respectively. As well as for the vehicle’s localization data, which could be obtained by a combination of sensors like GPS and USBL for example."
2103.12883,data,52,,,"agents also learned to bypass obstacles to arrive at a target, where traditional algorithms fail. We have validated two powerful approaches, which managed to understand the complex behaviors in realistic simulation and that can be used in the real world if guarantying range ﬁndings and relative localization data."
2103.12883,data,56,,,"In this work, we aim to navigate our described HUAUV autonomously from a starting point to a target point in a different environment without any environmental knowledge, preventing any collision with the scenario by using only range readings and the vehicle localization data. Its translation function, therefore, is deﬁned as:"
2103.12883,data,60,,,"[26] K. Kang, S. Belkhale, G. Kahn, P. Abbeel, and S. Levine, “Generalization through simulation: Integrating simulated and real data into deep reinforcement learning for vision-based autonomous ﬂight,” in Int. Conf. on Robotics and Automation (ICRA). IEEE, 2019, pp. 6008–6014."
2103.12883,data,76,,,"Our proposal differs from the discussed works by only using the vehicle’s relative localization data and not its explicit localization data. We also propose two Deep-RL approaches which we called 3DNDRL-D and 3DNDRL-S, a deterministic one and also a bias-stochastic one to further enhance our work. We compare them with the mapless navigation performed by a traditional geometric tracking controller that can be used for mapless navigation [16]."
2103.12883,github,3,,,1https://github.com/ricardoGrando/hydrone_deep_rl_icra
2103.12883,package,51,,,"[31] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “Uuv simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in OCEANS MTS/IEEE Monterey. IEEE, 2016, pp. 1–8."
2103.12883,package,55,,,"• We demonstrate that, with our approaches, the robot is capable to arrive at the desired target avoiding collisions. However, with a geometrically dependent tracking controller, the robot is unable to bypass the drilling risers. We also provide a completely built ROS package with a real-world described HUAUV."
2103.12883,python,63,,,"The whole system was implemented using ROS and Gazebo frameworks. The Deep-RL approaches were implemented using Python programming language, while the vehicles’ related plugins were partially implemented in C++ and Python. The implementation of the neural networks was carried out with the PyTorch2 library. The performance of our approaches can also be observed in a complementary video3."
2103.12895,"code, package, python, data, dataset, dataset provided",151,,,"the loading end), each named accordingly (front, middle, rear) in the dataset. Hence, there were 9 loggers in total for each instrumented shipment labeled with respect to the loggers’ location in the pallet and the pallets’ location in the container (front−top:FT, front−middle:FM, front−bottom:FB, ..., rear−bottom:RB). Figure 3 displays each sensor proﬁle separately for each of the 6 shipments. Please observe that these ﬁgures display real−world, noisy and complex multivariate time series as signature representatives of each shipment. Summary statistics for all variables from the shipments datasets are presented in Table 4. These statistics were obtained using the ‘Pandas’ Python package [8]. Python Code used for ﬁltering and analyzing these data will be provided based on request."
2103.12895,data,15,,,"series data for food engineering. Journal of Food Engineering, 298:110477, 2021."
2103.12895,data,16,,,2. Sense2Vec: Representation and Visualization of Multivariate Sensory Time Series Data [1]
2103.12895,data,173,,,"This article describes location aware temperature proﬁles from six strawberry shipments across the continental United States. Three pallets were instrumented in each shipment with three vertically placed loggers to take a longitudinal and latitudinal snapshot of 9 strategically different locations (including the top, middle and bottom layers of the pallets placed in the back, middle and the front of the shipping container) for a combined 54 measurement points across shipments of varying lengths. The sensors were instrumented in the ﬁeld, right at the point of harvest, recorded temperatures every every 5 to 10 minutes depending on the shipment, and uploaded their data periodically via cellular radios on each device. The data is a result of signiﬁcant collaboration between stakeholders from farmers to distributors to retailers to academics, which can play an important role for researchers and educators in food engineering, cold-chain, machine learning, and data mining, as well as in other disciplines related to food and transportation."
2103.12895,data,18,,,"3Possible sensor and data applications, and several future research trajectories are summarized in [1]."
2103.12895,data,18,,,Figure 3: Temperature proﬁles of multivariate time series data from precooling to the end of transportation.
2103.12895,data,19,,,Table 4: Time intervals for the temperature measurements for all sensor recordings used in the data collection.
2103.12895,data,20,,,1. Statistical and temporal analysis of a novel multivariate time series data for food engineering [5] ;
2103.12895,data,21,,,Figure 4: DeltaTrak’s Reusable Real−Time−Logger (RTL) Mini devices to log temperature data in real time.
2103.12895,data,21,,,[1] A. Abdella and I. Uysal. Sense2vec: Representation and visualization of multivariate sensory time series data.
2103.12895,data,21,,,"[3] Ricardo Badia-Melis, Ultan Mc Carthy, and Ismail Uysal. Data estimation methods for predicting temperatures of"
2103.12895,data,210,,,"The overall end-to-end data collection stages are highlighted by Figure 2. The ﬁrst stage depicts of the beginning of the strawberry cold chain at the ﬁeld, where they are harvested and placed into clamshells to build pallets. Pallets are then driven to the nearest processing facility to be precooled down to (0◦C). Finally, the strawberry is transported into different states. The second stage of the pipeline is inspired by a cross-industry standard process for data mining (CRISP-DM) methodology [6]. There are six phases to consider 1. Data collection directly from the GSM towers through cloud API. 2. Data description, data quality checking, outliers, and missing values analysis, data exploration, and ﬁnal preparation; 3. Feature engineering for time series data including scaling, sampling, correlation, and averaging; 4. Machine learning/ Deep learning modeling for time series analysis including representation, forecasting, classiﬁcation and clustering. 5. Choosing the right evaluation metric based on the problem deﬁnition and application (ensures that the model properly achieves the project objectives); Finally, 6.deploying the learned model into production for practical usage."
2103.12895,data,212,,,"Description of data collection: The loggers were instrumented inside the pallets of strawberries right at the point and time of harvest during the pallet buildup stage temperature recording was initiated. A total of three loggers were placed in a single pallet distributed equally along the vertical axis. Speciﬁcally one logger was placed closer to the bottom of the pallet (3rd layer from the bottom), another was placed closer to the middle layer of the pallet and a third was placed closer to the top layer of the pallet (3rd layer from the top) between the fruits. A total of three instrumented pallets were sent out with each of the six shipments. Similar to the placement of loggers within the pallet, the instrumented pallets inside the container were distributed equally along the horizontal axis. Hence, there were 9 loggers in total for each instrumented shipment labeled with respect to the loggers’ location in the pallet and the pallets’ location in the container (front−top, middle−middle, rear−bottom, etc). The data were transmitted in real time via GSM cellular networks which eliminate the need to collect the loggers at the end of the shipment."
2103.12895,data,22,,,Figure 2: Cold-chain time-series data collection pipeline. The images show the real environment for data collection in this work.
2103.12895,data,261,,,"We used DeltaTrak’s Reusable Real−Time−Logger (RTL) Mini devices as shown in Figure 4 to log both temperature and location data in real time. The data is transmitted in real time via cellular networks which eliminate the need to collect the loggers at the end of the shipment to be able to have access to the recorded data. The loggers have a wide temperature range of −30◦C to 95.55◦C with a temperature accuracy of +/−1◦C. The device also eliminates the need for any prior infrastructure setup to be able to automatically collect the data (such as readers for radio frequency identiﬁcation (RFID) transponders). More information about the hardware used in this study can be found in the appendix. The loggers were instrumented inside the pallets of strawberries right at the point and time of harvest during the pallet buildup stage. A total of three loggers were placed in a single pallet distributed equally along the vertical axis. A total of three instrumented pallets were sent out with each of the six shipments. Similar to the placement of loggers within the pallet, the instrumented pallets inside the container were distributed equally along the horizontal axis. Speciﬁcally, one instrumented pallet was placed at the front of the container (i.e., close to the front of the truck), another was placed near the middle of the container and a third was placed at the back of the container (i.e., close to"
2103.12895,data,3,,,2 Data Description
2103.12895,data,3,,,Data accessibility:
2103.12895,data,30,,,Subject: Agricultural Sciences Speciﬁc subject area: Food engineering and time series Type of data: Tabular data - CSV ﬁles Data format Mixed (raw and preprocessed)
2103.12895,data,36,,,"264 127 132 132 130 132 Table 2: Time sampling rate and the duration for all sensor recordings used in the data collection. The calculations include harvest, precooling, and transportation periods."
2103.12895,data,38,,,"Institution: University of South Florida and University of Florida Country:US States for collected samples/data: Florida, Georgia, Maryland, Pennsylvania, Virginia, South Carolina, North Carolina, Texas, California."
2103.12895,data,4,,,Data source location:
2103.12895,data,45,,,∗The authors would like to thank our collaborators for their support of the project including WishFarms for allowing us to conduct the shipping tests and coordinating all the logistics and DeltaTrak for donating the real−time loggers used to collect the data in the study.
2103.12895,data,5,,,1 Value of the Data
2103.12895,data,5,,,Stage 2: Data Collection
2103.12895,data,55,,,"• We hope that this data will motivate the food transportation research community to delve into developing more sophisticated regression and classiﬁcation algorithms for univariate and multivariate time series, better clustering methods, learning representations with dimensionality reduction, and a better mathematical and statistical understanding of what is happening in cargo hold."
2103.12895,data,6,,,Primary data sources: WishFarms.
2103.12895,data,69,,,"How data were acquired: Figure 1 shows a US Map that highlights the shipping routes that were monitored to acquire the data. Two of the shipments originated from Plant City, Florida with ﬁnal destinations in Florida and Georgia. Four shipments originated from Salinas, California with ﬁnal destinations in Maryland, Pennsylvania, Virginia, South Carolina, North Carolina, Georgia and Texas."
2103.12895,data,7,,,Source of Data Shipments 1 through 6
2103.12895,data,79,,,"[6] R. Wirth and J. Hipp. Crisp-dm: towards a standard process modell for data mining. 2000. [7] Maria Cecilia do Nascimento Nunes, Mike Nicometo, Jean Pierre Emond, Ricardo Badía Melis, and Ismail Uysal. Improvement in fresh fruit and vegetable logistics quality: berry logistics ﬁeld studies. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 372, 2014."
2103.12895,data,84,,,"Parameters for data collection: The strawberry cold chain beginning at the ﬁeld, the strawberries are harvested and placed into plastic clam shells packed into cardboard ﬂats with 8 clamshells (each weighing one pound) per ﬂat, which are subsequently stacked together to build pallets containing between 18−20 layers with 6 ﬂats per layer. Once the pallets are built in the ﬁeld on the back of a ﬂatbed trailer, they are driven to the nearest processing facility to be"
2103.12895,data,85,,,"• With a combined decades worth of research in perishable post-harvest logistics, the authors believe that there is still a lot of unknowns when it comes to what actually happens in the cargo hold throughout the shipment. Another reason why this data is novel and important is that unlike many other studies which involved a singular entity, it is the result of a signiﬁcant collaboration between all the stakeholders in the cold chain from growers to distributors to retailers to academics."
2103.12895,data,86,,,"The objective of the data collection process was to obtain the wide range of temperature proﬁles to which the strawberry shipments around the United States are subjected from the time of the harvest to the arrival at the distribution center and distribution of individual ﬂats to the retail stores as shown by Figure 2. In total there were six shipments as shown by Figure 3, which cover both short and long−distance transportation scenarios (more details are provided in Table 4). Two"
2103.12895,data,87,,,"• Develop temporal algorithms for solving regression, classiﬁcation and clustering tasks on time series and deploy advanced data analytics to predict the future behaviour of the data proﬁles during transportation from harvest to the DC, or to identify if a sensor proﬁle satisﬁed a speciﬁc quality control criteria for a retailer; • Educational purposes which include analysis of univariate or multivariate time series data using statistical methods for regression, deep learning for time series classiﬁcation, learning representation and location-based prediction; 3"
2103.12895,"data, data https",22,,,Please visit the following website for more information about the sensors used in collecting the data: https://www. deltatrak.com/reusable-real-time-loggers
2103.12895,"data, data https, supplementary data, data available",16,,,Supplementary data associated with this article can be found in the online version at Supplemental ﬁles
2103.12895,"data, dataset",7,,,Direct URL to data: Dataset link
2103.12895,"data, dataset",78,,,"• The analysis and processing of temperature time-series data for predictive tasks represent a signiﬁcant challenge especially when proﬁles may have variable lengths, high variability, and abnormalities as is common in many cold chain applications. The dataset will enable researchers from a wide array of ﬁelds and backgrounds to apply analytical tools such as machine learning and physical models in testing and comparing the performance of their predictive or diagnostic algorithms on the cold-chain."
2103.12895,"data, python",89,,,Instruments: DeltaTrak’s Reusable Real−Time−Logger (RTL) Mini devices are used to log both temperature and location data in real time. The RTLs have a wide operational temperature range of −30◦C to 95.55◦C with a temperature accuracy of +/−1◦C. More information about the hardware used in this study can be found in the appendix. Data was extracted via the cloud application which can establish secure communications with the GSM loggers. Python [4] was employed to perform subsequent data analysis.
2103.12895,dataset,13,,,A TIME-TEMPERATURE DATASET FOR THE STRAWBERRY COLD CHAIN ACROSS MULTIPLE SHIPMENTS AND LOCATIONS
2103.12895,dataset,196,,,"Monitoring and controlling the refrigeration of food during the cold-chain (transportation, storage, and distribution of perishable food items) are critical to reducing the amount of food waste.However, the cost of installation of the monitoring devices such as wireless sensor networks (WSNs) and radio frequency identiﬁcation (RFID) systems limits monitoring resolution in commercial applications generally to one per container [[2], [3]]. Hence, with signiﬁcant collaboration between the stakeholders from growers to distributors to retailers to academics, six strawberry shipments across the continental United States datasets are shared to help in overcoming this limitation. The datasets now made available were collected aiming at understanding the holistic temperature behavior of the strawberry cold-chain and the development of prediction models to predict the future behavior of the strawberries during transportation from harvest to the DC. Nevertheless, due to the temporal heterogeneity, complexity, similarity, and discrepancy characteristics of the variables included in these datasets, their use goes beyond this future prediction problem to location-based-prediction, binary control criteria, classiﬁcation, clustering, etc."
2103.12895,python,22,,,"[4] Guido Van Rossum and Fred L Drake Jr. Python tutorial. Centrum voor Wiskunde en Informatica Amsterdam, The"
2103.13219,data,117,,,"To apply a convnet trained from the synthesised data into the context of real recordings, a transfer learning approach can be used. It has been gaining more attentions in MIR for alleviating the data sparsity problem and its ability to be used for different tasks. For example, Choi et al. [21] obtained features from CNNs, which were trained for music tagging in the source task. These features outperformed MFCC features in the target tasks, such as genre and vocal/non-vocal classiﬁcation. We believe such strategy is suited to the challenges in detecting the sustain pedal from polyphonic piano music recorded in different acoustic and recording conditions."
2103.13219,data,125,,,"To identify which audio frames were played with the sustain pedal, we can use SVMs to classify the frame-wise convnet features into pedal on or off states. SVMs were chosen ﬁrst because we assume the features extracted from the carefullytrained model in the source task should be representative and separable. Second, the SVM algorithm was originally devised for classiﬁcation problems, involving ﬁnding the maximum margin hyperplane that separates two classes of data and has been shown ideal for such a task [28]. This allows us to focus on the quality of learnt features. SVMs were trained using a supervised learning method in the target task, where the detection was done on acoustic piano recordings."
2103.13219,data,154,,,"In our case, training a convnet with the synthesised data is considered as the source task. Then in the target task, we can use the learnt representations from the trained convnet as features, which are extracted from every frame of a real piano recording, to train a dedicated classiﬁer adapted to the actual acoustics of the piano and the performance venue used in the recording. This transfer learning approach is expected to better identify frames played with the sustain pedal. For the dedicated classiﬁer in the target task, we opt for SVM instead of multi-layer perceptron because SVM can greatly reduce the training time and yield better generalisation in classiﬁcation tasks [22]. In Section V-B, compared with ﬁne-tuning the last layer of the pre-trained convnet, transfer learning with SVM trained using the activations of multiple layers also achieves better performance."
2103.13219,data,165,,,"Given our large training data consisting of excerpts arranged in pedal/no-pedal pairs, binary classiﬁcation was chosen as a source task. This enabled the convnet to focus on variations in the nuances on sound played with/without the sustain pedal, while invariant to other musical elements such as pitch and loudness. Considering that the use of the sustain pedal can have effects on every piano string, this could lead to changes that affect the entire spectrum, i.e., take place at a global level. Therefore representations that reveal ﬁner details, such as short-time Fourier transform (STFT), may become inefﬁcient for training. The melspectrogram is a 2D representation that approximates human auditory perception through aggregating STFT bins along the frequency axis. This computationally efﬁcient input has been shown to be successful in MIR tasks such as music tagging [23]. For the above reasons, we consider melspectrogram an adequate input representation."
2103.13219,data,166,,,"Learning to use the piano pedals strongly relies on listening to nuances in the sound. Instructions with respect to when the pedal should be pressed and for what duration are required to develop critical listening. To facilitate the learning process, we pose a research question: “Can a computer point out pedalling techniques when a piano recording from a virtuoso performance is given?” Pedalling techniques change very speciﬁc acoustic features, which can be observed from their spectral and temporal characteristics on isolated notes. However, their effects are typically obscured by the variations in pitch, dynamics and other elements in polyphonic music. Therefore, automatic detection of pedalling techniques using hand-crafted features is a challenging problem. Given enough labelled data, deep learning models have shown the ability of learning hierarchical features. If these features are able to represent acoustic characteristics corresponding to pedalling techniques, the model can serve as a detector."
2103.13219,data,215,,,"not practical. We approach the sustain-pedal detection from the audio domain using transfer learning [5] as illustrated in Figure 2. Transfer learning exploits the knowledge gained during training on a source task and applies this to a target task [6]. This is crucial for our case, where the target-task data is obtained from recordings of a different piano, therefore it is difﬁcult to learn a “good” representation due to mechanical and acoustical deviations. In our source task, a convolutional neural network (denoted by convnet hereafter) is trained for distinguishing synthesised music excerpts with or without the sustain-pedal effect. The convnet is then used as a feature extractor, aiming to transfer the sustain-pedal effect learned from the source task to the target task. Support vector machines (SVMs) [7] are trained using the framewise convnet features from the acoustic piano recordings to ﬁnalise the feature representation transfer as the target task. SVMs can be used as a classiﬁer to localise which frames are played with the sustain pedal. The performance is expected to improve signiﬁcantly with the new feature representation. To sum up, the main contributions of this paper are:"
2103.13219,data,39,,,"Fig. 1. Different representations of the same note played without (ﬁrst note) or with (second note) the sustain pedal, including music score, melspectrogram and messages from MIDI or sensor data."
2103.13219,data,41,,,"[6] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345– 1359, 2010."
2103.13219,data,54,,,"only for detecting the pedalled frames, but also for learning the transients introduced by the sustain-pedal onset or even the offsets. More audio data including pieces by other composers and using various recording conditions should be tested to verify the robustness of our approach. This also constitutes our future works."
2103.13219,data,65,,,"[3] W. Goebl, S. Dixon, G. De Poli, A. Friberg, R. Bresin, and G. Widmer, “Sense in expressive music performance: Data acquisition, computational studies, and models,” Sound to Sense - Sense to Sound: A State of the Art in Sound and Music Computing, pp. 195–242, 2008."
2103.13219,"data, dataset",108,,,"These music excerpts were derived from pieces by 84 different composers from Baroque to the Modern period. Their durations distribute between 0.3 and 2.3 seconds. To prepare ﬁxed-length data for training, excerpts that are shorter or longer than 2 seconds were repeated or trimmed to create a 2-second excerpt. Considering the large size of our dataset, we randomly took a thousand samples from the excerpts of each composer. In total, 62424 excerpts form a smaller dataset3. This also helps to compare convnet of different architectures in a more efﬁcient way, since the training time can be signiﬁcantly reduced."
2103.13219,"data, dataset",135,,,"For the target task, the dataset consists of ten well known passages of Chopin’s piano music. A pianist was asked to perform the passages using a Yamaha baby grand piano situated in the MAT studios at Queen Mary University of London. The audio were recorded at 44.1 kHz and 24 bits using the spaced-pair stereo microphone technique with a pair of Earthworks QTC40 omnidirectional condenser microphones positioned about 50 cm above the strings. The positions were kept constant during the recording. Meanwhile, movement of the sustain pedal was recorded along with the audio with the help of the measurement system proposed in [4]. The audio data were annotated with frame-wise on or off labels as the ground truth, representing whether the sustain pedal"
2103.13219,"data, dataset",262,,,"Abstract—Detecting piano pedalling techniques in polyphonic music remains a challenging task in music information retrieval. While other piano-related tasks, such as pitch estimation and onset detection, have seen improvement through applying deep learning methods, little work has been done to develop deep learning models to detect playing techniques. In this paper, we propose a transfer learning approach for the detection of sustainpedal techniques, which are commonly used by pianists to enrich the sound. In the source task, a convolutional neural network (CNN) is trained for learning spectral and temporal contexts when the sustain pedal is pressed using a large dataset generated by a physical modelling virtual instrument. The CNN is designed and experimented through exploiting the knowledge of piano acoustics and physics. This can achieve an accuracy score of 0.98 in the validation results. In the target task, the knowledge learned from the synthesised data can be transferred to detect the sustain pedal in acoustic piano recordings. A concatenated feature vector using the activations of the trained convolutional layers is extracted from the recordings and classiﬁed into frame-wise pedal press or release. We demonstrate the effectiveness of our method in acoustic piano recordings of Chopin’s music. From the crossvalidation results, the proposed transfer learning method achieves an average F-measure of 0.89 and an overall performance of 0.84 obtained using the micro-averaged F-measure. These results outperform applying the pre-trained CNN model directly or the model with a ﬁne-tuned last layer."
2103.13219,"data, dataset, publicly available",279,,,"input.midpedal.wavno-pedal.wavexcerpts in pairsconvnetaudio recording.wavresultsconvnet featuresSVM classiﬁertransferlearningtrainsource tasktarget taskIII. DATASET For the source task, pedal and no-pedal versions of music excerpts are required to train a convnet, which is able to highlight the spectral or temporal characteristics that change with the sustain pedal instead of note events. For this reason, 1392 MIDI ﬁles publicly available from the Minnesota International Piano-e-Competition website1 were downloaded. They were recorded using a Yamaha Disklavier piano from the performance of skilled competitors. To render these MIDI ﬁles into high quality audio, the Pianoteq 6 PRO2 software was used. This physically modelled virtual instrument approved by Steinway & Sons can export audio using models of different instruments and recording conditions. We employed the Steinway Model D grand piano instrument and the closemiking recording mode. Audio with or without sustain-pedal effect was then generated with a sampling rate of 44.1 kHz and a resolution of 24 bits. These were rendered while preserving or removing the sustain-pedal message in the MIDI data. For each pedal-version audio, we can obtain the temporal regions when the sustain pedal is on or off by thresholding the MIDI message at 64 given its range of [0,127]. A pedalled segment is determined to start at a pedal onset (where the pedal state changes from off to on) and ﬁnish when the state returns to off. We can clip all the pedalled segments to form the pedal excerpts. The start and end times of the pedalled segments were also used to obtain no-pedal excerpts from the corresponding no-pedal-version of the audio."
2103.13219,database,64,,,The rest of this paper is organised as follows. We ﬁrst introduce related works in Section II. The process of database construction is described in Section III. The methods of sustain-pedal detection including convnet design and transfer learning are discussed in Section IV. Experiments and results are presented in Section V. We ﬁnally conclude our work in Section VI.
2103.13219,dataset,43,,,"2) A transfer learning method that allows the convnet trained from the source task to be adapted to the target task, where the recording instruments and room acoustics are different. This also allows effective learning with a smaller dataset."
2103.13219,"dataset, used dataset",321,,,"In this paper, we focus on detecting the technique of the sustain pedal, which is the most frequently used one among the three standard piano pedals. All dampers are lifted off the strings when the sustain pedal is pressed. This mechanism helps to sustain the current sounding notes and allows strings associated to other notes to vibrate due to coupling via the bridge. A phenomenon known as sympathetic resonance [1] is thereby enhanced and embraced by pianists to create a “dreamy” sound effect. We can observe how the phenomenon reﬂects on the melspectrogram in Figure 1, where note F4 is played without (ﬁrst) and with (second) the sustain pedal in two bars respectively. Note that the symbol under the second bar of the music score in Figure 1 can be used to indicate the sustain-pedal techniques. Yet, even if pedal notations are provided, pedalling in the same piano passage can be executed in many different ways. Playing techniques are typically adjusted to the performer’s sense of tempo, dynamics, as well as the location where the performance takes place [2]. Given that detecting pedalling nuances from the audio signal alone is a rather challenging task [3], several measurement systems have been developed to capture the pedal movement. For instance, the Yamaha Disklavier piano can encode this movement into MIDI messages (0-127) along with note events. A dedicated system proposed in [4] enables synchronously recording the pedalling gestures and the piano sound. This can be deployed on common acoustic pianos, and it is used to provide the ground truth dataset introduced in Section III. Detection of pedalling techniques from audio recordings is necessary in the cases where installing sensors on the piano is"
2103.13219,python,46,,,"[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel et al., “Scikit-learn: Machine learning in python,” Journal of machine learning research, vol. 12, no. Oct, pp. 2825–2830, 2011."
2104.00096,data,103,,,"Currently, more research is needed in the sensors and infrastructure used to sense these declines to improve the data quality. We do not want to use inaccurate data that could deeply impact the care and financial resources of an older adult. These technological advances can help older adults live safer, independent lives; however, we know less about how this type of information should be delivered. We need interdisciplinary approaches to investigate how findings on one’s decline can be verified and provided to older adults and their caregivers in an understandable and actionable format."
2104.00096,data,157,,,"We envision artificial intelligence (AI) systems that augment resources instead of replacing them - especially in under-resourced communities. Older adults rely on their caregiver networks and community organizations for social, emotional, and physical support; thus, AI should be used to coordinate resources better and lower the burden of connecting with these resources. Although sociotechnical smart systems can help identify needs of older adults, the lack of affordable research infrastructure and translation of findings into consumer technology perpetuates inequities in designing for diverse older adults. In addition, there is a disconnect between the creation of smart sensing systems and creating understandable, actionable data for older adults and caregivers to utilize. We ultimately advocate for a well-coordinated research effort across the United States that connects older adults, caregivers, community organizations, and researchers together to catalyze innovative and practical research for all stakeholders."
2104.00096,data,95,,,"Machine Learning and AI to Support Caregiving  Computing specialists have utilized sensors, computer networks, and artificial systems to assist older adults safely age in place. Essentially, these are tracking systems that can do everything from tell where an older adult is and alert someone if they wander out of a specific geographical location to send alerts to help older adults take their medication on schedule. When a system has enough data, it can also detect declines in older adults to assist stakeholders with assessing support and infrastructure needs."
2104.00096,data,99,,,"Outside of the design and evaluation process, older adults must be included in training data sets that inform intelligent system models. Smart home appliances with embedded AI are often advertised as ideal technology to help older adults age in place. Older adults can benefit from the social and companionship features some of these systems provide, however recent studies have shown major usability issues in smart voice assistants that do not wait for older adults to complete their command before the device starts the interaction - thus, the older adult becomes frustrated and the system"
2104.00622,data,1,,,Data
2104.00622,data,129,,,"For each image, we provide the following groundtruth data: depth map, instance segmentation, transparent object segmentation, intrinsic and extrinsic camera parameters, 2d/3d bounding box for each object, 6D pose for each object. Since the depth map created from ray-tracing is not accurate for transparent objects, we utilize a two-pass rendering strategy to solve it. Before the rendering, we randomly select some objects and list them as transparent candidates. During the ﬁrst pass, materials of all objects are set to opaque and we render all groundtruth data including depth map using real time ray-tracing. During the second pass, we set materials of transparent candidates to glass and render the RGB image using path tracing."
2104.00622,data,16,,,Table 5. Quantitative effect of training data on the generalization to real novel objects.
2104.00622,data,252,,,"synthetic data but achieves better performance. Implicit function learning. Our method takes inspiration from the implicit neural models [10, 35, 39, 18, 45, 52, 12], which are not restricted by topology and can represent the 3D surface continuously. Most of these methods [10, 35, 39] can not scale to scene level as they only use a single latent vector to encode the shape. Voxel-based implicit representations [6, 22, 40] address this limitation by learning local shape priors. However, they require coarse voxel grids or sparse point clouds as input, which already provide some information about 3D shapes. Our task is more challenging as the point cloud of transparent objects is totally missing. [36] propose to represent scenes as neural radiance ﬁelds, achieving impressive results on novel view synthesis. However, their method needs to be retrained for every new scene and takes a long time to render one image. Schwarz et al. [47] solves the ﬁrst drawback by learning a generative radiance ﬁeld, but their results are limited to single object or human face. Liu et al. [29] tackles the second problem by introducing sparse voxel grids, but they still need to sample points based on heuristic strategies while our method learns to estimate the possible terminating position for ray-voxel pairs."
2104.00622,data,270,,,"We set image resolution to 320 × 240 in all experiments. To simulate the noise pattern of real depth scans on our synthetic training data, we remove all depth values for transparent objects, part of the depth values for opaque objects, and create some random holes in the depth map for background. We also augment the color input by adding pixel noise, motion blur and random noise in HSV space. For training, our method only predicts depth for corrupted pixels mentioned above. For testing, depth values of all pixels are estimated so that our method does not rely on the segmentation of transparent objects. Our two stage networks are trained separately. The ﬁrst stage networks are trained for 60 epochs using Adam optimizer [25] with a ﬁxed learning rate of 0.001. After that, the ﬁrst stage networks are frozen and we train the reﬁnement networks for another 60 epochs using Adam optimizer. The ﬁrst 30 epochs are trained with learning rate 0.001 using the proposed loss function. The later 30 epochs are trained with learning rate 0.0001 using hard negative mining, where only top 10 percent of pixels with largest errors are considered. We set ωpos = 100 and ωsn = 10 for the ﬁrst stage and ﬁrst 30 epochs of the second stage. For the later 30 epochs of second stage, we set ωpos = 20 and ωsn = 2. ωprob is set to 0.5 for the ﬁrst stage and 0 for the depth reﬁnement model."
2104.00622,data,44,,,Figure 14. Qualitative results for training data. Point clouds are colored by surface normal and rendered in a novel viewpoint to better visualize the 3D shape. The red boxes highlights the interest area. Please zoom in to see details.
2104.00622,data,49,,,"[53] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth completion from sparse lidar In IEEE International data with depth-normal constraints. Conference on Computer Vision (ICCV), pages 2811–2820, 2019. 2"
2104.00622,data,64,,,"[43] Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3313–3322, 2019. 2"
2104.00622,data,69,,,There are several interesting directions for future works. We can extend our pipeline by treating each pixel’s projection as a cone to account for the lateral noise. Generating training data with a realistic depth noise model [37] helps to improve the robustness of our method. We also plan to investigate depth completion of transparent objects in cluttered scenes with heavy occlusion.
2104.00622,data,72,,,"Figure 6. Qualitative comparison to state-of-the-art methods. For all the baselines, we provide visualizations of the version retrained on our data for fair comparison. The point clouds are colored by surface normal and rendered in a novel viewpoint to better visualize the 3D shape. Red boxes in the groundtruth highlight areas where our method performs much better than all baselines. Zoom in to see details."
2104.00622,data,79,,,"Depth data captured from RGB-D cameras has been widely used in many applications such as augmented reality and robot manipulation. Despite their popularity, commodity-level depth sensors, such as structured-light cameras and time-of-ﬂight cameras, fail to produce correct depth for transparent objects due to the lack of light reﬂection. As a result, many algorithms utilizing RGB-D data cannot be directly applied to recognize transparent objects which are very common in household scenarios."
2104.00622,data,9,,,Table 11. Quantitative effect of training data.
2104.00622,data,98,,,"[1] CC0 TEXTURES. https://cc0textures.com/. 12 [2] NVIDIA Omniverse Platform. https://developer. nvidia.com/nvidia-omniverse-platform. 2, 5 [3] Sven Albrecht and Stephen Marsland. Seeing the unseen: Simple reconstruction of transparent objects from point cloud data. In Robotics: Science and Systems, 2013. 2 [4] Jonathan T Barron and Jitendra Malik. Intrinsic scene propIn IEEE Conference on erties from a single rgb-d image. Computer Vision and Pattern Recognition (CVPR), pages 17–24, 2013. 2"
2104.00622,"data, dataset",106,,,"and qualitative comparison on different training data respectively. We can see that training the model on both datasets can get best results. Ray Pooling. Table 12 shows that argmax performs consistently better than weighted sum on all types of testing data. Figure 15 also shows that argmax can better estimate missing depth of transparent objects on real images. Candidate points selection. Table 13 shows that directly learning offsets of candidate points is better than sampling points heuristically. Figure 16 further provides some examples on real images, showing that learning offset is more robust to strong background textures."
2104.00622,"data, dataset",226,,,"We compare our approach to several state-of-the-art methods in Table 1. For fair comparison, we evaluate all related works using their released checkpoints (denoted by method name) as well as retraining on our data (denoted by method name with subscript ours. All baselines are trained on both datasets together which is the same setting as our proposed method). RGBD-FCNours is a strong baseline proposed by ourselves. It directly regresses depth maps using fully convolutional networks from RGB-D images. We use Resnet34-8s [51] as the network architecture and train the network on our data. NLSPN [38] is the state-ofthe-art method for depth completion on NYUV2 [48] and KITTI [49] dataset. Cleargrasp [46] is the state-of-the-art method for depth completion of transparent objects. For our approach, we use the best model: LIDF plus the depth reﬁnement model. Our method achieves the best result on all datasets even when baseline methods are trained on the same data. It also shows that training on Omniverse Object dataset can boost the performance of baseline methods. In Figure 6, we provide qualitative comparison by rendering point cloud in a novel view. Our approach can generate more meaningful depth than baseline methods."
2104.00622,"data, dataset",344,,,"Function (LIDF) deﬁned on ray-voxel pairs consisting of camera rays and their intersecting voxels. The motivations for LIDF are: 1) The depth of a transparent object can be inferred from its color and the depth of its non-transparent In particular, color can provide useful vineighborhood. sual cues for the 3D shape and curvature while local depth helps to reason about the spatial arrangement and location of transparent objects. 2) Directly regressing the complete depth map using a deep network can easily overﬁt to the objects and scenes in the training data. By learning at the local scale (a voxel in our case) instead of the whole scene, LIDF can generalize to unseen objects because different objects may share similar local structures. 3) Voxel grids provide a natural partition of the 3D space. By deﬁning implicit function on ray-voxel pairs, we can signiﬁcantly reduce the inference time as the model only needs to consider occupied voxels intersected by the camera ray. Based on these motivations, we present a model to estimate the depth of a pixel by learning the relationship between the camera ray and its intersecting voxels given the color and local depth information. To further utilize the geometry of transparent object itself, we propose a depth reﬁnement model to update the prediction iteratively by combining the input RGB, input depth points and the predicted depth from LIDF. To train the whole pipeline, we create a large scale synthetic dataset, Omiverse Object dataset, using the NVIDIA Omniverse platform [2]. Our dataset provides over 60,000 images including both transparent and opaque objects in different scenes. The dataset is generated with diverse object models and poses, lighting conditions, camera viewpoints and background textures to close the sim-to-real gap. Experiments show that training on the Omniverse Object dataset can boost the performance for both our approach and competing methods in real-world testing cases."
2104.00622,"data, dataset",52,,,"Training Data. We analyze the effects of training data in Table 5. We ﬁnd training our method purely on ClearGrasp or Omniverse leads to similar results, but training on both datasets can improve the performance a lot. This indicates that Omniverse dataset can be a good complementary to"
2104.00622,"data, dataset",80,,,"more details about Omniverse Object dataset. The evaluation is done on the ClearGrasp dataset [46]. It has 4 types of different testing data: Synthetic images of 5 training objects (Syn-known); Synthetic images of 4 novel objects (Syn-novel); Real world images of 5 training objects (Realknown); Real world images of 5 novel objects (Real-novel), 3 of them are not present in synthetic data."
2104.00622,"data, dataset provided",10,,,Training Data. Table 11 and Figure 14 provide quantitative
2104.00622,"data, dataset, code, data https",193,,,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a selfcorrecting reﬁnement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs signiﬁcantly better than the current state-of-the-art In addimethods on both synthetic and real world data. tion, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp [46]. Code and dataset will be released at https: //research.nvidia.com/publication/202103_RGB-D-Local-Implicit."
2104.00622,"data, dataset, supplementary data",132,,,"Datasets Our full pipeline is trained on the ClearGrasp dataset [46] and a new dataset we generated using the Omniverse Platform [2], which we call Omniverse Object dataset. The dataset provides various supervisions for transparent and opaque objects in cluterred scenes. Figure 5 visualizes some examples of the dataset. 3D object models in Omniverse Object dataset are collected from ClearGrasp and ShapeNet [7]. To get natural poses of objects, we use NVIDIA PhysX engine to simulate objects falling to the ground. Then we randomly select some objects and set their materials to the glass. We also augment the data by changing textures for the ground and opaque objects, lighting conditions and camera views. See supplementary for"
2104.00622,"data, dataset, supplementary data",57,,,"In this section, we ﬁrst evaluate the effect of the depth reﬁnement model. After that, we compare several conﬁgurations for our ﬁrst stage networks. To focus on the generalization ability, we only report quantitative results on ClearGrasp Real-novel dataset. Please refer to the supplementary for results on other testing data."
2104.00622,dataset,10,,,Figure 5. Examples from our Omniverse Object Dataset.
2104.00622,dataset,108,,,"We have presented a novel framework for depth completion of transparent objects. Our method consists of a local implicit depth function deﬁned on ray-voxel pairs and an iterative depth reﬁnement model. We also introduce a large scale synthetic dataset for transparent objects learning, which can boost the performance for both our approach and other competing methods. Our pipeline is only trained on synthetic datasets but can generalize well to real world scenarios. We thoroughly evaluated our method compared to prior art and ablation baselines. Both quantitative and qualitative results demonstrate substantial improvements over the state-of-the-art in terms of accuracy and speed."
2104.00622,dataset,141,,,"Transparent objects. Transparent objects have been studied in various computer vision tasks, including object pose estimation [26, 32, 31, 41, 30], 3D shape reconstruction [3, 20, 42, 28, 46] and segmentation [23]. However, most of these works assume known background patterns [20, 42], known object 3D models [26, 32, 41], or multi view/stereo input [30, 28]. Our approach does not require any priors and can estimate the depth of transparent objects from a single view RGB-D image. Sajjan et al. [46] is the closest work to ours. However, they pretrain their networks on out-ofdomain real datasets while our method is trained purely on"
2104.00622,dataset,309,,,"Depth estimation. Depth estimation can be classiﬁed into three categories based on the input. Several methods have been proposed to directly regress the depth map from the color image using convolutional neural networks [14, 44, 27, 8, 16, 17, 19]. Most of them are trained on large scale datasets generated from RGB-D cameras, thus they can only reproduce the raw depth scan. Our method, on the contrary, focuses on the depth estimation for transparent objects where depth sensor typically fails. Another line of related work explores the task of depth completion given RGB images and sparse sets of depth measurements [33, 9, 43, 53, 11, 38]. These works improve the depth estimation over color-only methods, but they still produce low quality results because of limited information provided by sparse depth. Our method falls into the third category which tries to complete depth maps given noisy RGB-D images. Barron and Malik [4] propose a joint optimization for intrinsic images. Firman et al. [15] predict unobserved voxels from a single depth image using the voxlet representation. Matsuo and Aoki [34] reconstruct depths by ray-tracing to estimated local tangents. Recent works [56, 46] estimate surface normals and occlusion boundaries only from color images using deep networks and solve a global optimization based on those predictions as well as observed depths. The optimization is very slow and produces bad results if the network predictions are not accurate. We address these limitations by learning a implicit function using color and local depth jointly. Experiment shows that our method can achieve better results and 20× speedup compared to [46]."
2104.00622,dataset,47,,,"[44] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset IEEE Transactions on Pattern Analysis and Matransfer. chine Intelligence (TPAMI), 2020. 2"
2104.00622,dataset,6,,,D. Qualitative Results on NYUV2 Dataset
2104.00622,dataset,67,,,"Figure 18. Qualitative results on NYUV2 dataset. For every example, ﬁrst row from left to right: input RGB, input depth, predicted depth, groundtruth depth; second row from left to right: input point cloud, predicted point cloud, groundtruth point cloud. Point clouds are rendered in a novel viewpoint. Please zoom in to see details."
2104.00622,dataset,73,,,"We have done experiments on the NYUV2 dataset [48] to evaluate the performance of our method on general scenes and non-transparent objects. We corrupt the depth map by randomly creating some large holes. Our models are trained to predict the complete depth map given the corrupted depth map and RGB image. As shown in Figure 18, our method can predict reasonable missing depth in general scenes."
2104.00622,dataset,99,,,"Our contributions are summarized as follows: 1) We pro pose LIDF, a novel implicit representation deﬁned on rayvoxel pairs, leading to fast inference speed and good generality. 2) We present a two-stage system, including networks to learn LIDF and a self-correcting reﬁnement model, for depth completion of transparent objects. 3) We build a large scale synthetic dataset proved to be useful to transparent objects learning. 4) Our full pipeline is evaluated qualitatively and quantitatively, and outperform the current state-of-theart in terms of accuracy and speed."
2104.00622,"dataset, dataset provided",115,,,"In this section, we provide more details about our Omniverse Object Dataset. To generate the dataset, following categories from ShapeNet [7] are chosen: phone, bowl, camera, Following objects from ClearGrasp dataset [46] are chosen: cup-with-waves, ﬂower-bath-bomb, heart-bath-bomb, square-plastic-bottle, stemless-plastic-champagne-glass. Note that we only select training objects from ClearGrasp dataset to make sure testing objects are never seen during training. The background textures are randomly selected from the CC0 TEXTURES Dataset [1]. The textures for opaque objects are randomly selected from CC0 TEXTURES Dataset [1] and Describable Textures Dataset [13]."
2104.00622,"dataset, dataset provided",15,,,Real-Known dataset. We also provide qualitative comparison of ablation studies on real images.
2104.00622,"dataset, dataset provided",4,,,B. Omniverse Object Dataset
2104.00863,code,116,,,"CrypTFlow [11] is a system that converts TensorFlow (TF) code automatically into secure multi-party computation protocol. The system has three parts: a compiler, from TF code into two and three-party secure computations, an optimized three-party computation protocol for secure interference, and a hardwarebased solution for computation integrity. The most salient characteristic of CrypTFlow is the ability to automatically translate the code into MPC protocol, where the speciﬁc protocol can be easily changed and added. The optimized three-party computational protocol is speciﬁcally targeted for NN computation and speeds up the computation. This approach is similar to the holistic approach of [1]."
2104.00863,data,106,,,"DNNCoin (Deep Neural Network Coin), or SMCoin (Similitude Model Coin) can be structured in a distributed fashion similar Cryptocoins, where secret shares of the DNN coeﬃcients are distributed among servers, possibly with a common Merkle tree root residing on Blockchain and proof for the share belonging to the Merkle tree. Such that a request for inference using a particular DNNCoin/SMCoin is accompanied with (cryptocurrency) payment to the owner of the Big Data used to create the DNNCoin/SMCoin, while limitig the number of queries to avoid revealing the (possibly, polynomial representing) DNNCoin/SMCoin."
2104.00863,data,14,,,"Crypto-nets: Neural networks over encrypted data. ArXiv, abs/1412.6181, 2014."
2104.00863,data,164,,,"The rest of the paper is structured as follows; Previous relevant research is covered in Section 2. Section 3 discusses the most common activation functions and how they are approximated with polynomials. Section 4 discusses a way to approximate DNN with a single polynomial on a single computing node. The premise of the section is to establish a basis for a secure, communication-less multi-party computation, which is presented in Section 5. Section 6 summarizes the techniques for blindly computing a polynomial (some of its coeﬃcients being secret shares of zero) to obtain blind execution of DNN. Section 7 describes the way our polynomial neural network representation facilitate an eﬃcient execution of the inference by an untrusted third party, without revealing the (machine learning big) data the queries, and the results. Empirical experiments are described in Section 8 and, lastly, the paper is concluded in Section 9."
2104.00863,data,168,,,"Delegation of machine learning to a third party (e.g., cloud provider) without revealing anything about the (big) data (collected), the inputs/queries, and the outputs is an important goal we address here, by using FHE and the nested polynomial. In certain cases, for example when the neuron computes the max function, the nested polynomial can integrate actual FHE computation of the max over the inputs arriving from the previous layer, rather than a polynomial over these inputs. A neuron is computed as polynomial over input polynomials (values), and two (or more) results can be computed for each neuron: one a polynomial over the inputs to the neuron and one an FHE max value over the input. Then use an encrypted bit(s) to blindly choose among the results, i.e. between polynomial or “direct” FHE calculation of the neuron activation function."
2104.00863,data,178,,,"6. P. Derbeko, S. Dolev, and E. Gudes. Deep neural networks as similitude models for sharing big data. In 2019 IEEE International Conference on Big Data (Big Data), Los Angeles, CA, USA, December 9-12, 2019, pages 5728–5736. IEEE, 2019. 7. P. Derbeko, S. Dolev, and E. Gudes. Mldstore - dnns as similitude models for sharing big data (brief announcement). In S. Dolev, D. Hendler, S. Lodha, and M. Yung, editors, Cyber Security Cryptography and Machine Learning - Third International Symposium, CSCML 2019, Beer-Sheva, Israel, June 27-28, 2019, Proceedings, volume 11527 of Lecture Notes in Computer Science, pages 93–96. Springer, 2019. 8. C. Gentry. Fully Homomorphic Encryption Using Ideal Lattices. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing, pages 169–178, New York, NY, USA, 2009. ACM."
2104.00863,data,191,,,"In this paper, we consider a situation where the data owner collects the data, trains the model, and shares the model to be used by clients. The model is very valuable for the data owner, as the training process is resource-intensive and frequently performed over private and valuable data. Therefore, the goal of the data owner is to retain control of the model as much as possible after it was shared. The data owner will likely be willing to delegate the query service to (Machine Learning Data model Store MLDSore [7]) clouds, in a way that the cloud providers do for computing platforms. In such a scenario, the cloud providers should not be able to simply copy the model and reuse it. In addition, the data owner should have the ability to limit the number of queries executed on the model, such that a single, or a small team of colluding cloud providers (or servers) cannot execute an unlimited number of queries on the model. In fact,"
2104.00863,data,40,,,"The idea is to create a polynomial for the “ﬂow” of the data in the network instead of approximating every single neural unit with a polynomial. As an example, consider the network in Figure 4."
2104.00863,data,42,,,"9. R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and J. R. Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In ICML, 2016."
2104.00863,data,43,,,"Fig. 3. A high-level architecture of the auto-encoder neural network. Encoder transforms data from the original dimension to a much smaller encoding, while the decoder performs the opposite operation of restoring the original data from the encoded representation."
2104.00863,data,47,,,"The computation costs are increasing linearly with the polynomial degree (data not shown), where the original ReLu is similar to d = 1 degree polynomial. Thus, it makes sense to choose the lowest degree that still provides consistent and accurate results."
2104.00863,data,58,,,"CryptoDL [10] showed an implementation of Convolutional Neural Networks (CNN) over encrypted data using homomorphic encryption (HE). As fully homomorphic encryption is limited to addition and multiplication operations, the paper has shown approximation of CNN activation functions by low-degree polynomials due to the high-performance overhead of higher degree polynomials."
2104.00863,data,62,,,"Approximation of neural network units’ activation function with ﬁxed-point arithmetic was considered before in [9,10], where polynomial functions were suggested for approximation. In both works, the network was not distributed but rather was translated into ﬁxed-point calculations to run over encrypted data. However, the methods of approximation are similar to the MPC case."
2104.00863,data,65,,,"Taken together, those two results enable a somewhat surprising outcome: the data owner can train DNN models, pre-process, and share them with multiple cloud providers. The providers then can collaboratively calculate interference of the network on common or secret-shared inputs without ever communicating with each other. Thus, reducing the attack surface even further even for multilayer networks."
2104.00863,data,70,,,"Convolution Layer. The convolution layer is used in Convolutional Neural Networks (CNN), mainly for image recognition and classiﬁcation. Usually, this layer performs dot product of a (commonly) n × n square of data points (pixels). The idea is to calculate the local features. The layer performs multiplication and addition, which are directly translated into a polynomial."
2104.00863,data,75,,,"The above optimization of DNN evaluation targets the inference phase, which is done after the DNN-based model is shared and distributed across cloud providers. The network is not trained anymore, but only queried by the clients. At this phase, the performance issues do not impact the data owners, which could be resource-limited end-devices, but rather are relevant for the cloud providers that have as much larger resources."
2104.00863,data,75,,,"The goal of MPC calculations in the considered setup is to protect the published model from exposure to participating cloud providers. The model is trained by the data provider and has two components: architecture, which includes the layout, type, and interconnection of the neural units, as well as the weights of the input, which were reﬁned during the training of the network, i.e. backpropagation phase."
2104.00863,data,76,,,"of the computations is important for the secure sharing of DNN-based models. We think that this optimization method can enable more eﬃcient DNN calculations and further progress in the process of privacy-preserving data sharing. In particular, extra security and privacy factor is established, when no communication among the MPC participants is required, namely a participant cannot easily collude with other participants, as their identities are revealed to it."
2104.00863,data,89,,,"we show that a (D)NN can be represented by a (nested) polynomial, therefore enough queries (points on the polynomial) can reveal the neural network, and the ownership of the information (succinct model) is at risk. In practice, as data is constantly updated new data emerges and old data becomes obsolete, thus, frequent updates of the neural network are frequent enough to deﬁne a new polynomial for which the past queries are not relevant."
2104.00863,data,9,04/21/22,0,"encrypted data. CoRR, abs/1711.05189, 2017."
2104.00863,data,97,,,"Long Short-Term Memory (LSTM). LSTM is a subset of Recurrent Neural Network (RNN) architecture, whose goal is to learn sequences of data. LSTM networks are used for speech recognition, video processing, time sequences, etc. There are many diﬀerent variations of LSTM units with a usual structure, including several gates or functions, which enable the unit to remember values over several cell activations. A common activation function of LSTM units is the logistic sigmoid function, which we already considered in Section 3."
2104.00863,data,98,,,"Deep Neural Networks (DNN) are the state-of-the-art form of Machine Learning techniques these days. They are used for speech recognition, image recognition, computer vision, natural language processing, machine translation, and many other tasks. Similar to other Machine Learning (ML) methods, DNN is based on ﬁnding patterns in the data and, as such, the method embeds information about the data into a concise and generalized model. Subsequently, the sharing of the DNN model also reveals private and valuable information about the data."
2104.00863,database,42,,,"All tests were performed on the Fashion database of MNIST, which contains a training set of 60,000 and a testing set of 10,000 28x28 images of 10 fashion categories. The task is a multi-class classiﬁcation of a given image."
2104.00863,dataset,133,,,"Abstract. The structure and weights of Deep Neural Networks (DNN) typically encode and contain very valuable information about the dataset that was used to train the network. One way to protect this information when DNN is published is to perform an interference of the network using secure multi-party computations (MPC). In this paper, we suggest a translation of deep neural networks to polynomials, which are easier to calculate eﬃciently with MPC techniques. We show a way to translate complete networks into a single polynomial and how to calculate the polynomial with an eﬃcient and information-secure MPC algorithm. The calculation is done without intermediate communication between the participating parties, which is beneﬁcial in several cases, as explained in the paper."
2104.11641,data,102,,,"a) Number of Augmentations: For the data augmentation, we analyze the effect of the number of augmentations on the performance of AugInf. We successively apply one to eight augmentations while leaving the other parameters constant. The results of this are shown in Figure 2. When we apply four augmentations, the highest performance is achieved. After that, as the number of augmentations increase, the AUC score stabilises. Interestingly, the F1 score does not appear to be affected by the number of augmented graphs and remains stable between 0.73 and 0.74."
2104.11641,data,128,,,"[5] E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoeﬂer, and D. Soudry, “Augment your batch: better training with larger batches,” arXiv preprint arXiv:1901.09335, 2019. [6] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and Q. V. Le, “Unsupervised data augmentation for consistency training,” arXiv preprint arXiv:1904.12848, 2019. [7] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca, “Data augmentation using learned transformations for one-shot medical image segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2019, pp. 8543–8553."
2104.11641,data,147,,,"Before the joint training, we ﬁrst obtain augmentations of our data by adapting a similar idea to [4]. The approach for augmentation consists of two steps: (1) using the variational graph auto-encoder (VGAE) [9] to obtain edge probabilities for all possible and existing edges in graph G, (2) using the predicted edge probabilities, with a threshold set to stochastically add new edges, creating a modiﬁed graph Gm, which is used as input to the joint training process. The key idea of this augmentation approach is to use information inherent in the graph to predict which non-existent edges are likely to be added to the augmented graph to improve generalization. As a VGAE is a generative model, we utilize VGAE as part of our graph augmentation process."
2104.11641,data,199,,,"Fig. 1. The AugInf method. AugInf will ﬁrst obtain the predicted edge probability matrix M by using the VGAE [9]. A threshold hyperparameter will control the amount of edges added during the augmentation. We extend the work of [4] to perform multiple augmentations, randomly sampling a subset of edges to add to each original subgraph. (a) For train-time augmentation, the method will then integrate all augmentations together for the joint training process. The joint training model has two steps: the GAE step will learn the latent representations of the input data for the prediction stage; the prediction module (GAT or GCN) will produce the social inﬂuence predictions. The losses of these two stages (decoder loss and prediction loss) will be combined and propagated backwards to jointly update the model. (b) For test-time augmentation, AugInf will generate several augmentations of each test (validation) example, learning a representation for each before producing the inﬂuence prediction. AugInf will calculate the average of the predictions to produce the ﬁnal social inﬂuence prediction."
2104.11641,data,264,,,"and natural language processing (NLP) [8], the combination of data augmentation methods and deep neural networks have been shown to be effective. By performing data augmentation, model performance can be improved as it facilitates the neural network to learn generalizable features related to the task. While GNNs have become a popular research ﬁeld, little research has focused on using data augmentation for GNNs, especially in terms of using augmentation at both train- and test-time. Motivated by the success of data augmentation in CV and NLP, we study whether data augmentation at not only train-time, but also test-time, can improve the performance of GNNs, particularly on the task of social inﬂuence prediction. Extending the work of DeepInf [2], we have developed a method, AugInf, for social inﬂuence prediction with both trainand test-time augmentation for GNNs. In this method, the augmented graphs are ﬁrst generated by utilising a variational graph autoencoder (VGAE) [9] and then social inﬂuence is predicted by joint training of both a Graph Auto-Encoder (GAE) [9] and a GNN prediction module based on either a Graph Convolutional Network (GCN) [10] or a Graph Attention Network (GAT) [11]. We have compared the performance of AugInf with several state-of-the-art GNN approaches by experimenting on numerous social networks. Our experimental results show that AugInf can improve prediction performance on several of these social networks."
2104.11641,data,33,,,"[12] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,” IEEE Transactions on Knowledge and Data Engineering, 2020."
2104.11641,data,37,,,"[4] T. Zhao, Y. Liu, L. Neves, O. Woodford, M. Jiang, and N. Shah, “Data augmentation for graph neural networks,” arXiv preprint arXiv:2006.06830, 2020."
2104.11641,data,4,,,A. Data Augmentation
2104.11641,data,40,,,"[3] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation for deep learning,” Journal of Big Data, vol. 6, no. 1, p. 60, 2019."
2104.11641,data,43,,,"#1#2#3#4#5#6#7#8With GAT0.870.880.890.900.910.920.93AUCAUC0.680.700.720.740.76F1F1#1#2#3#4#5#6#7#8With GCN0.820.830.840.850.860.870.88AUCAUC0.620.630.640.650.660.670.680.690.70F1F1Data Mining, 2020, pp. 207–217."
2104.11641,data,44,,,"[21] W. Chen, Y. Yuan, and L. Zhang, “Scalable inﬂuence maximization in social networks under the linear threshold model,” in 2010 IEEE international conference on data mining. IEEE, 2010, pp. 88–97."
2104.11641,data,46,,,"[19] Y. Wang, W. Wang, Y. Liang, Y. Cai, J. Liu, and B. Hooi, “Nodeaug: Semi-supervised node classiﬁcation with data augmentation,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &"
2104.11641,data,47,,,"[22] J. Weng, E.-P. Lim, J. Jiang, and Q. He, “Twitterrank: ﬁnding topic-sensitive inﬂuential twitterers,” in Proceedings of the third ACM international conference on Web search and data mining, 2010, pp. 261–270."
2104.11641,data,55,,,"[2] J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, “Deepinf: Social inﬂuence prediction with deep learning,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, pp. 2110–2119."
2104.11641,data,68,,,"[28] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [29] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701– 710."
2104.11641,data,88,,,"A Graph Convolutional Network (GCN) [10] is a semisupervised learning algorithm for graph data, typically used for node and graph classiﬁcation, as well as link prediction. A GCN model is typically formed by stacking multiple GCN layers, and for each GCN layer, the inputs are the adjacency matrix A and the features matrix, H ∈ Rn×F , where n is the number of vertices, and F is the number of features. For each GCN layer:"
2104.11641,data,95,,,"[8] K. M. Yoo, Y. Shin, and S.-g. Lee, “Data augmentation for spoken language understanding via joint variational generation,” in Proceedings of the AAAI conference on artiﬁcial intelligence, vol. 33, 2019, pp. 7402–7409. [9] T. N. Kipf and M. Welling, “Variational graph autoencoders,” arXiv preprint arXiv:1611.07308, 2016. [10] ——, “Semi-supervised classiﬁcation with graph convolutional networks,” in International Conference on Learning Representations (ICLR), 2017."
2104.11641,"data, data available",123,,,"a) Graph Neural Networks: Graph Neural Networks (GNNs) [12, 13] have rapidly grown to become a popular research area, providing a highly competitive approach for tasks involving graph data. One line of research focuses on unsupervised models, e.g. VGAE [9] and Graphite [14]. These unsupervised variational models typically aim to use generative modelling of graphs for graph reconstruction, link prediction and clustering. Additionally, supervised models have attracted signiﬁcant attention, such as SCNN [15], ChebyNet [16], GAT [11] and GCN [10], which are widely used in tasks where labelled data is available."
2104.11641,"data, data available",168,,,"Graph neural networks (GNNs) [1] have been shown to be effective in various graph machine learning tasks, such as link prediction and node classiﬁcation. The rapid growth of online social networks has led to the development of numerous methods for studying social behaviour online. However, many learning tasks on social networks have relied heavily on manual feature extraction. GNNs have provided an alternative to this with their ability to automatically learn representations end-to-end. One such task of interest, which has been shown to be enhanced using GNNs, is social inﬂuence prediction [2]. Data augmentation [3], which increases the amount of data available by creating informative variations of existing data, can improve the performance of machine learning models and has been widely used in many machine learning tasks [4, 5, 6]. In the ﬁelds of computer vision (CV) [7, 3]"
2104.11641,"data, dataset",110,,,"• The Twitter dataset has been built by collecting Twitter data corresponding to tweets collected before, during and after the announcement of the discovery of the Higgs boson in 2012. The graph is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as whether a user retweets Higgs boson tweets. • The Weibo graph was built from 100 randomly selected users and their followers and followees. The social network is deﬁned as a friendship network, and the social action, which we are predicting, is deﬁned as retweeting behaviors in the Weibo social network."
2104.11641,"data, dataset",118,,,"Referring to the statistics of the datasets in Table I, we can see that the performance improvement of AugInf-GAT is particularly clear on the datasets with much fewer edges (Digg) but limited on datasets with more edges (Twitter and this is because the Twitter and Weibo). We believe that Weibo datasets contain enough edges to learn a sufﬁciently comprehensive representation, hence less beneﬁt is gained from the augmentation. We will further investigate the effect of removing edges from graphs as part of data augmentation in future work. Nonetheless, particularly for smaller graphs, we believe our proposed approach of train- and test-time augmentation can provide additional performance."
2104.11641,"data, dataset",151,,,"b) Data Augmentation: Data augmentation has been shown to be an effective approach in machine learning which expands a dataset by producing transformed copies of data, thereby making the model invariant to these transformations. Data augmentation has been widely used to improve generalizability of machine learning models in natural language processing (NLP) and computer vision (CV). Most of the work on data augmentation has focused on improving augmentation at the training phase, e.g., batch augmentation [5] and UDA [6]. There are also studies that focus on augmentation during the testing phase [17]. However, data augmentation for graph neural networks has only been recently studied, such as SUBG-CON [18] and NodeAug [19]. Particularly, there is no research on test-time augmentation for GNNs. Inﬂuence:"
2104.11641,"data, dataset",165,,,"Abstract—Data augmentation has been widely used in machine learning for natural language processing and computer vision tasks to improve model performance. However, little research has studied data augmentation on graph neural networks, particularly using augmentation at both train- and test-time. Inspired by the success of augmentation in other domains, we have designed a method for social inﬂuence prediction using graph neural networks with train- and test-time augmentation, which can effectively generate multiple augmented graphs for social networks by utilising a variational graph autoencoder in both scenarios. We have evaluated the performance of our method on predicting user inﬂuence on multiple social network datasets. Our experimental results show that our end-to-end approach, which jointly trains a graph autoencoder and social inﬂuence behaviour classiﬁcation network, can outperform stateof-the-art approaches, demonstrating the effectiveness of trainand test-time augmentation on graph neural networks for social inﬂuence prediction. We observe that this is particularly effective on smaller graphs."
2104.11641,"data, dataset",21,,,1OAG dataset details: www.openacademic.ai/ 2Digg dataset details: www.isi.edu/ lerman/downloads/digg2009.html/ 3Twitter dataset details: snap.stanford.edu/data/higgs-twitter.html 4Weibo dataset details: www.aminer.cn/inﬂuencelocality
2104.11641,dataset,1,,,Dataset
2104.11641,dataset,149,,,"inﬂuence prediction method, AugInf, which incorporates train- and testtime augmentation with a jointly trained graph neural network approach. During training, this method takes into account the losses of both the graph representation learning and downstream social inﬂuence prediction task. We improve performance by applying numerous augmentations to the graphs using variational graph auto-encoders at both train- and testtime. Via an ablation study we show that the jointly trained model obtains more effective latent feature representations by using the joint loss along with both the train- and test-time augmentations. We compare our proposed end-to-end method with the state-of-the-art on several social network datasets. The experimental results show that our proposed method, AugInf-GAT, can improve the performance of predicting social inﬂuence on a number of social networks, and in particular, on the smallest of the social network graphs."
2104.11641,dataset,2,,,A. Datasets
2104.11641,dataset,225,,,"In our experiments we apply three augmentations to each graph with the augmentation hyperparameter threshold value set to 0.8 and train for 500 epochs. We will discuss the performance of varying these parameters in a later section. For the GAE component, each of the two hidden layers contain 64 hidden units for Digg and Twitter, and 32 for OAG and Weibo. They are trained with the Adagrad optimizer, using a 0.2 learning rate for OAG and Weibo, 0.05 for Digg and 0.1 for Twitter. Weight decay is set to 0.0005 all datasets except Digg, where it is 0.001. Additionally, we use dropout rate of 0.2. For the GNN prediction module, the ﬁrst and second layers each contain 128 hidden units and the third layer, as the output layer, has two hidden units. There are eight attention heads in each GAT layer, which means each head needs to process 16 hidden units for Digg and Twitter, with four attention heads for OAG and Weibo, which means each head needs to process 32 hidden units. The nonlinear activation function we use for both augmentation and prediction (σ in Eq. 1 and 5) is the exponential linear unit (ELU) [31]."
2104.11641,dataset,23,,,"TABLE II THE PERFORMANCE OF TWO AUGINF MODELS ON DIFFERENT DATASETS, ALONG WITH THE PERFORMANCE OF THE BASELINES WITHOUT VERTEX FEATURES."
2104.11641,dataset,25,,,"We evaluate using four datasets across different social network domains, namely OAG1 (Open Academic Graph), Digg2, Twitter3 and Weibo4."
2104.11641,dataset,26,,,"1) Hyperparameter Analysis: We conduct an hyperparameter analysis on the Digg dataset with the same hyperparameters values mentioned previously, unless stated otherwise."
2104.11641,dataset,35,,,TABLE I THE STATISTICS OF THE DATASETS. |V | AND |E| ARE THE TOTAL NUMBERS OF NODES AND EDGES OF THE ORIGINAL DATASET RESPECTIVELY AND N IS THE NUMBER OF SUBGRAPHS AFTER PREPROCESSING.
2104.11641,dataset,44,,,"• The Digg dataset contains the timestamped voting behaviours of users on stories on a social news aggregation website. The edges of Digg graph are deﬁned as following relationships and the inﬂuence actions, which we are predicting, are voting behaviours."
2104.11641,dataset,66,,,"We have further evaluated AugInf-GAT and AugInf-GCN on the individual components of our approach, to determine the contribution of each component to the overall performance. There are three main components in our approach: (1) train-time augmentation (2) test-time augmentation and (3) the jointly trained model. We have evaluated the following combinations on the Digg dataset:"
2104.11641,dataset,67,,,"These datasets were used previously by Qiu et al. [2]. Qiu et al. [2] sampled the entire social network into sub-networks with 50 nodes in each sub-network by using a random walk with restart, extracted features for each node and provided a ground-truth for the dataset. The statistics of the three datasets are shown in Table I."
2104.11641,dataset,80,,,"b) The Threshold for Augmentation: Another parameter we analyze is the threshold that determines which edges may be added. The results of this are shown in Figure 3. When the threshold is set to 0.8 for Digg dataset, our method achieves the highest performance, while on average the number of edges per dataset increases by 2.7%. As we increase the number of added edges, the performance of our method decreases."
2104.11907,"code, publicly available, code available",9,,,4) The source code is publicly available.
2104.11907,"code, publicly available, code available, github, data, dataset",202,,,"Abstract—As an essential procedure of data fusion, LiDARcamera calibration is critical for autonomous vehicles and robot navigation. Most calibration methods rely on hand-crafted features and require signiﬁcant amounts of extracted features or speciﬁc calibration targets. With the development of deep learning (DL) techniques, some attempts take advantage of convolutional neural networks (CNNs) to regress the 6 degrees of freedom (DOF) extrinsic parameters. Nevertheless, the performance of these DL-based methods is reported to be worse than the non-DL methods. This paper proposed an online LiDAR-camera extrinsic calibration algorithm that combines the DL and the geometry methods. We deﬁne a two-channel image named calibration ﬂow to illustrate the deviation from the initial projection to the ground truth. EPnP algorithm within the RANdom SAmple Consensus (RANSAC) scheme is applied to estimate the extrinsic parameters with 2D-3D correspondences constructed by the calibration ﬂow. Experiments on KITTI datasets demonstrate that our proposed method is superior to the state-of-the-art methods. Furthermore, we propose a semantic initialization algorithm with the introduction of instance centroids (ICs). The code will be publicly available at https://github.com/LvXudong-HIT/CFNet."
2104.11907,data,156,,,"Semantic information is introduced for obtaining an ideal initial extrinsic parameter. SOIC [37] exploited semantic information to calibrate and transforms the initialization problem into the Perspective-n-Points (PnP) problem of the semantic centroid. Due to the 3D semantic centroid of the point cloud and the 2D semantic centroid of the image cannot match accurately, a matching constraint cost function based on the semantic elements of the image and the LiDAR point cloud is also proposed. The optimal calibration parameter is obtained by minimizing the cost function. Zhu et al. [38] proposed an online calibration system that automatically calculates the optimal rigid-body motion transformation between two sensors by maximizing the mutual information of their perceived data without adjusting the environment Settings. By formulating the calibration as an optimization problem with semantic features, the temporally synchronized LiDAR and camera are registered in real-time."
2104.11907,data,167,,,"is required to transform them to consistent size. To fulﬁll the input size of the network, that the input width and height is multiple of 32, we randomly crop the original image to 960×320. We generate the sparse LiDAR-image by projecting the LiDAR point cloud onto the original image plane and then crop the original RGB image and the sparse LiDAR-image simultaneously. We can obtain the input data of the network without changing the camera intrinsic parameters. Besides, because the crop process is random, we can increase training data with this operation. Data augmentation is performed on the cropped input data. We add color augmentations with 50 chance, where we perform random brightness, contrast, saturation, and hue shifts by sampling from uniform distributions in the ranges of [0.7, 1.3] for brightness, contrast, and saturation, [1 − 0.3/3.14, 1 + 0.3/3.14] for hue shifts."
2104.11907,data,18,,,"The above methods do not need calibration targets and are entirely data-driven calibration algorithms. Nevertheless, an"
2104.11907,data,283,,,"Similar to LCCNet, we employ a multi-range iterative reﬁnement method to improve the calibration accuracy further. We train ﬁve models with different initial error ranges ∆T , [−x, x] , x = {1.5m, 1.0m, 0.5m, 0.2m, 0.1m} for translation and [−y, y] , y = {20◦, 10◦, 5◦, 2◦, 1◦} for rotation. The calibration iterative reﬁnement process is shown in Algorithm 1. (cid:9), The inputs are camera frame I, LiDAR point clouds (cid:8)P L camera intrinsic K as well as initial calibration parameters Tinit. After processing the sensors data, we project the LiDAR (cid:9) to image plane to generate the sparse depth point clouds (cid:8)P L image Dinit and the projected 2D points {pinit }. Due to the input size of the Network is 320 × 960, we need to crop the original RGB image and the depth image simultaneously. To ensure the cropped depth image contains as many points as possible, we calculate the centroid of {pinit } to get the location of the crop window. Then, we use the output of N1(±1.5m, ±20◦) to rectify the coordinate of each initial projected point. The rectiﬁed 2D projected points and the corresponding valid LiDAR point clouds construct new 2D3D correspondences. By applying the EPnP algorithm within the RANSAC scheme, we calculate the extrinsic parameters to T1. We regard the transformation T1 Tpred and set as new initial extrinsic parameters Tinit the"
2104.11907,data,50,,,"[3] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for 3d object detection from rgb-d data,” in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 918–927."
2104.11907,data,80,,,"Recently, some approaches have attempted to apply deep learning to predict the relative transformation between LiDAR and the camera. The calibration errors of these methods are still signiﬁcant compared with the traditional methods. Thus, these learning-based calibration methods can not meet the calibration requirements. Besides, in practical application, when sensor parameters change, many training data are needed to ﬁne-tune the network model, which dramatically limits the algorithm’s generalization."
2104.11907,"data, dataset",231,,,"utilized in the above experiments, se3 error [36] is a more direct evaluation metric. The off-range of the test dataset T3 is a litter smaller than that in dataset T2. The metric MSEE for β-RegNet and RGGNet is smaller in T3 than in T2. Nevertheless, the MSEE is the same for CFNet in these two test datasets, which proves that CFNet is more robust than βRegNet and RGGNet with different off-range settings. In test dataset T4, the performance of β-RegNet degrades heavily, and RGGNet needs to re-train on an additional dataset by adding a small number of data from 2009 10 03 sequence to achieve a good calibration result 0.010 (83.22%). CFNet does not need any additional training dataset and re-train process. The calibration results 0.001 (98.08%) demonstrates that CFNet has good generalization capability. Thus, FNet outperforms all of the learning-based calibration algorithms, RegNet, CalibNet, RGGNet, and even the motion-based calibration method. We can also see that, compared to the motion-based algorithm [30], our proposed method is more generalized, without the requirements of hand-crafted features or the extra IMU sensor. Furthermore, the semantic initialization process does not require motion information which is vital to the hand-eye calibration."
2104.11907,dataset,10,,,TABLE IV THE COMPARISON RESULTS ON THE T1 TEST DATASET
2104.11907,dataset,101,,,"We use the odometry recordings from the KITTI dataset, speciﬁcally the left color image and Velodyne point clouds recordings. We use the sequence 06 to 21 for training and validation (29416 frames), sequence 01 to 05 for evaluation/test (4854 frames). The initial calibration off-range ∆T is (±1.5m, ±20◦). To compare with other learning-based (CNNbased) methods, we design four different test datasets on the raw recordings of the KITTI dataset. Each test dataset is independent of the training dataset with the following test name conﬁgurations:"
2104.11907,dataset,111,,,"In this paper, we presented a novel online LiDAR-camera extrinsic calibration algorithm. To represent the deviation from the initial projection of LiDAR point clouds to the ground truth, we deﬁne an image called calibration ﬂow. Inspired by the optical ﬂow network, we design a deep calibration ﬂow network CFNet. The initial projected points are rectiﬁed to construct accurate 2D-3D by the prediction of CFNet correspondences. EPnP algorithm within the RANSAC scheme is utilized to estimate the extrinsic parameters with iterative reﬁnement. Our experiments demonstrate the superiority of CFNet. The additional experiments on the KITTI360 datasets illustrate the generalization of our method."
2104.11907,dataset,111,,,"The evaluation results on the KITTI odometry dataset are shown in Table I. It can be seen that in all of these test sequences, the mean translation error Et < 2cm and the mean rotation error ER < 0.13◦. Figure 5 shows two examples of CFNet predictions. We can see that the reference objects in the projected depth image and RGB image align accurately after re-calibration. In all of these test sequences, the calibration error of sequence 01 is the largest. The main reason is that this sequence is collected from a high-way scene, which is not included in the training dataset."
2104.11907,dataset,15,,,"TABLE V THE COMPARISON RESULTS ON THE T2, T3, AND T4 TEST DATASET"
2104.11907,dataset,174,,,"We evaluate our approach on the KITTI benchmark dataset including RGB images and Velodyne point clouds [42], recordings collected from different scenes. The timestamps of LiDAR and camera are synchronized, so the images and point clouds in each sequence correspond. To train our proposed calibration ﬂow prediction network CFNet, we need to ensure the input, output, and corresponding calibration ﬂow ground truth. We deﬁne the extrinsic parameters ground truth Tgt between LiDAR and camera as the transformation matrix from the camera coordinate to the LiDAR coordinate. By adding a random variable ∆T , we can obtain the initial calibration parameters Tinit = ∆T · Tgt. The LiDAR point cloud is projected onto the image plane with initial extrinsic parameters Tinit and the camera intrinsic matrix K to generate the LiDAR-image Dinit. The network takes an RGB image I, and the corresponding projected LiDAR-image Dinit as input. The calibration ﬂow ground truth can be provided by Eq. 7."
2104.11907,dataset,21,,,Fig. 8. Examples of the reconstructed 3D color map on the KITTI360 datasets with the prediciton of CFNet.
2104.11907,dataset,3,,,A. Dataset Preparation
2104.11907,dataset,34,,,"The comparison results on the test datasets T2, T3, and T4 shown in Table V also illustrate the superior of the CFNet. Compared to the translation errors and the rotation errors"
2104.11907,dataset,49,,,Fig. 7. Examples of CFNet predictions with semantic initialization on the KITTI360 datasets.(First Row) 2D-IC. (Second Row) 3D-IC. (Third Row) Semantic Initialization. (Forth Row) CFNet Prediction. (Fifth Row) Ground Truth.
2104.11907,dataset,62,,,"KITTI360 datasets are utilized as an additional dataset to test our proposed LiDAR-Camera calibration algorithm CFNet. The models trained on KITTI odometry training datasets are regarded as the pre-trained models. We only use sequences 0000, 0002, and 0003 for training and validation during training to ﬁne-tune the CFNet models. Other sequences are selected as test datasets."
2104.11907,dataset,75,,,"We also test the performance of CFNet on the KITTI360 benchmark dataset. The results are shown in Table VI and Figure 7. Despite re-training on a tiny sub dataset with one epoch, excellent results are obtained in the test sequences. Therefore, when the sensor parameters change, such as the camera focal length or the LiDAR-Camera extrinsic parameters, an excellent prediction model can be obtained with simple re-training."
2104.11907,dataset,9,,,TABLE VI THE CALIBRATION RESULTS ON KITTI360 TEST DATASET
2104.14114,data,11,,,"Keywords: Scientiﬁc publications, Productivity prediction, Data model."
2104.14114,data,11,,,"on Big Data, 2(1), 18-30."
2104.14114,data,14,,,"collaboration patterns based on PNAS. EPJ Data Science, 7, 1-17."
2104.14114,data,15,,,"that integrates the advantages of both data-driven and model-based approaches, and the eﬀectiveness of"
2104.14114,data,18,04/21/22,0,"Following data processing, we obtained the time series of the number of publications by each author,"
2104.14114,data,19,,,"Additionally, an LSTM is well suited to making predictions based on time series data, especially for time"
2104.14114,data,19,,,"this study are described in Sections 2 and 3, respectively. The model and empirical data are described"
2104.14114,data,20,,,"features from raw data; these networks have also been applied to predict the above three indexes. Here,"
2104.14114,data,22,,,"a time series of 12 numerical data points, the LSTM outputs one value. A “Dense” layer is applied"
2104.14114,data,22,,,that of the 4 models. The data are fed to the LSTM in batches of 5 authors (batch-size=5).
2104.14114,data,23,,,"18. Cao X, Chen Y, Liu KR (2016) A data analytic approach to quantifying scientiﬁc impact. J"
2104.14114,data,23,,,"2. Clauset A, Larremore DB, Sinatra R (2017) Data-driven predictions in the science of science. Science,"
2104.14114,data,24,,,"32 GRU units. Given a time series of 12 numerical data points, the GRU outputs one value. A “Dense”"
2104.14114,data,3,,,5 Empirical data
2104.14114,data,9,,,model outperforms the tested data-driven and model-based approaches.
2104.14114,dataset,10,,,"in our model, as are the test datasets."
2104.14114,dataset,13,,,time series and a target of any author in the training dataset.
2104.14114,dataset,16,,,We extracted parts from the dataset dblp to construct training and test datasets for the experiments
2104.14114,dataset,16,,,"dataset is comprised of 315,677 publications produced by 441,501 authors, which have been published in"
2104.14114,dataset,16,,,"their annual number of publications in years 1951–2013, involving 105,806 publications. The test dataset"
2104.14114,dataset,17,,,"involving 83,302 publications. The test dataset consists of those the same as the training dataset and"
2104.14114,dataset,17,,,the power-law distribution. The training datasets used in these three experiments are the same as those
2104.14114,dataset,17,,,"this method was validated by applying it to a high-quality dblp dataset, demonstrating that the proposed"
2104.14114,dataset,18,,,"Here, we applied the piecewise Poisson model to the dataset. The training dataset consists of the"
2104.14114,dataset,18,,,"in Section 6. The training dataset consists of 5,741 authors who produced publications at year 2000 and"
2104.14114,dataset,18,,,"the piecewise Poisson model, the eﬀectiveness of which has been veriﬁed on the dblp dataset for the"
2104.14114,dataset,18,,,"the piecewise Poisson model. Herein, this model was applied to the dblp dataset and exhibited good"
2104.14114,dataset,18,,,"with a small training dataset; hence, integrating their advantages to design a mixed architecture can be"
2104.14114,dataset,19,,,function. RMSprop is chosen to optimize the network. The training dataset used here is the same as
2104.14114,dataset,19,,,the predicted number of publications. We applied this model to the datasets that are the same as those
2104.14114,dataset,20,,,"1951–2000, which count for about 94% authors in the dblp dataset. Notably, the hyperparameters in the"
2104.14114,dataset,20,,,of publications for each author. The dblp dataset is used to test the practicability of our model. The
2104.14114,dataset,20,,,"on the given dataset. This indicates a direction for improving our model. Second, in the distributions of"
2104.14114,dataset,21,,,"author s in years 1951–t. The proposed model is trained on every author s in the training dataset, where"
2104.14114,dataset,22,,,"the training dataset is randomly divided into 4 packets: one of the packets is used as the test set, while"
2104.14114,dataset,23,,,"The training dataset is the same as that in Section 6, but the test dataset is diﬀerent. It still consists of"
2104.14114,dataset,23,,,"[4, 5]. Statistical factor analysis seems to be useful for predicting this index, as it helps address datasets"
2104.14114,dataset,26,,,"In this model, the training dataset is divided into 4 parts, namely, Part I, Part II, Part III, and Part"
2104.14114,dataset,26,,,"hs(tX−1)} and the target hs(tX ) of any author s in the training dataset. For fourfold cross validation,"
2104.14114,dataset,32,,,"The dblp computer science bibliographic dataset was applied in this study, which provides open biblio graphic information on most of the journals and conference proceedings in computer science. The quality"
2104.14114,dataset,37,,,"of this dataset is guaranteed by a range of measures, such as applying several methods of name disam biguation, linking 60,000 manually conﬁrmed external IDs to dblp author bibliographies, and so on. The"
2104.14114,dataset,53,,,"We used the deep architecture of our model directly to predict the number of publications. The deep archi tecture, the LSTM, was applied to the test dataset, with the time series {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,56,,,"The training and test datasets used here are the same as those in Section 5. Figs. 15-17 show the com parisons between the GRU and our model on the test dataset, where the input is {hs(1989), hs(1990), ..., hs(2000)}"
2104.14114,dataset,6,,,requiring a large training dataset.
2105.00129,code,113,,,"Problem Statement – Given W , the objective is to produce the code for a workﬂow generator that generates realistic synthetic workﬂow instances. This workﬂow generator takes as input an integer, n ≥ minw∈W (|w|). It outputs a workﬂow w(cid:48) with n(cid:48) ≥ n vertices that is as realistic as possible. n(cid:48) may not be equal to n, because real worfklows for most scientiﬁc applications cannot be feasibly instantiated for arbitray numbers of tasks. Our approach guarantees that n(cid:48) is the smallest feasible number of tasks that is greater than n."
2105.00129,code,122,,,"generator for any given workﬂow application. WfChef takes as input a set of real workﬂow instances from an application, and outputs the code of a synthetic workﬂow generator for that application. WfChef analyzes the real workﬂow graphs in order to identify subgraphs that represent fundamental task dependency patterns. Based on the identiﬁed subgraphs and on measured task type frequencies in the real workﬂows, WfChef outputs a generator that can generate realistic synthetic workﬂow instances with an arbitrary numbers of tasks. In this work, we evaluate the realism of the synthetic workﬂows generated by our approach, both in terms of workﬂow structure and execution behavior. Speciﬁcally, this work makes the following contributions:"
2105.00129,code,153,,,"The pseudo-code for REPLICATEPOS is shown in Algorithm 3. It takes as input a desired number of vertices (n), a base workﬂow (base), the list of POs in the base workﬂow (bP Os), and the list of POs in the workﬂow whose number of vertices is the closest to n (cP Os). The intent is to replicate POs in the base workﬂow, picking which pattern to replicate based on the frequency of POs for that pattern in the closest workﬂow. At Line 2, the algorithm ﬁrst sets the generated workﬂow to be the base workﬂow. Lines 4-9 are devoted to computing a probability distribution. More speciﬁcally, for each PO in bP Os, the algorithm computes the probability with which this PO should be replicated. Given a PO in bP Os, nc"
2105.00129,code,22,,,The pseudo-code for WFCHEFGENERATE is shown in Algorithm 2. It takes as input a recipe (rcp) and a desired
2105.00129,code,267,,,"The pseudo-code for WFCHEFRECIPE is shown in Algorithm 1. Lines 2 to 16 are devoted to detecting all POs in W . For each w in W , the algorithm visits w’s vertices (Lines 515). An arbitrary unvisited vertex v is visited, and another arbitrary unvisited vertex v(cid:48) is found, if it exists, that has the same type-hash as v (Lines 6-7). If no such v(cid:48) exists then the algorithm visits another vertex v (Line 8). Otherwise, it marks v(cid:48) as visited (Line 9) and computes the set of closest common ancestor and successor vertices for v and v(cid:48) (Lines 10-11). The pseudo-code of the CLOSESTCOMMONANCESTORS and CLOSESTCOMMONDESCENDANTS functions is not shown as they are simple DAG traversals. If v and v(cid:48) do not have at least one common ancestor and one common descendant, then the algorithm visits another vertex v (Line 12). Otherwise, two POs have been found, which are constructed and appended to the list of POs that occur in w at Lines 13 and 14. The pseudocode for function SUBDAG is not shown. It takes as input a vertex in a DAG, a set of ancestors of that vertex, and a set of descendants of that vertex. It returns a DAG that contains all paths from all ancestors to all descendants to traverse v, but"
2105.00129,code,334,,,"Addressing these challenges requires a solid experimental methodology for evaluating and benchmarking workﬂow algorithms and systems. A fundamental component of this methodology is the availability of sets of representative workﬂow instances. One approach is to infer workﬂow structures from real-world execution logs. We have ourselves followed this approach in previous work [4], [5], resulting in a repository that provides ∼20 workﬂow instances for each of a handful of scientiﬁc applications. These instances have been used by researchers, often for driving simulation experiments designed to evaluate scheduling and resource management algorithms. Real workﬂow instances are by deﬁnition representative of real applications, but they cover only a limited number of scenarios. To overcome this limitation, in previous work we have developed tools for generating synthetic workﬂows by extrapolating the patterns seen in real workﬂow instances. The work in [4] presented a synthetic workﬂow generator for four workﬂow applications, which has been used extensively by researchers3. The method for generating the synthetic workﬂows was ad-hoc and based on expert knowledge and manual inspection of real workﬂow instances. Our more recent generator in [5] improves on the previous generator by using information derived from statistical analysis of execution logs. It was shown to generate more realistic workﬂows than the earlier generator, and in particular to preserve key workﬂow features when generating workﬂows at different scales [5]. The main drawback of these two generators is that implementing the workﬂow generation procedure is labor-intensive. Generators are manually crafted for each application, which not only requires signiﬁcant development effort (several hundreds of lines of code) but also, and more importantly, expert knowledge about the scientiﬁc application semantics that deﬁne workﬂow structures. As a result, this approach is not scalable if synthetic workﬂow instances are to be generated for a large number of scientiﬁc applications."
2105.00129,code,38,,,"The pseudo-code in this section is designed for clarity. Our actual implementation, described in the next section, is more efﬁcient and avoids all unnecessary re-computations (e.g., the probabilities computed in WFCHEFGENERATE)."
2105.00129,data,115,,,"WfChef consists of two main algorithms, WFCHEFRECIPE and WFCHEFGENERATE. The former is invoked only once and takes as input a set of workﬂow instances for a particular application, W , and outputs a “recipe”, i.e., a data structure that encodes relevant information extracted from the workﬂow instances. The latter is invoked each time a synthetic workﬂow instance needs to be generated. It takes as input a recipe and a desired number of vertices (as well as a seeded pseudorandom number generator), and outputs a synthetic workﬂow instance. Both these algorithms have polynomial complexity and implement several heuristics, as described hereafter."
2105.00129,data,160,,,"A short-term future work direction is to replace the handcrafted WorkﬂowHub generators developed in [5] and available on the WorkﬂowHub web site6 by generators automatically generated by WfChef. Another short-term future direction is to apply WfChef to more scientiﬁc workﬂow applications beyond those supported by WorkﬂowHub. A longer-term direction is to investigate whether machine learning techniques can be applied to solve the synthetic workﬂow generation problem, to compare these techniques to WfChef, and perhaps evolve WfChef accordingly. Our suspicion, however, is that the amount of training data necessary for machine learning approaches to be effective could be prohibitive. By contrast, the WfChef algorithms are able to analyze a few real workﬂow instances to discover patterns. In fact, another interesting research direction is to determine the minimum amount of training data that still allows WfChef to produce realistic synthetic workﬂow instances. In the results presented in this"
2105.00129,data,210,,,"Scientiﬁc workﬂow conﬁgurations, both inferred from realworld executions and synthetically generated, have been used extensively in the workﬂow research and development community, in particular for evaluating resource management and scheduling approaches. As scientiﬁc workﬂows are typically represented as Directed Acyclic Graphs (DAGs), several tools have been developed to generate random DAGs, based on speciﬁed ranges for various parameters [6]–[9]. For instance, DAGGEN [6] and SDAG [7] generate random DAGs based on ranges of parameters such as the number of tasks, the width, the edge density, the maximum number of levels that can be spanned by an edge, the data-to-computation ratio, etc. Similarly, DAGEN [8] generates random DAGS, but does so for parallel programs in which the task computation and communication payloads are modeled according to actual parallel programs. DAGITIZER [9] is an extension of DAGEN for grid workﬂows where all parameters are randomly generated. Although these generators can produce a very diverse set of DAGs, they may not resemble those of actual scientiﬁc workﬂows as they do not capture patterns deﬁned by applicationspeciﬁc semantics."
2105.00129,data,261,,,"Lines 17 to 23 are devoted to computing a set of “errors” resulting from using a particular (smaller) base workﬂow to generate a larger (synthetic) workﬂow. The WfChef approach consists in replicating POs in a base workﬂow to scale up its number of vertices while retaining a realistic structure. Therefore, when needing to generate a synthetic workﬂow at a particular scale, it is necessary to choose a base workﬂow as a starting point. To provide some basis for this choice, for each w ∈ W , the algorithm generates a synthetic workﬂow with |w| vertices using as a base each workﬂow in W with fewer vertices than w (Lines 12-22). The REPLICATEPOS function replicates POs in a base workﬂow to generate a larger synthetic workﬂow (it is described at the end of this section). The error, that is the discrepancy between the generated workﬂow and w, is quantiﬁed via some error metric (the ERROR function) and recorded at Line 21 (in our implementation we use the THF metric described in Section V-B). The way in which these recorded errors are used in our approach is explained in the description of WFCHEFGENERATE hereafter. Finally, at Line 24, the algorithm returns a recipe, i.e., a data structure that contains the workﬂow instances (W ), the discovered pattern occurrences (P Os), and the above errors (Errors)."
2105.00129,data,330,,,"An alternative to random generation is to generate DAGs based on the structure of real workﬂows for particular scientiﬁc applications. In [10], over forty workﬂow patterns are identiﬁed for addressing business process requirements (e.g., sequence, parallelism, choice, synchronization, etc.). Although several of these patterns can be mapped to some extent to structures that occur in scientiﬁc workﬂows [11], they do not fully capture these structures. In particular, they do not necessarily respect the ratios of different types of particular workﬂow tasks in these structures. This is important because a workﬂow structure is not only deﬁned by a set of vertices and edges, but also by the task type (e.g., an executable name) of each vertex. The work in [12] focuses on identifying workﬂow “motifs” based on observing the data created and used by workﬂow tasks so as to reverse engineer workﬂow structures. These motifs capture workﬂow (sub-)structures, and can thus be used for automated workﬂow generation. Unfortunately, identifying these motifs is an arduous manual process [12]. In our previous work [4], we developed a tool for generating synthetic workﬂow conﬁgurations based on realworld workﬂow instances. Although the overall structure of generated workﬂows was reasonably realistic, we found that workﬂow execution (simulated) behavior was not (see [5] and also results in Section V-C). In [5], we developed an enhanced version of that earlier generator in which task computational loads are more accurately captured by using statistical methods. As a result, the generated synthetic workﬂows are more realistic when compared to real-world workﬂows. While the task computational the DAG-generation procedure is labor-intensive because generators are manually crafted and rely on expert knowledge of the workﬂow application."
2105.00129,data,42,,,"structure, it samples task characteristics (i.e., task runtimes, input/output data sizes) from particular random distribution. Both WorkﬂowHub and WfChef do a similar sampling, but from distributions determined via statistical analysis of real workﬂow instances."
2105.00129,data,46,,,"[14] J. Liu, E. Pacitti, P. Valduriez, and M. Mattoso, “A survey of data intensive scientiﬁc workﬂow management,” Journal of Grid Computing, vol. 13, no. 4, pp. 457–493, 2015."
2105.00129,"data, data available",53,,,"work, for the purpose of evaluating WfChef and of comparing it to previously proposed approaches, we use as training data all available real workﬂow instances with fewer than the desired number of workﬂow tasks. But it may be that using fewer such instances would still lead to good results."
2105.00129,"data, data https, data available",205,,,"[17] M. Rynge, G. Juve, J. Kinney, J. Good, G. B. Berriman, A. Merrihew, and E. Deelman, “Producing an infrared multiwavelength galactic plane atlas using montage, pegasus and amazon web services,” in 23rd Annual Astronomical Data Analysis Software and Systems (ADASS) Conference, 2013. [Online]. Available: http://pegasus.isi.edu/ publications/2013/rynge-montage-pegasus-amazon-adass2013.pdf [18] E. Deelman, K. Vahi, G. Juve, M. Rynge, S. Callaghan, P. J. Maechling, R. Mayani, W. Chen, R. Ferreira da Silva, M. Livny, and K. Wenger, “Pegasus, a workﬂow management system for science automation,” Future Generation Computer Systems, vol. 46, no. 0, pp. 17–35, 2015. [19] K. Keahey, J. Anderson, Z. Zhen, P. Riteau, P. Ruth, D. Stanzione, M. Cevik, J. Colleran, H. S. Gunawi, C. Hammock et al., “Lessons learned from the chameleon testbed,” in 2020 USENIX Annual Technical Conference (USENIX ATC 20), 2020, pp. 219–233."
2105.00129,"data, publicly available",242,,,"Our ground truth consists of real Montage and Epigenomics workﬂow instances. These instances are publicly available on the WorkﬂowHub repository [5]. They were obtained based on logs of application executions with the Pegasus Workﬂow Management System [18] on the Chameleon academic cloud testbed [19]. Speciﬁcally, we consider 14 Montage workﬂow instances with between 105 and 9807 tasks, and 25 Epigenomics workﬂow instances with between 75 and 1697 tasks. We generate synthetic workﬂow instances with the same number of tasks as real workﬂow instances, so as to compare synthetic instances to real instances. Both WorkﬂowGenerator and WorkﬂowHub encode application-speciﬁc knowledge to produce synthetic workﬂow instances for any desired number of tasks, n. Instead, WfChef generators rely on training data, i.e., real workﬂow instances. We use a simple “training and testing” approach. That is, for generating a synthetic workﬂow instance with n tasks, we invoke WFCHEFRECIPE with all real workﬂow instances with < n tasks. For instance, say we want to use WfChef to generate an Epigenomics workﬂow with 127 tasks. We have real Epigenomics instances for 75, 121, and 127 tasks. We invoke WFCHEFRECIPE with the 75and 121-tasks instances to generate the recipe. We then invoke WFCHEFGENERATE, passing to it this receipt and asking it to generate a 127-tasks instance."
2105.00129,database,12,,,1The IEEE Xplore digital database includes 118 articles with both the words
2105.00129,github,12,,,"[20] “WRENCH Pegasus Simulator,” https://github.com/wrench-project/"
2105.00129,github,16,,,"[6] “DAGGEN: a synthetic task graph generator,” https://github.com/"
2105.00129,github,6,,,4https://github.com/wfcommons/workﬂow-schema 5https://github.com/tainagdcoleman/wfchef
2105.00129,"python, github, package",85,,,"We have implemented our approach in a Python package called wfchef. Speciﬁcally, this package deﬁnes a Recipe class. The constructor for that class takes as input a list of workﬂow instances and implements algorithm WFCHEFRECIPE. The workﬂow instances are provided as ﬁles in the WfCommons JSON format 4. The class has a public method duplicate that implements the WFCHEFGENERATE algorithm, and a private method duplicate_nodes that implements the REPLICATEPOS algorithm. This Python package is available on GitHub5."
2105.00129,"python, package",182,,,"Approximate Edit Distance (AED) – Given a real workﬂow instance w and a synthetic workﬂow instance w(cid:48), the AED metric is computed as the approximate number of edits (vertex removal, vertex addition, edge removal, and edge addition) necessary so that w = w(cid:48), divided by |w|. Lower values include a higher similarity between w and w(cid:48). We compute this metric via the optimize_graph_edit_distance method from the Python’s NetworkX package. Note that NetworkX also provides a method to compute an exact edit distance, but its complexity is prohibitive for the size of the workﬂow instances we consider. Even though the AED metric can be computed much faster, because it is approximate, we were able to compute it only for workﬂow instances with 865 or fewer tasks for Epigenomics and 750 or fewer tasks for Montage. This is because or RAM footprint issues (despite using a dedicated host with 192 GiB of RAM)."
2105.00775,"code, github, code available",50,04/21/22,2,"Copyright © 2021 O. Mesnard and L.A. Barba, released under a Creative Commons Attribution 4.0 International license. Correspondence should be addressed to Lorena A. Barba (labarba@gwu.edu) The authors have declared that no competing interests exist. Code is available at https://github.com/barbagroup/petibm-rollingpitching.."
2105.00775,"code, open-source, code available, python, github, data",323,,,"We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results. Admittedly, not everyone has access to an HPC cluster with GPU nodes and with Singularity installed. Lacking those resources, it becomes difficult to fully reproduce our workflow. However, we made the effort to deposit on Zenodo7 the primary data (directly output from our CFD solver) and post-processing scripts needed to reproduce the figures of the present manuscript. Once the Zenodo repository is downloaded, readers should be able to spin up a Docker container and run a Bash script to compute the secondary data and generate the figures, or generate different figures to explore the data in new ways. The Docker images produced and used for this study are stored on DockerHub8, under a basic free subscription. In the event Docker adopts a policy to automatically purge inactive images (those who have not been recently downloaded) from the Hub, the Dockerfiles are version-controlled on the GitHub repository and can be used to rebuild the images. We spent time engineering a transparent and reproducible workflow to produce the artifacts of this replication study. Surely, we cannot assert our steps will be fully reproducible in years from now; the software stack could very well become obsolete with new hardware generations. While the likelihood of the study being reproducible may decrease with the years, the transparency of the steps we took to generate the data shall remain constant."
2105.00775,"code, open-source, code available, python, github, data",342,,,"production, wake topology, and propulsive performance of a pitching and rolling wing. Although our numerical values do not fully match those from the original study Li and Dong5, we obtain the same trends and thus consider this replication attempt to be successful. A CFD solver typically outputs the solution of primary variables. For example, PetIBM outputs the pressure and velocity fields, as well as the body forces. We often use multiple post-processing scripts to generate the final data and figures reported and analyzed in the manuscript; it involves computing secondary data, such as the vorticity field, the aerodynamic power and forces. If the code is not made available, readers cannot inspect what has been done to produce these data; bugs introduced in these post-processing steps would go undetected. If no code is available, we cannot explain discrepancies observed between our replication and the original study. As Donoho and coworkers14 once said: “The only way we’d ever get to the bottom of such a discrepancy is if we both worked reproducibly and studied detailed differences between code and data.” We made our best efforts to ensure that our replication study is reproducible. Our computational application makes use of fully open-source tools, and we created a GitHub repository6 for this study. The repository contains the source code of the PetIBM application, as well as all input files of the simulations reported here, and pre- and postprocessing Python scripts. We adopted a reproducible workflow to run computational simulations; it makes use of Docker images and Singularity recipes to capture the computational environment. With Singularity, we ran container-based jobs on our universitymanaged HPC cluster. The GitHub repository also contains the job-submission scripts that were use to run the simulations on our cluster; they can be adapted to run on other platforms if readers are interested in reproducing our results."
2105.00775,"code, open-source, github, data, open-source code",327,,,"Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way. We have reimplemented the three-dimensional rolling and pitching kinematics in an open-source code shared on GitHub1 and prepared extensive reproducibility packages for all results."
2105.00775,"code, open-source, github, data, open-source code",333,,,"of this field are the Navier-Stokes equations, which are notoriously difficult to solve numerically, with computational experiments often taking a long time, even on parallel compute clusters. Both this difficulty and the history of the field (with early progress done in secret defense laboratories) contribute to rather poor standards of reproducibility. Research results are regularly communicated via published articles without accompanying software or data. In the past, we have undertaken a replication of results from our own research group on unsteady fluid dynamics3 and published the outputs and lessons learned from this exercise.4 We aim here to assess the effort needed to replicate the computational results from another research group and set our sights on a computational fluid dynamics study from Li and Dong5 that investigated the dynamics of pitching and rolling wings. While many prior studies have focused on the pitching and/or heaving motion of threedimensional wings, only few studies have looked at the combined rolling and pitching motion. As explained by Li and Dong5, the pitching-rolling kinematics has the potential to serve as a better canonical model for the hydrodynamics of bio-inspired flapping propulsors. The authors carried out a parametric study, using their own research code, to quantify the effects of the Reynolds number, Strouhal number, aspect ratio, and rolling/pitching phase difference on the wake topology and the propulsive performance of flapping wings. Table 1 lists the parameters and values considered in the original study. The deliverable from this study was the journal publication itself; the computational code, input data, and conditions of analysis used to produce the numerical results were not made available. Thus, referring back to Peng’s reproducibility spectrum, we consider the study to be not reproducible. Our objective was to replicate the scientific findings claimed in the original study and to do it in a reproducible way."
2105.00775,"code, publicly available, open-source, github, data, data repository",335,,,"The final product of the original study is a published manuscript in the journal Physics of Fluids. Although the manuscript is well detailed, the code and input data used to produce the computational results were not made publicly available by the authors. In that regard, we consider the study to not be reproducible. Thus, we aim to replicate the scientific findings claimed in the original study with our own research software stack and deliver reproducible results. PetIBM10 is developed in the open under the permissive (non-copyleft) 3-Clause BSD license, version-controlled with Git, and hosted on a public GitHub repository.2 Each major release of the software is archived on the data repository Zenodo. Our implementation of the three-dimensional rolling and pitching wing, which relies on PetIBM, is also open source and available on GitHub3 under the same license. The repository contains all input data and processing scripts that were used to produce the computational results reported in the next section. This allows anyone to inspect the code, to verify the steps that were taken to produce computational results, and to modify and re-use it for other applications. The repository also includes README files to guide readers that may be interested in re-running the analysis. Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others."
2105.00775,"code, publicly available, open-source, github, data, data repository",337,,,"Upon submission of the present manuscript, the application repository, as well as the data needed to reproduce the figures, have been archived on Zenodo. We leveraged our University high-performance-computing (HPC) cluster, called Pegasus, to run all simulations reported here. (We used computational nodes with Dual 20-Core 3.70GHz Intel Xeon Gold 6148 processors and NVIDIA V100 GPU devices.) To reduce the burden of building PetIBM and its applications on the cluster, we used the container technology from Docker11 and Singularity.12 Containers allow us to capture the conditions of analysis in a formatted image that can be shared with others. We have already used Docker containers in the past to create a reproducible workflow for scientific applications on the public cloud provider Microsoft Azure.13 Here, we aim to adopt a similar workflow on our local HPC cluster. Early in this replication study, we hit a snag: Docker is not available to users on Pegasus. Indeed, Docker is not available at most HPC centers for security reasons. Submitting container-based jobs with Docker implies running a Docker daemon (a background process) that requires root privileges that users do not and should not have on shared production clusters. Thus, we decided to leverage the Singularity container technology to conduct the replication study on Pegasus. Singularity is more recent than Docker, was designed from the ground up to prevent escalation of user privileges, and is compatible with Docker images. Our reproducible workflow starts with creating a Docker image that installs PetIBM and its applications, as well as all their dependencies. We then push the image to a public registry on DockerHub.4 Anyone interested in using the application code can now pull the image from the registry and spin up a Docker container to get a faithfully reproduced computational environment. Next, we use the cloud service Singularity Hub to build a"
2105.00775,data,188,,,"Figure 4. Comparison of the velocity profiles and profiles of the fluctuation of the kinetic energy in the wake of a flapping circular plate (AR = 1.27) at Reynolds number Re = 200 and Strouhal number St = 0.6. We report profiles at locations x/c = 1, 2, 3, 4, and 5. We compare the profiles obtained on the nominal grid, on the finer grid in space, and on the coarser grid in time. (a) Streamwise velocity (u1 − U∞) profiles in the x/y plane at z = S/2; (b) transverse velocity (u2) profiles in the x/y plane at z = S/2; (c) spanwise velocity (u3) in the x/z plane at y = 0; (d) 2)/2 in the x/y plane at z = S/2. We also report fluctuation of the kinetic energy (u(cid:48) 2 + u(cid:48) 2 1 digitized data from Fig. 4 of Li and Dong5."
2105.00775,data,311,,,"We also computed the three-dimensional flow at additional Reynolds numbers 100 and 400 for a circular flat plate (AR = 1.27) with Strouhal number St = 0.6 and a 90-degree phase difference between the rolling and pitching motions. Figure 12 shows a perspective view of the near-wake topology at t/T = 4.25 at Reynolds numbers 100 and 400. (Figure 6f shows the same view for Re = 200.) For all Reynolds numbers investigated, we note the formation of a double-loop vortex around the trailing edge of the plate. At lower Reynolds number (Re = 200), vortical structures dissipate more rapidly (due to increased viscous effects). At the higher Reynolds number (Re = 400), “double-C”-shaped vortex rings are convected downstream. As expected, vortex structures dissipate more rapidly for low Reynolds numbers. At Reynolds number Re = 400, “double-C”-shaped vortex rings propagate downstream. Overall, we observe similar features as those published in Li and Dong5 and confirm that the flow dynamics of the low-aspect-ratio flapping wing is insensitive to the Reynolds number, at least, for the range investigated here. Figure 13 displays the history of the force coefficients over two flapping cycles for Re = 100, 200, and 400. Table 7 reports the mean and peak thrust coefficients comparing to the data published in Li and Dong5. Although we obtained different statistics in the force coefficients (mean and peak values), we report similar trends as in the original study. First, the absolute peak value and mean value for the thrust coefficient increase with the Reynolds number. Second, the lift force peaks twice every half cycle, for all Reynolds"
2105.00775,data,49,,,"Replicability is obtaining consistent results across studies aimed at answering the same scientific question, each of which had obtained its own data. Two studies may be considered to have replicated if they obtain consistent results given the level of uncertainty inherent in the system under study."
2105.00775,data,50,,,Figure 11. Cycle-averaged thrust coefficient (CT ) and propulsive efficiency (η) for the baseline case as functions of the Strouhal number. Data were computed and averaged during the fifth flapping cycle. (See Fig. 11 of Li and Dong5 for comparison.)
2105.00775,data,50,,,"Table 3. Results of the grid-independence study for a flapping wing with AR = 1.27, St = 0.6, Re = 200, and ψ = 90o. We also report the results from Li and Dong5 (data digitized from figures of the original publication)."
2105.00775,data,56,,,Figure 19. Cycle-averaged thrust coefficient (CT ) and propulsive efficiency (η) for the baseline case as functions of the phase difference between the rolling and pitching motions. Data were computed and averaged during the fifth flapping cycle. (See Fig. 19 of Li and Dong5 for comparison.)
2105.00775,data,59,,,"mid-span are reported in Figure 4d. We also report digitized data from Li and Dong5 for comparison. Although there exist some differences with the results of the original study, we note that the profiles of the present study are visually similar when refining the spatial grid and when using fewer time steps per flapping cycle."
2105.00775,data,87,,,"Figure 3. History of the thrust (CT ), lift (CL), and spanwise (CZ ) coefficients over two flapping cycles of a circular plate (AR = 1.27) at Reynolds number Re = 200 and Strouhal number St = 0.6. We compare the instantaneous force coefficients obtained with nt = 2000 and nt = 1000 time steps per flapping cycle obtained on the nominal grid. Markers show digitized data from Fig. 9 of Li and Dong5."
2105.00775,"data, code",185,,,"Abstract This article reports on a full replication study in computational ﬂuid dynamics, using an immersed boundary method to obtain the ﬂow around a pitching and rolling elliptical wing. As in the original study, the computational experiments investigate the wake topology and aerodynamic forces, looking at the effect of: Reynolds number (100–400), Strouhal number (0.4–1.2), aspect ratio, and rolling/pitching phase difference. We also include a grid-independence study (from 5 to 72 million grid cells). The trends in aerodynamic performance and the characteristics of the wake topology were replicated, despite some differences in results. We declare the replication successful, and make fully available all the digital artifacts and workﬂow deﬁnitions, including software build recipes and container images, as well as secondary data and post-processing code. Run times for each computational experiment were between 8.1 and 13.8 hours to complete 5 ﬂapping cycles, using two compute nodes with dual 20-core 3.7GHz Intel Xeon Gold 6148 CPUs and two NVIDIA V100 GPU devices each."
2105.00775,"data, code",24,,,"Reproducibility is obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis."
2105.00775,"data, code",333,,,"θ(t) = −Aθ cos(2πf t + ψ) where Aθ is the pitching amplitude and ψ is the phase-difference angle between the pitching and rolling motions. For the present replication study, we use the same wing kinematics and numerically solve the three-dimensional Navier-Stokes equations (velocity/pressure formulation) for , where an incompressible viscous flow. The Reynolds number is defined as Re = U∞c U∞ is the incoming freestream speed and ν is the kinematic viscosity. The convective and diffusion terms of the partial differential equations are time-integrated using second-order accurate Adams-Bashforth and Crank-Nicolson methods, respectively. We enforce a Dirichlet condition (streamwise velocity set to the freestream speed U∞) on all boundaries, except at the outlet where we use a convective boundary condition (to carry vortical structures outside the computational domain). Our code base, PetIBM, solves the incompressible Navier-Stokes equations using a projection method, seen as an approximate block-LU decomposition of the fully discretized equations.6 To compute the flow around a moving object (e.g., a pitching-rolling wing), we use an immersed boundary technique. The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved)."
2105.00775,"data, code",337,,,"The fluid equations are solved over an extended domain that includes the interior of the immersed object. The boundary of the object is represented by a collection of Lagrangian markers that moves with a prescribed rigid kinematics and on which we enforce a no-slip condition. The presence of the body in the domain is taken into account by modifying the fluid equations in the vicinity of its boundary. This approach enables us to solve the equations on a simple fixed structured Cartesian grid. Different near-boundary treatments lead to different immersed boundary methods. The original study used a sharp-interface method with a ghost-cell methodology.7 PetIBM employs regularized delta functions to transfer data between the Lagrangian markers and the Eulerian grid points (on which the fluid equations are solved). Our code base includes several implementations of the immersed-boundary projection method;8 we use the formulation of Li et al.9 for all computations of the present study. These methods fall into the category of diffuse-interface methods, as the discrete delta function smears the solution over a few grid cells around the boundary. Each time step, we successively solve three linear systems for an intermediate velocity field, the Lagrangian forces, and the pressure field. The system for the velocity is solved using a stabilized bi-conjugate gradient method (from the PETSc library) with a Jacobi preconditioner and a convergence criterion based on the absolute L2-norm of the residual set to atol = 10−6. We solve the system for the Lagrangian forces with a direct solver (SuperLU_dist library). The pressure Poisson system is solved with a conjugate-gradient method using a classical algebraic multigrid technique (via the NVIDIA AmgX library); here, too, convergence is reached when the absolute L2-norm of the residual is 10−6. To quantify aerodynamic performance of the wing, we report the thrust, lift, and spanwise force coefficients, defined as"
2105.00775,"data, code, data available, code available",122,,,"The minimum requirement for computational research to be reproducible is to make code and data available to others. Peng2 introduced the concept of a reproducibility spectrum, in which reproducible research is a “minimum standard for judging scientific claims when full independent replication of a study is not possible” or not available. The two extremes on the reproducibility spectrum are “not reproducible” (when a published manuscript is the sole deliverable from a study) and “fully replicated” (the gold standard for a study). This paper addresses reproducibility and replicability in computational fluid dynamics, a mature field and one of the oldest branches of computational science. At the center"
2105.00775,github,10,04/21/22,2,2PetIBM: github.com/barbagroup/petibm 3PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 4DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,10,04/21/22,2,6PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching 7Repro-packs: doi.org/10.5281/zenodo.4732946 8DockerHub registry: hub.docker.com/repository/docker/mesnardo/petibm-rollingpitching
2105.00775,github,3,,,1PetIBM-rollingpitching: github.com/barbagroup/petibm-rollingpitching
2105.00775,open-source,216,,,"pitching-rolling plates.” In: Physics of Fluids 28.7 (2016), p. 071901. J. B. Perot. “An analysis of the fractional step method.” In: Journal of Computational Physics 108.1 (1993), pp. 51–58. R. Mittal, H. Dong, M. Bozkurttas, F. Najjar, A. Vargas, and A. Von Loebbecke. “A versatile sharp interface immersed boundary method for incompressible ﬂows with complex boundaries.” In: Journal of computational physics 227.10 (2008), pp. 4825–4852. K. Taira and T. Colonius. “The immersed boundary method: a projection approach.” In: Journal of Computational Physics 225.2 (2007), pp. 2118–2137. R.-Y. Li, C.-M. Xie, W.-X. Huang, and C.-X. Xu. “An efﬁcient immersed boundary projection method for ﬂow over complex/moving boundaries.” In: Computers & Fluids 140 (2016), pp. 122–135. P.-Y. Chuang, O. Mesnard, A. Krishnan, and L. A. Barba. “PetIBM: toolbox and applications of the immersedboundary method on distributed-memory architectures.” In: The Journal of Open Source Software 3.25 (May 2018), p. 558."
2105.00775,"python, github",316,,,"In the original study, the authors reported the results of a grid-independence study to justify the spatial and temporal grid resolutions used for the parametric study. They compared force coefficients, profiles of the velocity components, profiles of the fluctuating kinetic energy, and distances between vortical structures in the near wake, obtained with different grid resolutions. Here, we also report the results of our grid-independence study before moving on to the results of the parametric study. We use the same domain size as in the original study: 30c × 25c × 25c (where c is the chord length of the wing). The root of the wing (around which the plate undergoes the rolling/pitching motion) is located at the center of the computational domain. We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository)."
2105.00775,"python, github",331,,,"We keep the spatial grid uniform (with highest resolution) in the sub-area of the domain that covers the motion of the wing. Outside this area, we also add an extra uniform layer with grid-spacing size ∆x = 0.05c, in the sub-domain [−2c, 6c] × [−3c, 3c] × [−1c, 2c], which covers the near-wake region. (We opted for a smooth transition between the two uniform regions, in which the grid-cell widths are stretched with a constant ratio of 1.1 in all directions, except in the streamwise direction behind the wing where we used a ratio of 1.03.) Finally, the grid-cell width is stretched to the external boundaries with a constant ratio of 1.2. To the readers interested in further inspecting the geometric characteristics of the grids used in the present study: we used Python scripts to codify the grid parameters and saved them into PetIBM-readable yaml files (available on the GitHub repository). In the present study, we model the wing with a flat elliptical surface, discretized with Lagrangian markers uniformly distributed on its surface (with a similar resolution as the grid-spacing size of the background Eulerian grid). As in the original study, we consider the case of a circular wing (AR = 1.27) with Reynolds number Re = 200, Strouhal number St = 0.6, and phase-difference angle ψ = 90o, to assess independence in the numerical results. We investigated the effect of the grid-spacing size, the time-step size, and the convergence criterion of the iterative solvers, on the numerical solution. To assess the effect of the grid-spacing size ∆x in the vicinity of the wing on the solution, we computed five flapping cycles on three grids: coarse (∆x = 0.03c), nominal (∆x ="
2105.09146,data,110,,,"In Figure 10 and Table 2 we display the results of a baseline SINDy and SINDy+HNN approach for a noisy ideal pendulum. This trial followed the same procedure as outlined in the ideal mass-spring trial. The phase space trajectory predictions for the pendulum Figure 10 displays similar results to the mass-spring trial with the HNN and SINDy+HNN approaches outperforming the SINDy Baseline model on noisy data. The SINDy+HNN approach returned symbolic equations in line with or close to the underlying equation Table 2. As in the mass-spring trial, the SINDy baseline model was unable to recover meaningful symbolic equations when provided noisy input data."
2105.09146,data,119,,,"A HNN requires observations of a system’s canonical coordinates to train. This is generally provided to the network as input. If a system’s canonical coordinates are unknown, high dimensional data such as images can be paired with dimensionality reduction approaches to extract a representation of a system’s canonical coordinates. The Hamiltonian Generative Network architecture [19] has shown success in extracting representations of canonical coordinates and modeling classical physical systems from images. Here we will focus our discussion around the classic HNN architecture from [11] illustrated in Figure 4(b), that receives only a system’s canonical coordinates as input."
2105.09146,data,119,,,"The SINDy+HNN approach successfully regulated the noisy input and recovered symbolic equations inline with the underlying equations Table 1. The SINDy Baseline model in contrast lacked a noise mitigation approach and was unable to recover meaningful symbolic equations when faced with noisy input data. As previously discussed both models could be improved by the inclusion of a standard noise reduction approach on the input coordinates. The strong performance of the SINDy+HNN in the absence of a standard noise regulation approach suggests an HNN may offer utility as a physics informed noise regulator. In practical applications, it may be advantageous to follow up a standard noise reduction approach with a physical constraint embedded network."
2105.09146,data,140,,,"Figure 10: Pendulum phase trajectory predictions over the interval: t-span = [0,10]. The noise level increases from left to right: σ = 0 to σ = 0.03. The noisy input data is plotted as a grey background trajectory. (a) The Baseline SINDy model ﬁt directly on noisy input data. The baseline SINDy model struggles with the introduction of noise. (b) HNN model trained on noisy data. The HNN model learns to conserve energy however it begins to wobble at higher noise levels. (c) SINDy+HNN model, a SINDy model ﬁt on HNN predictions. The SINDy+HNN model recovered equation close to the underlying equation table: 2 and avoided the wobbling issue present at higher noise levels in the HNN model."
2105.09146,data,142,,,"Figure 9: Mass-spring phase trajectory predictions over the interval: t-span = [0,10]. The noise level increases from left to right: σ = 0 to σ = 0.03. The noisy input data is plotted as a grey background trajectory. (a) The Baseline SINDy model ﬁt directly on noisy input data. The baseline SINDy model struggles with the introduction of noise. (b) HNN model trained on noisy data. The HNN model learns to conserve energy however it begins to wobble at higher noise levels. (c) SINDy+HNN model, a SINDy model ﬁt on HNN predictions. The SINDy+HNN model recovered equation close to the underlying equation as shown in Table 1 and avoided the wobbling issue present at higher noise levels in the HNN model."
2105.09146,data,161,,,"SINDy with noise. SINDy receives a systems coordinates X and their derivatives ˙X as input. If ˙X is unknown it can be approximated from X with methods such as the ﬁnite difference. This works well for noiseless cases as in the mass-spring example. However, introducing noise into this system can hurt SINDy’s performance. This is illustrated in Figure 9, with the introduction of noise the baseline SINDy models fails to extrapolate past zero. Noise reduction approaches applied to X such as smoothing or the total variation regularization [31] numerical differentiation method applied in [29] can improve SINDy’s performance on noisy data. These approaches however do not utilize prior knowledge of a systems physical constraint in noise regulation. In the next section we will discuss the use of physical constraint embedded neural networks as an additional tool for regulating noisy input for SINDy."
2105.09146,data,17,,,[28] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data.
2105.09146,data,170,,,"components. We evaluated this approach on centered data generated from a selection of even, odd, and neither even nor odd functions. Here we demonstrated that our approach successfully inferred the even or odd symmetry without prior knowledge of the underlying symmetry and decomposed neither even nor odd functions into their even and odd components. We also demonstrated the noise resilient properties of physical constraint embedded neural networks and showed that they could be utilized as physics-informed noise regulators for symbolic regression. Here we utilized a conservation of energy embedded network as a preprocessing noise ﬁlter for a SINDy approach. We evaluated this approach on two simple noisy dynamic systems. Our physical constraint informed noise regulation approach returned symbolic equations for the systems time evolutions in line with the underlying equations and outperformed a baseline model. We believe that extending and incorporating physical constraint embedded neural networks into model architectures can provide a strong tool for inference and physics-informed noise reduction."
2105.09146,data,20,,,"data. Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(12):121107, Dec 2019."
2105.09146,data,202,,,"Conservation laws such as conservation of energy, momentum, mass, and electric charge are foundational concepts for understanding the behavior of classical physical systems and are intimately related to symmetry. Symmetry in physics refers to physical or mathematical features of a physical system that remain invariant during shifts in time or space. Even–odd symmetry refers to symmetry across the y-axis. An even function, such as such as x2 and cos(x), displays mirror symmetry whereas an odd function, such as such as x3 and sin(x), displays rotational symmetry. Given prior knowledge of a target system’s underlying even or odd symmetry, it may prove useful to provide this information directly to a neural network. Mattheakis et al.[15] proposed an even–odd hub neuron approach as a means of embedding these symmetries in neural networks. These hub neurons replaced the last hidden layer in a standard multilayer perceptron (MLP) and decomposed the network’s output to return the desired even or odd component. This decomposition proved resilient to noisy data and outperformed a standard feed-forward network."
2105.09146,data,214,,,"The even–odd decomposition network was able to successfully decompose a selection of neither even nor odd functions into their even and odd components. In Figure 3(c) the target function ex was decomposed by the network into its even and odd components the hyperbolic cosine and sine. In Figure 3(d) a target function containing polynomial terms and cosine and sine functions of varying frequencies was decomposed into its even and odd components. Decomposing a neural network parameterized function into its even and odd components may offer insight into the structure of the underlying function. This might have applications in equation discovery. For instance, an even–odd decomposition network could be used to split an unknown function into its even or odd components. These components could then be model separately with an equation discovery algorithm making use of the network-derived symmetry knowledge. The even–odd decomposition network is limited to centered data for non centered input data it does not guarantee the return of the simplest even or odd decomposition required to describe the data. Here a complexity metric may be needed to improve performance such as the number of terms present in a symbolic ﬁt on the even or odd components."
2105.09146,data,42,,,"Symbolic regression approaches, for the most part, ﬁt a model to data without regard to prior knowledge of a system’s underlying physical constraints. As in unconstrained neural networks, this can cause issues for extrapolation. In"
2105.09146,data,46,,,"[29] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identiﬁcation of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932– 3937, 2016."
2105.09146,data,62,,,"Figure 8: Hamiltonian neural networks as a noise regulator for SINDy. A HNN is ﬁrst trained on noisy coordinate data. The HNN model is then evaluated to return constraint informed coordinate predictions. Lastly, a SINDy model is ﬁt to the HNN coordinates predictions and a symbolic representation of the HNN parameterized time evolution functions is returned."
2105.09146,data,66,,,"Prior to this section, we have presented methods for embedding physical constraints into neural networks and neural network parameterizations of the underlying equations. In this section, we will present a data-driven method for discovering a system’s underlying symbolic equations i.e. symbolic regression, and propose the use of physical constraint embedded neural works as noise regulators for symbolic regression."
2105.09146,data,88,,,"From this formulation, the position and momentum can be evaluated at any time point through integration over time. Deriving the Hamiltonian formulation for a system from data can be challenging. Given data, it may be advantageous to parameterize a system’s Hamiltonian and time evolutions with a physics embedded modeling framework. Neural networks act as universal function approximators [16] and as such, they are a natural choice for parameterizing a system’s Hamiltonian and its time evolutions."
2105.09146,data,96,,,The even–odd hub neuron approach outperforms a standard feed-forward network and is resilient to noisy input data. This approach however suffers from limitations. It requires prior knowledge of a target system’s even or odd symmetry and cannot be applied to systems that are a mixture of even and odd components. To address these limitations we designed an Even–odd decomposition network for cases where the underlying symmetry is assumed to be unknown or is a mixture of even and odd components. Figure 2 shows the outline of the architecture.
2105.09146,data,97,,,Physical constraint embedded networks learn an underlying structure in the data. This allows them to return more robust and generalizable predictions than standard neural architectures. There are two common approaches used to embed physical constraints in neural networks. The ﬁrst involves structuring a network’s architecture to ensure the desired symmetry and the second involves placing constraints in the loss function to encourage the model to learn symmetry. We will present examples of both approaches and show that they can achieve similar results for embedding a generic even or odd symmetry.
2105.09146,"data, dataset",113,,,"The noisy observation data was used to train two models for each noise level: a HNN using the architecture from Section 3 and a baseline SINDy model. After training, each HNN model was used to generate 5000 coordinate predictions. These predictions were then used to ﬁt a SINDy+HNN model for each noise level. In total three models were returned for each noise level: a SINDy Baseline, a HNN, and a SINDy+HNN model. The SINDy+HNN approach is illustrated in Figure 8. For this example, the proposed SINDy+HNN approach to noise regulation is data-efﬁcient requiring only a small data set of 5k observations."
2105.09146,"data, dataset",117,,,"Neural network performance is highly dependent on the quantity and quality of the available training data. Experimental datasets are unavoidably noisy and scarce in quantity. This can limit a network’s ability to learn features and generalize. For neural networks modeling physical behaviors, this can be particularly problematic as it can lead to physically inconsistent predictions that violate governing laws. The black-box nature of the learned features and relations makes it challenging to assess if the network is accurately learning the underlying physics, constraints, and parameters from data. These issues of network’s interpretability and generalization ability limit their utility to model and simulate physical systems."
2105.09146,"data, dataset",174,,,"Neural networks often require large amounts of data to generalize and can be ill-suited for modeling small and noisy experimental datasets. Standard network architectures trained on scarce and noisy data will return predictions that violate the underlying physics. In this paper, we present methods for embedding even–odd symmetries and conservation laws in neural networks and propose novel extensions and use cases for physical constraint embedded neural networks. We design an even–odd decomposition architecture for disentangling a neural network parameterized function into its even and odd components and demonstrate that it can accurately infer symmetries without prior knowledge. We highlight the noise resilient properties of physical constraint embedded neural networks and demonstrate their utility as physics-informed noise regulators. Here we employed a conservation of energy constraint embedded network as a physics-informed noise regulator for a symbolic regression task. We showed that our approach returns a symbolic representation of the neural network parameterized function that aligns well with the underlying physics while outperforming a baseline symbolic regression approach."
2105.09146,dataset,163,,,"Implementation. A standard MLP and an MLP with an even–odd hub layer were implemented in PyTorch following the architecture described in [15]. The standard MLP model consisted of two fully connected layers of 5 neurons each, with sigmoid activations and one fully connected output layer. The MLP with the even–odd hub layer replaced the last hidden layer with an even–odd hub neuron. As in [15] we applied the models to a dataset generated from even (cosine) or odd (sine) functions with normally distributed noise N ∼ (µ = 0, σ = 0.2). The models were trained to minimize the mean squared error (MSE) of x(t) and ˆx(t). This loss function was then modiﬁed to include the symmetry metric term and an additional standard MLP model was trained. Figure 1 compares results for even symmetry function."
2105.09146,"open-source, package, python, data open-source , data",117,,,"[31] Rick Chartrand. Numerical differentiation of noisy, nonsmooth data. ISRN Applied Mathematics, 2011, 2011. [32] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445–22451, 2019. [33] Brian de Silva, Kathleen Champion, Markus Quade, Jean-Christophe Loiseau, J. Kutz, and Steven Brunton. Pysindy: A python package for the sparse identiﬁcation of nonlinear dynamical systems from data. Journal of Open Source Software, 5(49):2104, 2020."
2105.09146,python,142,,,"[24] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020."
2105.09146,python,149,,,"All SINDy models in this work were implemented using the PySINDy python library [33]. For each SINDy implementation, the same feature libraries were provided: a polynomial library containing terms up to the 2nd degree and a Fourier library limited to 1 frequency and containing both sine and cosine terms. The only argument that varied between the SINDy models was the threshold parameter provided to the STLSQ optimizer. Varying this parameter was necessary to ensure model performance and prevent the complete dropout of the terms in one model. For each model, the threshold parameter was selected as the value that dropped out the largest number of terms while maintaining the quality of the ﬁt. Each ﬁt was then integrated with an ODE solver to return the coordinate predictions. The same solver setup as in section 3 was used."
2105.09146,python,220,,,"Implementation. As in [11] we implemented two networks in PyTorch [22] a Baseline MLP Figure 4(a) and a HNN Figure 4(b). The Baseline MLP directly outputs the time derivatives and served as a point of comparison for the constraint embedded HNN network. The Baseline MLP consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 2 corresponding to ( dq dt ). The HNN consisted of two equation parameterizations, Equation 8 and 9. The former parameterized the Hamiltonian and consisted of two fully connected layers of 200 neurons each with Tanh activation functions and one fully connected output layer of size 1 corresponding to Hθ. The latter parameterized the time evolutions and used the python Autograd library [23] to return the partials of Hθ with respect to q and p, ( ∂H ∂q ). After training an ODE solver was used to integrate both the Baseline and HNN models to generate coordinate predictions. The SciPy [24] solve_ivp function was used to integrate the network with a tolerance of 1e-12 and a 4th order Runge-Kutta method was used for the solver [25]."
2105.09869,"code, github, code available",11,04/21/22,2,1The MATLAB code is available at https://github.com/amasoumi60/Robust Dynamic-Mode-Decomposition.
2105.09869,data,10,,,2) the sampled data are contaminated with outliers of
2105.09869,data,10,,,T HE sustained growth of data acquisition across all areas
2105.09869,data,100,,,"Fig. 11. Calculation of eigenvalues by the RDMD and the LTS-RDMD for the simple oscillator (64) in the following cases: (a) the measured data with noise and spikes; (b) discovered eigenvalues for noise-free data with spike levels µ = 1, p = 0.05; (c) discovered eigenvalues with spike levels µ = 1, p = 0.05 and Gaussian noise level η = 10−4; (d) discovered eigenvalues with spike levels µ = 1, p = 0.05 and Gaussian noise level η = 10−3"
2105.09869,data,107,,,"for k, j = {1, ..., N }, where lomed denotes a low median— that is, the (cid:0)(N +1)/2(cid:1)-th order statistic of N data points. Eq. (27) reads as follows: for each k, we compute the low median of |pk − pj| for j = {1, ..., N }. This yields N data points, the low median of which gives the ﬁnal estimate, (cid:98)s2. The factor 1.1926 is for consistency at the normal distribution."
2105.09869,data,111,,,"The problem of making the DMD robust to outliers is investigated. By casting the DMD problem in the robust statistics framework, it is solved by using a Schweppe-type Huber generalized maximum-likelihood estimator. The numerical results demonstrated the effectiveness of the proposed RDMD for a variety of dynamical systems when the sampled data are contaminated with outliers. Further, the proposed RDMD presented satisfactory performance in dealing with non-Gaussian noises. Finally, we noticed that the numerical results are signiﬁcantly improved by considering the symmetry of the problem; thus, we suggest the robustiﬁcation of the total least-squares method as a direction for future research."
2105.09869,data,117,,,"is necessary to modify the exact DMD method to construct a statistically robust DMD method. First, outliers must be detected and identiﬁed. To this end, we rely on projection statistics to derive weights over the interval [0, 1] that are used to bound the inﬂuence of outliers. Speciﬁcally, the farther an outlier is from the center of the data cloud, the smaller its assigned weight. All remaining data points not identiﬁed as outliers receive a weight equal to 1. These weights are incorporated into the Huber loss function to bound the inﬂuence of outliers in the estimation process. The details are presented next."
2105.09869,data,125,,,"The superior performance of the TDMD is attributed to the symmetric debiasing used by such a method, which makes it the best choice for noisy data. TDMD considers the uncertainties in both Y , Y (cid:48), so the problem becomes Y (cid:48) + ∆Y (cid:48) = A(Y + ∆Y ), whereas other methods consider only the uncertainties on the right-hand side of the problem. This explains why the TDMD method performs best for noisy data and, to a certain extent, suppresses the effect of outliers in the estimation process. The combination of the TDMD and the proposed N-RDMD methods is promising and will be addressed in future research."
2105.09869,data,125,,,"with µ = −0.05, λ = −1, and ρ(x1) = x2 1 ( [44]). In (60), the slow dynamics are dictated by the eigenvalue equal to −0.05, whereas the second state x2 quickly approaches the manifold x2 = ρ(x1). In this case, we simulate a case where the sampled data are contaminated with outliers of magnitude 0.2 between t = 1 and t = 1.05 seconds and between t = 2 and t = 2.05 seconds. Fig. 6 depicts the estimated eigenvalues. We observe that the N-RDMD performs best in capturing the eigenvalue equal to −0.05 and that corresponds to the slow dynamics."
2105.09869,data,126,,,"yk := g(cid:0)x[k](cid:1), where yk ∈ Rm is a vector of m measurements on (1) at time k. Note that, in principle, any measurement is a function of the state, x. In some applications, the measurement set is the state itself, such that yk = xk, and m = n. In this paper, we consider the more general case deﬁned in (4), where m (cid:54)= n. Suppose that one collects sampled measurements of (1) at time instances k = {0, 1, ..., N }. Deﬁne the data matrices as follows:"
2105.09869,data,158,,,"where Θ is a random matrix of the form randn(m, m) − hIm, and h is chosen such that all eigenvalues reside in the left half of the complex Cartesian plane. The data are collected for N = 200 time samples. The order of the truncated dynamics is considered to be c(cid:48) = 25 for all the simulated DMD methods. Note that ODMD [31] is not included in this case because no solution can be attained. Fig. 8 shows that the exact DMD and TDMD methods perform best when the data are outlier free; however, the N-RDMD outperforms other methods when the data are contaminated with outliers. Note that the TDMD estimates the eigenvalues with positive real parts, indicating unstable dynamics; therefore, we do not plot the TDMD response in Fig. 8(d)."
2105.09869,data,17,,,We investigate three cases as follows: 1) the sampled data are free of outliers;
2105.09869,data,17,,,"[2] S. L. Brunton and J. N. Kutz, Data-Driven Science and Engineering: Cambridge"
2105.09869,data,179,,,"a chi-square distribution with m degrees of freedom, χ2 m; hence, there is a probability of 1 − α that a data point pk such that d2 m,1−α is located within an ellipsoid given by d2 M = χ2 m,1−α that is centered at (cid:98)µ. This provides the rationale used to tag outliers—that is, an outlier is any data point for which the Mahalanobis distance is larger than a threshold, e.g., (cid:0)χ2 . But because dM is calculated via non-robust estimators of location and scale, it is vulnerable to the masking effect of multiple outliers, especially when the latter appear in clusters [37]. In other words, the corresponding ellipsoid is inﬂated to the point that it encompasses outliers, which can no longer be identiﬁed. To gain robustness, one can replace the sample mean and the sample covariance matrix in (21) by robust estimators of location and scale, respectively. This is discussed next."
2105.09869,data,18,,,"[1] T. Hey, S. Tansley, and K. Tolle, The Fourth Paradigm: Data-Intensive"
2105.09869,data,186,,,"The measurement snapshots are contaminated with the additive deviation ηw (t) + µs (t), where w(t) is the Gaussian noise. Also, the elements of the vector s(t) are obtained by multiplying a Bernoulli trial with small expectation p by a standard normal, which leads to a sparse noise; therefore, the snapshots are contaminated with a base Gaussian noise and some spikes of size µ with ﬁring rate p ( [28]). The comparison between our RDMD and the LTS-RDMD to discover the eigenvalues of the system is given in Fig. 11. Note that we repeated the simulations 200 times for each method. Simulations are performed for ﬁxed µ = 1 and p = 0.05 and for two different Gaussian noises levels: σ = 10−4 and σ = 10−3. It is observed that both methods can efﬁciently discover the true eigenvalues when the data are contaminated by the spikes (outliers). Also, in a number of simulations, the LTS-RDMD"
2105.09869,data,201,,,"Although the estimation of (cid:101)A from (55) is robust to outliers, it is not guaranteed that it includes all the dominant modes in the data. In other words, although each eigenvalue of (cid:101)A is also an eigenvalue of the original A, there is the possibility that the dominant eigenvalues are not included. This is mainly determined by how one selects the reduction matrix, T . An option is to choose T = U , as is done for the standard DMD method. Note that as previously explained, U is not a unitary matrix; therefore, robustness is ensured. On the other hand, because there are outliers in the data matrix, Y , such a reduction might not be able to cover all dominant modes. It is observed that for a low percentage of outliers among the data points, this selection captures the dominant eigenvalues. Another option is to preprocess the data matrices before performing the singular value decomposition. A comprehensive investigation on the choice of the similarity transformation in RDMD will be addressed in future research."
2105.09869,data,27,,,"Next, we assess the performance of the N-RDMD for larger data sets. First, we consider a random linear system of the form given by"
2105.09869,data,30,,,Fig. 4. Network of coupled oscillators when the sampled data are outlier free: (a) eigenvalues; (b) reconstruction of the state x2.
2105.09869,data,32,,,3) the sampled data are contaminated with outliers of magnitude 0.3 from t = 1 to t = 1.05 seconds and from t = 2 to t = 2.05 seconds.
2105.09869,data,33,,,"Hence, it is expected that a least-squares estimator provides strongly biased results when the samples contained in the data matrices Y and Y (cid:48) are contaminated with outliers. As"
2105.09869,data,34,,,"[4] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor, Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems. SIAM, 2016."
2105.09869,data,37,,,"[12] A. M. Avila and I. Mezi´c, “Data-driven analysis and forecasting of highway trafﬁc dynamics,” Nature Communications, vol. 11, no. 1, p. 2090, 2020."
2105.09869,data,41,,,"[6] P. Schmid and J. Sesterhenn, “Dynamic mode decomposition of numerical and experimental data,” in Sixty-First Annual Meeting of the APS Division of Fluid Dynamics, San Antonio, Texas, USA, 2008."
2105.09869,data,43,,,Fig. 5. Network of coupled oscillators when the sampled data are contaminated with outliers of magnitude 0.1 between t = 1 and t = 1.05 seconds: (a) eigenvalues; (b) reconstruction of the state x2.
2105.09869,data,43,,,"The detection and identiﬁcation of outliers are key steps in robust statistics. In statistical analysis, several methods have been proposed to detect an outlier based on its distance from the majority of the data point cloud, as explained next."
2105.09869,data,44,,,"[47] M. Netto and L. Mili, “A robust data-driven Koopman Kalman ﬁlter for power systems dynamic state estimation,” IEEE Transactions on Power Systems, vol. 33, no. 6, pp. 7228–7237, 2018."
2105.09869,data,44,,,"where (cid:98)µ = ((cid:98)µ1, ..., (cid:98)µN )T and (cid:98)S are, respectively, the sample mean and the sample covariance matrix of the data points in P."
2105.09869,data,46,,,"ω = [ω1 ... ωs]T, and D = diag(d1, ..., ds). The data are collected with a sample time of 0.01 second. The damping, dk, is set to 0.05 for all k."
2105.09869,data,49,,,"Deﬁnition 1 (Mahalanobis distance). Let a multivariate data set, P ⊆ Rm, be {p1, ..., pN }. The Mahalanobis distance between a data point, pk, and the data cloud comprising all data points in P is deﬁned as"
2105.09869,data,49,,,"In the multivariate case, however, it is challenging to align all multivariate data points such that a meaningful measure of distance can be obtained. A solution to this challenge stems from the fact that the Mahalanobis distance can be written as follows [39]:"
2105.09869,data,50,,,Fig. 6. Slow-manifold system when the sampled data are contaminated with outliers of magnitude 0.2 between t = 1 and t = 1.05 seconds and between t = 2 and t = 2.05 seconds. (a) Eigenvalues. (b) Reconstruction of state x2.
2105.09869,data,56,,,"Fig. 9. Estimated eigenvalues and reconstruction of state x2 for the generalized slow-manifold system in (63): (a), (b) outlier-free data; (c), (d) Sampled data contaminated with outliers of magnitude 0.2 from t = 1 to t = 1.05 seconds."
2105.09869,data,57,,,"Fig. 8. Estimated eigenvalues and reconstruction of state x2 for the random linear dynamical system in (62): (a), (b) outlier-free data; (c), (d) sampled data contaminated with outliers of magnitude 0.2 from t = 1 to t = 1.05 seconds."
2105.09869,data,58,,,"Yet, another challenge encountered in calculating the projection statistics is that considering all possible directions of v cannot be realized. To address this, Gasko and Donoho [42] suggest considering only the directions that originate from the coordinate-wise median vector, vmed, and that pass through each data point pk, yielding"
2105.09869,data,63,,,Fig. 10. State reconstruction of linear system (59) when the sampled data are contaminated with (a) Gaussian noise with variance of 0.01; (b) Laplace noise with variance of 0.01; (c) Student-t noise with 2 degrees of freedom; (d) Cauchy noise with half-width at half-maximum γ = 2.
2105.09869,data,65,,,"It can be shown (see [18]) that for a scalar b ∈ R, the set of data points for which d2 M < b lies inside an ellipsoid with the center at (cid:98)µ. Moreover, if the data points in P ⊆ Rm follow a multivariate normal distribution, then the values of d2 M follow"
2105.09869,data,67,,,"Fig. 7. Estimated eigenvalues and reconstruction of state x2 for the Van der Pol oscillator in (61): (a), (b) outlier-free data; (c), (d) sampled data contaminated with outliers of magnitude 0.3 from t = 1 to t = 1.15 seconds and from t = 3 to t = 3.05 seconds."
2105.09869,data,74,,,"[7] P. J. Schmid, “Dynamic mode decomposition of numerical and experimental data,” Journal of Fluid Mechanics, vol. 656, p. 5–28, 2010. [8] Y. Susuki and I. Mezi´c, “Nonlinear Koopman modes and coherency identiﬁcation of coupled swing dynamics,” IEEE Transactions on Power Systems, vol. 26, no. 4, pp. 1894–1904, 2011."
2105.09869,data,75,,,"The results depicted in Fig. 9 demonstrate the ability of all DMD methods to capture the dominant eigenvalues, located at µi, and to reconstruct the state x2 when the data are outlier free. Further, as shown in Fig. 9, the N-RDMD outperforms the DMD and the TDMD when the sampled data are contaminated with outliers of magnitude 0.1 from t = 1 to t = 1.05 seconds."
2105.09869,data,8,,,F. Dynamic Mode Decomposition of Large Data Sets
2105.09869,data,83,,,"[41] C. Croux and P. J. Rousseeuw, “Time-efﬁcient algorithms for two highly robust estimators of scale,” in Computational Statistics, Y. Dodge and J. Whittaker, Eds. Springer-Verlag, 1992, vol. 1, pp. 411–428. [42] M. Gasko and D. L. Donoho, “Inﬂuential observation in data analysis,” American Statistical Association Proceedings of the Business and Economic Statistics Section, pp. 104–109, 1982."
2105.09869,data,85,,,"[43] R. A. Willoughby, “Solutions of Ill-Posed Problems (A. N. Tikhonov and V. Y. Arsenin),” SIAM Review, vol. 21, no. 2, pp. 266–267, 1979. [44] E. Kaiser, J. N. Kutz, and S. L. Brunton, “Data-driven discovery of koopman eigenfunctions for control,” Machine Learning: Science and Technology, vol. 2, no. 3, p. 035023, 2021."
2105.09869,data,88,,,"where the sample mean of the data points in P, (cid:98)µP, is used as an estimator of location and the sample standard deviation of the data points in P, (cid:98)σP, is used as an estimator of scale. Note that d(pk) is often referred to as the z-score of the data point pk. The classic measure of distance given by (20) is generalized to the multivariate case by the Mahalanobis distance."
2105.09869,data,9,,,center of the data cloud is given by:
2105.09869,data,94,,,"Table II provides the eigenvalues of (cid:98)A computed for each case. Furthermore, the reconstruction of x2 in Cases 1 and 3 is depicted in Fig. 3. Table II shows that the accuracy of the RDMD is slightly less than that of other DMD methods in the outlier-free case. As discussed in previous sections, the RDMD presents high statistical efﬁciency under ideal scenarios while being robust to deviations from assumptions about the data. Indeed, the RDMD performs best for higher percentages of outliers."
2105.09869,data,96,,,"Next, we consider the network of coupled oscillators deﬁned in (58). Figs. 4 and 5 show the performance of the considered DMD methods. Fig. 4 shows that all methods capture the dominant eigenvalue of the system in an ideal case without outliers. Further, all methods yield an accurate reconstruction of state x2 = ω2. Conversely, Fig. 5 shows that the N-RDMD performs best when the sampled data are contaminated with outliers of magnitude 0.1 between t = 1 and t = 1.05 seconds."
2105.09869,data,98,,,"As is the case for the Mahalanobis distance, the projected statistics approximately follow a chi-square distribution with m degrees of freedom when the data points follow a multivariate normal distribution [18]; hence, the projection statistic of each data point, pk, is calculated, and if they exceed a threshold, e.g., d2 2,0.975, then the associated data point is tagged as an outlier. Note that in this paper, the data points, pk, are the columns of the data matrices given by (5)."
2105.09869,data,98,,,"The ﬁrst steps of the standard DMD method are outlined in Section II-B, where c denotes the rank of Y . Note that it is common to assume c ≤ m < N for large data sets. Moreover, particularly for model order reduction, one is interested in a projection (cid:101)A ∈ Rc(cid:48)×c(cid:48) , where c(cid:48) < c. In this case, one disregards (c−c(cid:48)) nonzero elements of Σ and (c−c(cid:48)) columns"
2105.09869,"data, data available",141,,,"A wealth of data science methods have been developed by researchers and made available to practitioners. Dynamic mode decomposition (DMD) stands out because of its connection with the Koopman operator theory [3], which reconciles data analysis and the mathematical knowledge of dynamical systems; the reader is referred to [4], [5] for more details. Since the publication of the paper authored by Schmid and Sesterhenn [6], [7], DMD has become the mainstream method for data-driven modeling of dynamical systems, mainly applied to ﬂuid mechanics [3], electric power grids [8], neuroscience [9], ﬁnance [10], climate science [11], and transportation [12], to name a few."
2105.09869,"data, data available",49,,,"of human activity is a crucial driver for the research and development of data science methods [1], [2]. This fact especially applies to complex dynamical systems for which ﬁrst-principles models are challenging to obtain while a large amount of data are available."
2105.09869,"data, dataset",131,,,"Abstract—This paper develops a robust dynamic mode decomposition (RDMD) method endowed with statistical and numerical robustness. Statistical robustness ensures estimation efﬁciency at the Gaussian and non-Gaussian probability distributions, including heavy-tailed distributions. The proposed RDMD is statistically robust because the outliers in the data set are ﬂagged via projection statistics and suppressed using a Schweppe-type Huber generalized maximum-likelihood estimator that minimizes a convex Huber cost function. The latter is solved using the iteratively reweighted least-squares algorithm that is known to exhibit a better convergence property and numerical stability than the Newton algorithms. Several numerical simulations using canonical models of dynamical systems demonstrate the excellent performance of the proposed RDMD method. The results reveal it outperforms several other methods proposed in the that literature."
2105.09869,"data, dataset",192,,,"It turns out that solving the sensitivity of DMD to deviations from the assumptions made about the data set is a challenging task [20] due to the vulnerability of the least-squares estimator to non-Gaussian noise and outliers, which is a great concern to practitioners. This fact motivated several independent investigations to assess the accuracy of the DMD in capturing the underlying system dynamics directly from the data set [21]–[24]. For instance, Dawson et al. [25] and Hemati et al. [26] address, respectively, the bias introduced by Gaussian noise and the bias resultant from asymmetrically processing snapshots. In Section IV of this paper, numerical experiments conﬁrm that the DMD variant proposed by Hemati et al. [26] has excellent performance in the presence of Gaussian noise and has good performance in the presence of non-Gaussian but symmetrically distributed noise. This is achieved thanks to a reformulation of the DMD using a total least-squares estimator [27]; however, this estimator is still vulnerable to outliers."
2105.09869,"data, dataset",224,,,"The vulnerability of the least-squares estimator to outliers is not directly solvable without data preprocessing; therefore, Askham et al. [28] reformulate DMD as an optimization problem and make use of a least trimmed squares (LTS) estimator, speciﬁcally the trimmed M-estimator introduced by Rousseeuw [29]. To the best of the authors’ knowledge, [28] is the only formulation of DMD that makes use of a robust estimator. In particular, the LTS estimator has a high breakdown point [29]—that is, this estimator is very robust from a statistical standpoint; however, the formulation in [28] lacks a mechanism to identify outliers. Indeed, identifying outliers without access to a system model is challenging but necessary in DMD. The formulation in [28] circumvents this challenge by making a blanket assumption that the time-series data can be represented “by the outer product of a matrix of exponentials, representing Fourier-like time dynamics, and a matrix of coefﬁcients, representing spatial structures.” Consequently, nonexponential dynamics in the data set are, therefore, classiﬁed as outliers. This fact precludes the application of the method proposed in [28] to dynamical systems that present nonexponential dynamics."
2105.09869,"data, dataset",267,,,"In the classic literature in robust statistics [16], [17], one deﬁnes robustness as insensitivity to deviations from the assumptions. In this sense, the least-squares estimator is not robust. Two cases of deviations from the assumptions are of particular concern. The ﬁrst case arises when the probability distribution of the observations is not Gaussian. The leastsquares estimator quickly loses its statistical efﬁciency (that is, accuracy) when the tails of the probability distribution of the observations become slightly thicker than the Gaussian distribution or when the probability distribution of the observations becomes slightly asymmetric. The second case arises when the probability distribution of the majority of the observations is Gaussian except for a few observations, which may take arbitrary values. In this respect, one deﬁnes an outlier as a data point that violates the underlying assumptions—in other words, it is a data point that is distant from the majority of the point cloud [18]. The least-squares estimator produces strongly biased results in the presence of a single outlier in the data set [19]. Both cases of deviations from the assumptions often occur in practice—for example, when the probability distribution of the observations is not known while being assumed to be Gaussian or when outliers arise because of instrumentation and communications errors or a poor experimental setup. This fact precludes the DMD from being applied to practical settings, especially for control purposes where pre-cleaning the data set is not an option."
2105.09869,"data, dataset",59,,,"Deﬁnition 2 (Median absolute deviation from median in the case of a univariate data set). Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A very robust estimator of scale [38] is the median absolute deviation from the median, which is deﬁned as"
2105.09869,"data, dataset",86,,,"Deﬁnition 3 (Median absolute deviation from the median in the case of a multivariate data set). Let a multivariate data set, P ⊆ Rm, be {p1, ..., pN }. The median absolute deviation from the median is deﬁned as (cid:0) (cid:12) (cid:12)pT for k, j = {1, 2, ..., N }, where v is the direction to which the data points are projected."
2105.09869,"data, dataset",87,,,"Let a univariate data set, P ⊆ R, be {p1, ..., pN }. A measure of the distance between a data point, pk ∈ P, and the center of the data cloud is given by pk−(cid:98)(cid:96) , where (cid:98)(cid:96) denotes an estimator of location, and (cid:98)s denotes an estimator of scale. A classic measure of distance in the univariate case is provided by"
2105.09869,"data, dataset, data available",116,,,"and the results are depicted in Fig. 7. Note that for the data collected from nonlinearly evolving signals, the approximated DMD modes are reﬂecting the behavior of the most dominant Koopman modes. As shown in Fig. 7, when there is no outlier contamination within the data set, all considered DMD methods calculate approximately the same dominant eigenvalues, and a good response of x2 is reconstructed; however, when there are some outliers among the data set, only N-RDMD can capture the same eigenvalues as found for the outlier-free data. In other words, the process of ﬁnding eigenvalues has been made robust against outliers."
2105.09869,dataset,56,,,"[26] M. S. Hemati, C. W. Rowley, E. A. Deem, and L. N. Cattafesta, “De-biasing the dynamic mode decomposition for applied Koopman spectral analysis of noisy datasets,” Theoretical and Computational Fluid Dynamics, vol. 31, no. 4, pp. 349–368, 2017."
2105.10037,code,2,,,A. Pseudo-code
2105.10037,code,68,,,"∈P Next, the learnt inverse model is used to augment ˆ with agent speciﬁc actions. Finally, these action augmented trajectories are used to learn the ﬁnal policy πTA via behavioral cloning. Note that our correspondence learning framework is agnostic to the imitation from observation algorithm used for learning the agent policy. The pseudo-code for training our framework is presented in Appendix A."
2105.10037,"code, github, code available",73,04/21/22,2,"In this section, we analyze the efﬁcacy of our proposed method on the xDIO task. We adopt MuJoCo (Todorov et al., 2012) as the experimental test-bed and evaluate on several cross-domain tasks, along with a thorough ablation study of different modules in our overall framework. Implementation details are presented in Appendix B. Code and videos are available at: https://driptarc.github. io/xdio.html."
2105.10037,data,101,,,"not be good enough given that our experiments involve didifferences in demonstration verse starting states, up to 1.5 × lengths, and varying task execution rates. Secondly, baselines which learn from unpaired data (CCA and CycleGAN), also fail due to the lack of a mechanism to preserve MDP task characteristics, which is taken care of in our method via temporal order preservation and domain alignment. Figure 6 illustrates the learnt state-maps for some of the crossdomain tasks. The proposed framework translates the expert states in a manner that preserves task semantics."
2105.10037,data,112,,,"(a) We propose an algorithm for cross-domain imitation learning by learning transformations across domains, modeled as Markov Decision Processes (MDP), from unpaired, unaligned, state-only demonstrations, thereby ameliorating the need for costly paired, aligned data. (b) Unlike previous work, neither do we utilize any costly RL procedure, nor do we require interactive querying of an expert policy. (c) We adopt multiple tasks in the MuJoCo physics engine (Todorov et al., 2012), and show that our framework can ﬁnd correspondences and align two domains across different viewpoints, dynamics and morphologies."
2105.10037,data,117,,,"Domain invariant latent space. To incentivize ψ, φ to generalize beyond proxy tasks, we use an encoder-decoder structure for the transformation function ψ. Concretely, represents an encoder ψ = DE ◦ SE → Z which maps a state in the expert domain to a domain agnostic latent space Z → SA represents the decoding EA is deﬁned similarly via the same function. φ = DA ◦ . Prior work (Gupta et al., 2017) has explored latent space learning such invariant spaces, but use paired data from both domains, which is a very strong and often unrealistic assumption, as explained above. Inspired from work based"
2105.10037,data,123,,,"Figure 2. Framework overview. An illustration of our MDP correspondence learning framework. We perform local alignment via state-transition distribution matching and cycle-consistency in the state space using Lj cyc, as well as in a learnt latent space using Lj M I (only proxy task is j shown here). The inverse cycle from agent to expert is omitted here for clarity. Global alignment is performed via consistency on the temporal position of states across the two domains, using the pre-trained position estimators P j A, P j E in Lj cyc inf - this prevents overﬁtting to the proxy tasks and makes the learnt transformation more robust and well-conditioned to the target data."
2105.10037,data,138,,,"We compare our framework to other methods which are able to learn state correspondences from unpaired and unaligned demonstrations without access to expert actions - Canonical Correlation Analysis (Hotelling, 1992), Invariant Features (Gupta et al., 2017) and CycleGAN (Zhu et al., 2017). Canonical Correlation Analysis (CCA) (Hotelling, 1992) ﬁnds invertible linear transformations to a space where domain data are maximally correlated when given unpaired, unaligned demonstrations. Invariant Features (IF) learns state maps via a domain agnostic space from paired and aligned demonstrations - we use Dynamic Time Warping (M¨uller, 2007) on the learned latent space to compute the pairings from the unpaired data. CycleGAN learns the state correspondence via adversarial learning with an additional"
2105.10037,data,287,,,"In this work, we propose a novel framework to tackle the xDIO problem, encompassing morphological, viewpoint and dynamics mismatch. We follow a two-step approach (see Fig. 1), where we ﬁrst learn a transformation across the domains using the proxy tasks (Gupta et al., 2017), followed by a transfer process and subsequent learning of the policy. Importantly, in contrast to previous work, we use unpaired and unaligned trajectories containing only states on the expert domain trajectories, to learn this transformation. Additionally, we do not assume any access to the expert policy or the expert domain except for the given demonstrations. To learn the state correspondences, we jointly minimize a divergence between the transition distributions in the state space as well as in the latent space between the expert and the agent proxy task trajectories, while learning to translate between the two domains with the unpaired data via cycle-consistency (Zhu et al., 2017). However, solely learning with such state cycle-consistency may only result in local alignment, and lead to difﬁculties in optimizing for complex environments. Thus, to impose global alignment, we enforce additional consistency on the temporal position of states across the two domains. This ensures that when a state is mapped from one domain to the other, the degree of completion associated by being in that state remains unchanged. Having learnt this mapping on the proxy tasks, we transfer demonstrations for a new inference task from the expert to the agent domain, which are subsequently utilized to learn a policy via imitation."
2105.10037,"data, dataset provided",131,,,"In this paper, we present a novel framework to tackle the xDIO task by learning a state-map across domains using both local and global alignment. Local alignment is performed via transition distribution matching and cycleconsistency in both the state and latent space, while global alignment is enforced via the idea of temporal position preservation. While previous approaches rely on paired data and expert actions, we provide a general framework that can learn the mapping from unpaired, unaligned demonstrations without expert actions. We demonstrate the efﬁcacy of our approach on multiple cross-domain tasks encompassing dynamics, viewpoint and morphological mismatch. Our future work will concentrate on extending our method for learning correspondence using random trajectories, thus mitigating the need for proxy tasks."
2105.10037,dataset,115,,,"The objective of xDIO is to learn an optimal policy πTA in in the agent domain, given state-only demonstrations the expert domain. In this paper, we propose to ﬁrst learn a transformation ψ : SE → SA between the domains and then leverage ψ to imitate from the expert demonstrations. Following prior work (Gupta et al., 2017; Liu et al., 2018; Kim et al., 2020), we assume access to a dataset consisting of expert-agent trajectories for M different proxy tasks: M j=1. Proxy tasks encompass simple ) D } primitive skills in both domains and are different from the inference task"
2105.10037,dataset,147,,,"We use the learned ψ to map the states in the inference task expert demonstrations to the agent domain. Given DM the set of transferred state-only demonstrations ˆ , we can use any imitation from observation algorithm to learn the ﬁnal policy. In this work, we follow the Behavioral Cloning from Observation (BCO) approach proposed in (Torabi et al., 2018). BCO entails learning an inverse dySA × SA → AA to infer missing acnamics model tion information. First, we collect a dataset of state-action A, st+1 triplets by random exploration. The A ) } inverse model is subsequently estimated by Maximum Like. lihood Estimation (MLE) of the observed transitions in P Assuming a Gaussian distribution over actions, this reduces to minimizing an (cid:96)2 loss as follows,"
2105.10037,dataset,63,,,"Given the alignment dataset D containing trajectories from the M proxy tasks, we ﬁrst pre-train the temporal position M j=1 using Equation 6. This is folestimators lowed by adversarial training of the state maps ψ, φ, where we use separate discriminators on the state space and latent space for each proxy task. The full objective is then:"
2105.10702,data,111,,,"The burden on diagnostic radiologists has been increasing quite rapidly with the advancement of a variety of modern imaging modalities, which, while allowing for higher resolution images in 3D and even 4D, dramatically increase the complexity of the diagnostic process. It has become common for radiologists to rely on various image analysis and automated decision support systems to facilitate the interpretation process. These computer aided diagnostic systems, or CAD, have been able to make great advances with the help of machine learning algorithms and a large amount of clinical imaging data, and have been successfully tested in clinical settings [1–4]."
2105.10702,data,121,,,"In the ﬁeld of computer vision, the use of human generated visually descriptive text to infer the contents of an image has primarily been applied to image caption generation. Earlier models relied on linking template-based language models to objects and spatial contexts in the image [6–8]. More recently, interest has moved to the combined potential of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for describing images using natural language [9, 5, 10–12]. The advantage of using neural networks for caption generation is that the model is not constricted by hard-coded language templates and is able to learn more freely from the training data."
2105.10702,data,50,,,"Table 1: BLEU n-gram and METEOR scores evaluated on report generation model trained on single image inputs (Baseline), max-aggregate of image features, and max-aggregate of image features extracted from BBox detections. Evaluated on training (tr) and test (te) data."
2105.10702,"data, data available",114,,,"Many of these machine learning algorithms are supervised learning approaches that require large amounts of annotated image data for training. Gathering suitable data is especially challenging in the medical domain as it is incredibly time consuming for radiologists to generate ground-truths of the standard and volume required for training a predictive model. An alternative approach is to use past clinical images and corresponding radiological reports available through a hospital’s picture archiving and communication system (PACS); the advantage being that, although this data is largely unstructured (free text), it is available to us in high volumes and removes the need for manual annotation."
2105.10702,database,14,,,"hierarchical image database. In CVPR, pages 248–255. IEEE, 2009."
2105.10702,database,55,,,"[13] Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, and Ronald M Summers. Interleaved Text / Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation. Journal of Machine Learning Research (JMLR), 17:1–31, 2016. doi: 10.1109/CVPR.2015.7298712."
2105.10702,dataset,189,,,"The knee X-ray dataset has been extracted from the PAC system of St Thomas Hospital (part of Guys and St Thomas NHS Foundation Trust) and has been fully anonymised to remove sensitive patient information. It comprises a total of 330 knee X-ray exams collected over the years 2015 and 2016. Each exam consists of a textual report and one or more X-ray images (left/right knees or both, taken from different views: anteroposterior (AP), lateral (L) and skyline (S), and different positions: weight-bearing (WB) and non-weight-bearing(nonWB)). The most common exam consists of both AP and L views of left and right knees separately, making up 42% of total exams. The reports vary in length between 2 and 145 words, with an average of 30 and standard deviation of 18.7; and between 1 and 16 sentences with an average of 2.7 per report. The X-ray images vary in sizes between 420 × 650 × 3 and 3056 × 3056 × 3."
2105.10702,dataset,2,,,3 Dataset
2105.10702,dataset,93,,,"We adopt the GoogLeNet [18] CNN architecture, pre-trained on the ImageNet dataset [19], and extract image features of each X-ray image view (V1 - VK) from the last spatial average pooling layer (R1024). The maximum value of each feature is aggregated across the exam images to create a ﬁxed-size input to the RNN of dimension R1024, which is then passed through a fully connected layer in order to reduce the dimension to R256, equal to the RNN input size."
2105.12306,data,188,,,"We compare REALISE with the following baselines: KUAS (Chang et al., 2015), NTOU (Chu and Lin, 2015), NCTU-NTUT (Wang and Liao, 2015), HanSpeller++ (Zhang et al., 2015), LMC (Xie et al., 2015) mainly utilize heuristics or traditional machine learning algorithms, such as n-gram language model, Conditional Random Field and Hidden Markov Model. Sequence Labeling (Wang et al., 2018) treats CSC as a sequence labeling problem and applies a BiLSTM model. FASpell (Hong et al., 2019) utilizes a denoising autoencoder (DAE) to generate candidate characters. Soft-Masked BERT (Zhang et al., 2020) utilizes the detection model to help the correction model learn the right context. SpellGCN (Cheng et al., 2020) incorporates the predeﬁned character confusion sets to the BERT-based correction model through Graph Convolutional Networks (GCNs). BERT (Devlin et al., 2019) is to directly ﬁne-tune the BERTBASE model with the CSC training data."
2105.12306,data,31,,,"Finally, we load the pretrained weights of the semantic encoder, phonetic encoder, and graphic encoder, and conduct the ﬁnal training process with the CSC training data."
2105.12306,data,4,,,4.1 Data and Metrics
2105.12306,data,53,,,"Following previous works (Wang et al., 2019; Cheng et al., 2020), we use the SIGHAN training data and the generated pseudo data (Wang et al., 2018, denoted as Wang271K) as the training set. We evaluate our model on the SIGHAN test sets"
2105.12306,data,61,,,"Chinese input methods do. We add a linear layer on the top of the encoder to transform the hidden states to the probability distributions over the Chinese character vocabulary. We pretrain the phonetic encoder with the pinyin of the sentences with spelling errors in the training data, and make it recover the character sequences without spelling errors."
2105.12306,"data, dataset",33,,,Table 2: Statistics of the used datasets. All the training data are merged to train the REALISE model. The test sets are used separately to evaluate the model performance.
2105.12306,"data, dataset",69,,,"in 2013, 2014 and 2015 (denoted as SIGHAN13, SIGHAN14 and SIGHAN15). Table 2 shows the data statistics. Originally, the SIGHAN datasets are in the Traditional Chinese. Following previous works (Wang et al., 2019; Cheng et al., 2020; Zhang et al., 2020), we convert them to the Simpliﬁed Chinese using the OpenCC tool2."
2105.12306,dataset,1,,,Dataset
2105.12306,dataset,14,,,Table 6: Ablation results of the REALISE model on each SIGHAN dataset.
2105.12306,dataset,28,,,"Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018. Being negative but constructively: Lessons learnt from creIn ating better visual question answering datasets."
2105.12306,dataset,284,,,"""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!! ""!#OITAnd the Yankees’ acepitcherThe number ofbabiesbornto womencontinues todecline(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:18415)(cid:12787)(cid:12713)!!!""!#OIT(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:16855)(cid:20043)(cid:7529)(cid:15919)!!! ""!#OITDataset"
2105.12306,dataset,383,,,"(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10449)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)1.001.001.001.001.000.971.000.991.001.001.001.001.000.200.200.140.340.180.130.370.250.190.310.250.230.170.090.040.130.120.080.060.520.090.040.040.060.080.05(cid:12655)(cid:12721)(cid:19189)(cid:23869)(cid:12655)(cid:17890)(cid:10450)(cid:13929)(cid:9001)(cid:8985)(cid:18025)(cid:17555)(cid:11983)!!! ""!#OIIplan to watchamoviewith mygirlfriendT(cid:10356)(cid:24485)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)0.991.001.001.000.991.000.991.000.991.000.990.260.230.240.170.370.340.210.210.210.300.210.220.620.240.070.180.230.090.060.040.050.05(cid:10356)(cid:8148)(cid:7613)(cid:13709)(cid:7724)(cid:24375)(cid:7560)(cid:16407)(cid:10926)(cid:9007)(cid:17533)The affair  also  happened  from  this  point!!!""!#OIT(cid:18619)(cid:7870)(cid:11870)(cid:12065)1.001.001.001.000.600.340.370.520.100.060.090.06(cid:8711)(cid:7870)(cid:11870)(cid:8402)!!! ""!#OITWish you happy(cid:20330)(cid:7538)(cid:15465)(cid:10072)(cid:25981)(cid:17890)(cid:17129)(cid:16810)(cid:12787)(cid:12713)1.001.001.001.001.001.001.001.001.001.000.180.260.300.250.230.120.240.240.200.150.130.220.060.030.140.090.050.530.070.06(cid:10469)(cid:10449)(cid:17890)(cid:17533)(cid:20496)(cid:10706)(cid:8349)(cid:7560)(cid:13518)(cid:8914)(cid:12895)(cid:20043)(cid:7529)(cid:15919)1.001.001.001.001.001.001.001.001.001.001.001.001.001.000.320.200.090.130.220.220.210.170.220.320.280.320.240.250.070.090.120.070.100.140.100.080.070.110.530.110.120.09(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:22512)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)1.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.140.130.170.190.340.290.280.300.320.250.250.180.350.240.360.090.070.100.050.130.100.470.050.070.100.090.090.160.070.08(cid:9046)(cid:27082)(cid:24351)(cid:7732)(cid:9790)(cid:7576)(cid:7844)(cid:12802)(cid:13996)(cid:26067)(cid:12702)(cid:7747)(cid:22537)(cid:10998)(cid:8559)Sima Qian was sentenced to imperial punishment for protecting Li Ling!!!"
2105.12306,github,10,,,1Code and model are available at https://github.
2105.12306,github,3,,,2https://github.com/BYVoid/OpenCC
2105.12309,"code, github, code available",55,04/21/22,2,the EKF Algorithm for RexROV becomes as shown in Algorithm (2). The algorithm has been applied to several test courses from [23] in the next section and their results are discussed. The source code for implementation of the EKF algorithm shown below is available in https:// gitlab.engr.illinois.edu/auvsl/submarine
2105.12309,data,230,,,"From the Figs. 6 to 8 and the Tables 7 to 9, the dynamic Kalman ﬁlter predictions were very close to the ground truth values. On comparison of the individual values predicted by the dynamic ﬁlter and the kinematic ﬁlter with the actual position of RexROV, the dynamic ﬁlter was more stable and reliable than the kinematic ﬁlter. The results were based on movement only in the two primary axes, x and y, similar to a ground vehicle. Further research is being conducted on the simultaneous triaxial movement to make the controller It is expected that the movement on the z-axis universal. should not affect the performance of the dynamic Kalman ﬁlter. Currently the model uses GPS data to correct position estimation. However research has indicated that the strength of the electromagnetic waves for the GPS signal reduces signiﬁcantly underwater [30]. Methods such as station keeping, SONARSLAM and vision systems are currently being explored as an alternative for position estimation. Alternative controller techniques are being researched to improve the performance of the model. The predictions were conducted at a frequency of 10Hz and the dynamic ﬁlter worked on the simulation without any performance issues. However, the ﬁlters need to be implemented in real-time hardware to assess the actual computational performance."
2105.12309,data,98,,,"is most commonly represented by a purely kinematic model (i.e., model describing vehicle motion without considering force and moments [5, 10, 11]). However kinematic models fail to capture highly non-linear behaviour often observed in underwater systems. Approaches based on dynamic models have been implemented successfully [4, 12, 13, 13] to get The study described in more accurate state estimates. Ref. [13] implemented a 3 DoF dynamic model along with INS and DVL sensor data to estimate state of vehicle."
2105.12309,open-source,108,,,"4x4 state matrix thereby reducing computational cost.The 4 DOF model is proposed in Ref. [19] and was successfully used by Ref. [20] in control development. Since motion predictions are being augmented with sensor readings, it is expected from this approach to work effectively with the problem proposed in this paper. A Robotic Operating System (ROS) Gazebo-based open-source marine vehicle simulator was used in this research [14]. The simulator incorporates the dynamic model by Fossen [14] with a vehicle model based on parameters derived from the works by Berg [17]."
2105.12309,open-source,238,,,"Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) are used for a wide variety of missions related to exploration and scientiﬁc research. Successful navigation by these systems requires a good localization system. Kalman ﬁlter based localization techniques have been prevalent since the early 1960s and extensive research has been carried out using them, both It has been found that the in development and in design. use of a dynamic model (instead of a kinematic model) in the Kalman ﬁlter can lead to more accurate predictions, as the dynamic model takes the forces acting on the AUV into account. Presented in this paper is a motion-predictive extended Kalman ﬁlter (EKF) for AUVs using a simpliﬁed dynamic model. The dynamic model is derived ﬁrst and then it was simpliﬁed for a RexROV, a type of submarine vehicle used in simple underwater exploration, inspection of subsea structures, pipelines and shipwrecks. The ﬁlter was implemented with a simulated vehicle in an open-source marine vehicle simulator called UUV Simulator and the results were compared with the ground truth. The results show good prediction accuracy for the dynamic ﬁlter, though improvements are needed before the EKF can be used on real time. Some perspective and discussion on practical implementation is presented to show the next steps needed for this concept."
2105.12309,open-source,30,,,"Experiments were conducted on open source simulation software Gazebo 7. For underwater scenario and sensors simulation, UUVSIM [14] is used in parallel with ROS Kinetic."
2105.12309,package,62,,,"[14] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschenbach, “UUV simulator: A gazebo-based package for underwater intervention and multi-robot simulation,” in Proceedings of OCEANS 2016 MTS/IEEE Monterey, 19-23 September, 2016, Monterey, CA, USA., IEEE, sep 2016."
2105.15074,data,10,,,3.5 Diffusion tensor imaging (DTI) data classiﬁcation results
2105.15074,data,111,,,"The children are from ﬁve different communities in Canada. The data was obtained by a foundation (Kids Brain Health Network, formerly called NeuroDevNet). We compared results with those obtained. Using SVMR and other methods. The data were divided into two groups: a control group and another that includes children who have been diagnosed with FASD. The FASD patient data consists of different neurological condition diagnoses within the FASD group. Such as FAS, pFAS, and ARND. Table 1 summarized the input data information, who many children were diagnosis with FASD and without it from the total data."
2105.15074,data,114,,,"This study measures the connectivity of the white matter in the corpus callosum. It comes from a structural MRI, so it does not involve tests . This study has 76.54% records classiﬁed as FASD and 46% control. Having little data means that this study did not achieve a good accuracy (less than 50%). The Leaky-ReLU function was added in the intermediate layers to improve. Managing to rise to 75% in a combination of 4 hidden layers of 64 and 128 neurons, a feature layer, and the training time was 100 epochs compared to the other models with 50 epochs training."
2105.15074,data,115,,,"The network selection only depended on the accuracy; we have not carried out cross-validation of each network to choose the best one. The best performance obtained in the conﬁgurations studied was combining 25 neurons in the input layer: a single hidden layer with 20 neurons and two neurons corresponding to the categories that have FASD and no FASD. With this combination, it was possible to obtain a 75.55 % accuracy with the test data, so it was selected to perform the data analysis. While other combinations showed performance more outstanding than 90% on training data, performance with unknown or test data was below 65%."
2105.15074,data,117,,,"This paper evaluated the usage of computational algorithms. Based on ANN to classify children with FASD and, this is why can be used for medical diagnosis. This paper estimated the prediction model using psychometric data, DTI, and saccadic eye movement. Most of the current research that implements ANN to predict FASD is focused on the study of images, the novelty of this paper is that used numerical data to estimate the model. Although Zhang et al. (2019) (Zhang et al., 2019) collect and use the same data, our results show a better performance using ANN instead of SVMR or other methods."
2105.15074,data,118,,,"The confusion matrix (Fig. 4) shows the performance of the model with test data. The model has a good performance in classifying children with FASD since the correctness is 38.46% in positive hits (FASD having FASD), and it has 7.69% false positives (a control having FASD). It should be noted that the total percentage of children diagnosed with FASD is 42.31%. So, the model allows classifying FASD in most cases. In classifying control patients, the model classiﬁes 51.0% of control patients who are really in control. And fails in 3.85% to classify patients with FASD as controls."
2105.15074,data,13,,,Figure 3: Precision and Loss using feature dense layer on psychometric data
2105.15074,data,14,,,"application, and perspective in the big data era. Methods, 166:4–21."
2105.15074,data,140,,,"The data used describe the performance of children in solve some task. Psychometric tests (NEPSY-II) (Paolozza et al., 2016) are tests designed regarding the psychosocial, intellectual behavior, memory, and performance of children. Saccadic eye movements is a simple method to infer structural or functional brain deﬁciencies, present in neurological disorders (Green et al., 2013). Ocular behavior such as ﬁxation and saccades are crucial for efﬁcient visual perception. Diffusion tensor image (DTI) is the only non-invasive method for characterizing the microstructural organization of brain tissue Jones and Leemans (2011). It measures the white matter connectivity in the corpus callosum in great detail with advanced diffusion magnetic resonance (MR) imaging schemes (Hagmann et al., 2006)."
2105.15074,data,17,,,"Prognostic Prediction Using Neuroimaging Data. Front. Aging Neurosci., 11. Publisher: Frontiers."
2105.15074,data,179,,,"Results show that our models present a better performance in precision, in all cases than the individual studies carried out by Zhang et al.. However, there are key differences that should be considered when it comes to comparing previous results. For instance, Zhang et al. analyzes the characteristics obtained from each test. And eliminates those that do not contribute to improving the performance of the prediction. In our study, all the features obtained from each study are left, and using a feature layer allows the algorithm to distinguish those characteristics. Which contribute to improving pathology’s prediction accuracy. In turn, we did not specify which will be the training and test group. But instead it is left with a random division (80% training data, 20% test data). Comparing the structure of the data of each test, we could note that some tests had signiﬁcantly more samples of control patients than FASD patients. Thus, the results"
2105.15074,data,2,,,2.1 Data
2105.15074,data,233,,,"Fetal alcohol spectrum disorder (FASD) is a syndrome whose only difference compared to other children’s conditions is the mother’s alcohol consumption during pregnancy. An earlier diagnosis of FASD improving the quality of life of children and adolescents. For this reason, this study focus on evaluating the use of the artiﬁcial neural network (ANN) to classify children with FASD and explore how accurate it is. ANN has been used to diagnose cancer, diabetes, and other diseases in the medical area, being a tool that presents good results. The data used is from a battery of tests from children for 5-18 years old (include tests of psychometric, saccade eye movement, and diffusion tensor imaging (DTI)). We study the different conﬁgurations of ANN with dense layers. The ﬁrst one predicts 75% of the outcome correctly for psychometric data. The others models include a feature layer, and we used it to predict FASD using every test individually. The models accurately predict over 70% of the cases, and psychometric and memory guides predict over 88% accuracy. The results suggest that the ANN approach is a competitive and efﬁcient methodology to detect FASD. However, we could be careful in used as a diagnostic technique."
2105.15074,data,252,,,"In this research, ANN algorithms were developed to classify: psychometric data, DTI, and Saccadic eye movement of children/young people diagnosed with FASD. These data were trained in different network conﬁgurations. Measuring their accuracy in training and test data (data not used during training). We achieve an exactness of over 70% on all models with the network conﬁguration selected. Its performance in training was over 90%. The selected model was studied based on the loss and accuracy functions. Evaluating its performance in predicting FASD, control, and false positives or false negatives. Currently, there is an underestimation of people with FASD. This syndrome affects a signiﬁcant percentage of the world population. However, its diagnosis requires the certainty of consumption during embryonic development. It could be undiagnosed in our control population due to the clinical similarities with other neuro-development diseases. In conclusion, we suggest that ANN algorithms improve their performance with a suitable network conﬁguration -increase in the number of neurons, hidden layers, change in optimization functions, and feature layer- and the activation algorithm used to incorporate new data into training. The combination with other data groups even allows the increase of its performance, which is why ANN can be a good alternative in machine learning algorithms. However, a deeper study of the different network conﬁgurations must be undertaken to improve the prediction."
2105.15074,data,252,,,"Zhang et al. (2019) (Zhang et al., 2019) utilized methods, especially SVMR, require fewer parameters to optimize its performance, reducing the possibility of over-tuning training data, and increasing actual performance. According to the training data results -unlike ANN- the study of different conﬁgurations can change its operation and performance in general. The increase in training data allows ANN to improve its performance. Without changing parameters. Nevertheless a new training improves accuracy, unlike SVMR. SVMR is faster and more stable than other ANN. So the implementation of one or the other model depends on the change in data or other characteristics rather than on the accuracy of each one. Consequently, this paper explored the usage of numerical data, instead of images for diagnosing FASD. First, for psychometric data, a basic network conﬁguration was obtained with correctness with test data of 75.5% with a Leaky-ReLU activation function. Which is recommended for data without normalization, as in this case. It is expected that the models using machine learning will predict the output data in at least 90% of the cases. However, the model and data used, it was not achieved, remaining below expectations, by a 15%. Subsequently, a change was made in the conﬁguration of the networks. And a feature layer was added. This layer allows identifying"
2105.15074,data,253,,,"Machine learning (ML) is a sub-discipline of artiﬁcial intelligence (AI). ML techniques focus on developing algorithms. Capable of learning or adapting to their structure based on observed data. This learning occurs when the optimization function adjusting the weights by calculating the gradient of the loss function. Also known as the cost function, which tells us how good the model is Sajda (2006). Deep Learning within the ML methods framework, whose theoretical foundation is centered on classical neural networks . Unlike traditional neural networks, these have a group of hidden neurons and layers (H1...Hm) (Fig. 1) Ravi et al. (2017); Jo et al. (2019) . The input data (I1...In) passes through the layers in a non-linear combination of their outputs. Besides, can approximate any arbitrary function through a learning process. To a set of parameters in output projected from input space (O1...Ow) Shukla et al. (2017); Deng et al. (2013). The output of each neuron is described as a mathematical formula Villada et al. (2016) (See equation (1)), where xi are the weights without apathetic which the weight xi inputs, ϕ is the neuron activation function and n is the number of neurons connected to the input."
2105.15074,data,277,,,"In consequence, this paper will try to ﬁll the gap of the use of ML techniques for an improved FASD diagnosis. Speciﬁcally, this work is focuses on developing computational algorithms based on ANN to classify children with FASD. The research question is whether an ANN model could be used for medical diagnosis. One of the importance of testing ANN is the easy implementation and simplicity in applying existing data. This model will be evaluated based on its performance. In terms of precision, completeness, and performance comparison with other machine learning techniques. Used for differentiation using various tests such as psychometric, saccadic eye movement, and diffusion tensor imaging (DTI). It is believed that techniques based on learning machines are a good tool. For grouping and comparing the clinical information obtained from a set of psychometric data. Thus expanding the possibility of detecting differences in FASD patients. To our knowledge, the use of machine learning as FASD diagnostic tools has not been tested. However there are current studies that have been used them such as Zhang et al. (2019). However they have used support vector machine regression (SVMR) algorithms to analyze psychometric data and DTI and other methods for the remaining test. This paper is organized as follows: the next section presents materials and methods are used to carry out this study. Additionally describing the experiment execution. Section 3 presents the results obtained from the experiment. Section 4 discusses these results and Section 5 concludes this paper."
2105.15074,data,28,,,"Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep"
2105.15074,data,335,,,"the essential characteristics. This signiﬁcantly contribute to the classiﬁcation, achieving 88% accuracy. So with this conﬁguration; it was close to the desired prediction value of at least 90%, being a preliminary diagnosis and not achieving a reasonable prediction could have consequences. This is why for the other studies carried out, we used a network conﬁguration with a feature layer to observe the operation of these networks. With other data of numerical origin related to FASD, achieving in all cases an accuracy greater than 70% above from achieved by . The confusion matrices obtained demonstrate that the model predicts at least 71% of patients diagnosed with FASD. From the around 50% that this group represents. However, the prediction of control patients has over 77 % of patients who have not been diagnosed with FASD (except Prosaccadic task). These matrices suggest that the model must be recalibrated or overlearned from the data of patients with FASD, or else some control patients may have FASD but have not been diagnosed. These results cannot be proven. So according to the data, the use of ANN as a diagnostic tool must be improved before its use can be suggested. One caveat is the occurrence of the false-positive. That is to diagnose a person with FASD that does not have it since the data between FASD and control patients can be similar. It is essential to mention that some patients are not correctly diagnosed with FASD or have some cognitive problems similar to FASD. Which can also lead to many false negatives. This result is because collecting data on alcohol consumption during pregnancy is not accessible. Yet it is requires the mother’s declaration about the consumption. Likewise, over time the early samples of FASD decrease and are dependent on the time of exposure to alcohol."
2105.15074,data,338,,,"With other data of numerical origin related to FASD, achieving in all cases an accuracy greater than 70% above from achieved by . The confusion matrices obtained demonstrate that the model predicts at least 71% of patients diagnosed with FASD. From the around 50% that this group represents. However, the prediction of control patients has over 77 % of patients who have not been diagnosed with FASD (except Prosaccadic task). These matrices suggest that the model must be recalibrated or overlearned from the data of patients with FASD, or else some control patients may have FASD but have not been diagnosed. These results cannot be proven. So according to the data, the use of ANN as a diagnostic tool must be improved before its use can be suggested. One caveat is the occurrence of the false-positive. That is to diagnose a person with FASD that does not have it since the data between FASD and control patients can be similar. It is essential to mention that some patients are not correctly diagnosed with FASD or have some cognitive problems similar to FASD. Which can also lead to many false negatives. This result is because collecting data on alcohol consumption during pregnancy is not accessible. Yet it is requires the mother’s declaration about the consumption. Likewise, over time the early samples of FASD decrease and are dependent on the time of exposure to alcohol. And also the phase of gestation during which the ingestion occurred, and the amount of it. The neuropsychological and the developmental deﬁcits which some patients present (between groups diagnosed with FASD and other pathologies) do not show signiﬁcant differences, disabilities, or behavioral problems. Therefore, the diagnosis of FASD cannot be made lightly and should be considered other results such as magnetic resonance imaging, eye movement, among others."
2105.15074,data,39,,,"Prosaccade tests measure reaction time, performance accuracy, viability, and parameters which corresponding to the main sequence. For the classiﬁcation of FASD patients with these data from the 186 records, only 38% correspond to"
2105.15074,data,4,,,3.1 Pyscometric data classiﬁcation
2105.15074,data,4,,,3.2 Antisaccad data classiﬁcation
2105.15074,data,41,,,The proposed network consists of two or more dense layers interconnected for binary classiﬁcation. A feature layer was added as the ﬁrst layer to extract information from the input data. This feature layer was allowing the network to learn
2105.15074,data,42,,,"the range of 1-10 and others of 70-100. Consequently, there is no normalization, and a ReLU activation function could not be the same. Adequate for performance to exceed 80 % assertiveness in data not observed during training."
2105.15074,data,43,,,"After evaluating the networks’ performance using psychometric data, we could see that the networks did not show a remarkable improvement. This less-than-expected increase is explained partly because the conﬁguration chosen by adding a feature layer to the ANN model."
2105.15074,data,43,,,Table 2: Network conﬁguration for performance evaluation with psychometric data. a(IL) Input Layer (HL) Hidden Layer. b(PT) Accuracy in training. (PV) Accuracy in test (unobserved data).
2105.15074,data,5,,,3.3 Prosaccade task data classiﬁcation
2105.15074,data,50,,,"Fig. 3a shows the loss function on the psychometric data. In this case, the loss function has a decreasing curve in both training and testing. This result suggests that the network may have issues classifying, some data not used during training but not signiﬁcantly."
2105.15074,data,52,,,"A study was carried out with numerical data from different tests.To evaluate neural networks’ functioning as an alternative for predicting some neuromental pathology. Then, variations of the type of networks and layers used were made to improve the model’s prediction, along with a feature layer."
2105.15074,data,54,,,"Rodriguez, C. I., Vergara, V. M., Davies, S., Calhoun, V. D., Savage, D. D., and Hamilton, D. A. (2021). Detection of prenatal alcohol exposure using machine learning classiﬁcation of resting-state functional network connectivity data. Alcohol, 93:25–34."
2105.15074,data,54,,,"Zhang, C., Paolozza, A., Tseng, P.-H., Reynolds, J. N., Munoz, D. P., and Itti, L. (2019). Detection of Children/Youth With Fetal Alcohol Spectrum Disorder Through Eye Movement, Psychometric, and Neuroimaging Data. Front. Neurol., 10."
2105.15074,data,56,,,"For the activation of neurons, ""Leaky ReLU"" (Equation (2)) was used. It is a function that improves the performance of ReLU, which is the most used in ANN. In the study case, the psychometric data are not in the same range; some are in"
2105.15074,data,6,,,2.2 Data analysis using machine learning
2105.15074,data,6,,,2.3.1 Network conﬁguration for psychometric data
2105.15074,data,66,,,"The confusion matrix (Fig. 5) results show a high rate of prediction of classifying FASD compared to the amount of data evaluated. However, some of the classiﬁcation characteristics tend to confuse control children with the possibility of having FASD. These results make it necessary to evaluate the degree of FASD these children have and the reason for this classiﬁcation."
2105.15074,data,75,,,"These tasks are related to following the objective order; When there are errors, the subject cannot follow the order that initiates the saccade by the second objective more than on the ﬁrst. In these tasks, time and errors are measured Zhang et al. (2019). We took 61 records by classiﬁcation, achieving 88% in test data, one of the highest in the study."
2105.15074,data,79,,,"Table 2 shows layered neuron conﬁgurations in networks. And performance evaluation during training and with the validation data. The psychometric input data is 20. So the neurons in the input layer varied from the same amount of input data to 200. In the output layer, two neurons represent the classiﬁcation of FASD or not FASD. We made a variation in the number of hidden layers and the number of neurons in them."
2105.15074,data,8,,,Table 1: Data summary from all test
2105.15074,data,83,,,"Each of the conﬁgurations used 1,000 training epochs for performance validation. The evaluation of the model using the training data. We used 75% of the data for training and 25% for performance tests. Due to the model result not undergoing improvement, we decided to add a feature layer and increase the number of neurons per layer. Also, we reduce the number of training periods and improving the model by more than 80 % 1."
2105.15074,data,83,,,"The confusion matrix (Fig. 6) for this case shows that the model predicts FASD patients quite well, but only in 50% of the cases control patients being controls. This confusion matrix allows us to observe that we just made some adjustments to improve the exactness to predict control patients or verify the eliminated data. We should better understand the present differentiating characteristics from control patients to adequately identiﬁed during training reﬂected in the test data."
2105.15074,data,9,,,2.3.2 Network conﬁguration with addition of eye movement data
2105.15074,data,9,,,Number of features (input data) Total data
2105.15074,"data, data available",212,,,"This research aimed to evaluate the performance of a classiﬁcation algorithm based on ANN of children with FASD and without the syndrome, using the result of psychometric, DTI, and saccade tests and comparing the accuracy of the model with the SVMR developed by Zhang et al. et al. (2019). Our results suggest that ANN can make a preliminary diagnosis of pathologies reasonably. When numerical data are available. Most of the studies carried out to classify FASD patients using machines learning, study brain images or natural language Fang et al. (2006); Wozniak and Muetzel (2011); Suttie et al. (2018). Only the study conducted by Zhang et al. et al. (2019) uses numerical data from psychometric and eye movement studies and is freely available. Then, the results obtained with the use of ANN were compared with the results obtained by these authors in their study (Fig. 2). The purpose of this study was to evaluate, whether ANN can be used to classify patients with FASD from data obtained non-invasively and that do not require many studies for a preliminary diagnosis."
2105.15074,"data, data available",28,,,"In this section, we analyzed the data available and evaluated the performance of the ANN method and the conﬁguration of the neural network with reasonable accuracy."
2105.15074,"data, dataset",110,,,"The data used for testing included psychometric tests. Associated with analyzing social behavior, memory activities, language delay and all altered behavioral factors in FASD children. Also, the data set included other developmental diseases, making the classiﬁcation process more difﬁcult. This paper used two models of dense networks using this type of data. The ﬁrst model archived an accuracy of 75.55%. However, the model did not show signiﬁcant improvements, despite the variation in the layers and neurons in the layers. In the second model, a feature layer was added, achieving an accuracy of 88.46%."
2105.15074,"data, dataset",129,,,"Once the network conﬁguration was chosen, training and testing or validation behavior are shown in the model precision and loss functions. The loss function with a high result indicates that the neural network has a poor performance and a low result, that it is doing a good job. Fig. 3a shows the accuracy for each of the data set. In terms of the number of that it was aspects found and related to the number of individuals evaluated. We attempted to ﬁnd a model with no signiﬁcant difference, between the labeled data and the prediction. Nonetheless, with the implemented conﬁguration, the training data has increased accuracy. In addition, the validation data are also increasing accuracy."
2105.15074,"data, dataset",147,,,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model."
2105.15074,"data, dataset",159,,,"Prenatal alcohol exposure causes brain damage. And the neuropsychological consequences are deep. These deﬁcits in cognitive functions include: difﬁculties in planning, organization, and attention, consequential learning failures, and memory deﬁciencies. Some have speech and/or language difﬁculties, visuospatial functions and spatial memory, that are increased by exposure to prenatal alcohol (Glass et al., 2017; Green, 2007; Mohammad et al., 2020). Those characteristics could be seen in the task and the evaluation. It is a vital diagnosis based on the abnormalities founded and not only in the traditional facial characteristic (Wozniak et al., 2009). The novelty of the Zhang et al. (2019)’s data set is that it includes numeric and image data. This selection observes a deep learning operation in numerical data because most of the studies are based on images."
2105.15074,"data, dataset",253,,,"In the ﬁrst model, the data set did not include data regarding sex and age to observe the inﬂuence of these characteristics. In the prediction of the syndrome, since modiﬁes the data based the age to improve the performance of the models. The results of the evaluation of these models with data not observed during training did not exceed 55% accuracy. So these characteristics (sex and age) were incorporated in the following models. The general precision improved, exceeding 80% in all trained models. Depending on the study data, we made different combinations of neurons and layers. Compared to those of the ﬁrst psychometric study. The models used more neurons in the layers. However, the number of epochs in which the model was trained was lower than the other model. For the Antisaccade movement, Prosaccade used a dense network with two hidden layers of 128 neurons with ""ReLu"" activation. For Memory-guide saccade, Diffusion tensor imaging (DTI), and Psychometric data, we used four interleaved hidden layers of 64 and 128 neurons with sigmoid, ReLu activation, and Leaky ReLu just for DTI. The models trained for a total of 50 epochs. We used Leaky ReLu just in one case (DTI), which increasing the perfection. In other cases, the results not improved and we discarded to use the optimization function."
2105.15074,"data, dataset, data available",83,,,"The data used in this research is based on an open-access dataset collected by (Zhang et al., 2019). This research will use psychometric, saccadic eye movement and DTI data. From an open data set collected and analyzed by (Zhang et al., 2019) to address the research question. This data set contains children’s information from 3 to 18 years old. Including subject that are clinically diagnosed with or without FASD."
2105.15074,"data, python, dataset provided",181,,,"The input data are the features of each test. And the output is binary to classify FASD or control. The amount of data must be large enough to provide training examples. From which a large set of parameters can be drawn. Only a large number of parameters give rise to the wealth of class functions that model implicit knowledge Faust et al. (2018). Unfortunately, there are little data on children with FASD, making the classiﬁcation difﬁcult. The Keras library of TensorFlow was used in Python to implement the algorithm (Ketkar, 2017). We designed various models with dense layer connections, changing the number of neurons in each input layer, hidden layer, and output layer. To compile the model, we used “backpropagation with optimization Adam” and loss “sparse categorical cross-entropy” in the ﬁrst model. And “Binary Crossentropy” in another model, which are ideal conﬁgurations for classiﬁcation of categories in the FASD and non-FASD cases."
2105.15074,github,14,04/21/22,2,1The algorithms and functions used are available in the GitHub repository https://github.com/vjduarte/ANN_FASD
2105.15074,python,18,,,"Ketkar, N. (2017). Deep Learning with Python: A Hands-on Introduction. Apress."
2106.03907,code,113,,,"In this section, we report the empirical performance of the DFPV method. First, we present the results of estimating structural functions; we design two experimental settings for low-dimensional treatments and high-dimensional treatments, respectively. Then, we show the result of applying PCL methods to the bandit oﬀ-policy evaluation problem with confounding. We include the results for problems considered in prior work in Appendix E. The experiments are implemented using PyTorch [27]. The code is included in the supplemental material. All experiments can be run in a few minutes on Intel(R) Xeon(R) CPU E5-2698 v4 2.20GHz."
2106.03907,data,104,,,"We have proposed a novel approach for proxy causal learning, the Deep Feature Proxy Variable (DFPV) method, which performs two-stage least squares regression on ﬂexible and expressive features. Motivated by the literature on the instrumental variable problem, we showed how to learn these feature maps adaptively with deep neural networks. We also showed that PCL learning can be used for oﬀ-policy evaluation in the bandit setting with confounding, and that DFIV performs competitively in this domain. This work thus brings together research from the worlds of deep oﬄine RL and causality from observational data."
2106.03907,data,107,,,"We only compare PCL methods here, as it is not straightforward to apply CEVAE to the oﬀ-policy evaluation problem. We evenly split the data for Stage 1, Stage 2, and policy evaluation (i.e we set n = m = n(cid:48)).We ran 20 simulations for each setting. Results are summarized in Figure 4, in which DFPV performs better than existing PCL methods. This is not surprising since, as demonstrated in the structural function experiments, DFPV can estimate complex bridge functions, which results in a more accurate estimation of the value function."
2106.03907,data,110,,,"In this section, we develop a consistency result for causal parameters discussed in the main body. First, we consider the structural function. Given estimated bridge function ˆh(a, w) = ˆu(cid:62)(ψˆθA(2) (w)), we estimate the structural function by taking the empirical mean over W . To make the discussion simple, }nW we assume the access to an additional data sample SW = {wextra i=1, such that the estimated structural function is given as ˆfstruct(a) = ˆu(cid:62)(ψˆθA(2)"
2106.03907,data,112,,,"The complete procedure is presented in Algorithm 1. Note that we may use any sophisticated gradient-based learning method to optimize, such as Adam [10]. As reported in Xu et al. [35], we observe that the learning procedure is stabilized by running several gradient descent steps on the stage 1 parameters (θA(1), θZ) before updating the stage 2 features (θA(2), θW ). Furthermore, we may use mini-batch updates, which sample subsets of the data at the beginning of each iteration and only use these subsamples to update the parameters."
2106.03907,data,117,,,"Here, function Fig returns the corresponding image for the latent parameters, and η, ε are noise variables generated from ηA ∼ N (0.0, 0.1I) and ε ∼ N (0.0, 0.5). Each element of the matrix B ∈ R10×4096 is generated from Unif(0.0, 1.0) and ﬁxed throughout the experiment. From the data generation process, we can see that A and Y are confounded by posY. Treatment variable A is given as a ﬁgure corrupted with Gaussian random noise. The variable posY is not revealed to the model, and there is no observable confounder. The structural function for this"
2106.03907,data,122,,,"In both experiments, DFPV consistently outperforms existing methods. This suggests that DFPV is capable of learning complex structural functions by taking the advantage of the ﬂexibility of neural networks. KPV and PMMR perform similarly in all settings, but KPV tends to perform slightly better when the data size is large. This might be because KPV expresses its solution using a larger number of parameters than PMMR. Although CEVAE also learns a ﬂexible model with a neural network, it’s highly unstable in the demand design experiment and underperforms kernel methods in the dSprite experiment. This is because CEVAE does not take advantage of the relations between proxy variables and the structural function."
2106.03907,data,125,,,"If observations from the joint distribution of (A, Y, Z, W ) are available in both stages, we can tune the regularization parameters λ1, λ2 using the approach proposed in Singh et al. [30], Xu et al. [35]. Let the complete data of stage 1 and stage 2 be denoted as (ai, yi, zi, wi) and (˜ai, ˜yi, ˜zi, ˜wi). Then, we can use the data not used in each stage to evaluate the out-of-sample performance of the other stage. Speciﬁcally, let Algorithm 1 converges at t = T , and the regularizing parameters are given by"
2106.03907,data,14,,,"[3] B. Deaner. Proxy controls and panel data, 2018."
2106.03907,data,151,,,"Here, ψp(W,Z|L), ψp(A|L), µp(Y |A,L) are neural networks. We provide the structure of neural nets in Table 4 and 5. Following the orignal work [17], we train all neural nets by Adamax [10] with a learning rate of 0.01, which was annealed with an exponential decay schedule. We also performed early stopping according to the lower bound on a validation set. To predict structural function, we obtain q(L) by marginalizing q(L|A, Z, W, Y ) by observed data A, Z, W, Y . We then (cid:2)EY ∼p(Y |A=a,L) [Y ](cid:3). output ˆfstruct(a) = EL∼q(L)"
2106.03907,data,16,,,"i=1 Note that empirically, we can use outcome-proxy data in S1 instead of SW ."
2106.03907,data,16,,,"i=1, stage 3 data S3 = { ˇwi, ˇci}n(cid:48)"
2106.03907,data,17,,,"i=1, stage 2 data S2 = {(˜yi, ˜ai, ˜zi)}n"
2106.03907,data,18,,,"outcome-proxy variable data SW = {wextra 1 − 7δ, we have (cid:107)f ∗"
2106.03907,data,182,,,"In this paper, we propose a novel Deep Feature Proxy Variable (DFPV) method, which is the ﬁrst work to apply neural networks to the PCL problem. The technique we employ builds on earlier work in instrumental variable (IV) regression, which is a related causal inference setting to PCL. A range of deep learning methods has recently been introduced for IV regression [1, 6, 35]. We propose to adopt the Deep Feature Instrumental Variable method [35], which learns deep adaptive features within a two-stage regression framework. Details of DFPV are given in Section 3. In Section 4, we empirically show that DFPV outperforms other PCL methods in several examples. We further apply PCL methods to the oﬀ-policy evaluation problem in a confounded bandit setting, which aims to estimate the average reward of a new policy given data with confounding bias. We discuss the setting in Section 3, and show the superiority of DFPV in experiments in Section 4."
2106.03907,data,19,,,"In this section, we present the data generation process of experiments and the detailed settings of hyper-parameters."
2106.03907,data,192,,,"Proxy causal learning (PCL) is a method for estimating the causal eﬀect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the ﬁrst stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the eﬀect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal eﬀect, subject to identiﬁability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be applied to oﬀ-policy evaluation for the confounded bandit problem, in which DFPV also exhibits competitive performance."
2106.03907,data,202,,,"In this section, we develop the DFPV algorithm, which learns adaptive features modeled by neural nets using a technique similar to Xu et al. [35]. As in Mastouri and Zhu et al. [18], we assume that we do not necessarily have access to observations from the joint distribution of (A, Y, Z, W ). Instead, we are given m observations of (A, Z, W ) for Stage 1 and n observations of (A, Z, Y ) for Stage 2. We denote the stage 1 observations by (ai, zi, wi) and the stage 2 observations by (˜ai, ˜zi, ˜yi). If observations of (A, Y, Z, W ) are given for both stages, we can evaluate the out-of-sample loss of Stage 1 using Stage 2 data and vice versa, and these losses can be used for hyper-parameter tuning of λ1, λ2 (Appendix A). We ﬁrst introduce two-stage regression with adaptive feature maps and then describe the detailed learning procedure of DFPV."
2106.03907,data,26,,,"Again, we use data (P, V ) to compute the empirical average of h(πP (P ), V )."
2106.03907,data,28,,,"function. Here, we assume access to SW for proving consistency results, but empirically, we can use stage 1 data to compute this mean."
2106.03907,data,32,,,"Here, we show the result for the synthetic setting proposed in Mastouri et al. [18]. The data generating process for each variable is given as follows:"
2106.03907,data,33,,,"To correct this confounding bias, we introduce cost-shifter C1, C2 as a treatment-inducing proxy, and views V of the reservation page as the outcomeinducing proxy. Data is sampled as"
2106.03907,data,36,,,"when the data is low-dimensional and the relations between the variables are smooth. We would like to note, however, DFPV outperforms CEVAE, which shows that the proxy setting is still required."
2106.03907,data,37,,,"Theorem 3. Let Assumptions 3, 7 and 8 hold. Let η = supa∈A ηa. Given stage 1 data S1 = {(wi, ai, zi)}m i=1, additional"
2106.03907,data,39,,,"Experiments for Oﬄine Policy Evaluation We now describe the oﬄine policy evaluation experiment based on the demand design data. We set up synthetic experiments to evaluate two types of policy. In the ﬁrst case, a policy"
2106.03907,data,41,,,"We use the same data (Y, P, C1, C2, V ) in demand design for policy evaluation experiments. We consider two policies. One is a policy depends on costs C1, C2 which is"
2106.03907,data,42,,,"Theorem 4. Let Assumptions 3, 7, 8, 9 hold. Given stage 1 data S1 = {(wi, ai, zi)}m i=1, with at least probability 1 − 7δ, we have"
2106.03907,data,43,,,"Algorithm 2 Deep Feature Instrumental Variable with Observable Confounder Input: Stage 1 data (ai, zi, wi, xi), Stage 2 data (˜ai ˜zi, ˜yi, ˜xi), Regularization X(2))."
2106.03907,data,43,,,"Lemma 3. Under Assumption 7, and given stage 1 data S1 = {(ai, zi, wi)}m for any δ > 0, with at least probability 1 − 2δ, we have (cid:115)"
2106.03907,data,45,,,"[36] L. Yao, S. Li, Y. Li, M. Huai, J. Gao, and A. Zhang. Representation learning for treatment eﬀect estimation from observational data. In Advances in Neural Information Processing Systems, volume 31, 2018."
2106.03907,data,57,,,"parameters (λ1, λ2). Initial values θ(0) = (θ(0) , wextra Learning rate α, additional data (xextra i Output: Estimated structural function ˆfstruct(a) 1: t ← 0 2: repeat 3: 4: Update (θ(t+1)"
2106.03907,data,58,,,"Proposition 4. Let Assumption 3 and Assumptions 7, 8, 9 in Appendix C hold. Denote the minimizers of ˆL2 as (ˆu, ˆθA(2), ˆθZ). Given stage 1 data S1 = {(wi, ai, zi)}m i=1, and data for policy evaluation"
2106.03907,data,65,,,"In this appendix, we report the results of two additional experiments. One is a synthetic setting introduced in Mastouri et al. [18], which has a simpler data generating process. The other is based on the real-world setting introduced by Deaner [3]. In both setting, DFPV performs similarly to or better than existing methods."
2106.03907,data,65,,,"πC1,C2 (C1, C2) = 23 + C1C2. To conduct oﬄine-policy evaluation, we use data (C1, C2, V ) to compute the empirical average of h(πC1,C2 (C1, C2), V ). In our second experiment, the policy depends on current price P , which is given as"
2106.03907,data,67,,,"Proposition 2. Let Assumption 3 and Assumptions 7 and 8 in Appendix C hold. Given stage 1 data S1 = {(wi, ai, zi)}m i=1, and i=1, then for the minimizer of ˆL2 }nW additional output-proxy data SW = {wextra denoted as ( ˆu, ˆθA(2), ˆθZ), we have"
2106.03907,data,7,,,E.1 Experiments with Simpler Data Generating Process
2106.03907,data,70,,,"Algorithm 1 Deep Feature Proxy Causal Learning Input: Stage 1 data S1 = {ai, zi, wi}, Stage 2 data S2 = {˜ai ˜zi, ˜yi}, Additional }, Regularization parameters (λ1, λ2). Ini outcome-proxy data SW = {wextra Z , θ(0) tial values θ(0) = (θ(0)"
2106.03907,data,74,,,"From Figure 6, we can see that DFPV and CEVAE methods perform worse and have larger variances than KPV and PMMR methods. This is not surprising, since DFPV tends to require more data than KPV and PMMR, as needed to learn the neural net feature maps (rather than using ﬁxed pre-deﬁned kernel features). Hence, we can say that we should favor KPV and PMMR over DFPV"
2106.03907,data,74,,,"Theorem 2. Let Assumptions 3 and 7 hold. Given stage 1 data S1 = {(wi, ai, zi)}m i=1 and stage 2 data S2 = {(˜yi, ˜ai, ˜zi)}n 1 − 6δ, we have (cid:107)h∗(A, W ) − ˆh(A, W )(cid:107)P (A,W )"
2106.03907,data,77,,,"Lemma 4. Under Assumption 7, given stage 1 data S1 = {(ai, zi, wi)}m stage 2 data S2 = {(˜ai, ˜zi, ˜yi)}n ( ˆu)(cid:62)(ψˆθA(2) (a) ⊗ ψˆθW we have (cid:13) EY |A,Z [Y ] − ˆEW |A,Z (cid:13) (cid:13)"
2106.03907,data,82,,,"where ε1, ε2, ε3, ε4 ∼ N (0, 1). From observations of (Y, P, C1, C2, V ), we estimate ˆfstruct by PCL. For each estimated ˆfstruct, we measure out-of-sample error as the mean square error of ˆf versus true fstruct obtained from Monte-Carlo simulation. Speciﬁcally, we consider 10 evenly spaced values of p ∈ [10, 30] as the test data."
2106.03907,data,84,,,"From observations of (Y, W, Z, A), we estimate ˆfstruct by PCL. For each estimated ˆfstruct, we measure out-of-sample error as the mean square error of ˆf versus true fstruct obtained from Monte-Carlo simulation. Speciﬁcally, we consider 20 evenly spaced values of A ∈ [0.0, 1.0] as the test data. The results with data size n = m = {500, 1000} are shown in Figure 6."
2106.03907,data,91,,,"where ˆV (θ), ˆu(θ) are given in (7). Given these losses, (θA(1), θZ) are minimized with respect to ˆLDFPV (θ), and (θA(2), θW ) are minimized with respect to ˆLDFPV (θ). Finally, we take the empirical mean of ψθ(t) based on additional 2 }nW output-proxy data SW = {wextra i=1, which is used for estimating the structural"
2106.03907,data,94,,,"In causal learning, we aim to estimate the eﬀect of our actions on the world. For example, we may be interested in measuring the impact of ﬂight ticket prices on sales [2, 34], or the eﬀect of grade retention on cognitive development [4]. We refer to our action as a treatment, which results in a particular outcome. It is often impossible to determine the eﬀect of treatment on outcome from observational data alone, since the observed joint distribution of treatment and"
2106.03907,"data, data available",245,04/21/22,0,"One common assumption to cope with confounding bias is to assume no unobserved confounders exist [8], or more generally, the ignorable treatment assignment assumption [28], which states that the treatment assignment is independent of the potential outcomes caused by the treatment, given the background data available. Although a number of methods are proposed based on this assumption [7, 9, 36], it can be too restrictive, since it is often diﬃcult to determine how the confounder aﬀects treatment assignments and outcomes. A less restrictive assumption is that we have access to proxy variables, which contain relevant side information on the confounder. In the ﬂight tickets example, we can use the number of views of the ticket reservation page as a proxy variable, which reﬂects peoples’ desire for ﬂights. Note that if we can completely recover the confounder from proxy variables, the ignorable treatment assignment assumption can be satisﬁed. Motivated by this, Lee et al. [14] and Louizos et al. [17] aim to recover the distribution of confounders from proxy variables using modern machine learning techniques such as generative adversarial networks [5] or variational auto-encoders (VAE) [11]. Although these methods exhibit powerful empirical performance, there is little theory that guarantees the correct recovery of the causal eﬀects."
2106.03907,"data, data available",74,,,"where each element of the matrix B ∈ R10×4096 was generated from Unif(0.0, 1.0) and ﬁxed throughout the experiment. We ﬁxed the shape parameter to heart and used other parameters as the treatment-inducing proxy Z. We sampled another image that shared the same posY as treatment A, which is used as output-inducing proxy W . Details of data generation process can be found in Appendix F.2."
2106.03907,"data, dataset",105,,,"Here, we describe the data generation process for the dSprites dataset experiment. This is an image dataset parametrized via ﬁve latent variables (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. In this experiment, we ﬁxed the shape parameter to heart, i.e. we only used the heart-shaped images. The other latent parameters take values of scale ∈ [0.5, 1], rotation ∈ [0, 2π], posX ∈ [0, 1], posY ∈ [0, 1]."
2106.03907,"data, dataset",109,,,"Table 1 shows the result for this dataset. In this setting, the performance of DFPV matches KPV and PMMR. As in the experiment described in the revious section, the setting is low-dimensional (one-dim treatment variable, three-dim treatment-inducing proxy, four-dim outcome-inducing proxy) and the generative model is smooth (the ""ground truth"" being a generalized additive model and a Gaussian mixture model). For these reasons, we might again expect this data to favor kernel methods, such as KPV and PMMR; nonetheless, our method matches them. DFPV again outperforms CEVAE in this setting."
2106.03907,"data, dataset",145,,,"To test the performance of DFPV in a more realistic setting, we conducted the experiment on the Grade Retention dataset introduced by Deaner [3]. This aims to estimate the eﬀect of grade retention based on the score of math and reading on the long-term cognitive outcomes, in which we use scores in elementary school as a treatment-inducing proxy (Z) and cognitive test scores from Kindergarten as the an outcome-inducing proxy (W). Following Mastouri et al. [18], we generate a synthetic ""ground truth"" by ﬁtting a generalized additive model to learn a structured causal model (SCM), and a Gaussian mixture model to learn unmeasured confounder based on the learned SCM. Note, this is needed since for real-world data there is no measured ground truth."
2106.03907,"data, dataset",274,,,"We compare the DFPV method to three competing methods, namely KPV [18], PMMR [18], and an autoencoder approach derived from the CEVAE method [17]. In KPV, the bridge function is estimated through the twostage regression as described in Section 2, where feature functions are ﬁxed via their kernel functions. PMMR also models the bridge function using kernel functions, but parameters are learned by moment matching. CEVAE is not a PCL method, however, it represents a state-of-theart approach in correcting for hidden confounders using observed proxies. The causal graph for CEVAE is shown in Figure 2, and CEVAE uses a VAE [11] to recover the distribution of confounder U from the “proxy” Q. We make two modiﬁcations to CEVAE to apply it in our setting. First, we include both the treatment-inducing proxy Z and output-inducing proxy W as Q in CEVAE (we emphasize that this does not follow the causal graph in Figure 2, since there exist arrows from Q to A, Y ). Second, CEVAE is originally used in the setting where Q is conditioned on a particular value, whereas we marginalize Q. See Appendix F.4 for the choice of the network structure and hyper-parameters. We tuned the regularizers λ1, λ2 as discussed in Appendix A, with the data evenly split for Stage 1 and Stage 2. We varied the dataset size and ran 20 simulations for each setting. Results are summarized in Figure 3."
2106.03907,"data, dataset, dataset provided, data available",147,,,"Experiments for Structural Function We present two structural function estimation experiments. One is a demand design experiment based on a synthetic dataset introduced by Hartford et al. [6], which is a standard benchmark for the instrumental variable regression. Here, we modify the data generating process to provide a benchmark for PCL methods. We consider the problem of predicting sales Y from ticket price P , where these are confounded by a potential demand D ∈ [0, 10]. To correct this confounding bias, we use the fuel price (C1, C2) as the treatment-inducing proxy, which has an impact on price P , and the number of views of the ticket reservation page V as the outcome-inducing proxy. Details of the data generation process can be found in Appendix F.1."
2106.03907,dataset,108,,,"treatment variables. We test this using the dSprite dataset [19], which is an image dataset described by ﬁve latent parameters (shape, scale, rotation, posX and posY). The images are 64 × 64 = 4096-dimensional. Based on this, Xu et al. [35] introduced the causal experiment, where the treatment is each ﬁgure, and the confounder is posY. Inspired by this, we consider the PCL setting that learns the same structural functions with nonlinear confounding, which is not possible to handle in the instrumental variable setting. Speciﬁcally, the"
2106.03907,dataset,14,,,"From this dataset, we generate the treatment variable A and outcome Y as"
2106.03907,dataset,31,,,5: 6: until convergence 7: Compute ˆu(θ(t)) from (20) 8: Compute mean feature for W using stage 1 dataset
2106.03907,dataset,39,,,5: 6: until convergence 7: Compute ˆu(θ(t)) from (7) 8: Compute mean feature for W using stage 1 dataset: µθW ← 1 n (cid:18)
2106.03907,dataset,59,,,"If ˆRS1(H1) → 0 and Corollary 3. Let Assumption 7 hold and κ1, κ2 = 0. ˆRS2 (H2) → 0 in probability as the dataset size increases, ˆh converges to h∗ in probability with respect to (cid:107) · (cid:107)P(A,W )."
2106.03907,dataset,6,,,E.2 Experiments using Grade Retention dataset
2106.03907,dataset,62,,,"Proposition 6. [Theorem 3.3 24, with slight modiﬁcation] Let S be a measurable space and H be a family of functions mapping from S to [0, M ]. Given ﬁxed dataset S = (s1, s2, . . . , sn) ∈ S n, the empirical Rademacher complexity is given by"
2106.03907,dataset,8,,,Table 1: Results of grade retension dataset
2106.03907,"dataset, github, data https",31,,,"[19] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. URL https://github.com/deepmind/ dsprites-dataset/."
2106.06026,data,60,,,"with some meta-data. Vertices must be valid conﬁgurations H, meaning vectors of H have 1s in speciﬁed coordinates of H. Edges between conﬁgurations of G change up to one vector and/or some coordinates, and we think of edges as performing operations on conﬁgurations. We ensure the graph is undirected by choosing operations that are invertible."
2106.06064,"code, code available",10,,,Code to reproduce our experiments is available at https:
2106.06064,data,100,,,"We also allow for the possibility that there is access to a graph G = (V, E), where V is the set of N nodes and E ⊂ V × V denotes the set of edges. In this case, each node corresponds to one time-series. The edges indicate probable predictive relationships between the variables, i.e., the presence of an edge (i, j) suggests that the historical data for time-series i is likely to be useful in predicting time-series j. The graph may be directed or undirected."
2106.06064,data,114,04/21/22,0,"In this work, we model multivariate time-series as random realizations from a nonlinear state-space model, and target Bayesian inference of the hidden states for probabilistic forecasting. The general framework we propose can be applied to univariate or multivariate forecasting problems, can incorporate additional covariates, can process an observed graph, and can be combined with data-adaptive graph learning procedures. For the concrete example algorithm deployed in experiments, we build the dynamics of the state-space model using graph convolutional recurrent architectures. We develop an inference procedure that employs particle ﬂow, an alternative to particle ﬁlters, that can conduct more effective inference for high-dimensional states."
2106.06064,data,140,,,"With the proposed formulation, we can modify recurrent graph convolutional architectures when designing the function g. When a meaningful graph is available, such architectures signiﬁcantly outperform models that ignore the graph. For example, we conduct experiments by incorporating into our general model the Adaptive Graph Convolutional Gated Recurrent Units (AGCGRU) presented in (Bai et al., 2020). The AGCGRU combines (i) a module that adapts the provided graph based on observed data, (ii) graph convolution to capture spatial relations, and (iii) a GRU to capture evolution in time. The example model used for experiments thus employs an L-layer AGCRU with additive Gaussian noise to model the system dynamics g: xt = AGCGRU (L) yt = Wφxt + wt ."
2106.06064,data,153,,,"The goal is to construct a model that is capable of processing, for some time offset t0, the data Yt0+1:t0+P , Zt0+1:t0+P +Q and (possibly) the graph G, to estimate Yt0+P +1:t0+P +Q. The prediction algorithm should produce both point estimates and prediction intervals. The performance metrics for the point estimates include mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean squared error (RMSE). For the prediction intervals, the performance metrics include the Continuous Ranked Probability Score (CRPS) (Gneiting & Raftery, 2007), and the P10, P50, and P90 Quantile Losses (QL) (Salinas et al., 2020; Wang et al., 2019). Expressions for these performance metrics are provided in the supplementary material."
2106.06064,data,16,,,"data: 1:P +Q}n∈Dtest 1:P , z(n)"
2106.06064,data,16,,,• HA (Historical Average): uses the seasonality of the his torical data.
2106.06064,data,32,,,"Chen, C., Petty, K., and Skabardonis, A. Freeway performance measurement system: Mining loop detector data. Transport. Research Record, 1748, Jan. 2000."
2106.06064,data,38,,,"Daum, F. and Huang, J. Nonlinear ﬁlters with log-homotopy. In Proc. SPIE Signal and Data Process. Small Targets, pp. 669918, San Diego, CA, USA, Sep. 2007."
2106.06064,data,38,,,"Li, Y., Yu, R., Shahabi, C., and Liu, Y. Diffusion Convolutional Recurrent Neural Network: Data-Driven Trafﬁc Forecasting. In Proc. Int. Conf. Learning Rep., 2018."
2106.06064,data,45,,,"Karl, M., Soelch, M., Bayer, J., and Van der Smagt, P. Deep variational Bayes ﬁlters: Unsupervised learning of state space models from raw data. In Proc. Int. Conf. Learning Rep., 2016."
2106.06064,data,48,,,"Song, C., Lin, Y., Guo, S., and Wan, H. Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting. In Proc. AAAI Conf. Artiﬁcial Intell., pp. 914–921, Apr. 2020."
2106.06064,data,50,,,"Huang, Y., Weng, Y., Yu, S., and Chen, X. Diffusion convolutional recurrent neural network with rank inﬂuence learning for trafﬁc forecasting. In Proc. IEEE Int. Conf. Big Data Science And Engineering, pp. 678–685, Aug 2019."
2106.06064,data,53,,,"Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., and Bengio, Y. A recurrent latent variable model for sequential data. In Proc. Adv. Neural Info. Process. Systems, pp. 2980–2988, 2015."
2106.06064,data,59,,,"Pan, Z., Liang, Y., Wang, W., Yu, Y., Zheng, Y., and Zhang, J. Urban trafﬁc prediction from spatio-temporal data using deep meta learning. In Proc. ACM SIGKDD Int. Conf. Knowl. Discov. & Data Mining, pp. 1720–1730, 2019."
2106.06064,data,59,,,"Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X., and Zhang, C. Connecting the dots: Multivariate time series forecasting with graph neural networks. In Proc. ACM SIGKDD Conf. Knowl. Discov. & Data Mining, pp. 753–763, 08 2020."
2106.06064,data,75,,,"Spatio-temporal forecasting has many applications in intelligent trafﬁc management, computational biology and ﬁnance, wireless networks and demand forecasting. Inspired by the surge of novel learning methods for graph structured data, many deep learning based spatio-temporal forecasting techniques have been proposed recently (Li et al., 2018; Bai et al., 2020). In addition to the temporal patterns present in the data, these approaches can effectively learn"
2106.06064,"data, data https",1,,,tlc-trip-record-data.page
2106.06064,"data, dataset",102,,,"Point forecasting results on non-graph datasets : We evaluate our proposed ﬂow-based RNN on the Electricity and Trafﬁc datasets, following the setting described in Appendix C.4 in (Oreshkin et al., 2020). We augment the results table in (Oreshkin et al., 2020) with the results from an FC-GRU (a fully connected GRU encoder-decoder) and GRU+ﬂow. We use a 2 layer GRU with 64 RNN units in both cases. We follow the preprocessing steps in (Oreshkin et al., 2020). In the literature, four different data splits have"
2106.06064,"data, dataset",157,,,"Spatio-temporal forecasting has numerous applications in analyzing wireless, trafﬁc, and ﬁnancial networks. Many classical statistical models often fall short in handling the complexity and high non-linearity present in time-series data. Recent advances in deep learning allow for better modelling of spatial and temporal dependencies. While most of these models focus on obtaining accurate point forecasts, they do not characterize the prediction uncertainty. In this work, we consider the time-series data as a random realization from a nonlinear state-space model and target Bayesian inference of the hidden states for probabilistic forecasting. We use particle ﬂow as the tool for approximating the posterior distribution of the states, as it is shown to be highly effective in complex, high-dimensional settings. Thorough experimentation on several real world time-series datasets demonstrates that our approach provides better characterization of uncertainty while maintaining comparable accuracy to the state-of-theart point forecasting methods."
2106.06064,"data, dataset",310,,,"Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets. Some qualitative visualization of the conﬁdence intervals for 15-minute ahead predictions for the PeMSD3, PeMSD4, PeMSD7, and PeMSD8 datasets are shown in Figures 7, 8, 9, and 10 respectively. We observe that the conﬁdence intervals from the proposed algorithm are considerably tighter compared to its competitors in most cases, whereas the coverage of the ground truth is still ensured."
2106.06064,"data, dataset",344,,,"In Table 1 of the main paper, we report the average MAE of the top 10 algorithms. The detailed comparisons in terms of MAE, MAPE, and RMSE with all the baseline algorithms on the four PeMS datasets are provided in Tables 10, 11, 12, and 13. We observe that statistical models such as HA, ARIMA, and VAR and basic machine learning models such as SVR, FNN, and FC-LSTM show poor predictive performance as they cannot model the complex spatio-temporal patterns present in the real world trafﬁc data well. Graph agnostic deep learning models such as DeepGLO and NBEATS perform better than the statistical models, but they cannot incorporate the graph structure when learning. FCGAGA has lower forecasting errors as it is equipped with a graph learning module. The spatio-temporal graph-based models (especially AGCRN, GMAN, GWN, and LSGCN) display better performance. These models either use the observed graph or learn the graph structure from the data. In general, the deep learning based probabilistic forecasting algorithms such as DeepAR, DeepFactors, and MQRNN do not account for the spatial relationships in the data as well as the graph-based models, although MQRNN is among the best performing algorithms. DeepAR and DeepFactors aim to model the forecasting distributions and thus do not perform as well in the point forecasting task. The training loss function (negative log likelihood of the forecasts) does not match the evaluation metric. However, MQRNN shows better performance, possibly because it does target learning the median of the forecasting distribution along with other quantiles. The proposed AGCGRU+ﬂow algorithm demonstrates comparable prediction accuracy to the best-performing spatio-temporal models and achieves the best average ranking across the four datasets. Figure 6 demonstrates that the proposed AGCGRU+ﬂow has lower average MAE in most of the nodes compared to the second best performing AGCRN algorithm, for all four PeMS datasets."
2106.06064,"data, dataset",47,,,"both for the ground-truth test data, and samples of forecasts, and then computing the (normalized) CRPS on the summed data. The results are summarized in Table 7. We observe that the proposed GRU+ﬂow achieves the lowest CRPSsum for all datasets."
2106.06064,"data, dataset",71,,,"test split is set at 70/10/20% chronologically and standard normalization of the data is used as in (Li et al., 2018). We use one hour of historical data (P = 12) to predict the trafﬁc for the next hour (Q = 12). Graphs associated with the datasets are constructed using the procedure in (Huang et al., 2020)."
2106.06064,"data, dataset",90,,,"We address the task of discrete-time multivariate time-series prediction, with the goal of forecasting multiple time-steps ahead. We assume that there is access to a historical dataset for training, but after training the model must perform prediction based on a limited window of historical data. Let yt ∈ RN ×1 be an observed multivariate signal at time t and Zt ∈ RN ×dz be an associated set of covariates. The i-th element of yt is the observation associated with time-series i at time-step t."
2106.06064,"data, dataset, data available",113,,,"5.2. Inference We assume that a dataset Dtrn is available for training. Although this data may be derived from a single time-series, because our task is to predict yt0+P +1:t0+P +Q using a limited historical window yt0+1:t0+P , we splice the time-series and thus construct multiple training examples, denoted by (y(m) P +1:P +Q). In the training set, all of these observations are available; in the test set yP +1:P +Q are not. In addition, the associated covariates z1:P +Q are known for both training and test sets."
2106.06064,"data, dataset, data available",168,,,"In Table 6, we observe that the ﬂow based approach performs comparably or better than the state-of-the-art NBEATS algorithm for the Electricity dataset, even with a simple GRU as the state transition function. The better performance of the univariate N-BEATS compared to the multivariate models suggests that most time-series in these datasets do not provide valuable additional information for predicting other datasets. This is in contrast to the graphbased datasets, where the performance of N-BEATS was considerably worse than the multivariate algorithms. The proposed ﬂow-based algorithm achieves prediction performance on the Trafﬁc dataset that is comparable to N-BEATS except for one split with limited training data. Across all datasets and split settings, our ﬂow-based approach signiﬁ cantly outperforms the FC-GRU. The proposed algorithm outperforms TRMF, DeepAR, DeepState and DeepGLO. It outperforms DeepFactors for the Electricity dataset, but is worse for the Trafﬁc dataset (for the same split with limited available training data)."
2106.06064,"data, dataset, publicly available",125,,,"6.1. Datasets We evaluate our proposed algorithm on four publicly available trafﬁc datasets, namely PeMSD3, PeMSD4, PeMSD7 and PeMSD8. These are obtained from the Caltrans Performance Measurement System (PeMS) (Chen et al., 2000) and have been used in multiple previous works (Yu et al., 2018; Guo et al., 2019; Song et al., 2020; Bai et al., 2020; Huang et al., 2020). Each of these datasets consists of the trafﬁc speed records, collected from loop detectors, and aggregated over 5 minute intervals, resulting in 288 data points per detector per day. In non-graph setting, we use Electricity (Dua"
2106.06064,dataset,1,,,Dataset
2106.06064,dataset,1,,,gluon-ts/tree/mv_release/datasets
2106.06064,dataset,10,,,Table 9. Summary statistics of the multivariate non-graph datasets
2106.06064,dataset,12,,,Dataset Algorithm 11.41/13.11/14.62/16.27 DeepAR 14.16/15.87/17.59/18.99 DeepFactors GRU+ﬂow 11.23/12.70/13.98/15.25 DCGRU+ﬂow 11.21/12.14/12.87/13.64 AGCGRU+ﬂow 10.53/11.39/12.03/12.47
2106.06064,dataset,127,,,"Probabilistic forecasting results on non-graph datasets : For comparison with state-of-the-art deep learning based probabilistic forecasting methods on standard non-graph time-series datasets, we evaluate the proposed GRU+ﬂow algorithm following the setting in (Rasul et al., 2021). The results reported in Table 1 of (Rasul et al., 2021) are augmented with the results of the GRU+ﬂow algorithm. We use a 2 layer GRU with 64 RNN units in each case. We follow the preprocessing steps as in (Salinas et al., 2019; Rasul et al., 2021). The evaluation metric is (normalized) CRPSsum (deﬁned in the supplementary material), which is obtained by ﬁrst summing across the different time-series,"
2106.06064,dataset,141,,,"In this experiment, we compare the proposed state-space model with different learnable noise variance at each node (parameterized by the softplus function in eq. (9) in the main paper with ﬁxed and uniform noise standard deviation γ = 0.01/0.05/0.10 at all nodes. Other hyper-parameters and the training setup remain unchanged. The results in Table 18 demonstrate that the learnable noise variance approach is not particularly beneﬁcial in comparison to a uniform, ﬁxed variance approach in most cases. However, we note that the probabilistic metrics reported in Table 19 are the lowest for the learnable noise variance model in all cases. This suggests that different time-series in these road trafﬁc datasets have different degrees of uncertainty which cannot be effectively modelled by the uniform, ﬁxed noise variance approach."
2106.06064,dataset,15,,,Table 8. Summary statistics of the PeMS road trafﬁc datasets PeMSD3 PeMSD4 PeMSD7 PeMSD8
2106.06064,dataset,17,,,We perform experiments on four graph-based and four nongraph based public datasets to evaluate proposed methods.
2106.06064,dataset,177,,,"posed AGCGRU+ﬂow algorithm achieves on par or better performance with the best-performing spatio-temporal models, such as GWN, GMAN and AGCRN. We present a comparison of the average rankings across datasets in Figure 3. Our proposed method achieves the best average ranking and signiﬁcantly outperforms the baseline methods. Table 3 summarizes the results for probabilistic forecasting. We observe that in most cases, the proposed ﬂow based algorithms outperform the competitors. MQRNN also shows impressive performance in predicting the forecast quantiles, as it is explicitly trained to minimise the quantile losses. In particular, comparison of GRU+ﬂow with the DeepAR model reveals that even without a sophisticated RNN architecture, the particle ﬂow based approach shows better characterization of prediction uncertainty in most cases. Figure 4 provides a qualitative comparison of the uncertainty characterization, showing example conﬁdence intervals for 15-minute ahead prediction for the PeMSD7 dataset. We see that the proposed algorithm provides considerably tighter intervals, while still achieving coverage of the observed values."
2106.06064,dataset,2,,,Dataset Algorithm
2106.06064,dataset,2,,,PeMS datasets
2106.06064,dataset,27,,,"6.2. Preprocessing For the PeMS datasets, missing values are ﬁlled by the last known value in the same series. The training, validation and"
2106.06064,dataset,28,,,"For the experiments on the PeMS road trafﬁc datasets, we compare the proposed AGCGRU+ﬂow algorithm with four different classes of forecasting techniques, listed as follows:"
2106.06064,dataset,283,,,"6.4. Hyperparameters and training setup For our model, we use an L = 2 layer AGCGRU (Bai et al., 2020) as the state-transition function. The dimension of the learnable node embedding is de = 10, and the number of RNN units is dx = 64. We treat ρ and σ as ﬁxed hyperparameters and set ρ = 1 and σ = 0 (no process noise). We train for 100 epochs using the Adam optimizer, with a batch size of 64. The initial learning rate is set to 0.01 and we follow a decaying schedule as in (Li et al., 2018). Hyperparameters associated with scheduled sampling (Bengio et al., 2015), gradient clipping, and early stoppng are borrowed from (Li et al., 2018). We set the number of particles Np = 1 during training and Np = 10 for validation and testing. The number of exponentially spaced discrete steps (Li & Coates, 2017) for integrating the ﬂow is Nλ = 29. For each dataset, we conduct two separate experiments minimizing the training MAE (results are used to report MAE, MAPE, RMSE, and P50QL) and the training negative log posterior probability (results are used to report CRPS, P10QL, and P90QL). We also experiment with alternative state transition functions, including the DCGRU (Li et al., 2018) and GRU (Chung et al., 2014). For these, the hyperparameters are ﬁxed to the same values as presented above."
2106.06064,dataset,3,,,THE PEMS DATASETS
2106.06064,dataset,3,,,https://archive.ics.uci.edu/ml/datasets/
2106.06064,dataset,32,,,"Table 25. Execution time, memory consumption (during training) and model size for AGCRN-ensemble, GMAN-ensemble and AGCGRU+ﬂow for the four PeMS datasets. Lower numbers are better."
2106.06064,dataset,36,,,Table 6. Normalized Deviation on Electricity and Trafﬁc datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better.
2106.06064,dataset,38,,,Figure 3. Boxplot of ranks of the top 10 algorithms across the four trafﬁc datasets. The means of the ranks are shown by the black triangles; whiskers extend to the minimum and maximum ranks.
2106.06064,dataset,40,,,Figure 6. Scatter-plots of average MAE at each node for AGCGRU+ﬂow v.s. that of AGCRN on PeMS datasets. The AGCGRU+ﬂow has lower average MAE compared to AGCRN at most of the nodes for all four datasets.
2106.06064,dataset,40,,,"Table 7. Average CRPSsum for Electricity, Trafﬁc, Taxi, and Wikipedia datasets. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better"
2106.06064,dataset,42,04/21/22,0,"3) we show that the proposed method provides a superior characterization of the prediction uncertainty compared to existing probabilistic multivariate time-series forecasting methods, both for datasets where a graph is available and for settings where no graph is available."
2106.06064,dataset,42,,,"Table 10. Average MAE, MAPE and RMSE for PeMSD3 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,,,"Table 11. Average MAE, MAPE and RMSE for PeMSD4 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,,,"Table 12. Average MAE, MAPE and RMSE for PeMSD7 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,42,,,"Table 13. Average MAE, MAPE and RMSE for PeMSD8 dataset for 15/30/45/60 minutes horizons. The best and the second best results in each column are shown in bold and marked with underline respectively. Lower numbers are better."
2106.06064,dataset,49,,,"Figure 4. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at node 4 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors, which leads to lower quantile error."
2106.06064,dataset,59,,,"Figure 10. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 1, 17, 95, and 164 of PeMSD8 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,,,"Figure 7. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 37, 54, 100, and 187 of PeMSD3 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,,,"Figure 8. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 2, 44, 57, and 213 of PeMSD4 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,59,,,"Figure 9. 15 minutes ahead predictions from the probabilistic forecasting algorithms with conﬁdence intervals at nodes 43, 108, 163, and 201 of PeMSD7 dataset for the ﬁrst day in the test set. The proposed AGCGRU+ﬂow algorithm provides tighter conﬁdence interval than its competitors in most cases, which leads to lower quantile error."
2106.06064,dataset,64,,,"For spatio-temporal predictions using the graph-based recurrent architectures, this can be done if the graph can be partitioned meaningfully. For non-graph datasets, we can use the cross-correlation among different time-series to group them into several lower-dimensional problems. Alternatively, we can train a univariate model based on all the time-series as in (Rangapuram et al., 2018)."
2106.06064,dataset,7,,,10. Description and statistics of datasets
2106.06064,dataset,78,,,"& Graff, 2017) (hourly time-series of the electricity consumption), Trafﬁc (Dua & Graff, 2017) (hourly occupancy rate, of different car lanes in San Francisco), Taxi (Salinas et al., 2019), and Wikipedia (Salinas et al., 2019) (count of clicks to different web links) datasets. The detailed statistics of these datasets are summarized in the supplementary material."
2106.06064,dataset,82,,,The statistics of the PeMS datasets and the non-graph datasets used in our experiments are summarized in Tables 8 and 9 respectively. The description of the PeMS datasets are provided in Section 6.1 of the main paper. The Electricity dataset contains electricity consumption for 370 clients. The Trafﬁc dataset is composed of 963 time-series of lane occupancy rates. The Taxi dataset contains counts of taxis on different roads and the Wikipedia dataset speciﬁes clicks to web links.
2106.06064,dataset,9,,,Dataset No. nodes No. time steps Interval
2106.06064,dataset,92,,,"If our focus is on obtaining a point estimate, then we can perform optimization on the training set with respect to a loss function derived from Mean Absolute Error (MAE) or Mean Square Error (MSE). The point forecast ˆy(m) P +1:P +Q is obtained based on a statistic such as the mean or median of the samples {yj,(m) j=1. The MAE loss function on a dataset indexed by D can then be expressed as:"
2106.06064,dataset,93,,,"Table 25 summarizes the run time, GPU usage during training, and the size of the learned model for AGCRN-ensemble, GMAN-ensemble, and the proposed AGCGRU+ﬂow for the four PeMS datasets. We observe that if we choose the ensemble size so that the algorithms have an approximately equal execution time, then the model-size of the ensemble algorithms are comparable to our approach as well. However, our method requires more GPU memory compared to the ensembles during training because of the particle ﬂow in the forward pass."
2106.06064,dataset,93,,,"The novel contributions in this paper are as follows: 1) we propose a graph-aware stochastic recurrent network architecture and inference procedure that combine graph convolutional learning, a probabilistic state-space model, and particle ﬂow; 2) we demonstrate via experiments on graph-based trafﬁc datasets that a speciﬁc instantiation of the proposed framework can provide point forecasts that are as accurate as the state-of-the-art deep learning based spatio-temporal models. The prediction error is also comparable to the existing deep learning based techniques for benchmark non-graph multivariate time-series datasets;"
2106.06064,"dataset, code, publicly available",152,,,"6.5. Results and Discussion Comparison with baselines : Results for the point forecasting task are summarized in Table 1. We observe that most of the spatio-temporal models perform better than graph agnostic baselines in most cases. Moreover, the pro Some of the recent spatio-temporal models such as (Chen et al., 2020; Zhang et al., 2020; Park et al., 2020) do not have publicly available code. Although the codes for (Wu et al., 2020; Song et al., 2020; Pan et al., 2019) are available, these works use different datasets for evaluation. We could not obtain sensible results from these models for our datasets, even with considerable hyperparameter tuning. The code for (Kurle et al., 2020; de B´ezenac et al., 2020) is not publicly available."
2106.06064,"dataset, used dataset",29,,,"been used for the Electricity dataset, and three different splits have been used for the Trafﬁc dataset. The evaluation metric is P50QL (Normalized Deviation)."
2106.06064,github,1,,,//github.com/networkslab/rnn_flow
2106.06064,github,3,,,https://github.com/mbohlkeschneider/
2106.14178,"data, code, publicly available, code available",122,,,"In this work, we proposed a novel residual moment loss function to extract location information in medical image segmentation. Motivated by image moments, we explicitly encoded the coordinate information of pixels (or voxels) to the RM loss, which is simple but can capture the target location eﬀectively. In addition, our method is also easy to optimize with high computational eﬃciency. The experimental results demonstrated that our method could be adapted to various data types and network architectures. The method can also be easily embedded into other network training strategies and used in diﬀerent practical problems, which will be investigated in our future research. Source code will be publicly available."
2106.14178,dataset,128,,,"The LA dataset contains 100 training cases and 54 testing cases. Following the experimental setting in [10], we randomly selected 16 cases for training and 20 cases for testing for a fair comparison with [10]. We cropped all cases centering at the heart region to alleviate the problem of unbalanced categories. All cases are normalized by subtracting the mean and dividing by the standard deviation. We use 3D V-Net [12] as the baseline which is optimized by SGD with the 0.01 learning rate. To prevent overﬁtting, we use dropout during training but turn it oﬀ in the inference stage. Besides, the RM loss weight is α = 0.01 in Eq. 6."
2106.14178,dataset,19,,,Table 2. Left atrial segmentation accuracy with mean (standard deviation) on the LA MRI dataset.
2106.14178,dataset,2,,,Dataset Method
2106.14178,dataset,20,,,Table 1. Optical cup and disk segmentation accuracy with mean (standard deviation) on the GS dataset.
2106.14178,dataset,39,,,"Fig. 3. Visualization of the left atrial segmentation results on the LA MRI dataset. The ﬁrst column is ground-truth. The second and third columns are the results of baseline and our method, respectively."
2106.14178,dataset,43,,,"To verify the eﬀectiveness and generalization of our method, we apply the residual moment loss to 2D and 3D neural networks with various public datasets. All our experiments are implemented with the PyTorch (1.3.0) framework [14]."
2106.14178,dataset,45,,,"We evaluate our method on two datasets, which are the 2D optic cup and disk segmentation dataset: Drishti-GS (GS) [16] and the left atrial (LA) 3D gadolinium-enhanced magnetic resonance imaging (MRI) [19]."
2106.14178,dataset,55,,,"16. Sivaswamy, J., Krishnadas, S., Joshi, G.D., Jain, M., Tabish, A.U.S.: Drishti-GS: Retinal image dataset for optic nerve head (ONH) segmentation. In: IEEE 11th International Symposium on Biomedical Imaging. pp. 53–56. IEEE (2014)"
2106.14178,dataset,68,,,"Fig. 2. The optic cup and disk results of the GS dataset. The green and blue lines represent the contours of ground-truth and the segmentation results, respectively. The ﬁrst row is the results of the baseline and the second row is our method, while the ﬁrst two columns are the cup results and the last two columns are the disk results."
2106.14178,dataset,77,,,"For the GS dataset, we use 50 images for training and 50 for testing. These images are resized to 512 × 512 for computational eﬃciency. We apply the 2D U-Net [15] as the baseline and set the weight of RM loss in Eq. 6 as α = 1. The model is trained by the stochastic gradient descent (SGD) with the learning rate of 0.001 for 2000 iterations."
2106.14178,dataset,87,,,"Our method can be easily extended to 3D. Table 2 and Fig. 3 show the quantitative and qualitative results of the LA dataset. Here, we compare with some implicit position embedding methods, which are boundary loss [8], Hausdorﬀ distance loss [7] and signed distance function loss [20]. The experimental results of these three comparison methods are directly taken from Ma et al. [10] and our experimental settings followed their work."
2106.14178,"dataset, publicly available",228,,,"Abstract. Location information is proven to beneﬁt the deep learning models on capturing the manifold structure of target objects, and accordingly boosts the accuracy of medical image segmentation. However, most existing methods encode the location information in an implicit way, e.g., the distance transform maps, which describe the relative distance from each pixel to the contour boundary, for the network to learn. These implicit approaches do not fully exploit the position information (i.e., absolute location) of targets. In this paper, we propose a novel loss function, namely residual moment (RM) loss, to explicitly embed the location information of segmentation targets during the training of deep learning networks. Particularly, motivated by image moments, the segmentation prediction map and ground-truth map are weighted by coordinate information. Then our RM loss encourages the networks to maintain the consistency between the two weighted maps, which promotes the segmentation networks to easily locate the targets and extract manifold-structure-related features. We validate the proposed RM loss by conducting extensive experiments on two publicly available datasets, i.e., 2D optic cup and disk segmentation and 3D left atrial segmentation. The experimental results demonstrate the eﬀectiveness of our RM loss, which signiﬁcantly boosts the accuracy of segmentation networks."
2107.05429,data,224,,,"As a data-driven supervised learning approach, DNN-based speech enhancement can be mainly categorized into timefrequency domain [2–4] and time domain [5–7] methods. The time-frequency (T-F) domain methods aim to extract the acoustic features (e.g., complex spectrum or logarithmic power spectrum) of clean speech from the features of noisy speech. Common training targets include ideal ratio mask (IRM) [8] and target magnitude spectrum (TMS) [3], etc. The phase spectrum is also considered to beneﬁt the speech quality [9]. However, it is difﬁcult to estimate phase spectrum directly because of its unstructured characteristic. Phase-sensitive mask (PSM) [4] was proposed to exploit phase information for speech enhance ment. More recent methods, such as PHASEN [10], make use of the inter-connection between the magnitude and phase spectrum for better phase estimation. Some other methods retrieve phase implicitly by optimizing the real and imaginary parts of the complex spectrum [11] or estimating complex ratio mask (CRM) [12]. Since complex-valued weights are suitable for modeling the inherent information of the spectrum, complexvalued neural networks [13] have also been used for speech enhancement."
2107.05429,data,57,,,"[28] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 5220–5224."
2107.05429,"data, dataset",113,,,"In order to test the performance under various unknown noise, we also used the test set from WSJ-0 [29] as the test speech. It contains 651 utterances from 8 speakers. There are two noise datasets used for test; one is the music data from MUSAN [30], the other is babble, factory1 and f16 from NOISEX92 [31]. The SNR range of the test noisy speech is the same as the training set. We also evaluated the model on the development test set and blind test set provided by DNS challenge. All the audio used is sampled at 16kHz."
2107.05429,database,51,,,"[31] A. Varga and H. Steeneken, “Assessment for automatic speech Ii. noisex-92: A database and an experiment to recognition: study the effect of additive noise on speech recognition systems,” Speech Communication, vol. 12, pp. 247–251, 1993."
2107.05429,database,52,,,"[27] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,” The Journal of the Acoustical Society of America, vol. 133, p. 3591, 2013."
2107.05429,dataset,112,,,"We trained the DPCRN on the Interspeech 2021 DNS challenge dataset. 60000 clips of reverberant speech (about 500 h) were generated, with 55000 clips for training and 5000 for validation. The noise clips were mainly generated from Audioset [26], DEMAND [27] and Freesound1. In training stage, we randomly split the waves into 5-second segments and convolved them with room impulse responses (RIRs) randomly-selected from openSLR26 and openSLR28 [28]. Then the noisy speech was generated by mixing reverberant speech and noise. The SNR range of the mixture is set between -5 and 5 dB."
2107.05429,dataset,115,,,"The widespread noise and reverberation may seriously degrade the performance of automatic speech recognition (ASR) systems and decrease speech intelligibility in communication. Speech enhancement aims at separating clean speech from background interference for higher speech intelligibility and perceptual quality. Despite the rapid progress of DNN-based speech enhancement recently, its performance in real applications still faces the challenges such as low signal-to-noise ratio (SNR), high reverberation and far-ﬁeld pickup. The Interspeech 2021 deep noise suppression (DNS) challenge [1] is organized to foster more competitive speech enhancement system in adverse environments, and training datasets and evaluation metrics are provided for such purpose."
2107.05429,dataset,119,,,"convolutional layers. The CRM is output from the last transposed convolutional layer. We evaluate the DPCRN on the Interspeech 2021 DNS challenge dataset. Experimental results show that the DPCRN outperforms the baseline models, including NSNet2 [17], DTLN [18] and DCCRN [13]. On simulated test datasets, our model achieves competitive results as baseline models and show better performance in the case of low SNR. With only 0.8M parameters, our model achieves an overall MOS of 3.57 according to the ITU-T P.835 [19] subjective evaluation on DNS challenge blind test set, and reaches the third place in the wide band scenario track."
2107.05429,dataset,131,,,"The performance on simulated WSJ0-MUSAN test set is presented in Table 1. It can be seen that when the SNR is greater than or equal to 0 dB, the performance of DPCRN-1 is slightly weaker than DCCRN, but better than DTLN. It should be noted that DPCRN-1 performs better than DCCRN at lower SNR. Table 2 shows the results on WSJ0-NOISEX92 test set. Under more disruptive noise from NOISEX92, the DPCRN-1 exceeds the baseline models in terms of all three metrics, demonstrating the beneﬁt of the DPRNN module for spectrogram modeling. On both datasets, DPCRN-2 has better performance than DPCRN-1 in terms of PESQ and STOI but its SDR is slightly worse, indicating that including the time-frequency MSE in the"
2107.05429,dataset,3,,,3.1. Datasets
2107.05429,dataset,68,,,"[26] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 776–780."
2107.05429,dataset,95,,,"Inspired by the successful application of DPRNN and CRN, we propose a deep learning-based speech enhancement model in the time-frequency domain, named as DPCRN. It combines the local pattern modeling capability of CNN and the long-term modeling capability of DPRNN. Compared with CRN, DPCRN demonstrates the beneﬁt of RNN for spectrum modeling. With only 0.8M parameters, our model achieves competitive results on various unknown noise datasets. In the future, we will try to reduce the computational complexity of the model for wider band spectrum processing."
2107.09477,data,11,,,Table 1: Summary of the data conditions in VCC2020.
2107.09477,data,17,,,"the statistical properties of the training data, resulting in a collapsed, averaged prosodic style."
2107.09477,data,21,,,"[28] Data Baker China, “Chinese standard mandarin speech cor pus,” accessed 05 May 2020."
2107.09477,data,3,,,4.1. Data
2107.09477,data,31,,,"[46] L. van der Maaten and G. Hinton, “Visualizing data using t SNE,” JMLR, vol. 9, pp. 2579–2605, 2008."
2107.09477,data,43,,,"[15] L. Sun, K. Li, H. Wang, S. Kang, and H. Meng, “Phonetic posteriorgrams for many-to-one voice conversion without parallel data training,” in Proc. ICME, 2016, pp. 1–6."
2107.09477,data,44,,,"training on even approximately 5 minutes of data. To train a textbased synthesizer, human-labeled text is used. However, since annotating ground-truth BNF and VQW2V is impossible, we use the trained recognizer to extract the representations for synthesizer training."
2107.09477,data,45,,,The TP module can be trained jointly with the GST-TTS by using a stop-gradient operator. It was shown that the TP module can generate natural speech that well reﬂects the variations of the training data using the text input alone [14].
2107.09477,"data, dataset, open-source, used dataset",94,,,"All recognizers were trained with the LibriSpeech dataset [24]. For the multispeaker TTS dataset, we used the “clean” subsets LibriTTS dataset [25], except in the text-based system for task 2, where we merged open-source single-speaker TTS datasets in Finnish [26], German [27], and Mandarin [28], as in [4]. For each of the two tasks, a separate neural vocoder was trained with the training data of the source and target speakers."
2107.09477,"data, dataset, used dataset",115,,,"We used the VCC2020 dataset [3], which contained two tasks in our evaluation. The data conditions are summarized in Table 1. Both tasks share the same two source English male and female speakers whose data were not used. There were two target male and female speakers of English in task 1 whereas in task 2 there were one male and one female speaker each of Finnish, German, and Mandarin. During conversion, the source speaker’s voice in the source language was converted as if it was uttered by the target speaker while keeping the linguistic contents unchanged. For each target speaker, 70"
2107.09477,"data, open-source",179,,,"For each system, ﬁve random utterances were chosen for each conversion pair. In the naturalness test, recordings of the target speakers were also included and served as the upper bound. In the similarity test for task 2, following [3], we selected three English recordings and two L2 language recordings as the natural reference for the ﬁve converted utterances. All subjective evaluations were performed using the open-source toolkit [44] that implements the ITU-T Recommendation P.808 [45] for subjective speech quality assessment in a crowd using the Amazon Mechanical Turk (Mturk) and screens the obtained data for unreliable ratings. We recruited more than 100 listeners from the United States and had each sample rated by ﬁve different participants on average. Note that to reduce cost, we eliminated SPT systems that freeze GST in the listening tests, since they yield inferior performance compared with systems that do not freeze GST, as shown in Section 5.1. Audio samples are available online9."
2107.09477,dataset,13,,,"Speech Datasets for 10 Languages,” 2019, pp. 1566–1570."
2107.09477,dataset,216,,,"that SPT can be a sub-optimal strategy for prosody modeling in ASR+TTS-based VC. First, the target-speaker-dependent TTS training causes a mismatch between training and conversion, because the speech of the target speaker is used as input to the reference encoder during training but that of the source is used during conversion. A speaker adversarial classiﬁer can alleviate this issue [5, 13] but requires careful hyperparameter tuning. Second, there are scenarios where SPT is not desired, such as emotion VC or accent conversion. In this work, we examine two prosody modeling methods for ASR+TTS-based VC. In addition to SPT, we propose a novel technique, which we refer to as target text prediction (TTP). We borrow the idea from [14] and train a text prediction (TP) module to generate the prosody embedding from the text derived from the source speech. Figure 1b illustrates this process. The TP module is ﬁrst pretrained with a GST-TTS on a multispeaker dataset, and further ﬁne-tuned in a target-speaker-dependent manner. As a result, TTP does not suffer from a mismatch between training and conversion unlike SPT. Our contributions in this work are as follows."
2107.09477,dataset,23,,,"[27] Munich Artiﬁcial Intelligence Laboratories GmbH, “The MAILABS speech dataset,” 2019, accessed 30 November 2019."
2107.09477,dataset,53,,,"The ASR and TTS models adopt sequence-to-sequence (seq2seq) structures, which were shown to improve conversion similarity by modeling the long-term dependencies in speech. Note that the two models can be separately trained and thus beneﬁt from advanced techniques and a wide variety of datasets in their own ﬁelds."
2107.09477,dataset,82,,,"ASR: A multispeaker dataset DASR ensures the speaker independence of the recognizer. TTS: Synthesizer training involves a pretraining and a ﬁne-tuning stage. Pretraining is performed on a multispeaker TTS dataset DTTS, which is followed by ﬁne-tuning on the limited target speaker dataset Dtrg. This is a common practice in building modern neural TTS models, as pretraining ensures stable quality and ﬁne-tuning retains high speaker similarity [21]. Such a training strategy allows for"
2107.09477,github,3,,,5https://github.com/pytorch/fairseq/tree/master/
2107.09477,github,3,,,8https://github.com/kan-bayashi/ParallelWaveGAN
2107.09477,github,3,,,9https://unilight.github.io/Publication-Demos/
2107.09477,github,6,,,6https://kaldi-asr.org/models/m8 7https://github.com/espnet/espnet/tree/master/
2107.09477,github,8,,,implementation on ESPnet: https://github.com/espnet/espnet/tree/master/ egs/vcc20
2107.09477,open-source,135,,,"All synthesizers map their respective inputs to 80-dimensional mel ﬁlterbanks with 1024 FFT points and a 256-point frame shift (16 ms). The x-vector [37] was used as the speaker embedding, and we used the pretrained model provided by Kaldi 6. The average of all x-vectors of the training utterances of each speaker was used during inference. Synthesizers with discrete input including text and VQW2V had a Transformer-TTS architecture [38] with detailed settings [4, 17]. For the BNF-based synthesizer, we adopted the Voice Transformer Network (VTN) [39, 40] and followed the ofﬁcial implementation7. For the neural vocoder, we adopted the Parallel WaveGAN (PWG) [41] and followed the open-source implementation8."
2107.09477,open-source,180,,,"For task 2, all systems showed performance degradation brought about by SPT, and TTP signiﬁcantly outperformed SPT for all representations. For systems based on frame-level features, TTP could even outperform the baseline and was also comparable to the textbased system. To investigate this gap, we plotted the breakdown per target language in the bottom graphs of Figure 3. We suspect that the relative performance change was again correlated with the text preprocessing, as stated in Section 5.1.2. As in [4], thanks to the open-source community, we utilized G2P tools to convert both English and Mandarin text into phonemes, resulting in a better acoustic model and a larger improvement brought about by TTP. On the other hand, because of the lack of linguistic knowledge, characters were used for Finnish and German, resulting in degradation when combined with TTP. We thus conclude that TTP is an effective method for improving naturalness in task 2, if the input representation is properly processed."
2107.09477,open-source,33,,,"[44] B. Naderi and R. Cutler, “An Open Source Implementation of ITU-T Recommendation P.808 with Validation,” in Proc. Interspeech, 2020, pp. 2862–2866."
2107.09477,open-source,58,,,"[30] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, “Espnet-TTS: Uniﬁed, Reproducible, and Integratable Open Source End-toin Proc. ICASSP, 2020, pp. End Text-to-Speech Toolkit,” 7654–7658."
2107.09477,publicly available,108,,,"The system was implemented using ESPnet, a well-developed opensource end-to-end (E2E) speech processing toolkit [29, 30]. Following [4], the ASR model for the text-based system was based on the Transformer [31–33] with joint CTC/attention loss [34], and a RNNbased language model for decoding4. The ASR model for BNF extraction was based on TDNNF-HMM [35], where we concatenated 40-dimensional MFCCs and 400-dimensional i-vectors as input. For VQW2V, we used the publicly available pretrained model provided by fairseq [36]5, as in [17]."
2108.05075,"code, github",15,,,"[52] “Code for synthesizing adversarial patch attack,” https://github.com/A-LinCui/"
2108.05075,"code, github",21,,,"[41] “Code for lgs defense,” https://github.com/metallurk/local_gradients_smoothing. [42] https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial."
2108.05075,"code, github",61,,,"[44] “Code for patchguard defense,” https://github.com/inspire-group/PatchGuard. [45] X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang, “Interpretable deep learning under fire,” in 29th {USENIX} Security Symposium ({USENIX} Security 20), 2020."
2108.05075,"code, publicly available, github, code available",9,,,Our code is publicly available at https://github.com/DependableSystemsLab/
2108.05075,data,110,,,"We assume that the defender has exclusive access to a hold-out set, which can be created by sampling a series of images from the data distribution. The attacker is allowed to replace a contiguous region of an image with the adversarial patch. As in most prior work, we primarily consider square patches [15, 19, 24–26]. That said, Jujutsu can generalize to other shapes such as circular and rectangular patches, as we show in Section 4.7. The attacker’s goal is to generate a robust and universal adversarial patch that can trigger targeted misclassification in the DNNs."
2108.05075,"data available, dataset",133,,,"45%. We do not consider patches of larger size because a 7% patch is already able to achieve very high attack success rates (average 99%) and a larger patch will make the adversarial samples become visually more suspicious. For each patch, we train it for 30 epochs on a training set with 2000 images and evaluate the attack success rate on a separate test set and choose the one with the highest success rate. For the attack evaluation on ImageNette and CelebA, we use the entire test set in each dataset; for ImageNet and Place365, we use 10000 images from the validation set for each. Examples of adversarial samples for each dataset can be found in Fig. 6."
2108.05075,database,43,,,"[35] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places: A 10 million image database for scene recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017."
2108.05075,database,44,,,"[32] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE conference on computer vision and pattern recognition."
2108.05075,dataset,1,,,Dataset
2108.05075,dataset,101,,,"2.2 Threat Model This work assumes a white-box attacker, who has full knowledge of the victim DNN such as its structure, parameters. We assume however that the attacker has no knowledge of the exact inputs to the DNN, but instead has access to a surrogate dataset, which follows the same distribution as the legitimate inputs. This is similar to the assumptions in universal attack studies, and it is shown that the knowledge of the input distribution often suffices for the attacker to generate universal adversarial perturbations [2, 3, 5]."
2108.05075,dataset,103,,,"Our evaluation shows that AT’s performance degrades as the number of target classes increases, on both robust accuracy and FP. This is because with more target classes, the learning objective for AT becomes increasingly difficult - this is similar to how common DNNs would yield lower accuracy on a 1000-class dataset than on a simple 10-class dataset. On the other hand, we see that Jujutsu achieves consistently high performance in terms of both robust accuracy and FP across attacks targeting different classes. Further, Jujutsu yields significantly better performance than AT in all cases."
2108.05075,dataset,107,,,"CelebA. CelebA [34] is a large-scale face dataset with more than 200k celebrity images. We follow the implementation in [36] to first create a 307-class subset from the original dataset, where each class represents a celebrity identity and contains at least 15 images. The resulting dataset contains 4263 training images and 1215 validation images. We use a pre-trained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 30 epoches, and we use the SGD optimizer with an initial learning rate of 0.01 and momentum of 0.9."
2108.05075,dataset,11,,,ImageNet. ImageNet [32] is 1000-class datasets with high-resolution
2108.05075,dataset,111,,,"4.1.3 Attack Setup. The attacker’s goal is to synthesize adversarial patches that achieve both high attack success rate and remain stealthy. Therefore, for each dataset, we generate patches of different sizes, occupying 5%, 6% and 7% of the image pixels. We use 𝑥% patch to refer to a patch that occupies 𝑥% of the pixels of the image. We do not consider patches of smaller size because we find that they are unable to universally cause misclassification, e.g., use of a 4% patch on CelebA degraded the attack success rate by more than"
2108.05075,dataset,12,,,"• Evaluate Jujutsu on 4 datasets (ImageNet, ImageNette, CelebA"
2108.05075,dataset,134,,,"Benign inputs are the same as the adversarial inputs, except that they do not have the adversarial patch. We consider an image adversarial if and only if the predicted label for it is identical to that of both the hold-out images implanted with the suspicious features. Table 1 shows Jujutsu’s detection performance on all 4 datasets. Detection success recall. Jujutsu is able to consistently detect adversarial samples, with a detection success recall rate of over 93% across patch sizes (in most cases). The detection success recall increases with the size of the patch as a larger patch has a higher attack success rate. On average, Jujutsu can detect around 96% of the adversarial samples on all the datasets."
2108.05075,dataset,139,,,"4.1.2 Datasets. We evaluate Jujutsu on ImageNet [32], ImageNette [33], CelebA [34] and Place365 [35]. ImageNet is a 1000-class dataset and we use the pre-trained ResNet-50 from the torchvision library. ImageNette is a 10-class subset of ImageNet and we train a ResNet18 on this dataset. CelebA is a facial dataset with diverse celebrity faces. We created a 307-classes subset from the original set and train a ResNet-18 model following [36] to perform identity classification. Place365 is a 365-class dataset containing common natural sceneries (e.g., patio, restaurant) and we use the pre-trained ResNet-50 from [37]. All images are resized to 224*224. We provide more details of the datasets in Appendix A.1."
2108.05075,dataset,160,,,"Mitigation success recall: While masking alone is able to achieve higher detection recall compared to masking and inpainting when the masking percentage is small, the difference becomes negligible when the masking percentage increases. This is because when the masking percentage is low, the masked images are more likely to have a label different from those of the original images; while the inpainted images are more likely to have the same label as the original image - this is similar to the reason why robust accuracy from masking alone is higher than that from masking and inpainting for 25% masking. However, when the masking percentage increases, both the masked and inpainted images are likely to have labels that are different from that of the original image - thus the difference becomes negligible between both approaches. We see that Jujutsu is highly effective in detecting adversarial samples on all the datasets."
2108.05075,dataset,163,,,"derivative. The parametric softplus function can be expressed as: 𝑓 (𝑥) = 1 𝛼 log(1 + exp(𝛼𝑥)), where 𝑥 is the original input to the ReLu function, and 𝛼 is the hyper-parameter to control the shape of the curve. We follow Xie et al. [47] to empirically set 𝛼 as 10 in our experiment. Finally, we only use the parametric softplus for backward propagation, and use ReLU for the normal forward pass. To be conservative, we consider the 7% patch, which allows the attacker to inject larger perturbations to evade Jujutsu. We choose 200 samples for training the adversarial patch, 500 steps per sample and 20 epochs in total. For each dataset, we choose 𝛽 ∈ [0.1, 0.5, 1, 5] and choose the one yielding the highest attack success rate."
2108.05075,dataset,207,,,"(2) Mitigation FPR is the (reduced) FPR from the two-staged combination of detection and mitigation (explained in Section 3.3.3). (3) Mitigation success recall is the detection recall from the combination of detection and mitigation (explained in Section 3.3.3) - we distinguish this from the detection success recall, which is the detection recall from the detection technique alone. Mitigation success recall gives the final amount of adversarial samples detected by Jujutsu. Result. Table 2 shows Jujutsu’s mitigation performance on all 4 datasets. The results are averaged across patches of different sizes (5% to 7%). The detection performance is higher on larger patches as these patches have higher attack success rate (difference between the largest and smallest patch is about 9%) and the mitigation performance is consistent across different patch sizes (differences on robust accuracy and FP are both less than 2%). We consider two mitigation techniques, (1) masking alone, and (2) masking with inpainting (our mitigation technique). We discuss the results in terms of the 3 aforementioned metrics."
2108.05075,dataset,22,,,Jujutsu is able to detect an average of 95.93% of the adversarial samples with 3.33% FPR on 4 datasets.
2108.05075,dataset,235,,,"Equation 5 requires several forward and backward passes for calculating the saliency map ˆ𝑀∗ 𝑗 (𝑥), which is much more timeconsuming than the original optimization (Equation 3). Therefore, we reduce the sampling size 𝑛 in Equation 4 from 50 to 5 for faster training. We experimentally verified that the smaller sampling size 𝑛 does not significantly affect the resulting saliency map, and that we can still find all the salient features. Under this setting, it took around 18 days to generate an adversarial patch on Place365 dataset, compared to about 540 days if we had followed our previous setup. Result. We compare the attack success rate of the patches generated from the undefended models and the ones guarded by Jujutsu in Fig. 9. With the protection of Jujutsu, the adaptive attacker who attempts to evade Jujutsu’s detection suffers a significant drop in attack success rate, from 99% to just 4.9% (on average). This is because in Equation 5, the first term aims to increase the influence on the final prediction to manipulate the output label; while the second term reduces the influence on the output. This equation constrains the adaptive attacker, who cannot evade detection without also significantly degrading the attack’s effectiveness."
2108.05075,dataset,30,,,This section shows Jujutsu’s performance when using different number of hold-out images for attack detection on the ImageNet dataset. The results are presented in Table 9.
2108.05075,dataset,33,,,"accuracy on ImageNette, as it is a 10-class dataset, and performing correct image classification on this dataset is easier than on the other complicated datasets such as the 1000-class ImageNet."
2108.05075,dataset,36,,,"When 75% of the perturbations are masked, it is almost infeasible for the attacker to generate a successful adversarial patch, and hence the success rate is near 0% on all datasets."
2108.05075,dataset,45,,,"We train the rectangular patches on each dataset using a 7% patch (36*96). We change the square bounding box to a rectangular one, which occupies around 20% of pixels as before. Note that we do not"
2108.05075,dataset,48,,,Place365. Place365 [35] is a large-scale dataset containing 365 unique scene categories. We use the pre-trained ResNet-50 model from [37]. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments.
2108.05075,dataset,49,,,Detection FPR. Jujutsu yields an average FPR of 3.3% on the 4 datasets. We find that the FPRs on the two object-recognition datasets (ImageNet and ImageNette) are higher than that on the facial and scenery datasets. This is because the salient features in
2108.05075,dataset,56,,,know the exact width/height ratio of the rectangular shape created by the attacker. Our detection bounding box has a width/height ratio of 6:4. Table 7 shows the results. We find that Jujutsu is able to achieve high detection and mitigation performance on rectangular patches with a very low FPR across different datasets.
2108.05075,dataset,74,,,"We evaluate Jujutsu on four diverse datasets (ImageNet, ImageNette, CelebA and Place365), and show that Jujutsu achieves superior performance and significantly outperforms existing techniques. We find that Jujutsu can further defend against different variants of the basic attack, including 1) physical-world attack; 2) attacks that target diverse classes; 3) attacks that construct patches in different shapes and 4) adaptive attacks."
2108.05075,dataset,74,,,"object-recognition datasets might contain the entire object (e.g., a small bird), which can cause the model to continue to assign the same label to the transplanted image. However, for the facial and scenery datasets, the salient features only contain a fraction of the image pixels (e.g., a partial face), which is unlikely to result in the same label on the transplanted image."
2108.05075,dataset,76,,,"Under the VGGFace2 dataset, Jujutsu achieves a robust accuracy of 37.26%2, which is significantly higher than that of 0.2%3 by Februus. This is because Februus relies on the pre-defined threshold to identify the regions associated with adversarial patch. This method would fail to locate the adversarial patch if the patch’s influence to the prediction is lower than the threshold, and our experiment validates this."
2108.05075,dataset,77,,,"4.6 RQ5 - Attacks Targeting Different Labels This section evaluates Jujutsu against attacks that target different class labels. For each target label, we need to perform training to generate the universal adversarial patches. Unfortunately, training is highly time-consuming, and hence, for each dataset, we train five 7% patches targeting different labels. Note that training is only needed for creating the adversarial patches, and not for Jujutsu."
2108.05075,dataset,77,,,"We evaluate Jujutsu on 4 datasets and show that Jujutsu achieves superior detection and mitigation performance, and significantly outperforms existing techniques. We also demonstrate Jujutsu’s effectiveness in defending against different variants of the patch attack, including: 1) physical-world attacks; 2) attacks that use patches in various shapes; 3) attacks that target diverse classes. Finally, Jujutsu can thwart adaptive attacks in fooling the DNNs."
2108.05075,dataset,78,,,"We also notice that the robust accuracy by Jujutsu on CelebA is lower than that on the other datasets, which is because the inpainting technique needs to synthesize the correct facial features belonging to a particular celebrity’s face to enable correct identity prediction. This is a much more challenging task for image inpainting than for the other three datasets, and hence Jujutsu yields a lower robust accuracy. Jujutsu achieves the highest robust"
2108.05075,dataset,81,,,"Mitigation FPR: Masking with inpainting achieves low FPR, because the inpainted inputs are more similar to the original benign inputs than the masked inputs (in the latter case many features are simply masked). Therefore predictions on the original and inpainted inputs are more likely to be the same, which is not the case for inputs that are merely masked. We also see that Jujutsu consistently achieves very low FPRs on all the datasets."
2108.05075,dataset,9,,,Figure 6: Adversarial samples for each dataset.
2108.05075,"dataset, code",151,,,"Jujutsu has a lower robust accuracy on VGGFace2 than those on the other datasets due to the insufficient performance yielded by the inpainting technique (PICNet [29]). This is because we need to train the PICNet from scratch on VGGFace2, which is very time-consuming as VGGFace2 is a very large dataset. We trained the PICNet on a small subset of the dataset for a week and used it in our evaluation due to time constraint. The performance of Jujutsu can be further improved with more resources to train the PICNet (e.g., increase the size of training set and number of epoches). 3To ensure the code was implemented correctly, we verified that the code was able to reproduce the results reported in the original paper for trojan attack. We then used the code to evaluate against patch attacks."
2108.05075,"dataset, code, github",79,,,"[48] “Code for strip defense,” https://github.com/garrisongys/STRIP. [49] “Code for februus defense,” https://github.com/AdelaideAuto-IDLab/Februus.git. [50] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “Vggface2: A dataset for recognising faces across pose and age,” in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)."
2108.05075,"dataset, code, github, data https",39,,,"[37] “Place365 dataset,” https://github.com/CSAILVision/places365. [38] “Code for smoothgrad,” https://github.com/hs2k/pytorch-smoothgrad. [39] “Code for image inpainting technique,” https://github.com/lyndonzheng/"
2108.05075,"dataset, dataset provided",30,,,A APPENDIX A.1 Dataset Details We provide the details for each of the 4 datasets in our evaluation below. All the images are resized to 224*224.
2108.05075,"dataset, github, data https",12,,,"[51] “Imagenette dataset,” https://github.com/fastai/imagenette."
2108.05075,"dataset, github, data https",13,,,"[36] “Celeba dataset,” https://github.com/ndb796/CelebA-HQ-Face-Identity-and Attributes-Recognition-PyTorch."
2108.05075,"dataset, github, data https",53,,,"[33] “Imagenette dataset,” https://github.com/fastai/imagenette. [34] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of International Conference on Computer Vision (ICCV), December 2015."
2108.05075,"dataset, used dataset",116,,,"images. We use the pre-trained ResNet-50 model from the torvision library. We use 2000 images from the validation set for attack generation and 10000 images for attack evaluation in our experiments. ImageNette. ImageNette [33] is a 10-class subset of ImageNet dataset, with 9469 training images and 3925 validation images. We used this dataset to compare with adversarial training. We use a pretrained ResNet-18 model from the torchvision library and retrain the entire model on this dataset. The training consists of 20 epoches, and we use the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.0013 and momentum of 0.9."
2108.05075,"python, dataset",192,,,"4.1.4 Defense Setup. For saliency map generation, we use the implementation from [38]. For image inpainting, we use the implementation from [39] and their pre-trained models for each dataset. The original implementation does not support pixel-wise random masking, and we added this feature. We use cv2.blur from the Python cv2 library with a filter size of 51 to pre-process the saliency map to make it robust to noise. We then identify the point with the highest value in the saliency map as the center of the detection box. The length of the detection box is set to 102, which is around 20% of the pixels in the images. We sample a total of 1000 random images from the test set as the hold-out dataset. For each image, we empirically choose 2 random images from the the hold-out dataset for feature transfer, as we find that this setup balances the detection success recall and FPR. We show in Appendix A.2 Jujutsu’s performance when we use different numbers of random images."
2108.09408,data,169,,,"Multi-scale feature aggregation based. The problems mentioned above are also partly caused by inappropriate aggregation and fusion. Thus, many articles looked for more eﬃcient and eﬀective methods to aggregate low-level features with detailed information and high-level features with semantic clues. Feng et al. [8] employ a Dilated Convolutional Pyramid Pooling (DCPP) module to generate a coarse prediction based on global contextual knowledge. In MINet [16], the aggregate interaction module can eﬃciently utilize the features from adjacent layers through mutual learning, while the self-interaction module makes the network adaptively extract multi-scale information from data and better deal with scale variation. Li et al. [10] present a residual reﬁnement network with semantic context features, including a global attention module, a multireceptive block module, and a recurrent residual module. These methods are mainly innovative in feature extraction and fusion, but they do not explore features with more possibilities."
2108.09408,data,176,,,"In general, there is no need to down-sample the feature at 7 × 7 scale because it has a suﬃciently large receptive ﬁeld. However, the experiment results in Table. 2 is somewhat counterintuitive. It shows that there is still some diﬀerent global semantic information waiting to be discovered. The version for comparison is a U-shaped vanilla model, which is trained with the initial learning rate 10−4. Here we use the same learning rate to train the parameters in the backbone and modules designed in our method. The rise of M AE, mF , Sm and Em reﬂects ADM’s reliability and validity. Fig. 4 shows the comparison of visualized results between the vanilla network with and without ADM. UENs generate multiscale features on account of the input level and integrate the features with edge information, which guides the model to pay attention to the boundary areas. The experimental data also reveals that the UENs make greater improvements in various aspects."
2108.09408,dataset,1,,,Datasets
2108.09408,dataset,111,,,"Datasets and implementation details. The model is trained on the DUTSTR with 10553 images. In detail, we trained the model using the SGD optimizer with initial learning rate 3e-5, 0.9 momentum, 5e-4 weight decay, and batch size 16. Because the ResNet-50 parameters are pre-trained on ImageNet, the learning rate of this part is a tenth of the randomly initialized parts which is set as 3e-5. Then, the trained model is tested on ﬁve datasets, including DUTS-TE with 5019 images, DUT-OMROM with 5168 images, HKU-IS with 4447 images, ECSSD with 1000 images and PASCAL-S with 850 images."
2108.09408,dataset,133,,,"Abstract. Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at diﬀerent scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edgebased U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods."
2108.09408,dataset,334,,,Datasets mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em mF MAE Sm Em Metrics .745 .049 .862 .860 .868 .044 .911 .914 .692 .064 .809 .837 .871 .038 .907 .937 BMPM[30] .751 .059 .839 .861 .889 .059 .893 .914 .713 .062 .814 .846 .874 .045 .888 .931 RAS[3] .785 .057 .834 .867 .914 .040 .910 .929 .747 .063 .815 .850 .893 .036 .895 .939 R3Net[5] PiCANet[13] .749 .051 .867 .852 .885 .044 .917 .910 .710 .065 .835 .834 .870 .039 .908 .934 MLMSNet[26] .799 .045 .856 .882 .914 .038 .911 .925 .735 .056 .817 .846 .892 .034 .901 .945 .777 .051 .854 .869 .906 .042 .912 .920 .736 .066 .824 .853 .882 .037 .903 .940 PAGE[23] .805 .043 .869 .886 .917 .037 .918 .925 .747 .056 .825 .866 .891 .034 .905 .944 CPD[28] .756 .048 .866 .884 .880 .037 .916 .921 .756 .056 .836 .869 .895 .032 .909 .946 BASNet[19] .840 .035 .888 .902 .925 .033 .924 .927 .766 .053 .838 .870 .840 .062 .855 .859 F3Net[24] .799 .040 .879 .881 .910 .042 .917 .921 .739 .055 .832 .858 .885 .032 PoolNet[12] .941 .767 .048 .865 .879 .880 .040 .918 .922 .739 .059 .837 .854 .878 .038 .907 .942 TDBU[22] .815 .039 .875 .891 .920 .041 .918 .927 .755 .052 .818 .867 .898 .031 .918 .948 EGNet[31] .792 .044 .861 .886 .892 .033 .928 .924 .761 .054 .847 .871 .896 .031 .916 .948 U2Net[18] .828 .037 .884 .917 .924 .033 .925 .953 .756 .055 .833 .873 .908 .028 .920 .961 MINet[16] .855 .034 .892 .910 .930 .034 .924 .925 .773 .051 .838 .873 .914 .027 .919 .954 LDF[25] .870 .031 .904 .917 .936 .028 .934 .929 .790 .052 .851 .881 .917 .026 .925 .956 Ours
2108.09408,dataset,35,,,Table 3. Quantitative comparison with state-of-the-art methods on ﬁve datasets. The best results are highlighted in bold. The best and the second best results are highlighted in red and green respectively.
2108.09408,dataset,46,,,"• We build an eﬃcient framework to fully combine and fuse edge information, detailed information and semantic clues. Many experiments are conducted to illustrate the validity of our algorithm and this model could surpass most models on four large-scale salient object detection datasets."
2108.09408,dataset,93,,,"Quantitative comparison. Table. 3 shows the quantitative evaluation results of the SOTA methods mentioned above and our model in terms of mF , M AE, Sm, and Em. The proposed method consistently performs better than all the competitors across four metrics on four datasets. In terms of Em, our method achieves the second best overall performance, which is slightly inferior to MINet. It is worth noting that MEUNet achieves the best performance in terms of the mean F-measure and structure quality evaluation Sm."
2109.03162,data,146,,,"Abstract argumentation frameworks can be encoded in trivial graph format (TGF) as well as in aspartix format (APX). The following data structures are populated during the parsing of the input graph G: a list arg of the arguments in arg(G); a dictionary argToIdx, mapping each argument x to its position in arg; a dictionary att, mapping each argument x to the set {y | xy ∈ att(G)}; a dictionary attR, mapping each argument x to the set {y | yx ∈ att(G)}. Within these data structures, theories (7)–(13) are constructed in amortized linear time. Single extension computation and extension enumeration is then demanded to the underlying circumscription solver [6]."
2109.03162,python,119,,,"PYGLAF [2] is implemented in Python and uses CIRCUMSCRIPTINO (http://alviano.com/software/circumscriptino/), a circumscription solver extending the SAT solver GLUCOSE [3]. Linear reductions are used for all semantics [4]. For the ideal extension, the reduction requires the union of all admissible extensions of the input graph; such a set is computed by means of iterative calls to CIRCUMSCRIPTINO. The communication between PYGLAF and CIRCUMSCRIPTINO is handled in the simplest possible way, that is, via stream processing. This design choice is principally motivated by the fact that the communication is often minimal, limited to a single invocation of the circumscription solver."
2109.03162,python,48,,,"[4] Mario Alviano. Ingredients of the argumentation reasoner pyglaf: Python, circumscription, and glucose to taste. In Marco Maratea and Ivan Serina, editors, RCRA 2017, volume 2011 of CEUR Workshop Proceedings, pages 1–16. CEUR-WS.org, 2017."
2109.03162,python,92,,,"The PYGLAF reasoner takes advantage of circumscription to solve computational problems of abstract argumentation frameworks. In fact, many of these problems are reduced to circumscription by means of linear encodings, and a few others are solved by means of a sequence of calls to an oracle for circumscription. Within PYGLAF, Python is used to build the encodings and to control the execution of the external circumscription solver, which extends the SAT solver GLUCOSE and implements algorithms taking advantage of unsatisﬁable core analysis and incremental computation."
2109.04359,data,116,,,"Table 3, illustrates the clusters identification by Normal Mixture algorithm for turbine T01. For this study six clusters as operational modes were selected, which has been illustrated in details (Figure 4). Reviewing the charts, seven or eight clusters very correctly describe the operations as well, which might be operationally more accurate. However, for the purpose of this study six operational modes were accurate enough, since did not need to differentiate between two classes of idling modes. Reviewing the operational diagram with eight clusters although, splits Sub-Rated data points, while might be a correct assessment is not the intention of this study."
2109.04359,data,125,,,"Wind energy industry has been reviewing power curve designs for effectiveness in analysis (Sohoni et al., 2016), exploring methods for using new analytics techniques (Yang et al., 2014). For this analysis, a third element of rotor speed has been added as the third axis to the power curve visualization, while four parameters have been used to cluster the overall operational data points (fourth element being blade or pitch angle). Figure 4, illustrates the extended power curve, including the rotor speed, in a three-dimensional scatterplot space. As can be seen the extended power curve includes effects of rotor speed among with generated power and wind speed more clearly."
2109.04359,data,154,,,"1. Abstract The main objective of this paper is finding effective gearbox condition monitoring methods by using continuously recorded monitoring SCADA (Supervisory Control and Data Accusation) data points. Typically for wind turbine gearbox condition monitoring; temperature readings, high frequency sounds and vibrations in addition to lubricant condition monitoring have been used. However, collection of such data, require shutting down equipment for installation of costly sensors and measuring lubricant quality. Meanwhile, operational data usually collected every 10 minutes, comprised of wind speed, power generated, pitch angle and similar performance parameters can be used for monitoring health of wind turbine components such as blades, gearbox and generator. This paper uses gear rotational speed for monitoring health of gearbox teeth; since gearbox teeth deterioration can be measured by monitoring rotor to generator rotation ratios over extended period of time."
2109.04359,data,155,,,"4. Operating modes and Operational Parameters Industrial equipment and processes run in multiple operational modes, depending on the physical characteristics of process input (e.g., wind speed) and output (e.g., electrical generation) parameters (Pandit & Infield, 2018a). The operating modes may include stop, start, partial or full production, depending on the actual operating requirements such as electrical demand from grid operators. In order to analyze industrial operations, the data points must be grouped based on natural characteristics implied within data values, which uniquely identify the operating mode (Xie et al., 2013). Data points from multiple operating modes, can be combined, for analysis if the operating modes have similar characteristics such as power generation and wind speed (Turnbull et al., 2020), specifically towards the objectives of the analysis."
2109.04359,data,161,,,"Normal Mixture uses AIC (Akaike information criterion) and BIC (Bayesian information criterion) as measuring indicators of fit among clustering models (Aho et al., 2014). Both AIC and BIC are known as penalized-likelihoods, sum of errors, to the center of clusters, with different sensitivity towards number of parameters. BIC penalized models with larger parameters, while disadvantageous for analysis of larger data sizes (Brewer et al., 2016). Since the turbine data clustering comprised of only four parameters and large number of data points, both methods of AIC or BIC result very similar measurements of fit (Proust, 2021). Authors selected AIC as indicator of fit for this paper, which produced similar results as BIC. Discussion of selecting the best clustering method and indicator of fit can be considered an independent research topic and have been excluded from this paper."
2109.04359,data,40,,,"Zhang, Z. Y., & Wang, K. S. (2014). Wind turbine fault detection based on SCADA data analysis using ANN. Advances in Manufacturing, 2(1), 70–78. https://doi.org/10.1007/s40436-014-0061-6"
2109.04359,data,48,,,"Turnbull, A., Carroll, J., & McDonald, A. (2020). Combining SCADA and vibration data into a single anomaly detection model to predict wind turbine component failure. Wind Energy, 24(3), 197–211. https://doi.org/10.1002/we.2567"
2109.04359,data,48,,,"Yin, S., Ding, S. X., Xie, X., & Luo, H. (2014). A Review on Basic Data-Driven Approaches for Industrial Process Monitoring. IEEE Transactions on Industrial Electronics, 61(11), 6418–6428. https://doi.org/10.1109/tie.2014.2301773"
2109.04359,data,51,,,"7. Conclusion Condition based monitoring requires precise and accurate measurement data captured over a time period covering variety of conditions (e.g., wind speed and power production), as well as failures in order to be able to detect minute change in the behavior of overall process."
2109.04359,data,53,,,"In order to analyze the gear ratios, the overall operating time has been divided into modes with similar physical characteristics. The process of splitting data into the operating modes is performed by clustering algorithms, which can be categorized under unsupervised machine learning (Yin et al., 2014)."
2109.04359,data,60,,,"Kim, K., Parthasarathy, G., Uluyol, O., Foslien, W., Sheng, S., & Fleming, P. (2011). Use of SCADA Data for Failure Detection in Wind Turbines. ASME 2011 5th International Conference on Energy Sustainability, Parts A, B, and C. Published. https://doi.org/10.1115/es2011-54243"
2109.04359,data,65,,,"The data recordings contain components of wind turbines illustrated in Table 1, total of 85 data elements, including minimum, maximum and standard deviations of 10-minute intervals. In this paper only a small portion of the data elements have been used which were related to rotor to generator speed ratio and the main factors such as wind speed and power produced."
2109.04359,data,99,,,"The objective of clustering is finding the operating modes, for data points with similar characteristics, which will simulate the actual operations over the timeframe of study. For this research six clusters produced a reasonable total error, which each cluster could be considered an operational mode. Interestingly as Figure 3, illustrates, AIC for Turbine T01 is close to lowest and for turbine T09 the highest total error respectively. Lower AIC for turbine T01 means the data points could be assigned to clusters with less error for turbine T01 than turbine T09."
2109.04359,"data, data available",74,,,"Operational Modes and Physical Characteristics Each different operational mode causes different types of gears behavior based on wind intensity, torque and vibration frequencies (Table 6). The data available for this analysis based on 10 minutes intervals cannot be used for analysis of dynamic forces for each operational mode; however, can be used for measuring gear ratios, as an indication of gear tooth wear over time."
2109.04359,"data, data https",39,,,"Yang, W., Court, R., & Jiang, J. (2013). Wind turbine condition monitoring by the approach of SCADA data analysis. Renewable Energy, 53, 365–376. https://doi.org/10.1016/j.renene.2012.11.030"
2109.04359,"data, data https",42,,,"Liu, X., Lu, S., Ren, Y., & Wu, Z. (2020). Wind Turbine Anomaly Detection Based on SCADA Data Mining. Electronics, 9(5), 751. https://doi.org/10.3390/electronics9050751"
2109.04359,"data, data https",45,,,"Zaher, A., McArthur, S., Infield, D., & Patel, Y. (2009). Online wind turbine fault detection through automated SCADA data analysis. Wind Energy, 12(6), 574–593. https://doi.org/10.1002/we.319"
2109.04359,"data, data https",47,,,"Wilkinson, M., Darnell, B., Delft, T., & Harman, K. (2014). Comparison of methods for wind turbine condition monitoring with SCADA data. IET Renewable Power Generation, 8(4), 390–397. https://doi.org/10.1049/iet-rpg.2013.0318"
2109.04359,"data, data https",49,,,"Du, M., Yi, J., Mazidi, P., Cheng, L., & Guo, J. (2017). A Parameter Selection Method for Wind Turbine Health Management through SCADA Data. Energies, 10(2), 253. https://doi.org/10.3390/en10020253"
2109.04359,"data, data https",51,,,"Qiu, Y., Chen, L., Feng, Y., & Xu, Y. (2017). An Approach of Quantifying Gear Fatigue Life for Wind Turbine Gearboxes Using Supervisory Control and Data Acquisition Data. Energies, 10(8), 1084. https://doi.org/10.3390/en10081084"
2109.04359,"data, dataset",66,,,"3. Operational Wind Farm Data The data used in this analysis consist of 5 turbines from EDP Renewables Spain, open data center dedicated to wind energy research. The dataset contains 521, thousand data points of five wind turbines, 2MW production capacity each. The data recordings are for the period of beginning 2016 to end of 2017, 10-minute intervals."
2109.04359,"data, dataset",80,,,"Clustering Operational Modes Clustering is an unsupervised machine learning technique, used for grouping of data points, which have similar characteristics. For wind turbine analytics the similarities are based on the four identified parameters, which are also closely related to the operational modes of the turbines. The algorithm used in this analysis is Normal Mixture, which is an effective classification algorithm for large datasets with overlapping clusters (Fraley & Raftery, 2007)."
2109.04359,dataset,16,,,Grid Integration Nacelle Tower Pitch Rotor Meteorological Table 1. Wind turbine components within the dataset
2109.04359,dataset,39,,,"Using the parameters defined in Figure 1, wind turbine operating modes can be identified using statistical clustering methods. For clustering the wind turbines dataset four parameters illustrated in Figure 1, were selected as the following:"
2109.04359,dataset,55,,,"9. Acknowledgements Author would like to express gratitude to EDP Renewables Spain, for providing datasets such as wind turbines and solar panels, free of charge available to research community. Also great gratitude to CFREF (Canadian First research Excellent Fund) for providing the opportunity working on green energy technologies."
2109.04359,dataset,76,,,"As nature of wind is turbulent with rapid fluctuations, a wind turbine may operate in variety of modes within relatively short period of time. Monitoring rotational speed ratio over time, requires consistent operational conditions such as wind speed and torques within the gearbox. This paper also introduces the concept of clustering such as Normal Mixture algorithm for dividing operating datasets into consistent subgroups, which are used for long term monitoring."
2109.04359,dataset,86,,,"There have been extensive research into gear tooth wear, fatigue, pitting and microcracks conducted with reliability engineers for decades. In order to analyses different types of gear tooth failures, more detailed dataset with higher sampling frequency in addition to oil analysis is required (Zeng et al., 2020). Given the available dataset, analysis of gear ratio change over time is feasible, which is an indication of gear tooth wear (Jiang et al., 1999)."
2109.04359,dataset,94,,,"Power Curve Analysis Power curves (active power generated versus wind speed) have been used within the wind energy industry for performance measuring and production prediction under different wind regimes (Sohoni et al., 2016). Table 2, illustrates the power curves of the wind turbines within the analysis dataset. Although power curves are similar in shape, they also exhibit differences which analysis of the detailed cause, are out of scope for this paper. As illustrated in Table 2, Turbine T06 exhibits the most clearly"
2109.05366,code,98,,,"In this work we show that these limitations are not inherent in GPUfs design, and demonstrate how to achieve high performance that is singificantly faster than the original design, and even exceeds the throughput of a commonly-used traditional CUDA-based [8] baseline where files are accessed from the CPU code. The key to our optimization is a GPU-side I/O readahead prefetcher and page cache replacement mechanism. Together they alleviate the GPUfs performance limitations, by adjusting the GPU I/O layer to match the characteristics of the operating system I/O mechanisms and PCIe."
2109.05366,data,101,,,In the experiments we compare three implementations: (1) CPU I/O – standard CPU I/O (1 CPU thread) that reads the whole file at once with cudaMemcpy to transfer data to the GPU; (2) GPUfs with 64KB pages to show the upper bound on the performance as it results in the best performance; (3) GPUfs with 4KB pages and the prefetcher configured to 64KB. (4) The original GPUfs with 4KB pages without the prefetcher. GPUfs is configured with 2GB page cache in all of the cases.
2109.05366,data,113,,,"Unfortunately, using GPUfs for sequential accesses results in low I/O performance. Here, GPUfs API dictates that every GPU threadblock reads one file stride at a time in a data parallel manner, and then processes that stride in the threadblock. This access is similar to the one observed in multi-core and multi-node workloads [1], but results in hundreds of concurrent I/O accesses to the disk. Our analysis reveals a number of non-trivial performance problems emerging due to intricate interplay between the GPU I/O access pattern, the performance characteristics of the PCIe bus for CPU-GPU transfers, and the CPU OS readahead prefetching mechanism."
2109.05366,data,114,,,"00.511.522.54KB16KB64KB128KB256KB512KB1MB2MB4MBGB/sRequest SizeGPUCPU_GPUpatt051015204ΚΒ16ΚΒ64ΚΒ128KB256KB512KB1MB2MB4MB#spins in the queueRequest Sizethread0thread1thread2thread30123454KB16KB64KB128KB256KB512KB1MB2MB4MBGB/SREQUEST SIZEthe GPU read request, it transfers the data into a staging buffer in the GPU memory, and each threadblock retrieves the page that fulfills its own request as part of the GPU I/O call. The purpose of the GPU prefetcher is to move data to the GPU in larger chunks which include more pages in addition to the one in the request. Thus, by the time the GPU threadblock retrieves the requested page, the prefetched data is already in the GPU, hence the benefits of asynchronous prefetching are diminished. Thus, our design uses synchronous prefetching."
2109.05366,data,138,,,"GPU I/O readahead prefetcher in the CPU?. The location of the prefetch logic is an important design question. One option is to place the prefetcher on the CPU side. Thus, for every GPU I/O request, the CPU might move larger blocks to the CPU. This design, however, requires the CPU to be aware of the internal memory allocation in the GPU file system layer and the page cache, and is hard to implement without having PCIe support for atomic operations. Our design uses GPUfs itself to request larger chunks from the CPU, thereby allowing the on-GPU GPUfs logic to allocate memory, and ensure consistent access to data. This design is also similar to the way the OS prefetcher is implemented in the CPU."
2109.05366,data,139,,,"Moreover, for smaller reads the GPU I/O pattern enables more efficient use of the prefetched data. When a CPU thread issues sequential read requests as in Figure 4, certain requests collide with the request of the prefetcher, therefore they end up waiting for data to arrive. For example, consider the case in which a CPU thread reads 4KB data from offset 0. The prefetcher will issue a read I/O request that will fetch 16KB of data. The thread will be blocked for the first page (4KB) and the rest of the data will be fetched asynchronously. If the thread immediately reads additional 4KB (from offset 4K), the previous asynchronous I/O request by the prefetcher has not yet completed, thus the thread will block."
2109.05366,data,150,,,"When a threadblock issues an I/O request, GPUfs divides the request into pages stored in the GPU page cache. For every page, the GPU page cache is checked and if there is a hit, the data of the page are copied to the GPU application buffer. In case there is a miss, the threadblock submits a request to the shared CPU-GPU request queue (RPC in figure 1) and waits for the CPU thread to read the data from disk or the CPU OS page cache. Once the CPU completes the read, it sends the data to the GPU and signals the threadblock that the request has been served and the data is ready for use. After the threadblock receives the signal it copies the data to the page in the GPU page cache and to the user-level buffer."
2109.05366,data,18,,,"(3) If the data is there, it is used to fulfill the read request and"
2109.05366,data,20,,,[20] K. J. Nesbit and J. E. Smith. 2005. Data cache prefetching using a global history
2109.05366,data,203,,,"CPU thread, therefore, will examine how many GPUfs pages will be delivered to the GPU. The main operations are setting their metadata (e.g. size, offset per page) and placing the data in the staging buffer. Why per-threadblock private buffers? The original version of GPUfs enables file accesses at threadblock-level granularity. Therefore per-threadblock private buffers made the design, implementation and integration of the GPU I/O readahead prefetcher more efficient. Additionally, the private buffer is a reasonable solution because it is quite likely that the same threadblock will access the prefetched pages. Last, having a private buffer eliminates the page cache contention among the threadblocks, thereby reducing the overhead of dealing with prefetching. In fact, our original design used the page cache as a prefetcher target instead of the private buffers, but it suffered from significant synchronization overheads. Lack of a global prefetching scheme. The GPU I/O readahead prefetcher does not implement any synchronization mechanisms between the threadblocks for coordinated prefetching. Therefore, there is a possibility that different threadblocks may prefetch the same page if the access pattern is not sequential."
2109.05366,data,3,,,prefetched data.
2109.05366,data,37,,,"be smaller than the total input size. Specifically, we configure it to be 500MB (except for setting the page cache size to 256MB for 3DCONV because the input data size is 512MB)."
2109.05366,data,39,,,Fig. 5. GPU vs. CPU I/O bandwidth performance without PCIe data transfers included. CPU is accessing the file with the GPU I/O access pattern which was observed from the execution of the GPUfs host threads.
2109.05366,data,4,,,3.4 PCIe data transfers
2109.05366,data,40,,,"(7) When the data is transferred back from the CPU, the threadblock updates the page cache with the 4KB page that is currently requested and the rest of the pages are placed in its private buffer."
2109.05366,data,40,,,"GPUs have been increasingly used in data-intensive workloads, such as databases [7] and graph processing [27]. Therefore, enabling efficient GPU access to files and storage devices has become an important optimization goal."
2109.05366,data,46,,,"The CPU modifications required for the integration are mainly to enable the CPU handle the management of the multiple GPUfs page cache pages. When the corresponding pread finishes its execution, it returns the actual size of the data that has been loaded. The"
2109.05366,data,47,,,"With the I/O pattern of the GPU execution, the access to different file portions are interleaved. This interleaving enables the prefetcher to issue asynchronous I/O requests early enough such that by the time the data is accessed it is already in the page cache."
2109.05366,data,50,,,"We seek to improve GPUfs performance with 4KB pages, and observe that the main bottleneck is the PCIe bus. Thus, the main optimization opportunity is to increase the size of PCIe transfers by prefetching the data in advance into the GPU page cache in larger chunks."
2109.05366,data,51,,,"[24] Sagi Shahar and Mark Silberstein. 2016. Supporting Data-Driven I/O on GPUs Using GPUfs. In Proceedings of the 9th ACM International on Systems and Storage Conference (Haifa, Israel) (SYSTOR ’16). ACM, New York, NY, USA."
2109.05366,data,54,,,"The microbenchmark we use for our evaluation is a GPU application configured with 120 threadblocks, 512 threads per threadblock. The application reads 1 GB from a 10GB file. Every threadblock issues the same number of sequential I/O read requests into its own strides of the file in a data-parallel manner."
2109.05366,data,58,,,"Data prefetching has been extensively studied and used for many cases. To the best of our knowledge, it is the first time that a systemlevel, I/O readahead prefetcher is proposed and integrated with the GPU I/O system stack. In this section we present a subset of the related work that exists in bibliography."
2109.05366,data,60,,,"Our new page cache replacement mechanism is optimized to avoid page cache management overheads for large files that are read sequentially. Our main idea is that each threadblock keeps its own set of active pages. Assuming that when the data is accessed sequentially it will be used only once, each threadblock effectively performs local replacement mechanism."
2109.05366,data,63,,,"[29] Dong Hyuk Woo and Hsien-Hsin S. Lee. 2010. COMPASS: A Programmable Data Prefetcher Using Idle GPU Shaders. In Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems (Pittsburgh, Pennsylvania, USA) (ASPLOS XV). ACM, New York, NY, USA, 297–310."
2109.05366,data,64,,,"[14] Robert Cooksey, Stephan Jourdan, and Dirk Grunwald. 2002. A Stateless, ContentDirected Data Prefetching Mechanism. In Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems (San Jose, California) (ASPLOS X). ACM, New York, NY, USA, 279–290. https://doi.org/10.1145/605397.605427"
2109.05366,data,67,,,"This is a significant degradation that we seek to avoid. Note that some applications may exhibit mixed access pattern, whereas some files are accessed sequentially, and some are not. For example, in the collage application, the original picture used as the basis for the collage is read sequentially, whereas the tiny images are accessed in a complex data-dependent manner."
2109.05366,data,67,,,"To isolate the impact of PCIe data transfers from the file I/O, we perform the same experiment as above while the data is stored in memory in RAMfs [4]. Figure 7 shows, as expected, that larger page sizes perform much better than the small ones. This is in conflict with earlier observations that smaller pages benefit the system performance."
2109.05366,data,69,,,"We first focus on the interaction of GPU-generated I/O pattern with the file I/O layer on the CPU. Therefore, in this experiment we generate the I/O requests from the GPU kernel and send them to the CPU, however disable GPU data transfers and GPU page cache handling. Thus, we exercise the same GPU access pattern but without moving the data to the GPU."
2109.05366,data,80,,,"On the CPU, the host CPU threads poll the shared CPU-GPU request queue, which is logically divided between them. When a thread finds the request in the queue, it issues the corresponding pread() OS call, one GPUfs page at a time. When the pread completes, the host threads place the data in their staging buffers, and transfer them to the GPU, batching, opportunistically, multiple chunks together."
2109.05366,data,81,,,"2 files, 1 GB each 1 file, 256 MB total data read 3.25 GB 1 file, 1.1 GB 1 file, 768 MB 2 files, almost 1 GB each 2 files, almost 1 MB and 952 MB 1 file, 1 GB 1 file, 1 GB 1 file, 512 MB 1 files, almost 1 GB 1 file, almost 1 GB 1 file, almost 1 GB 1 file, almost 1 GB"
2109.05366,data,84,,,"We run the GPU with 120 threadblocks, each 512 threads. Each threadblock reads 8MB stride using GPUfs. This configuration fully utilizes the GPU compute capacity, and represents the preferred way to read data into GPUs using GPUfs, as explored in prior work [26]. GPUfs is configured with 4KB page size and 4 CPU threads for file system accesses. The CPU baseline also uses 4 threads to match the number of CPU threads in GPUfs."
2109.05366,data,85,,,"GPU I/O prefetcher vs bigger GPU page cache page sizes. All the new mechanisms introduced in this paper enable faster operation of GPUfs for 4KB pages in case of sequential accesses. However, we observe that configuring GPUfs with 64KB pages achieves higher performance. This is expected, as the use of 4KB pages in GPUfs incurs higher page cache management overheads compared to managing 64KB pages, and this paper does not deal with optimizing the page cache data structures."
2109.05366,data,90,,,"A common technique for optimizing I/O performance in sequential accesses is readahead prefetching. Linux implements a read-ahead prefetching mechanism that is capable of optimizing strided sequential accesses in multithreaded programs. The two main features that the Linux Readahead Prefetcher employs are the support of multiple streams per file descriptor and the asynchronous prefetching operation. For every file descriptor, there is an allocated readahead data structure which contains the basic information needed in order to determine the sequentiality of the read accesses and the prefetch size."
2109.05366,data,91,,,"We evaluate the GPUfs with the new readahead prefetcher and page cache replacement mechanism on 14 benchmarks derived from the RODINIA, PARBOIL and POLYBENCH benchmark suites. We follow the methodology used in prior works to evaluate GPU I/O performance [30] as follows. The input for the GPU kernel in the original benchmarks is first stored in a file, and then the benchmarks are invoked when the data is read from the file into GPU memory. The benchmark time measurement has been modified to include"
2109.05366,data,95,,,"We observe and conclude that the GPUfs workload scheduler, which maps the GPU I/O requests to the host threads, creates a detrimental effect on the GPU I/O performance, for request sizes greater than or equal to 128KB. Furthermore, the GPU memory limitations also cause performance degradation for applications that operate on files that do not fit in the GPU page cache and thus, we introduce a new page cache replacement mechanism that reduces significantly the number of allocations/deallocations of pages and therefore, the data structure handling overheads."
2109.05366,"data, code",154,,,"I/O prefechers. [15] introduces a new algorithm that makes the minimum number of I/O instructions to service the I/O requests. [11] proposes an I/O signature-based prefetcher which mainly targets to detect the I/O pattern of an application and issues the requests as early as possible. [21] raises the issue of performing more aggressive I/O prefetching in terms of speculation and data size. Our work follows this direction but also considers other factors as well, like the PCIe data transfers, because of the complexity of a heterogeneous CPU-discrete GPU system. [13] pre-executes a fragment of code, via a pre-execution thread that runs at the same time as the main thread, in order to prefetch I/O requests. This solution cannot be efficiently applied for GPUs because it would require code divergence and would raise performance issues."
2109.05366,"data, data available",43,,,"(6) If the data is not found in the private buffer, then the threadblock issues a read request to the CPU with size equal to GPUfs PAGE_SIZE + PREFETCH_SIZE (PREFETCH_SIZE is static and is defined before execution)."
2109.05366,"data, data available",58,,,"(4) If there is a page cache miss, the threadblock allocates a new page and searches if the data is available in its private buffer. (5) If the data is in its private buffer, it updates the page cache, returns the data to the user-level buffer and continues execution."
2109.05366,"data, dataset",114,,,"GPUfs [26] is the recently introduced system infrastructure which allows GPU threads to access files directly from GPU kernels. GPUfs provides standard POSIX-like APIs (e.g., (read()/write()) for GPU threads to perform file I/O, thus reducing the programming complexity, it implements a local page cache in the GPU memory, allows access to very large datasets and enables applications to perform data-driven, e.g., indexed-based, data accesses efficiently. GPUfs passes the I/O requests to the CPU, yet these remain invisible to the programmer, who only uses a standard and convenient I/O abstraction from GPU kernels."
2109.05366,"data, dataset provided",70,,,We run a simple experiment to evaluate the sequential I/O performance of GPUfs by using it to move 960MB file into GPU memory. The goal is to compare the effective I/O bandwidth that GPUfs can achieve with the bandwidth of reading the same file from the CPU (no data transfer to GPU). We expect that GPUfs would provide full pipe-lining and therefore achieve similar bandwidth.
2109.05366,"data, dataset, data available",71,,,"Sequential pattern is among the most common. For example, it is employed in deep neural networks [9] inference and training when reading the input dataset [5]. Such sequential access to data is very popular in high performance computing. In addition, I/O benchmark suites for manycore/multi-node architectures [1] consider sequential access as one of the most popular in data-intensive applications."
2109.05366,"data, dataset, used dataset",203,,,"GPUs are broadly used in I/O-intensive big data applications. Prior works demonstrate the benefits of using GPU-side file system layer, GPUfs, to improve the GPU performance and programmability in such workloads. However, GPUfs fails to provide high performance for a common I/O pattern where a GPU is used to process a whole data set sequentially. In this work, we propose a number of system-level optimizations to improve the performance of GPUfs for such workloads. We perform an in-depth analysis of the interplay between the GPU I/O access pattern, CPU-GPU PCIe transfers and SSD storage, and identify the main bottlenecks. We propose a new GPU I/O readahead prefetcher and a GPU page cache replacement mechanism to resolve them. The GPU I/O readahead prefetcher achieves more than 2× (geometric mean) higher bandwidth in a series of microbenchmarks compared to the original GPUfs. Furthermore, we evaluate the system on 14 applications derived from the RODINIA, PARBOIL and POLYBENCH benchmark suites. Our prefetching mechanism improves their execution time by up to 50% and their I/O bandwidth by 82% compared to the traditional CPU-only data transfer techniques."
2109.05366,database,77,,,"To evaluate the performance impact of the page size in such applications, we run the Mosaic benchmark [23] which creates an image collage from multiple tiny images fetched at input-dependent location from a large database (19GB). Each tiny image is 4KB. We run the application with GPUfs configured to use 4KB and 64KB pages. We observe that smaller pages result in 45% higher performance compared to 64KB."
2109.1263,data,10,,,"Journal of Direct, Data and Digital Marketing Practice."
2109.1263,data,10,,,student-generated social media data. Journal of Marketing Management.
2109.1263,data,11,,,"2.2.1. Issues in data collection during ""Market Research"""
2109.1263,data,116,,,"Although the all-encompassing goal of text analytics and mining is transforming the unstructured textual data format into actionable information by applying natural language processing (NLP) and investigation, such terms’ deﬁnitions They indicated ""text analytics"" to be a broader idea which includes the retrieval of information (e.g., the scanning and recognizing of pertinent reports for a speciﬁc key term arrangement), as data mining, information extraction, and Web mining, whereas ""text mining"" was indicated to fundamentally be focused on identifying new, high-value knowledge from textual data sources:[83] T ext Analytics Data M ining + W eb M ining"
2109.1263,data,12,,,# Return a frame of data containing the respective sentence and score
2109.1263,data,122,,,"1- Using another social media platforms contents rather than twitter. 2- Exploiting qualitative research side by side of the data mining methodology like mixed method approaches. 3- Trying to extend the scope of the CGCs gathered, in both geography and industry. 4- Changing the objectives of the research and do it all again and to see if they can get better or at least acceptable results. 5- Using various ML algorithms and seeking better and more optimized insights. 6- Trying to check another ways of analytics to the same CGCs. 7- One can use semantic lexical chaining methods to clustering obtained data, such as NMF and GSDMM which have oﬀered by [21]"
2109.1263,data,135,,,"1- Based on the consumer sentiment analysis of the company, and inﬂuencer and their correlation, have companies chosen suitable celebrities for their former ad campaigns? 2- Based on the consumer sentiment analysis performed on the company, and inﬂuencer and their correlation, which one of the two celebrities are more suitable and eﬀective for the company’s future ad campaigns? To ﬁnd answers to these questions, challenges regarding text classiﬁcation techniques (lexicon-based (naïve)) were summarized for the sentiment analysis that is performed on social media data today. Then, the research methodology was outlined, empirically evaluating the machine learning (Naïve Bayes) and lexicon-based approaches through a large CGC sample of brand pages on Twitter and related hashtags and keywords."
2109.1263,data,146,,,"The semantic extraction of the textual content of Social Media represents a further step towards better understanding and getting insight out of the true context of a consumer’s content as a social network user[2]. For success in text mining, as [96] remarked that in Text mining process we try to ﬁnd hidden patterns in any textual content including CGCs and better understanding patterns found to be analyzed, investigations should pursue the best rehearse-dependent sound technique. A standardized process model, e.g., Cross-Industry Standard Process, is required for Data Mining (CRISP-DM) - it is the business standard for data mining practices. Although CRISP-DM pieces are mostly material to text mining practices, a particular data mining process model would include information preprocessing activities with signiﬁcantly more details. [83]"
2109.1263,data,153,,,"The importance of decision-making in a market strategy and marketing is dependent on the conclusions of market research. The market research procedure varies from a form to another but has a set of steps. The standard procedure of market research helps avoid errors, misunderstanding, and uncertainty risk. One can deﬁne the procedure of marketing research as “The scientiﬁc and systematical procedure which involves deﬁning the problem, collecting data, and analyzing and reporting the data and results of a speciﬁc issue when the problem is encountered by the organization” [56] Marketing research procedure overview: the procedure of marketing research includes a step-by-step loom in which each step is to respond to the scrupulous questions [56]: Why is it required to perform the research? What does research require to perform? How can the research objectives be accomplished?"
2109.1263,data,17,,,"adoption of big data technologies. Scandinavian Journal of Management, 34(2):129–140."
2109.1263,data,181,,,"and algorithms like the ones we used in this article such as: Sentiment Analysis, Polarity Analysis, etc. 6- [35] Showed in their article that getting the most out of SMM or SMA and big data can be so useful in predicting the future behaviors of the consumers, and in their speciﬁc case ""Anticipating the international tourist arrivals"". Like that we presented the fact that managers can Improve Decision Making, Celebrity Branding And Market Research: Using Social Media Mining (SMM). In fact we can probably come to this conclusion that data mining in any aspects like our case of interest; Social Media, could be helpful and their outputs as metrics to us, prove their value in increasing the forecasting accuracy of diﬀerent models in any industry. 7- Those who take advantage of the approaches mentioned in the research in hand can easily map the insights obtained to the actor roles mentioned in [37] ﬁndings and get a more holistic output."
2109.1263,data,198,,,"Decision-making is an essential activity in every organization—perhaps the most important activity. Decision-making impacts the success/failure and performance of organizations its performances. It is becoming diﬃcult to make decisions due to internal and external factors. Making proper decisions can yield very high rewards. The same case applies to losses of inappropriate decisions. It is unfortunately diﬃcult to make a decision. Several decision types exist, each of which needs a diﬀerent approach for decision-making. For instance, McKinsey & Company management consultants[25] classiﬁed organizational decisions into four groups: Big-bet, high-risk decisions. Cross-cutting decisions, which allow for repetition but pose risk and require group work. Ad hoc decisions, which appear episodically. Decisions to delegated to people or small groups. Moreover, market studies, which incorporate social and feeling studies, are the methodical translation and get-together of data on people or associations that adopt measurable and investigative strategies and processes of the applied sociologies to understand or improve decision-making. The downsides of the market research include a high cost, high time-consumption, and inexactness.[82]"
2109.1263,data,20,,,"[98] Wang, S. and Shi, W. (2012). Data mining and knowledge discovery."
2109.1263,data,213,,,"The next aspects to be discussed are those that may appear during the assortment and estimation of information and distress chiefs and decision-makers: data cannot be accessed. Therefore, the model is built of dependent on probably-mistaken evaluations. It may be costly to acquire information. The information can be suﬃciently precise or exact. The estimation of information is subjective regularly. Information may be unreliable. The important information that impacts the outcomes may be qualitative (delicate). An excessive amount of information may exist (that is, data over-burden). The results (or outcomes) might take place during an inclusive period. Therefore, costs, proﬁts, and incomes are recorded at various focuses on time. In order to tackle this diﬃculty, one can adopt a present-esteem approach when the outcomes cannot be quantiﬁed. Future information is expected to be similar to historical data. In case this is not the circumstance, the change’s nature should be predicted and included in the investigation. After ﬁnishing the preliminary investigation, it is possible to decide whether an issue really exists, where it is identiﬁed, and how important it is."
2109.1263,data,250,,,"such that the chore of ﬁnding the required information perfectly satisﬁes the research purpose. Research objectives consist of three crucial parts: Research Question The research question deﬁnes the information that is required by the organization that is accountable for decision-making. It represents that the in a sequence should be obtained in accordance with the research purpose. [56] Hypothesis The hypothesis represents the researcher’s perspective of the likely answer to the question of the research. The Researcher can generate = the possible research question results in one of the previous stages and perform the research to answer whether a hypothesis proposed at the beginning of the research was correct. Research Scope Hypothesis development helps keep the research procedure more precise and focused to contribute to the research purpose. It is another important aspect of the research to indicate the boundaries limitations of the research. [56] Step 3: Value estimation of research information Once the purpose, objectives, and scope of the research have been deﬁned, it is similarly very important to gauge the estimation of the required data or that of the research issue to be conceivably answered in the research question. Step 4: Research Design The research design is generally the research’s structure or framework developed to conduct the research. Step 5: Data Collection Step 6: Data Analysis Step 7: Reporting Results and Presentation."
2109.1263,data,26,,,"[12] Caesarius, L. M. and Hohenthal, J. (2018). Searching for big data: How incumbents explore a possible"
2109.1263,data,275,,,"Naïve Bayes is a simple classiﬁcation technique based on probability (a machine-learning method applied to prediction problems of classiﬁcation type) that is derived from the Bayes theorem. In this method, the output variable should be of nominal values. For this reason, the present study employed this technique as the machine-learning approach. Despite the fact that the input variables may be a combination of numeric and nominal types, it is required to discretize the numeric output variable by using a binning method prior to using it in a Bayes classiﬁer. The term “Naïve” arises from the strong and, to some extent, unrealistic independence assumption among input variables. In simple terms, in a Naïve Bayes classiﬁer, the input variables are assumed to not be dependent on each other. It also assumes the presence and absence of a speciﬁc variable in the predictor combinations to not be associated with other variables’ presence or absence. One can develop classiﬁcation models of Naïve Bayes type very eﬀectively (completely accurately) and eﬃciently (i.e., relatively quickly with small computational eﬀort) in a supervised machine-learning setting. In other words, through a collection of training data (which is not necessarily to be large), one can obtain the parameters of classiﬁcation models of Naïve Bayes type by utilizing the maximum likelihood method -t that is, due to the assumption of independence, Naïve Bayes models can be developed without strictly complying with the Bayes theorem’s entire rules and requirements. [83]"
2109.1263,data,287,,,"1- Decision Making And Market Research are so vital to businesses and managers. One of the ways that could help them signiﬁcantly are the approaches of the current study. Another help as [40] mentioned in their results, would be companies exploiting the big data existing in social media to build a clear connection with strategic planning and competitive positioning. 2- We are completely agree with [42] that an detailed concentration on the consumergenerated content could specially help web based retailing brands manage customer relationships eﬀectively and eﬃciently. Thoughts and encounters shared by clients via social media platforms permit organizations to associate with them and remain side by side of what is happening on Twitter. 3- If we consider the suggested Business SMA framework by [41], we can get to the four overarching end goals mentioned -Intelligence Fetching, Making Sense out of Data, Insight Producing, and Decision Making- through the current research. 4- One may use the WordCloud approach we used as an easier way to get to related texts instead of more complex ways like lexical chain generation algorithm to cluster texts from an account suggested by [21]. 5- As [18] mentioned in their article, link prediction is a signiﬁcant approach in recognizing relations in business and social media. To anticipate the possible relations in multi-relational social networks like twitter and other systems, one should consider the importance and eﬀect between diﬀerent kinds of relations, for instance by taking advantage of social media CGCs. So for the sake of this important subject, managers should use a handful of tools"
2109.1263,data,292,,,"is a widely used approach to perform classiﬁcation via a set of weighted words [92] as an approach to the analysis of sentiment within the community of marketing research [11], since it does not need the classiﬁer to be pre-processed or trained. On the other hand, the use of machine learning for sentiment analysis, which is also considered as a supervised learning method, is mostly reported to have accuracy [16] Also, it has been employed in marketing research [79]. The machine learning approach, however, needs a training phase, which is performed by either the researchers or the provider of the sentiment software. Since each one of these methods oﬀers its speciﬁc advantages and limitations, researchers and marketers should verify classiﬁcation accuracy to avoid any action on inaccurate outcomes of data analysis [14]. In addition, considering the wide platform range in social media and their particularities on content type that can be created by consumers (e.g. Twitter tweets, Facebook comments, emoticons, hashtags, emojis, abbreviations, and slang language), the available sentiment analysis techniques, which is typically examined on English texts, need to be carefully validated before they are put in use on social media data by marketers. SMM makes diﬀerent contributions to market research, such as almost freeness, quick accessibility, and including no information bias, which may be analyzed using various algorithms. It also helps make decisions in many manners, and it is mostly the market research outcome based on SMM, Eventually, SMM can contribute to celebrity branding."
2109.1263,data,31,,,"[96] Usai, A., Pironti, M., Mital, M., and Aouina Mejri, C. (2018). Knowledge discovery out of text data:"
2109.1263,data,321,,,"The aim of the present study is to assess and compare the performance of ﬁrms and celebrities (i.e., inﬂuencers that with the experience of being in an ad campaign of those companies) with the automated sentiment analysis that was employed for CGC at social media while exploring the feeling of the consumers toward them to observe which inﬂuencer (of two for each company) had a closer eﬀect with the corresponding corporation on consumer minds. For this purpose, a number of consumer tweets from the pages of brands and inﬂuencers were utilized to make a comparison of machine learning and lexicon-based approaches to the sentiment analysis through the Naïve algorithm (lexicon-based) and Naïve Bayes algorithm (machine learning method) and obtain the desired results to assess the campaigns. The ﬁndings suggested that the approaches were dissimilar in terms of accuracy; the machine learning method (Naïve Bayes algorithm) yielded higher accuracy. Finally, the results showed which inﬂuencer was more appropriate according to their existence in previous campaigns and helped choose the right inﬂuencer in the future for our company and have a better, more appropriate, and more eﬃcient ad campaign subsequently. It is required to conduct further studies on the accuracy improvement of the sentiment classiﬁcation. This approach should be employed for other social media CGC types, e.g., Instagram feeds. The results revealed decisionmaking for which sentiment analysis methods (or their combinations) are the best approaches for the analysis of social media. It was also found that companies should be aware of their consumers’ sentiments and choose the right person every time they think of a campaign. Keywords: data mining, social media mining, big data, sentiment analysis, decision making, market research, celebrity branding Paper type: Research paper"
2109.1263,data,338,,,"The volume of discussions concerning brands within social media provides digital marketers with great opportunities for tracking and analyzing the feelings and views of consumers toward brands, products, inﬂuencers, services, and ad campaigns in the consumer-generated content (CGC). The aim of the present study is to assess and compare the performance of ﬁrms and celebrities (i.e., inﬂuencers that with the experience of being in an ad campaign of those companies) with the automated sentiment analysis that was employed for CGC at social media while exploring the feeling of the consumers toward them to observe which inﬂuencer (of two for each company) had a closer eﬀect with the corresponding corporation on consumer minds. For this purpose, a number of consumer tweets from the pages of brands and inﬂuencers were utilized to make a comparison of machine learning and lexicon-based approaches to the sentiment analysis through the Naïve algorithm (lexicon-based) and Naïve Bayes algorithm (machine learning method) and obtain the desired results to assess the campaigns. The ﬁndings suggested that the approaches were dissimilar in terms of accuracy; the machine learning method (Naïve Bayes algorithm) yielded higher accuracy. Finally, the results showed which inﬂuencer was more appropriate according to their existence in previous campaigns and helped choose the right inﬂuencer in the future for our company and have a better, more appropriate, and more eﬃcient ad campaign subsequently. It is required to conduct further studies on the accuracy improvement of the sentiment classiﬁcation. This approach should be employed for other social media CGC types, e.g., Instagram feeds. The results revealed decisionmaking for which sentiment analysis methods (or their combinations) are the best approaches for the analysis of social media. It was also found that companies should be aware of their consumers’ sentiments and choose the right person every time they think of a campaign."
2109.1263,data,339,,,"For example, social media provides consumers with a voice equal to or even higher than brands, thereby disrupting marketing procedures and bringing immense challenges and dilemmas to marketers [22]. Managers in branding are no longer able to ignore the essential online voice of their consumers [36]. They also enjoy new opportunities to exploit the unfettered consumer-generated content (CGC) that are available on platforms in social media. The term social media refers to the strengthening advances in social interactions among people in which information, opinions, and ideas are shared and exchanged within virtual networks and communities. Social media consists of a set of internet-based programming applications manufacturing on Web 2.0 innovative and ideological establishments and allowing user-generated content to be generated and traded[50]. Since digital marketing is currently considered to be a “many-to-many conversation” between companies and customers as well as between customers [61], the classic one-way businessto-customer transmissions have begun becoming outdated. The tracking and analysis of the feelings and views of customers on particular brands, services, or products associated with the CGC on social media are a recent trend in the environment of digital marketing analytics. The aim is to categorize positive and negative CGC, commonly text-based (naïve) content, in accordance with a number of manual or automated categorization methods (i.e., naïve Bayes). Marketers, for instance, can receive timely customer feedback on a newly-oﬀered service or product by assessing customer sentiment stated in the comments of a Facebook post or those of tweets with a particular hashtag associated with the service/product. Considering the immense CGC volume, which is typically known as “Big Data” and has expanded along with the employment of social media platforms, it is no longer feasible to qualitatively and manually analyze the sentiment of consumers that is expressed in online brand-associated content."
2109.1263,data,341,,,"They also enjoy new opportunities to exploit the unfettered consumer-generated content (CGC) that are available on platforms in social media. The term social media refers to the strengthening advances in social interactions among people in which information, opinions, and ideas are shared and exchanged within virtual networks and communities. Social media consists of a set of internet-based programming applications manufacturing on Web 2.0 innovative and ideological establishments and allowing user-generated content to be generated and traded[50]. Since digital marketing is currently considered to be a “many-to-many conversation” between companies and customers as well as between customers [61], the classic one-way businessto-customer transmissions have begun becoming outdated. The tracking and analysis of the feelings and views of customers on particular brands, services, or products associated with the CGC on social media are a recent trend in the environment of digital marketing analytics. The aim is to categorize positive and negative CGC, commonly text-based (naïve) content, in accordance with a number of manual or automated categorization methods (i.e., naïve Bayes). Marketers, for instance, can receive timely customer feedback on a newly-oﬀered service or product by assessing customer sentiment stated in the comments of a Facebook post or those of tweets with a particular hashtag associated with the service/product. Considering the immense CGC volume, which is typically known as “Big Data” and has expanded along with the employment of social media platforms, it is no longer feasible to qualitatively and manually analyze the sentiment of consumers that is expressed in online brand-associated content. For example, Twitter produces more than 500 million tweets every day, and Facebook hosts 4.75 billion content pieces each day. This adds to the need for developing automated instruments to identify and analyze text-expressed customer sentiment [98]. There are two eminent approaches to the automated analysis of sentiment. It"
2109.1263,data,342,,,"Before we go towards social media mining, ﬁrst we have to comprehend social media. Social media is characterized as any platform that encourages user-generated content and ongoing shared communication. In previous articles many of the researchers explicitly characterize social media as a set of web and mobile instruments and applications that stimulate interpersonal communication and opinion sharing, and the creation and dissemination of user-generated content. All the world’s signiﬁcant brands are presently active via social media[39] and so many papers investigated that in many ways including the usage analysis of social media for competitive business outcomes [15]. Social media data have recently attracted signiﬁcant attention as a rising voice of the client as it has quickly become a channel for trading and storing customer-generated, huge amount, and unregulated voices about different aspects of business[45][26]. The noticeable advancements in social in the past decade, as well as digital channels’ profusion, e.g., social network websites (such as Facebook), microblogs (such as Twitter) and media sharing (such as Instagram or YouTube), have brought a revolution not only in the communications of brands with their customers but also in the customers’ roles within the marketing procedure like eWOM [1]. For example, social media provides consumers with a voice equal to or even higher than brands, thereby disrupting marketing procedures and bringing immense challenges and dilemmas to marketers [22]. Managers in branding are no longer able to ignore the essential online voice of their consumers [36]. They also enjoy new opportunities to exploit the unfettered consumer-generated content (CGC) that are available on platforms in social media. The term social media refers to the strengthening advances in social interactions among people in which information, opinions, and ideas are shared and exchanged within virtual networks and communities."
2109.1263,data,43,,,"The present study combined two sentiment analysis approaches, demonstrating that the companies should work on these kinds of data mining seriously since the minds of consumers are very diﬀerent from companies’ assumptions and achievements of their conventional market research practice."
2109.1263,data,45,,,"[46] Jimenez-Marquez, J. L., Gonzalez-Carrasco, I., Lopez-Cuadrado, J. L., and Ruiz-Mezcua, B. (2019). Towards a big data framework for analyzing social media content. International Journal of Information Management, 44:1–12."
2109.1263,data,45,,,"[82] Proctor, T. (2007). Essentials of marketing research, volume 12. [83] Ramesh Sharda, Dursun Delen, E. T. (2019). Analytics, Data Science, Artiﬁcial Intelligence Systems for Decision Support."
2109.1263,data,6,,,3.6. Data gathering and steps
2109.1263,data,6,,,Table 1: Data cleaning process
2109.1263,data,6,,,Table 2: Data cleaning process
2109.1263,data,6,,,approaches to analyzing unstructured data.
2109.1263,data,60,,,"[26] Del Vecchio, P., Mele, G., Passiante, G., Vrontis, D., and Fanuli, C. (2020). Detecting customers knowledge from social media big data: toward an integrated methodological framework based on netnography and business analytics. Journal of Knowledge Management, 24(4):799–821."
2109.1263,data,76,,,"[39] Harrigan, P., Miles, M. P., Fang, Y., and Roy, S. K. (2020). The role of social media in the engagement and information processes of social CRM. International Journal of Information Management, 54:102151. [40] He, W., Wang, F. K., and Akula, V. (2017). Managing extracted knowledge from big social media data"
2109.1263,data,78,,,"According to [43], [62], [7], [101], [70], [87] ,[4] we can clearly see that there is a connection between consumers’ feelings and celebrities’ icon and consumer behavior and decision making according to the perceived sentiment so it is really important to examine all the accessible resources specially big data contents because people are technology oriented these"
2109.1263,data,80,,,"A key aspect is whether the data framework reports an issue or solely an issue’s side eﬀects. For instance, if the reports suggest that sales have reduced, there is a problem; however, the situation undoubtedly indicates symptoms of the problem. The actual problem must be known. It sometimes may be a perception problem, organizational procedures, or incentive mismatch, instead of a low-performance decision model.[83]"
2109.1263,"data, data open-source , open-source",328,,,"includes a huge number of people, responses are commonly very quick. This platform facilitates humans’ fundamental social instincts. Through sharing on Twitter, users can easily express their opinions on everything at any time. Connected friends or followers (on Twitter) immediately obtain the information on the current situations in the lives of people. This, in turn, contributes to another emotion of humans—i.e., the innate need for knowing the current life situations of people. The user interface (UI) of Twitter is not only real-time but also easy to use. It is understood instinctively and naturally -i.e., the Twitter UI has a very intuitive nature. If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes."
2109.1263,"data, data open-source , open-source",346,,,"If the tweets of users can properly be mined and analyzed, Twitter can serve as an excellent advertisement and marketing tool for so many things including successful social media marketing strategies [77]. However, the information that is provided by Twitter includes a larger domain. As Twitter is naturally non-symmetric in terms of followers and followings, it helps better understand user interests than its inﬂuence on the social network. One can consider an interest graph as a way of learning the connections of individuals and their various interests. The degree calculation of the association or correlations of the interests of individuals and the potential advertisements is among the most essential applications of an interest graph. On the basis of such correlations, users can be aimed to obtain a maximum response to advertisement campaigns, along with the recommendations of followers. A vast sample of customer tweets of ﬁfteen twitter brands and inﬂuencer pages are mined and utilized for the comparison of lexicon-based and machine learning techniques to the sentiment analysis via the Naïve algorithm (lexicon-based) and the Naïve Bayes algorithm (i.e., a machine learning method) and for obtaining the desired outcomes. These brands are chosen since they are all located in the USA, and they have a good, appropriate, and active twitter account that can be dug to obtain information, and had many campaigns over the years. Regarding celebrities, almost the same reason was the case, and their reputation and connection to the companies were a major concern. The present study utilized R studio, R language, and Twitter developer API for data mining as it is an open-source instrument, has many useful packages, and has result visualization ability. Appropriate data were gathered by searching hashtags and keywords relating to companies and celebrities. Then, after they were cleansed and made them meaningful, the analysis part was carried out through the two above-mentioned methods."
2109.1263,"data, dataset provided",148,,,"The current data age is known for the rapid development in the measure of data and information that are electrically collected, stored, provided. A considerable portion of business information is stored in basically-unstructured text documents. According to Merrill Lynch and Gartner, 85% of a corporate datum is captured and stored in an unstructured format. The equivalent study also suggested that the size of such non-structurally-stored information is increasing every 18 months. Since knowledge provides power in the current business world and is derived from information and data, organizations which make eﬀective and eﬃcient use of their text information sources enjoy the crucial knowledge to make better choices, inducing a competitive advantage over the organizations that lag behind. Thus, the need for text analytics and mining becomes part of the all-inclusive view of the current organizations."
2109.1263,"data, dataset provided",162,,,"Today, as [12] mentioned, ""Big Data” opportunities are a lot and in fact they provide manual ways for illogically dealing with sentiment analysis and add to the need for making robotized devices for the analysis of textually-expressed consumer sentiment. The present study selected Twitter as the case-study social media since it can be seen as an internet-based short message service (SMS) extension. IAs Jack Dorsey, the co-founder and co-creator of Twitter, said: ""...We came across the word ’twitter’, and it was just perfect. The deﬁnition was ’a short burst of inconsequential information,’ and ’chirps from birds’. And that’s exactly what the product was."" Twitter functions as a utility via which individuals can send SMSs throughout the world. It allows for continuously becoming heard and receiving answers. As the audience of Twitter"
2109.1263,"data, package",194,,,"11- Install Rstem and sentiment packages 12- Make use of the classify_emotion function that returns a class data frame object, consisting of seven columns (including disgust anger, joy, fear, surprise, sadness, and best_ﬁt) and a row for each of the documents 13- Replace such NA values 14- Employ another function, i.e., classify_polarity(), that is provided by the sentiment package, for the classiﬁcation of tweets into two groups, namely neg (negative sentiment) and pos (i.e., positive sentiment) (It should be noted that the overall tweet sentiment is considered as neutral when this ratio is found to be 1) 15- Create consolidated results in a data frame from these two functions 16- Sort and rearrange the data within the frame 17- Generate a single function so that it is used by the tweets of each business and the sentiment can be plotted for each of the businesses 18- Likewise, plot the polarity distribution of the tweets 19- Try to obtain a sense of the tweets’ overall content through the word clouds."
2109.1263,download,108,,,"8- Download both negative and positive English opinion/sentiment words (nearly 16000 words) 9- Add some industry-speciﬁc and/or particularly emphatic terms, depending on the requirements 10- Generate a function in R that calculates the raw sentiment by the simple matching algorithm by considering the following: # Remove digits, punctuations, and control characters # Convert them all into the lower sentence case # Divide each of the sentences by using space-delimiter words # Obtain the Boolean matches of the words with the negative and positive opinion-lexicon # Obtain the score in the form of the total positive sentiment dedicated from the total negative sentiment"
2110.0282,code,10,,,E.1. Randomized Powering algorithm. The pseudo-code for estimating
2110.0282,code,72,,,"E.2. Adaptive rank selection algorithm. The pseudocode for adaptive rank selection by a-priori error estimation is given in Algorithm E.2. The code is structured to reuse use the previously computed Ω and Y , resulting in signiﬁcant computational is estimated from q iterations of the randomized power method E savings. The error (cid:107) (cid:107) U ˆΛU T . on the error matrix A −"
2110.0282,data,105,,,"6.3.1. Background. Cross-validation is an important machine-learning technique to assess and select models and hyperparameters. Generally, it requires re-ﬁtting a model on many subsets of the data, so can take quite a long time. The worst culprit is leave-one-out cross-validation (LOOCV), which requires running an expensive training algorithm n times. Recent work has developed approximate leave-one-out cross-validation (ALOOCV), a faster alternative that replaces model retraining by a linear system solve [12, 26, 40]. In particular, these techniques yield accurate and computationally tractable approximations to LOOCV."
2110.0282,data,119,,,"For µ = 10−8, block Nystr¨om PCG reduces the number of iterations substantially, but the speedup is negligible. The data matrix A is sparse, which reduces the beneﬁt of the Nystr¨om method. Block CG also beneﬁts from the presence of multiple righthand sides just as block Nystr¨om PCG. Indeed, O’Leary proved that the convergence of block CG depends on the ratio (λs + µ)/(λn + µ), where s is is the number of right-hand sides [25]. Consequently, multiple right-hand sides precondition block CG and accelerate convergence. We expect bigger gains over block CG when A is dense."
2110.0282,data,128,,,"Abstract. This paper introduces the Nystr¨om PCG algorithm for solving a symmetric positivedeﬁnite linear system. The algorithm applies the randomized Nystr¨om method to form a low-rank approximation of the matrix, which leads to an eﬃcient preconditioner that can be deployed with the conjugate gradient algorithm. Theoretical analysis shows that preconditioned system has constant condition number as soon as the rank of the approximation is comparable with the number of eﬀective degrees of freedom in the matrix. The paper also develops adaptive methods that provably achieve similar performance without knowledge of the eﬀective dimension. Numerical tests show that Nystr¨om PCG can rapidly solve large linear systems that arise in data analysis problems, and it surpasses several competing methods from the literature."
2110.0282,data,19,,,"positive-semideﬁnite matrix from streaming data, in NIPS, vol. 30, 2017, pp. 1225–1234."
2110.0282,data,19,,,"solution ˆx(aj), we compute the relative error the relative error over 100 data-points aj."
2110.0282,data,33,,,"[35] W. T. Stephenson, M. Udell, and T. Broderick, Approximate cross-validation with low rank data in high dimensions, in NeurIPS, vol. 33, 2020."
2110.0282,data,37,,,"Fig. 1: Ridge regression: CG versus Nystr¨om PCG. For the shuttle-rf data set, Nystr¨om PCG converges to machine precision in 13 iterations while CG stalls. See subsections 1.3 and 6.2."
2110.0282,data,45,,,"6.5. Kernel ridge regression. Our last application is kernel ridge regression (KRR), a supervised learning technique that uses a kernel to model nonlinearity in the data. KRR leads to large dense linear systems that are challenging to solve."
2110.0282,data,67,,,"1.2. Guarantees. This paper contains the ﬁrst comprehensive study of the preconditioner (1.3), including theoretical analysis and testing on prototypical problems from data analysis and machine learning. One of the main contributions is a rigorous method for choosing the rank (cid:96) to guarantee good performance, along with an adaptive rank selection procedure that performs well in practice."
2110.0282,data,79,,,"The next century has since arrived, and one of the most fruitful developments in matrix computations has been the emergence of new algorithms that use randomness in an essential way. This paper explores a topic at the nexus of preconditioning and randomized numerical linear algebra. We will show how to use a randomized matrix approximation algorithm to construct a preconditioner for an important class of linear systems that arises throughout data analysis and scientiﬁc computing."
2110.0282,data,86,,,"6.2. Ridge regression. In this section, we solve the ridge regression problem (1.7) described in subsection 1.3 on some standard machine learning data sets (Table 2) from OpenML [38] and LIBSVM [6]. We compare Nystr¨om PCG to standard CG and two state-of-the-art randomized preconditioning methods, the sketch-andprecondition method of Rokhlin and Tygert (R&T) [29] and the Adaptive Iterative Hessian Sketch (AdaIHS) [19]."
2110.0282,data,96,,,"1.5. Roadmap. Section 2 contains an overview of the Nystr¨om approximation and its key properties. Section 3 studies the role of the Nystr¨om approximation in estimating the inverse of the regularized matrix. We analyze the Nystr¨om sketchand-solve method in Section 4, and we give a rigorous performance bound for this algorithm. Section 5 presents a full treatment of Nystr¨om PCG, including theoretical results and guidance on numerical implementation. Computational experiments in Section 6 demonstrate the power of Nystr¨om PCG for three diﬀerent applications involving real data sets."
2110.0282,"data, dataset",137,,,"where H(ˆθ) Rd×d is the Hessian of the loss function at the solution ˆθ, for each datapoint aj. The main computational challenge is computing the inverse Hessian vector product H −1(ˆθ) θl(ˆθ, aj). When n is very large, we can also subsample the data and average (6.1) over the subsample to estimate ALOOCV. Since ALOOCV solves the same problem with several right-hand sides, blocked PCG methods (here, Nystr¨om blocked PCG) are the tool of choice to eﬃciently solve for multiple righthand sides at once. To demonstrate the idea, we perform numerical experiments on ALOOCV for logistic regression. The datasets we use are all from LIBSVM [6]; see Table 5."
2110.0282,"data, dataset",179,,,"C.3. Kernel Ridge Regression. We converted the binary classiﬁcation problem to a regression problem by constructing the target vector as follows: We assign +1 to the ﬁrst class and -1 to the second class. For multi-class problems, we do one-vs-all classiﬁcation; this formulation leads to multiple right hand sides, so we use block PCG for both methods. We did no data pre-processing except for EMNIST, MiniBooNE, MNIST, and Santander. For EMNIST and MNIST the data matrix was scaled by 255 so that its entries lie in [0, 1], while for MinBooNE and Santander the features were normalized by their z-score. The number of random features, mrf from 103 for j = 1, . . . , 9. For adaptive Nystr¨om PCG we capped the linear grid mrf = j the maximum rank for the preconditioner at (cid:96)max = and used a tolerance of 40 for the ratio ˆλ(cid:96)/nµ on all datasets."
2110.0282,"data, dataset",63,,,"6.4. Large scale ALOOCV experiments. Table 7 summarizes results for block Nystr¨om PCG and block CG on the larger datasets. When µ = 10−4, block Nystr¨om PCG oﬀers little or no beneﬁt over block CG because the data matrices are very sparse (see Table 5) and the rcv1 problem is well-conditioned (see Table 13)."
2110.0282,"data, dataset",83,,,"C.1. Ridge regression experiments. Most of the datasets used in our ridge regression experiments are classiﬁcation datasets. We converted them to regression problems by using a one-hot vector encoding. The target vector b was constructed by setting bi = 1 if example i has the ﬁrst label and 0 otherwise. We did no data pre-processing except on CIFAR-10, where we scaled the matrix by 255 so that all entries lie in [0, 1]."
2110.0282,"data, dataset",83,,,"Table 13: For µ = 10−4 the Hessian is well-conditioned for both datasets, so there is little value to preconditioning. For µ = 10−8, the ill-conditioning of the Hessian increases signiﬁcantly, making preconditioning more valuable. Furthermore, as ALOOCV uses Block PCG on at least several batches of data points, the cost of constructing the preconditioner is negligible compared to the cost of solving the linear systems (see Table 7 in subsection 6.3)."
2110.0282,"data, dataset",87,,,"6. Applications and experiments. In this section, we study the performance of Nystr¨om PCG on real world data from three diﬀerent applications: ridge regression, kernel ridge regression, and approximate cross-validation. The experiments demonstrate the eﬀectiveness of the preconditioner and our strategies for choosing the rank (cid:96) compared to other algorithms in the literature: on large datasets, we ﬁnd that our method outperforms competitors by a factor of 5–10 (Table 3 and Table 10)."
2110.0282,dataset,1,,,Dataset
2110.0282,dataset,105,,,"6.3.2. Experimental overview. We perform two sets of experiments in this section. The ﬁrst set of experiments uses Gisette and SVHN to test the eﬃcacy of Nystr¨om sketch-and-solve. These datasets are small enough that we can factor H(θ) using a direct method. We also compare to block CG and block PCG with the computed Nystr¨om approximation as a preconditioner. To assess the error due to θl(ˆθ, aj). For any putative an inexact solve for datapoint aj, let x(cid:63)(aj) = H −1(θ)"
2110.0282,dataset,11,,,Table 8: Kernel ridge regression datasets and experimental parameters.
2110.0282,dataset,123,,,"6.5.3. Experimental results. Tables 9 to 11 summarize the results for the KRR experiments. Table 9 shows that both versions of Nystr¨om PCG deliver better performance than random features preconditioning on all the datasets considered. Nystr¨om PCG also uses less storage. Table 10 shows that Nystro¨om PCG yields better performance than random features PCG on the larger scale datasets when both are restricted to ranks of 1, 000. Table 11 shows the adaptive strategy proposed in subsection 5.4.2 to select (cid:96) works very well. In contrast, it is diﬃcult to choose mrf for random features preconditioning: the authors of [2] provide no guidance except for the polynomial kernel."
2110.0282,dataset,141,,,"version uses the oracle best value of (cid:96) found by grid search (from the same grid used to select mrf) to minimize the total runtime, and the second is the adaptive algorithm described in subsection 5.4.2. The grid for (cid:96) and mrf is restriced to less than 10, 000 to keep the preconditioners cheap to apply and store. The adaptive algorithm for each dataset was initialized at (cid:96) = 2, 000, which is smaller than 0.05n for all datasets. For 105, we restricted both (cid:96) and mrf to 1, 000, which corresponds the datasets with n to less than 0.01n. We then run both algorithms till they reach the desired tolerance or the maximum number of iterations are reached."
2110.0282,dataset,172,,,"We now give the details of the random features experiments. For Shuttle-rf we used random features corresponding to a Gaussian kernel with bandwidth parameter σ = 0.75, we set µ = 10−8/n. For smallNORB-rf we used ReLU random features 10−4. For Higgs we normalized the features by their z-score and we with µ = 6 used random features for a Gaussian Kernel with σ = 5 and regularization µ = 10−4. Similarly for YearMSD, we normalized the matrix by their z-score and used random features for a Gaussian kernel with σ = 8 and µ = 10−5. The sketch size for R&T was d, 2d selected from , to prevent the cost of the forming and applying preconditioner } { from becoming prohibitive. We selected the AdaIHS parameter ρ from the same grid used for the regularization path experiments. We also capped the sketch size for AdaIHS for each dataset by the sketch sized used for R&T."
2110.0282,dataset,19,,,Table 7: ALOOCV: Large datasets. Block Nystr¨om PCG outperforms block CG as µ becomes small.
2110.0282,dataset,31,,,We run two sets of experiments. For the datasets with n < 105 we run oracle random features PCG against two versions of the Nystr¨om PCG algorithm. The ﬁrst
2110.0282,dataset,32,,,"Table 6: ALOOCV: Small datasets. The error for a given value of µ is the maximum relative error on 100 randomly sampled datapoints, averaged over 20 trials."
2110.0282,dataset,39,,,"6.5.1. Background. We brieﬂy review KRR [32]. Given a dataset of inputs R for i = 1, . . . , n and a kernel function R in the associated reproducing kernel Hilbert"
2110.0282,dataset,43,,,"Table 3: Ridge regression: Nystr¨om PCG versus AdaIHS and R&T PCG. Nystr¨om PCG outperforms AdaIHS and R&T PCG in iteration and runtime complexity for both datasets. Additionally, Nystr¨om PCG requires much less storage."
2110.0282,dataset,54,,,"For the large scale problems the adaptive algorithm for Nystr¨om PCG was initialized at (cid:96)0 = 500 and is capped at (cid:96)max = 4000. We set the solve tolerances for both algorithms to 10−10. As before, we sample 100 points randomly from each dataset."
2110.0282,dataset,56,,,"C.2. ALOOCV. The datasets were chosen so that n and d are both large, the challenging regime for ALOOCV. The ﬁrst three datasets are binary classiﬁcation problems, while SVHN has multiple classes. For SVHN we created a binary classiﬁcation problem by looking at the ﬁrst class vs. remaining classes."
2110.0282,dataset,62,,,"6.2.3. Random features regression. Tables 3 and 4 compare the performance of Nystr¨om PCG, AdaIHS, and R&T PCG for random features regression. Table 3 shows that Nystr¨om PCG performs best on all datasets for all metrics. The most striking feature is the diﬀerence between sketch sizes: AdaIHS and R&T require much"
2110.0282,dataset,62,,,"The second set of experiments uses the larger datasets real-sim and rcv1.binary and small values of µ, the most challenging setting for ALOOCV. We restrict our comparison to block Nystr¨om PCG versus the block CG algorithm, as Nystr¨om sketchand-solve is so inaccurate in this regime. We employ Algorithm E.2 to construct the preconditioner for block Nystr¨om PCG."
2110.0282,dataset,69,,,"consider the shuttle-rf dataset (subsection 6.2). The matrix G has dimension 43, 300 × 10, 000, while the preconditioner is based on a Nystr¨om approximation with rank (cid:96) = 800. Figure 1 shows the progress of the residual as a function of the iteration count. Nystr¨om PCG converges to machine precision in 13 iterations, while CG stalls."
2110.0282,dataset,7,,,Dataset CIFAR-10 Guillermo smallNorb-rf shuttle-rf Higgs-rf YearMSD-rf
2110.0282,dataset,7,,,Table 2: Ridge regression datasets.
2110.0282,dataset,73,,,"All datasets either come with speciﬁed test sets, or we create one from a random 80-20 split. The PCG tolerance, σ, and µ were all chosen to achieve good performance on the test sets (see Table 11 below). Both methods were allowed to run for a maximum of 500 iterations. The statistics for each dataset and the experimental parameters are given in Table 8."
2110.0282,dataset,8,,,Dataset ijcnn1 MNIST Sensorless SensIT MiniBooNE EMNIST-Balanced Santander
2110.0282,dataset,82,,,"6.5.2. Experimental overview. We use Nystr¨om PCG to solve several KRR problems derived from classiﬁcation problems on real world datasets from [6, 38]. For 2/(2σ2)). We all experiments, we use the Gaussian kernel (cid:107) compare our method to random features PCG, proposed in [2]. We do not compare to vanilla CG as it is much slower than Nystr¨om PCG and random features PCG."
2110.0282,dataset,86,,,"6.1. Preliminaries. We implemented all experiments in MATLAB R2019a and MATLAB R2021a on a server with 128 Intel Xeon E7-4850 v4 2.10GHz CPU cores 105), every numerical and 1056 GB. Except for the very large scale datasets (n experiment in this section was repeated twenty times; tables report the mean over the twenty runs, and the standard deviation (in parentheses) when it is non-zero. We highlight the best-performing method in a table in bold."
2110.0282,dataset,9,,,Table 5: ALOOCV datasets and experimental parameters.
2110.05239,data,104,,,"For each network and experimental set up we perform a class wise ROC analysis, yielding eight receiver operator curves and corresponding AUROCs for each network. We subtract the AUROC for the image data alone from the AUROC when combining the image features and metadata. This class-level measure of improvement or degradation is represented as boxplots presented in Fig. 3. There is an overall increase the unprocessed images in AUROC in all cases except when using a vgg16 network which shows a considerable degradation. When using augmented images vgg16 shows an enhancement in line with the other networks."
2110.05239,data,12,,,"1Spencer A. Thomas is with the Data Science group, National Physical"
2110.05239,data,123,,,"The metadata for the images, M are mapped such that they contain only numerical values to be compatible with standard neural networks. The mapping function G (M ) converts the data to ASCII decimal introduced in [3]. The conversion is performed element wise for an input string to allow maximum ﬂexibility, for example, distinguishing upper and lower case letters, and mixed numerical and text inputs. When the input data differ in length, all instances are padded with trailing white space to the same size as the largest input string prior to conversion. Any missing entries in the ﬁelds are recorded as not a number (zero in ASCII decimal)."
2110.05239,data,124,,,"Deep learning has emerged as a powerful suite of tools for image classiﬁcation [1], and has a huge potential to solve challenges in healthcare settings. The use of deep neural networks is successful at tasks such as classiﬁcation of medical images [2], analysis of electronic health records [3]– [5] and segmenting data from emerging medical technologies [6], [7]. This enormous potential comes with the caveat that very large amounts of data are required to train robust models that generalise beyond the training set. This requirement is unfortunately difﬁcult to satisfy in the majority of biological and medical studies due to barriers to data availability."
2110.05239,data,139,,,"Transfer learning has emerged as a promising method for circumventing the need for vast amounts of data to train deep networks [8]. For domains with limited data, transfer learning utilises networks pre-trained on similar tasks with large amounts of data [9]. Transfer learning is often used in medical imaging [2], [10], [11] due to the limited availability of data that require expert labeling [12]. Transferring the image features from one domain to another can at least match the performance of models trained directly on the new domain [13]. However the conﬁguration of the transfer can be performed in a number of ways [12], [14] and more research is needed in this area."
2110.05239,data,145,,,"Consider the input data as X ∈ RN ×D where Xi is a D dimensional data point with N instances of the data. For imaging analysis X is the imaging data with D pixels and N images. Deep learning takes X as an input and applies a series of transformations through hidden layers typically in the form of convolutions. Following the notation of [21], a matrix W k ∈ Rdk−1×dk is used to linearly transform the output of the (k − 1)th layer, Xk−1 ∈ RN ×dk−1 , into a dk dimensional space, Xk−1W k ∈ RN ×dk , at the kth layer. The linear transformations are followed by a non-linear function, σk(z), at each layer. The output of a network with K layers is given by"
2110.05239,data,148,,,"This classiﬁcation model is trained using gradient descent for a maximum of 2000 epochs or when the gradient falls below 10−6. In this work we compare the performance of the transfer learning based classiﬁcation of the ISIC image data, to the performance of transfer learning when images are combined with their associated metadata. In the former case, we extract the image features, F(X), from each network pretrained on ImageNet, which are then passed to the softmax function to classify the images. In the latter case, we combine F(X) and G(M ) as in Eq. (2), and pass H to the softmax classiﬁer. In all experiments the data are split into 70:30 training:testing sets that are ﬁxed for all networks for comparability of results."
2110.05239,data,188,,,"We report the macro average (mean class) performance in order to concisely summarise the ﬁndings of our experiments. In all of our experiments, we ﬁnd that combining metadata with the image features improves classiﬁcation performance for all networks compared to classifying using only image features. This enhancement is observed in all metrics indicating this may be a general characteristic of deep networks. This is clearly illustrated in Fig. 2 where positive values indicate an improvement when including metadata. The only degradation observed was in the sensitivity of a vgg16 network when using data augmentations. However, this decrease is small and this network exhibits relatively low improvements with augmented data compared to the other networks in this work. Improvements in accuracy and speciﬁcity are relatively small in all cases, though substantial improvements in the other metrics are seen in all networks. Speciﬁcally, googlenet, densenet201 and inceptionresnetv2 show improvements of more than 10 percentage points, meaning a score of 0.7 when using only image data increases to ≥ 0.8, a signiﬁcant improvement."
2110.05239,data,21,,,"[17], and data driven methods can identify patterns of patients [3], [4]."
2110.05239,data,44,,,"[9] S. J. Pan, Q. Yang, W. Fan, and S. J. Pan, “A survey on transfer learning,” IEEE Transactions on Knowledge and Data Engineering, vol. 22, pp. 1345–1359, 2010."
2110.05239,data,58,,,"The metadata ﬁelds are integrated with the image data by concatenating the image features obtained by the CNN at its deepest layer prior to classiﬁcation, F(X) (blue vector in Fig. 1), with the encoded metadata inputs, G(M ) (red vector in Fig. 1)."
2110.05239,data,62,,,"[3] S. A. Thomas, N. Smith, V. Livina, I. Yonova, R. Webb, and S. de Lusignan, “Analysis of primary care computerized medical records (cmr) data with deep autoencoders (dae),” Front. Appl. Math. Stat., vol. 5, pp. 1–12, 2019."
2110.05239,data,75,,,"Medical imaging data often has associated metadata used by clinicians in patient assessments. These metadata are multi type (numeric, categorical, etc) and are essential for maintaining the value of archived data [15]. The information may be content related, e.g. scanner parameters, or relevant extracts from computerised medical records (CMR). These resources contain rich information relating to diseases [16],"
2110.05239,data,81,,,"Classiﬁcation tasks based on the combination of imaging with genomics data has been shown to surpass clinical experts in digital pathology [18]. Combining relevant information about the sample, e.g. patient demographics, with imaging data has also yielded high accuracy scores in binary classiﬁcation tasks [19]. However, the effect of combining these data is unknown and an assessment of any improvements or degradation to the networks in these frameworks is needed."
2110.05239,data,81,,,"It is worth noting that these improvements come at a negligible cost as seen in Table II. The training time for the softmax classiﬁer when using the combined data is comparable to when using the image features alone. Moreover, this is insigniﬁcant compared to the feature extraction time in all networks, smaller by up to two orders of magnitude. The low time cost makes this a practical extension of current methods where metadata are available."
2110.05239,data,83,,,Fig. 1. Combination of imaging and non-imaging data in deep networks. A series of convolution and pooling operations (orange) yield a lower dimensional feature vector (blue) for image data. The non-imaging data are encoded numerically by mapping to ASCII decimal [3] providing a metadata feature vector (red). The imaging and non-imaging feature vectors are concatenated (purple) and used as input for a softmax classiﬁer (green).
2110.05239,data,92,,,"[5] A. Avati, K. Jung, S. Harman, L. Downing, A. Ng, and N. H. Shah, “Improving palliative care with deep learning,” vol. 18, p. 122, 2018. [6] S. A. Thomas, Y. Jin, J. Bunch, and I. S. Gilmore, “Enhancing classiﬁcation of mass spectrometry imaging data with deep neural networks,” in 2017 IEEE Symposium Series on Computational Intelligence (SSCI), 2017, pp. 1–8."
2110.05239,data,94,,,"For the augmentation experiments, we introduce a subset of image manipulations, X (cid:48) = Ω (X) , where Ω represents the augmentation to the image prior to passing it to the network. The augmentation function introduces a random shift in the image of up to 30 pixels from its origin along the X axis and separately along the Y axis, random reﬂections in X and/or Y, and random rotations up to 90 degrees. This transformation is applied to the training and testing data."
2110.05239,"data, data available",87,,,"Clinicians will typically base diagnosis on several information sources either implicitly or explicitly. Demographic factors such as age can inﬂuence the likelihood of disease prevalence. In this work we investigate the combination of imaging data with related metadata to enhance classiﬁcation performance evaluated by several metrics. We utilise transfer learning due to the limited volumes of data available, comparing the performance with and without metadata. Additionally we repeat the experiments with and without data augmentation during the training of the model."
2110.05239,"data, dataset",155,,,"Abstract— In this work, we compare the performance of six state-of-the-art deep neural networks in classiﬁcation tasks when using only image features, to when these are combined with patient metadata. We utilise transfer learning from networks pretrained on ImageNet to extract image features from the ISIC HAM10000 dataset prior to classiﬁcation. Using several classiﬁcation performance metrics, we evaluate the effects of including metadata with the image features. Furthermore, we repeat our experiments with data augmentation. Our results show an overall enhancement in performance of each network as assessed by all metrics, only noting degradation in a vgg16 architecture. Our results indicate that this performance enhancement may be a general property of deep networks and should be explored in other areas. Moreover, these improvements come at a negligible additional cost in computation time, and therefore are a practical method for other applications."
2110.05239,"data, dataset provided",58,,,Extraction is the time to obtain the dK dimensional features from the network processing over 48 CPU cores. Training times refer to time to train the softmax classiﬁer based on either input features from F or H. Times for the unprocessed (left) and augmented (right) data are provided respectively for each case.
2110.05239,dataset,119,,,"We compare several state-of-the-art deep convolutional neural network architectures for obtaining F. All the networks used here have been pretrained using the ImageNet [22] dataset, and the network weights transferred to the ISIC image dataset. In this conﬁguration, we are using the networks as feature extractors. Speciﬁcally we evaluate alexnet [23], densenet201 [24], resnet50 [25], inceptionresnetv2 [26], vgg16 [27] and googlenet [28] each with and without augmentation added to the input images. To account for the difference in input size to each network, all images are resized to the required dimensions using bi-linear interpolation."
2110.05239,dataset,71,,,"[12] T. Rai, A. Morisi, B. Bacci, N. J. Bacon, S. A. Thomas, R. M. L. Ragione, M. Bober, and K. Wells, “Can imagenet feature maps be applied to small histopathological datasets for the classiﬁcation of breast cancer metastatic tissue in whole slide images?” in Medical Imaging 2019: Digital Pathology, vol. 10956, 2019."
2110.05239,dataset,73,,,"[11] H. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, and R. M. Summers, “Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35, no. 5, pp. 1285–1298, May 2016."
2110.05239,"dataset, database",117,,,"A large collection of digital skin images from the International Skin Imaging Collaboration (ISIC) Melanoma Project [20] have been collated, processed and classiﬁed by expert dermatologists. The HAM1000 dataset from the ISIC database contains 10,015 digital images of skin lesions, each belonging to one of eight classes of skin conditions. Additionally the images have associated metadata containing clinical and acquisition information. The clinical ﬁelds contain a small amount of patient information including diagnosis of the images, an example is shown in Table I. Speciﬁcally, these are, age (numerical), sex (categorical) and anatomical site of the lesion (text)."
2110.10422,data,107,,,"Synthetic CAR example. We a rectangle by subdividing it into 10 rows and 15 columns. Unlike the two examples above, kernel, where we work with here we generate data using the CAR model φ ∼ N (0, Q−1), Q = τ (D − αA), reliant on the adjacency matrix A. Parameter α is being assigned the Uniform(0.4, 1) prior. This prior is chosen to study the question whether VAE can be trained well on CAR samples, displaying spatial correlation which corresponds to the values of α away from zero."
2110.10422,data,118,,,"To test the performance of the trained VAE, ground truth data is generated by ﬁxing the value of α at 0.7, and adding normal i.i.d. noise term at each area with variance of 0.5. This data is then used to ﬁt the original CAR model, as well as the model using a VAE trained on CAR prior samples. Both models use the prior Uniform(0.01, 1) for the variance of the noise term. For both models we run MCMC with 1000 warm-up iterations and 2000 iterations to sample from posterior. The average ESS of the spatial random eﬀect in the VAE-CAR model is 4650 which took 9"
2110.10422,data,131,,,"Most commonly, VAEs have been used in the literature to learn and generate observed data. We propose using spatial priors x = fGP, evaluated at the required spatial conﬁguration (a grid or neighbourhood structure) as training examples instead of observed data. The trained VAE then enables us to compute ˆx = fVAE. Remarkably, unlike dealing with observed data, this approach does not have issues with the quality nor the quantity of the training examples. The amount of training data is unlimited since we can draw as many GP realisations for training as required. Similarly, there is no issue of data quality as we can create exact GP draws, free from noise, characteristic for any real-life observations."
2110.10422,data,140,,,"can use directly the simulated counts y arising from the Poisson distribution with rate λ = exp(fGP). The encoder now maps these counts into the latent variable z, as, before, and the decoder is enhanced with a probabilistic step: it maps z into a positive rate ˆλ, and calculates the reconstruction loss as p(y|ˆλ) : y ∼ Pois(ˆλ). Results of such a model are shown on Figure 8. We have generated training data y for the VAE over a regular grid with n = 100 points according to the distribution y ∼ Pois(exp(fGP)). The hyperparameters of the GP were identical to our one-dimensional GP example presented above. VAE architecture included two hidden layers with elu"
2110.10422,data,141,,,"An alternative approach to training a VAE for non-Gaussian likelihoods Our previously described approach, where we calculate the linear predictor and then ﬁt model to the data using a link function, follows the long standing tradition in statistics driven by the interest to measure associations between predictors and an outcome. In the previous two examples we have trained the VAE directly on the GP draws (fGP) and used the Gaussian distribution to compute the reconstruction loss p(x|ˆx). When the outcome data is not Gaussian or not even continuous, there is an alternative way of training a VAE. Let us consider, for instance, count data which we would like to model using the Poisson distribution. Instead of using fGP draws as training data of the VAE, we"
2110.10422,data,147,,,"Statistical models of areal data described above assign the same characteristics to each location within every area Bi. This assumption is unrealistic for many settings as heterogeneity might be present within each Bi, as long as the size of the area is non-negligible. Areal data can be viewed as an aggregation of point data and a series of approaches began to emerge recently to address this issue (Johnson et al. (2019); Arambepola et al. (2022)). Hence, it is reasonable to use a data augmentation step to sample from the posterior distribution of the exact locations, and then to aggregate results. Future directions of research include an approach where presented above models, reliant on the adjacency structure, are substituted with continuous kernel approaches and a VAE trained using them."
2110.10422,data,156,,,"or its extensions (Li and Turner, 2016). The ﬁrst term in ELBO is the reconstruction loss, measured by likelihood quantifying how well ˆx and x match. The second term is a Kullback-Leibler (KL) divergence which ensures that z is as similar as possible to the prior distribution, a standard normal. It serves as a regulariser ensuring parsimony in the latent space and thus leads to uncorrelated variables in the latent space. New data can be generated by sampling from the latent space with the trained decoder network. Once the numerically optimal network parameters ˆψ have been obtained, new realisations can be generated in two steps. As the ﬁrst step, we draw from the standard normal distribution z ∼ N (0, I), and as the second step, apply the deterministic transformation D ˆψ(z)."
2110.10422,data,173,,,"Suppose we are given outcome data y1, . . . , yn, corresponding to a set of observed disjoint areas {Bi}, i = 1, . . . , n, covering the domain of interest G = ∪n i=1Bi. These areas may be the cells of a computational grid, pixels of a raster image, or they may correspond to existing administrative units. Outcome data {yi}n i=1 can represent either counts aggregated over an area, such as number of disease such as disease cases, continuous bounded data, prevalence (i.e. a number between zero and one), or continuous unbounded data, such as land surface temperature in degrees Celsius (◦C). Hence, in applied ﬁelds, it is common to use generalised linear models to unify the modelling approach for all such types of outcome. Accounting for mixed eﬀect model structure, their Bayesian hierarchical formulation can be expressed as follows"
2110.10422,data,173,,,"respectively. To perform cross-validation, we created a 5-fold split of the data. To measure performance, we have used MSE between the continuous predicted rate λ and the observed count y. The mean MSE of the BYM model across ﬁve runs was 426, with standard deviation of 131, and the mean MSE of the VAE-BYM model was 414, with standard deviation of 171. Average ESS of the VAE-BYM random eﬀect was ∼3850, and average ESS of the BYM random eﬀect was ∼630. Inference times were 3s on average (0.2s standard deviation) for the VAE-BYM runs, and 33s on average (3s standard deviation) for the BYM runs. These experiments conﬁrm the consistency of our observations: even when the dimension of the latent variable z is the same as the number of the counties, there is a beneﬁt to using VAE-BYM over BYM it achieves comparable performance while displaying much higher ESS and shorter inference times."
2110.10422,data,196,,,"Gaussian processes implemented through multivariate Gaussian distributions for a ﬁnite collection of data, are the small-area approach most spatial In this context they are used to encode correlation structures over space and can generalise in interpolation tasks. Despite their well ﬂexibility, oﬀ-the-shelf GPs present serious computational limit their scalability and practical usefulness in applied settings. Here, we propose a novel, deep generative modelling approach to tackle this challenge: for a particular setting, we approximate a class spatial sampling through prior of GP priors and subsequent ﬁtting of a variational autoencoder (VAE). Given a trained VAE, the resultant decoder allows spatial inference to become incredibly eﬃcient due to the independently distributed low dimensional, latent Gaussian space representation of the VAE. Once trained, inference using the VAE decoder replaces the GP within a Bayesian approach sampling provides tractable and easy-to-implement means of approximately encoding spatial priors and facilitates statistical inference. We demonstrate the utility of our VAE two stage approach on Bayesian, small-area estimation tasks. Keywords: spatial modelling, Gaussian process prior, small-area estimation, Bayesian inference,"
2110.10422,data,205,,,"it learns a probabilistic a latent representation z, distribution p(z|x), from which a latent representation can be sampled. This distribution is chosen in such a way, that the latent variables z are independent the and normally distributed. encoder outputs a pair of d-dimensional vectors µγ, σ2 γ to characterise the mean and variance of z, respectively. The latent variable then follows the Gaussian distribution z ∼ N (µγ, σ2 γI). The loss function is modiﬁed to include a KL-divergence term implied by a standard N (0, I) prior on z: L(x, ˆx) = p(x|ˆx) + KL(N (µγ, σ2 γI)||N (0, I)). The KL term can be interpreted as a regulariser ensuring parsimony in the latent space. New data can be generated by sampling from the latent space with the trained decoder network. This feature of VAEs has led to a series of successful applications with the goal of generating new samples from the approximated distribution of real data (Liu et al., 2018)."
2110.10422,data,211,,,"An autoencoder (Hinton and Salakhutdinov, 2006) the is a neural network architecture used for supervised dimensionality reduction and task of representation learning. It is comprised of three components: an encoder network, a decoder network, layer containing a latent and a low dimensional representation. The ﬁrst component, the encoder Eγ(·) with parameters γ, maps an input x ∈ Rp into the latent variable z ∈ Rd, where d is typically lower than p. The decoder network Dψ(·) with parameters ψ aims to reconstruct the original data x from the latent representation z. Therefore, an autoencoder imposes a ‘bottleneck’ layer in the network which enforces a compressed representation z of the original input x. This type of dimensionality reduction technique is particularly useful when some structure exists in the data, such as spatial correlations. This structure can be learnt and consequently leveraged when forcing the input through the network’s bottleneck. Parameters of the encoder and decoder networks are learnt through the minimisation of a reconstruction loss function p(x|ˆx) expressed as the likelihood of the observed data x given the reconstructed data ˆx."
2110.10422,data,227,,,"the ﬁxed eﬀects design matrix (a set of covariates), β are the ﬁxed eﬀects, and η is the linear predictor combining ﬁxed and random eﬀects. Equation (4) provides an observational model, where u is a link function characterising the mean of the distribution (e.g. logit for binomial data, exponential for positive data). A common modelling assumption is that the observations yi are conditionally independent p(y|f, θ) = (cid:81)n i=1 p(yi|η(fi), θ), and the spatial It structure is captured latently by function f . is common to choose a GP prior over f , and as a consequence, ﬁnite realisations fGP are jointly normally distributed with mean µ and covariance matrix Σ. Since fGP always enters the model via the linear predictor, without loss of generality (aﬃne transformation) we can consider fGP to have zero mean (µ = 0) and be distributed as fGP ∼ N (0, Σ). Structure of the covariance matrix Σ depends on the spatial setting of the problem and the model which we chose for the random eﬀect. Once the model for fGP has been deﬁned, the linear predictor can be computed as"
2110.10422,data,23,,,Figure 16: Posterior density the spatial random eﬀect for one area obtained during MCMC inference on HIV prevalence data from Zimbabwe.
2110.10422,data,23,,,Figure 17: HIV Zimbabwe prevalence data. Autocorrelations of MCMC samples of a spatial random eﬀect modelled via the VAE-CAR model.
2110.10422,data,23,,,Figure 18: HIV Zimbabwe prevalence data. Autocorrelations of MCMC samples of a spatial random eﬀect modelled via the CAR model.
2110.10422,data,232,,,"An autoencoder is a neural network that is trained by unsupervised learning, with the aim of dimensionality reduction and feature learning. It consists of an encoder network, Eγ(·) with parameters γ, a decoder network Dψ(·) with parameters ψ and a bottleneck layer containing a latent vector z. The encoder maps input x ∈ Rp into the latent vector z ∈ Rd, and the decoder network maps z back into the input space by creating a reconstruction of the original data ˆx = Dψ(Eγ(x)). The network is trained by optimisation to learn reconstructions ˆx that are close to the original input x. An autoencoder can including borrow any neural network architecture, multilayer perceptrons, convolutional or recurrent layers. A VAE is a directed probabilistic graphical model whose posterior is approximated by a neural forming an autoencoder-like architecture. network, The goal of VAEs is to train a probabilistic model in the form of p(x, z) = p(x|z)p(z) where p(z) = N (0, I) is a prior distribution over latent variables z and p(x|z) is the likelihood function that generates data x given latent variables z. The output of the"
2110.10422,data,241,,,"of disease cases recorded per county, regardless of the exact locations where within the county the cases occurred or got recorded, constitutes an example of areal data. State-of-the-art models of areal data rely on ‘borrowing strength’ from neighbours and use a hierarchical Bayesian formulation to do so. Widely used models of such data capture spatial structure via the adjacency matrix A. Each element aij of it is either 1, if areas Bi and Bj are neighbours, and 0 if not. Point-referenced (geostatistical) data represents measurements of a spatially continuous phenomenon at a set of ﬁxed locations. Number of cases recorded at a set of hospitals with known geographical position is an example of such data. Modelling of the underlying continuous process relies on pair-wise distances between observed and unobserved locations. GPs in such models use continuous kernels. Point pattern data consist of precise locations of events. An example of a point pattern is a collection of GPS coordinates of households of newly occurred malaria cases. One way of modelling point pattern data is to cover the entire study area with a ﬁne computational grid and view each grid cell as a location. Modelling of the continuous underlying process is typically done using continuous GP kernels. All of the three types of data can be modelled using the proposed novel approach."
2110.10422,data,30,,,"Gelfand, A. E. and Vounatsou, P. (2003). Proper multivariate conditional autoregressive models for spatial data analysis. Biostatistics, 4(1):11–15."
2110.10422,data,30,,,"Hinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. science, 313(5786):504–507."
2110.10422,data,312,,,"One-dimensional GP on regular and irregular In this ﬁrst example we use VAE to perform grids. inference on continuous data {yi}n i=1 over a regular one-dimensional grid. The grid consists of n = 400 points in the (0, 1) interval. Training prior samples are drawn as evaluations of a Gaussian process with zero mean and squared exponential kernel k(h) = σ2e−h2/l2 . This model is useful for small-area estimation when the covariance matrix is Euclidean distance-based. To allow for hyperparameter learning, we impose hierarchical structure on the model by using hyperpriors on variance σ2 ∼ LogNormal(0, 0.1) and lengthscale l ∼ InverseGamma(4, 1). This hierarchical structure allows the VAE to be trained on a range of values of hyperparameters. The average lengthscale, according to these priors, is around 0.3. Since this is much larger than the distance between two neighbouring points on the grid (0.0025), there is enough redundancy in the data to expect a lower dimensional embedding. Realisations fGP are presented on Figure 1(left). We trained a VAE with two hidden layers of dimensions 35 and 30, respectively, and the bottleneck layer of dimension 10. As an activation function we used the rectiﬁed linear unit (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011) for all nodes in the hidden layers. The priors learnt by the VAE are presented in Figure 1(right). Figure 1 shows that the typical shape of the prior, as well as the mean have been learnt well. Amount of uncertainty (second moment) displayed by the VAE priors is lower."
2110.10422,data,343,,,"Since this is much larger than the distance between two neighbouring points on the grid (0.0025), there is enough redundancy in the data to expect a lower dimensional embedding. Realisations fGP are presented on Figure 1(left). We trained a VAE with two hidden layers of dimensions 35 and 30, respectively, and the bottleneck layer of dimension 10. As an activation function we used the rectiﬁed linear unit (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011) for all nodes in the hidden layers. The priors learnt by the VAE are presented in Figure 1(right). Figure 1 shows that the typical shape of the prior, as well as the mean have been learnt well. Amount of uncertainty (second moment) displayed by the VAE priors is lower. This is an expected phenomenon as vanilla VAEs are known to produce blurred and over-smoothed data due to compression; on the other hand, they perform well on denoising tasks (Kovenko and Bogach (2020)) i.e. reconstructing underlying truth given corrupt data. Empirical covariance matrices of the original and trained VAE priors show similar patterns (Supplement Figure 12). To perform inference, we generate one GP realisation, use it as the ground truth, and simulate observed data by adding i.i.d. noise. We allow the number of observed data points to vary as 0.5% (2 data points), 1% (4 data points), and 1.5% (6 data points) of the total number of the grid points and recover the true function. The model used for inference is y ∼ N (fVAE, s2), where the amount of noise is given the half-Normal prior s ∼ N +(0.1). Inference results are presented on Figure 2. The higher the"
2110.10422,data,37,,,"Martins, T. G., Simpson, D., Lindgren, F., and Rue, H. (2013). Bayesian computing with INLA: new features. Computational Statistics & Data Analysis, 67:68–83."
2110.10422,data,38,,,Figure 15: Posterior predictive distributions of the case count data for each out of 56 counties in Scotland. Each posterior distribution is represented by its point estimate (mean) and 95% Bayesian credible intervals
2110.10422,data,39,,,"In this paper we propose a novel use of VAEs: we learn uncorrelated representations of spatial GP priors for a predeﬁned spatial setting, and then use the trained decoder to perform Bayesian inference on new data."
2110.10422,data,41,,,"Johnson, O., Diggle, P., and Giorgi, E. (2019). A spatially discrete approximation to log-gaussian cox processes for modelling aggregated disease count data. Statistics in medicine, 38(24):4871–4887."
2110.10422,data,42,,,Figure 4: Inference results on a synthetic CAR example: data is generated by adding a noise term to the ground truth. We perform MCMC inference on the data by using both the original CAR and the VAE-CAR models.
2110.10422,data,45,,,"Figure 8: Inference results obtained via a VAE trained directly on the count data y generated from the Poisson distribution Pois(λ), λ = exp(fGP). The estimated rate is in good agreement with the ground truth."
2110.10422,data,5,,,2.3 Models of areal data
2110.10422,data,50,,,HIV prevalence in Zimbabwe. We consider household survey data from the 2015-2016 Populationbased HIV Impact Assessment Survey in Zimbabwe Sachathep et al. (2021). The observed positive cases yi among all observed cases ni in each area Bi are modelled according to the Binomial distribution:
2110.10422,data,55,,,"Another natural way to model areal data, especially gridded surfaces with small area sizes, is modelling an approximately as covariance between areas spatially continuous process, for example based on the pairwise distances between the centroids. Typical kernels for distance-based covariance matrices include squared exponential, exponential, periodic or Mat´ern."
2110.10422,data,56,,,Figure 14: Results of MCMC inference on an irregular grid on noisy GP data by using the trained VAE priors fVAE. The posterior mean of our model is shown in green with the 95% credible intervals shown in blue. Quality of the estimation improves with the growing number of data points.
2110.10422,data,57,,,"• We introduce a two stage process for inference. First, we train a VAE to create an uncorrelated representation of complex spatial priors. Next, we use the learnt latent distribution in the model instead of the GP prior to perform MCMC inference on new data, while keeping the trained decoder ﬁxed."
2110.10422,data,6,,,2.1 Three types of spatial data
2110.10422,data,69,,,"Figure 11: We perform MCMC inference on the projected COVID-19 incidence in the UK at the LTLA level. The leftmost plot displays raw data, the middle plot shows the mean of the posterior predictive distribution using the the CAR spatial random eﬀect fCAR, the rightmost plot shows the mean of the posterior predictive distribution obtained from the model with the fVAE-CAR spatial random eﬀect."
2110.10422,data,69,,,"Figure 9: We perform MCMC inference on the observed number of HIV-positive individuals among observed individuals in Zimbabwe. The leftmost plot displays raw data, the middle plot shows the mean of the posterior predictive distribution using the the CAR spatial random eﬀect fCAR, the rightmost plot shows the mean of the posterior predictive distribution obtained from the model with the fVAE-CAR spatial random eﬀect."
2110.10422,data,69,,,"There are three types of spatial data: areal (or lattice), geostatistical (or point-references) and point patterns. Models of all three of them rely on the notion of GPs and their evaluations in the form of the multivariate normal distribution. Areal data arise when a ﬁxed domain is partitioned into a ﬁnite number of subregions at which outcomes are aggregated. Number"
2110.10422,data,70,,,Figure 2: We perform MCMC inference on data generated from a noise-free GP and added i.i.d. noise by using the trained VAE priors fVAE. The leftmost plot shows VAE prior. The plots on the right show posterior mean of our model in green with the 95% credible intervals shown in blue. Quality of the estimation improves with the growing number of data points.
2110.10422,data,71,,,"Figure 10: Traceplots of the spatial random eﬀect for one area obtained during MCMC inference on HIV prevalence data from Zimbabwe. Posterior samples produced by the VAE-CAR model display very little autocorrelation, which allows to achieve high ESS and fast inference. Posterior samples produced by the original CAR model, on the contrary, show high auto-correlation, leading to low ESS and longer computation time."
2110.10422,data,76,,,"0.00.20.40.60.81.0x2.01.51.00.50.00.51.01.52.0y=fGP(x)GP priors we want to encodeGP drawsmean95% HPDI0.00.20.40.60.81.0x2.01.51.00.50.00.51.01.52.0y=fVAE(x)Priors learnt by VAEVAE drawsmean95% HPDI0.00.20.40.60.81.021012VAE priorprior drawsmeanVAE prior: 95% BCI0.00.20.40.60.81.021012n datapoints =2posterior drawspredicted meanground truthVAE posterior: 95% BCIobserved data0.00.20.40.60.81.021012n datapoints =4posterior drawspredicted meanground truthVAE posterior: 95% BCIobserved data0.00.20.40.60.81.021012n datapoints =6posterior drawspredicted meanground truthVAE posterior: 95% BCIobserved dataSemenova, Xu, Howes, Rashid, Bhatt, Mishra, Flaxman"
2110.10422,data,81,,,"and then linked to the observed data via the likelihood. Unless the random eﬀect is chosen to be trivial set of variables resulting from Σ = I), (i.e. the random eﬀect fGP represents the computational challenge. Further we describe some options for spatial random eﬀect priors in the context of smallarea estimation, and propose a method to substitute its calculation at the inference stage with another variable, leading to increased inference eﬃciency."
2110.10422,data,98,,,"In this paper we have proposed a novel application of VAEs to learn spatial GP priors and enable smallSuch an approach leverages the area estimation. power of deep learning to fuel inference for wellestablished statistical models. Uncorrelated latent parameters of the VAE make consequent Bayesian inference with MCMC highly eﬃcient at successfully exploring the posterior distribution. An advantage of the proposed approach, as compared to traditional VAE applications, is that there is no limitations in neither quality nor quantity of the training data as any number of training samples can be generated by"
2110.10422,"data, data available",129,,,"number of points, the closer is the estimated mean to the ground truth curve. Areas without any data available in their proximity, show higher uncertainty than areas informed by closely located data points. Eﬀective sample size (ESS) is an important measure of the eﬃciency of MCMC sampling (Martino et al., 2017). For example, we have run inference with 1000 warm-up and 1000 posterior MCMC samples for diﬀerent number of data points. Average ESS for the posterior of the function evaluated at the observed points increased together with the number of points, while inference time remained constant. The original GP model displayed the reverse trend: average ESS remained constant, while computation time increased."
2110.10422,"data, data available",130,,,"Spatially referenced data come in a variety of forms, including exact geographical coordinates such as a latitude and longitude or predeﬁned geographical areal units such as a village, administrative unit or pixel of a raster image. The latter are known as areal unit data, and are found in ﬁelds such as epidemiology, environmental and political science; a variety of relevant methods come under the banner of small-area statistics (Rao and Molina, 2015). There are many motivations for modelling such data, from surveillance program evaluation to identifying environmental risk factors for disease. Small-area statistics are particularly relevant to informing policy decisions, which are often made at the areal unit level (Clements et al., 2006)."
2110.10422,"data, dataset",144,,,"Scottish lip cancer dataset. The Scottish lip cancer dataset, originally presented by Kemp et al. (1985), has become a benchmark dataset for areal models. It has been used to demonstrate performance and implementations of CAR, ICAR, BYM and its variations (Duncan et al., 2017; Morris et al., 2019). The dataset consists of the observed and expected numbers of cases (y and E, respectively) at 56 counties in Scotland, as well as a covariate measuring the proportion of the population engaged in agriculture, ﬁshing, or forestry (aﬀ ). The covariate is related to exposure to sunlight, which is a risk factor for lip cancer. We model the count data y as following a Poisson distribution with the log-Normal rate λ"
2110.10422,"data, dataset",171,,,"Statistical modelling of spatial data is routinely (GP) priors performed using Gaussian process (Williams and Rasmussen, 2006). GPs have gained popularity in a variety of applied ﬁelds due to their ﬂexibility, ease of implementation, and their inherent ability to characterise uncertainty. However, GPs also present a number of practical challenges. For example, basic inference and prediction using a GP requires matrix inversions and determinants - both of which scale cubicly with data size. This makes applications of GPs prohibitive for large datasets. Moreover, kernel design of a GP requires substantial domain knowledge in order to reﬂect characteristics of the process of interest (Stephenson et al., 2021). Hence, the choice of an inference method is of great importance when it comes to dealing with GPs. The theoretic asymptotic convergence properties and diversity of Markov chain Monte Carlo (MCMC) approaches make it the most reliable method for Bayesian inference."
2110.10422,"data, dataset",270,,,"However, MCMC scales poorly, and struggles to deal with the high degree of auto-correlation inherent limiting its utility in large spatial to spatial data, settings (Rue et al., 2009). A range of more tractable approximate methods have been developed, such as Laplace approximation, variational inference (Hoﬀman et al., 2013), expectation propagation (Minka, 2013), or inducing variables in sparse GPs (Titsias, 2009). However, few of these approximate methods have asymptomatic guarantees or yield consistently accurate posterior estimates (Yao et al., 2018) over a wide range of challenging datasets. Userfriendly software packages, such as R-INLA (Martins et al., 2013) provide extremely convenient interfaces to a large set of predeﬁned models, but do not provide enough ﬂexibility for custom model development, and hence, have limitations for speciﬁc classes of applied research. There is an unmet need for approaches that can be easily implemented and customised in popular probabilistic programming languages such as Stan (Carpenter et al., 2017), NumPyro (Phan et al., 2019), PyMC3 (Salvatier et al., 2016) or Turing.jl (Ge et al., 2018), while still scaling favourably to large datasets. Here, we propose a novel computational technique which leverages the variational autoencoder model from deep learning and combines it with Bayesian inference (Mishra et al., 2020a; Fortuin et al., 2020) with the goal of small-area estimation."
2110.10422,"data, dataset",82,,,"Here we present results concerned with the Scottish lip cancer dataset, produced by models with i.i.d., BYM and VAE-BYM random eﬀects. Figure 15 shows posterior predictive distributions, produced by the three models when all of the available data was used to ﬁt the models. We observe that the i.i.d. model already achieves a relatively good ﬁt. BYM and VAE-BYM models are able to capture the remaming spatial dependence and are very similar between themselves."
2110.10422,"data, dataset",96,,,"drawing from the noise-free spatial priors. In addition, there is no need to retain the whole training data set as every batch can be generated on the ﬂy. Our method is beneﬁcial even if the latent space dimension is the same as the number of data points, unlike some other approximation methods, which rely on information redundancy in the data. As the HIV prevalence in Zimbabwe example shows, by reducing the autocorrelation between posterior MCMC samples, we can obtain drastic gains in both ESS and computation speed."
2110.10422,"data, dataset provided",206,,,"The VAE is trained on the spatial random eﬀect BYM priors fBYM = φ1 + φ2 to obtain the fVAE-BYM representation. We can use any parametrisation of BYM, as we only rely on its generative properties (and this is one of the advantages of our approach). We notice that a model with i.i.d. random eﬀect φ1 already produces a relatively good ﬁt (see Supplement, Figure 15(a)). It is only the remaining discrepancies that the spatially-structured random eﬀect φ2 needs to explain. We account for this observation in our priors for τ1 and τ2, as well as the dimension of the latent variable z: as there is only a moderate spatial signal, there is no redundancy in the data for spatial eﬀect estimation. To be able to provide good quality VAE-BYM priors, we opt to not compress the spatial prior and choose the dimension of z to be equal to the number of counties. We train a network with one hidden layer with 56 hidden nodes and use the exponential linear unit (Clevert For et al., 2015), or elu, activation function."
2110.10422,"data, package",136,,,COVID-19 incidence in the UK. We consider the projected number of COVID-19 infections in the UK at Lower Tier Local Authority (LTLA) level (Mishra et al. (2020b)). These estimates are available from the public website 1 and are based on the Scott et al. (2020) package. The model behind the package (Flaxman et al. (2020)) relies on a self-renewal equation and does not take spatial correlations into account. Here we demonstrate how spatial modelling using the proposed approach can be performed on the incidence data. Number of infections yi in each LTLA during the period between the 21st of January and the 5th of February 2022 is modelled via the Poisson distribution according to the model
2110.10422,dataset,24,,,• We demonstrate the usage of VAE-priors on a range of simulated and real-life datasets to show their performance in small-area estimation tasks.
2110.10422,dataset,254,,,"optimisation, we use the variational R´enyi bound that extends traditional variational inference to R´enyi αdivergences as proposed by Li and Turner (2016). By using α = 0 we opt for an importance weighted autoencoder (Burda et al., 2015). We performed two assessments to evaluate whether the VAE-BYM produces similar inference results as the original BYM model. First, we used both models for inference on the entire dataset to compare results for mapping (the most typical task in epidemiology and policy informing work), and second, we performed 5-fold cross-validation. Posterior predictive distributions of the rates λBYM, λVAE-BYM obtained by the models where fBYM and fVAE-BYM have been used to capture the spatial random eﬀect, are very close to each other: Figure 5 displays the two distributions, where each of them is represented by its point estimate (mean) and 95% Bayesian credible interval (BCI). Uncertainty intervals produced by the two models are remarkably close to each other for most of the counties. Figure 6 demonstrates very good agreement in the point estimates. Figure 7 presents the obtained maps: models with fBYM and fVAE-BYM produce very close spatial patterns. The average ESS of the spatial eﬀects in the BYM model is ∼150, and in the VAE-BYM model it is ∼1030. MCMC elapsed time shows the same trend: 402s and 12s for the BYM and VAE-BYM"
2110.10422,dataset,5,,,7 Scottish lip cancer dataset
2110.10422,dataset,50,,,"Figure 6: Scottish lip cancer dataset: point estimates (means) and uncertainty intervals (50% BCIs) of the rates λBYM and λVAE-BYM produced by models with fBYM and fVAE-BYM random eﬀects, respectively. There is a very good agreement between the two models."
2110.10422,dataset,6,,,8 HIV prevalence in Zimbabwe dataset
2110.10422,dataset,60,,,"The rest of this paper is organised as follows. In Section 2, we lay out the methodology of the twostage approach. In Section 3, we demonstrate the approach on synthetic and real datasets. In Section 4, we conclude with a discussion and provide a broader outlook on the potential impact of this work."
2110.10422,dataset,63,,,"posterior Scottish lip cancer dataset: Figure 5: predictive distributions of the rates λBYM, λVAE-BYM produced by models with fBYM and fVAE-BYM random eﬀects, respectively. The distributions are represented (mean) and uncertainty by the point estimates intervals (95% BCI) for each county. There is a very good agreement between the two models."
2110.10422,github,3,,,1https://imperialcollegelondon.github.io/
2110.10422,package,246,,,"priors for the spatial hyperparameter α can be pretrained, and then used for model selection - a step enabled by fast inference times when using VAEbased models. We have trained ﬁve VAEs using hyper-priors α ∼ U (0, 0.2), U (0.2, 0.4), U (0.4, 0.6), U (0.6, 0.8), U (0.8, 0.99), correspondingly. Each of the resulting VAE-priors were used to ﬁt a model. Model selection was performed based on the widely applicable information criterion (WAIC; Watanabe and Opper (2010)) using the arviz package (Kumar et al. (2019)). The best model was the one trained with α ∼ U (0.8, 0.99). To obtain smooth maps, we used 2000 MCMC iterations and performed inference using both the original CAR, as well as the VAECAR models with priors α ∼ Uniform(0.8, 0.99) and τ ∼ Gamma(6, 2). Resulting maps are presented on Figure 11: models with fCAR and fVAE-CAR produce similar spatial patterns. Characteristics of the two model ﬁts are presented in Table 1: there is no signiﬁcant diﬀerence between the absolute errors (p-value of the paired t-test is 0.05), while VAECAR has achieved much higher ESS at much shorter computation times."
2110.10422,package,45,,,"Scott, J. A., Gandy, A., Mishra, S., Unwin, J., epidemia: Flaxman, S., and Bhatt, S. (2020). Modeling of epidemics using hierarchical bayesian models. R package version 1.0.0."
2110.10422,python,32,,,"Salvatier, J., Wiecki, T. V., and Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2:e55."
2110.10422,"python, open-source",46,,,"Kumar, R., Carroll, C., Hartikainen, A., and Martin, O. (2019). Arviz a uniﬁed library for exploratory analysis of bayesian models in python. Journal of Open Source Software, 4(33):1143."
2111.02679,data,141,,,"We visualize the original training sample, the randomly augmented two views and the mixed image in Fig. 5, where multiple examples are shown. Note that, we adopt the same set of data augmentation strategies as in SimSiam (Chen and He 2021), including geometric augmentation, color augmentation and blurring augmentation. Concretely, the procedure of data augmentation is described as the PyTorch(Paszke et al. 2019) notations: we ﬁrst use RandomResizedCrop to select a random patch of the images and resize the patch to 224 x 224 with RandomHorizontalFlip; then we adjust brightness, contrast, saturation and hue of 0.8 probability as a color distortion, and perform grayscale conversion of 0.2 probability with RandomGrayscale; ﬁnally, we apply Gaussian blur to the patches."
2111.02679,data,141,,,"generalization of the visual representations. I-Mix (Lee et al. 2021) introduces virtual labels in a batch and mix data instances and their corresponding virtual labels in the input and label spaces, respectively. Similarly, MixCo (Kim et al. 2020) adopts a mix-up training strategy in contrastive learning to build semi-positive samples. Un-Mix (Shen et al. 2020) randomly mixes all the samples in a mini-batch with the smooth label to smooth decision boundaries and prevent the learner from becoming over-conﬁdent. In this paper, we adopt image mixture in the image space to obtain the hard mixed images for training. Instead of directly predicting the mixed images to the mixed labels, we further force the representation of the mixed images maintaining the most discriminative representation."
2111.02679,data,201,,,"Introduction Learning discriminative image representation in an unsupervised/ self-supervised manner has attracted increasing interest (Agrawal, Carreira, and Malik 2015; Doersch, Gupta, and Efros 2015; Xie et al. 2021), for it gets rid of the costly manually-labeled data and achieves promising performance on many down-stream tasks (Larsson et al. 2019; Hung et al. 2019; Doersch and Zisserman 2017). These methods generally design pretext tasks and learn the representation from the label generated by the tasks, such as rotation predicting (Komodakis and Gidaris 2018), jigsaw (Noroozi and Favaro 2016; Kim et al. 2018), in-painting (Pathak et al. 2016), colorization (Zhang, Isola, and Efros 2016; Larsson, Maire, and Shakhnarovich 2017) and clustering (Noroozi et al. 2018; Caron et al. 2018). Among them, many stateof-the-art methods employ the principle of contrastive learning (Grill et al. 2020; Chen et al. 2020a; Chen and He 2021; He et al. 2020) and achieve remarkable progress."
2111.02679,data,315,,,"In recent years, the rapid development of contrastive learning (Hadsell, Chopra, and LeCun 2006; Wu et al. 2018; Tian, Krishnan, and Isola 2020; Zhuang, Zhai, and Yamins 2019; Misra and Maaten 2020) has brought a greater breakthrough to self-supervised learning. The main idea of contrastive learning is to pull the positive pairs closer, while pushing the negative pairs away from each other in the embedding space. Previous works demonstrate that contrastive learning beneﬁts from observing sufﬁcient negative samples (Jaiswal et al. 2021). Follow this principle, InstDist (Wu et al. 2018) builds a memory bank storing all representations of instances, which increases the size of negative pairs. In MoCo (He et al. 2020), a momentum update mechanism is adopted to maintain a queue of negatives to build a stable data distribution with a large number of negative samples. Following this work, in SimCLR (Chen et al. 2020a), large-scale batch size is adopted to provide negative samples instead of the memory bank. In recent, some studies (Grill et al. 2020; Chen and He 2021) achieve promising performance with only positive pairs. BYOL (Grill et al. 2020) adopts a siamese network where one branch is a momentum encoder. It directly predicts the output of one view from another view. SimSiam (Chen and He 2021) further improved by adopting a stop gradient that iteratively receives gradient information parameters to prevent the siamese network from collapsing. In this paper, we make an attempt beyond the original siamese framework, where another branch with the mixed harder sample as input is utilized to predict invariant discriminative representation."
2111.02679,data,327,,,"Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677. Grill, J.-B. ; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P.; Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.; Gheshlaghi Azar, M.; Piot, B.; kavukcuoglu, k.; Munos, R.; and Valko, M. 2020. Bootstrap Your Own Latent - A New Approach to SelfSupervised Learning. In NeurIPS, volume 33, 21271–21284. Guo, H.; Mao, Y.; and Zhang, R. 2019. Mixup as locally linear out-of-manifold regularization. In AAAI, volume 33, 3714–3722. Hadsell, R.; Chopra, S.; and LeCun, Y. 2006. Dimensionality reduction by learning an invariant mapping. In CVPR, 1735–1742. He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020. Momentum contrast for unsupervised visual representation learning. In CVPR, 9729–9738. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778. Henaff, O. 2020. Data-efﬁcient image recognition with contrastive predictive coding. In ICML, 4182–4192. Hung, W.-C.; Jampani, V.; Liu, S.; Molchanov, P.; Yang, M.-H.; and Kautz, J. 2019. Scops: Self-supervised co-part segmentation. In CVPR, 869–878. Jaiswal, A.; Babu, A. R.; Zadeh, M. Z.; Banerjee, D.; and Makedon, F. 2021. A survey on contrastive self-supervised learning. Technologies, 9(1): 2."
2111.02679,data,346,,,"Chen, X.; Fan, H.; Girshick, R.; and He, K. 2020b. baselines with momentum contrastive learning. arXiv:2003.04297. Chen, X.; and He, K. 2021. Exploring simple siamese representation learning. In CVPR, 15750–15758. Doersch, C.; Gupta, A.; and Efros, A. A. 2015. Unsupervised visual representation learning by context prediction. In ICCV, 1422– 1430. Doersch, C.; and Zisserman, A. 2017. Multi-task self-supervised visual learning. In ICCV, 2051–2060. Donahue, J.; Kr¨ahenb¨uhl, P.; and Darrell, T. 2017. Adversarial feature learning. In ICLR. Feng, W.; Wang, Y.; Ma, L.; Yuan, Y.; and Zhang, C. 2021. Temporal Knowledge Consistency for Unsupervised Visual Representation Learning. In ICCV. Goyal, P.; Doll´ar, P.; Girshick, R.; Noordhuis, P.; Wesolowski, L.; Kyrola, A.; Tulloch, A.; Jia, Y.; and He, K. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677. Grill, J.-B. ; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P.; Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.; Gheshlaghi Azar, M.; Piot, B.; kavukcuoglu, k.; Munos, R.; and Valko, M. 2020. Bootstrap Your Own Latent - A New Approach to SelfSupervised Learning. In NeurIPS, volume 33, 21271–21284. Guo, H.; Mao, Y.; and Zhang, R. 2019. Mixup as locally linear out-of-manifold regularization. In AAAI, volume 33, 3714–3722. Hadsell, R.; Chopra, S.; and LeCun, Y. 2006. Dimensionality reduction by learning an invariant mapping. In CVPR, 1735–1742."
2111.02679,data,349,,,"References Agrawal, P.; Carreira, J.; and Malik, J. 2015. Learning to see by moving. In ICCV, 37–45. Berthelot, D.; Carlini, N.; Goodfellow, I.; Papernot, N.; Oliver, A.; and Raffel, C. 2019. MixMatch: A Holistic Approach to SemiSupervised Learning. In NeurIPS. Bossard, L.; Guillaumin, M.; and Van Gool, L. 2014. Food-101– mining discriminative components with random forests. In ECCV, 446–461. Caron, M.; Bojanowski, P.; Joulin, A.; and Douze, M. 2018. Deep clustering for unsupervised learning of visual features. In ECCV, 132–149. Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; and Joulin, A. 2020. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In NeurIPS. Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a. A simple framework for contrastive learning of visual representations. In ICML, 1597–1607. Chen, X.; Fan, H.; Girshick, R.; and He, K. 2020b. baselines with momentum contrastive learning. arXiv:2003.04297. Chen, X.; and He, K. 2021. Exploring simple siamese representation learning. In CVPR, 15750–15758. Doersch, C.; Gupta, A.; and Efros, A. A. 2015. Unsupervised visual representation learning by context prediction. In ICCV, 1422– 1430. Doersch, C.; and Zisserman, A. 2017. Multi-task self-supervised visual learning. In ICCV, 2051–2060. Donahue, J.; Kr¨ahenb¨uhl, P.; and Darrell, T. 2017. Adversarial feature learning. In ICLR. Feng, W.; Wang, Y.; Ma, L.; Yuan, Y.; and Zhang, C. 2021. Temporal Knowledge Consistency for Unsupervised Visual Representation Learning."
2111.02679,"data, dataset, code",207,,,"Recently contrastive learning has shown signiﬁcant progress in learning visual representations from unlabeled data. The core idea is training the backbone to be invariant to different augmentations of an instance. While most methods only maximize the feature similarity between two augmented data, we further generate more challenging training samples and force the model to keep predicting discriminative representation on these hard samples. In this paper, we propose MixSiam, a mixture-based approach upon the traditional siamese network. On the one hand, we input two augmented images of an instance to the backbone and obtain the discriminative representation by performing an element-wise maximum of two features. On the other hand, we take the mixture of these augmented images as input, and expect the model prediction to be close to the discriminative representation. In this way, the model could access more variant data samples of an instance and keep predicting invariant discriminative representations for them. Thus the learned model is more robust compared to previous contrastive learning methods. Extensive experiments on large-scale datasets show that MixSiam steadily improves the baseline and achieves competitive results with state-of-the-art methods. Our code will be released soon."
2111.02679,dataset,122,,,"The structure of MixSiam is shown in Fig. 2 (b). The two augmented images of the original instance and their mixed image are fed into the encoder network, where the three encoders directly share weights with each other. The output of the predictor network in the right is forced to share the representation with the discriminative representation obtained by the two encoder networks in the left. The strong similarity constraint helps to extract robust features with discrimination ability. The experiments on various datasets demonstrate that the proposed method can consistently boost the performance over the SimSiam (Chen and He 2021) baseline by learning from the mixed images and the discriminative representations."
2111.02679,dataset,208,,,"In this paper, we propose MixSiam, a mixture-based approach upon the siamese contrastive. By investigating the state-of-the-art siamese-based method, we ﬁnd it difﬁcult to deal with large intra-class variations as it only learns from augmented views with little variance. To this end, we present a mix-based feature learning strategy upon the siamese structure. On the one hand, the hard virtual samples generated by mixup are introduced to enhance the robustness. On the other hand, we capture the discriminative representation of an instance from its corresponding augmented views. By learning to minimize the similarity between the hard virtual sample and the discriminative representation, the model learns to be invariant to the hard samples. Experiments are conducted on standard ImageNet and several small datasets, demonstrating our superiority comparing with the state-of-the-art methods. The consistent improvement over the siamese baseline further veriﬁes the effectiveness of our method. In the future, the augmentation strategies will be investigated to select the appropriate ones for generating the mixed images. More experiments will be conducted on more downstream tasks such as object detection and object segmentation to further validate the generalization ability of the proposed model."
2111.02679,dataset,37,,,"For the transfer learning experiment, the encoder is pretrained on ImageNet, and we transfer them to different datasets for training the linear classiﬁer. The training detail is the same as that of CIFAR10."
2111.02679,dataset,51,,,"(3) Our method achieves a stronger backbone network, which has been effectively veriﬁed on the standard ImageNet and multiple small-scale datasets such as CIFAR10, CIFAR100, and Food101. With a much smaller batch size training, MixSiam achieves competitive classiﬁcation accuracy than existing state-of-the-art methods."
2111.02679,dataset,60,,,"We conduct two settings of experiments. First, we transfer the ImageNet pre-trained model on CIFAR10/100 to show the generalization ability and make a fair comparison with state-of-the-art methods. Speciﬁcally, the linear classi Figure 3: Linear classiﬁcation accuracy of Top-1 and Top5 on ImageNet dataset. The results of SimSiam and our method are reported."
2111.02679,dataset,65,,,"Experiment In this section, we evaluate our method on various unsupervised datasets. We ﬁrst employ the standard linear evaluation protocol to learn representations on ImageNet (Russakovsky et al. 2015). Then we transfer the learned model to multiple ﬁne-grained image classiﬁcation datasets. Finally, we analyze our method with ablation studies to demonstrate the effectiveness of each design."
2111.13023,code,48,,,"[38] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 2"
2111.13023,data,152,,,"The model is trained with the Adam optimizer for 1000 epochs with an initial learning rate of 10−5, decayed by a factor of 0.5 every 100 epochs. We do not use data augmentation as in general the typical data augmentation applied to images does not have a corresponding augmentation on the mesh. Therefore, using data augmentation where the input data is deﬁned on the group Z2 and the output data is deﬁned on the group SO(3) is ill-deﬁned. Further, ignoring this and applying data augmentation to the input images, such as rotations, without applying the corresponding rotation to the output mesh will guide the learning process of the model to being invariant to such transformations, which is undesirable. We train on a Titan Xp with a batch size of 8 with input image size of 256 × 256."
2111.13023,data,155,,,"GNNs were ﬁrst introduced as a means to learn on graph structured data using neural networks where the data is irregularly structured [15, 22, 40, 48]. GNNs become more widely used when they were developed to scale better with the size of input graph [4, 13, 21, 31, 53]. Since then GNNs have been used to learn about graphs [7,42,43,50,58], point clouds [19, 28, 39, 57, 60], and meshes [16, 24, 25]. Graphmesh- and point-convolutions are relevant here as they can be used in the decoder as an inductive bias to build into the model. They provide the inductive bias of locality in the model, meaning that features are updated only based on neighbouring nodes. On the other hand, using an MLP does"
2111.13023,data,202,,,"Our work follows the general structure of previous works on learning to generating 3D hand meshes from RGB images in that it uses an encoder to embed the image into a latent space and a decoder to generate the 3D meshes. Unlike previous works we consider the symmetries in the problem and create a rotation equivariant encoder and decoder. In addition, we add two new modules a vector mapping function and a 3D projection function. The vector mapping function maps from a latent image space on the group C8 to an SO(2) representation space. The 3D projection function maps from an SO(2) vector space to an SO(2) vector space and scalar space which allows us to introduce the third inferred dimension of the 3D space in a controlled way. By ensuring rotation equivariance at each stage of the model a 2D rotation of the input image corresponds to a 2D rotation of the output mesh about the third axis. Ensuring rotation equivariance within the model reduces the dependency on very large data sets and removes undesirable deformations of the mesh under rotation of the image."
2111.13023,data,216,,,"Similarly to the developments of equivariance for image data on the plane R2, equivariance has been considered for graphs and point clouds in 3D space. In general for 3D point clouds or graphs the group considered is that of 3D rotations, SO(3), 3D rotations and translations, SE(3), or 3D rotations, reﬂections and translations, E(3) [19, 32, 47, 49, 51]. The group representations for SO(3) are orthogonal matrices that can be decomposed as ρ(g) = QT [⊕lDl(g)] Q, where Q is a change-of-basis matrix and Dl is a Wigner-D matrix [19]. In work on E(3) equivariant models the message passing graph neural network [21] is modiﬁed such that edge updates use the squared distance and position is updated using a vector ﬁeld in a radial direction to guarantee equivariance to the group E(n). In addition to 3D point clouds, an equivariance constraint can be placed over a network for 2D rotational groups, such as SO(2) or SE(2) [17]."
2111.13023,data,230,,,"Previous approaches use a convolutional encoder and graph based decoder to generate meshes. We introduce a rotation-equivariant convolutional encoder and an SO(3)equivariant decoder. In addition, we introduce two new novel components into the framework for 3D mesh generation from RGB images which we term a vector mapping function and 3D projection function. Instead of encoding images into a latent vector where no structure is enforced, as is done in previous works, we introduce a vector mapping function which maintains the rotation equivariance enforced in the encoder while projecting the data into a latent vector space. This guarantees that a rotation of the input image maps to a rotation of the latent space and therefore enforces some structure in the latent space. Furthermore, we introduce a 3D projection function that maintains the rotation equivariance enforced in the prior components of the model by utilising an SO(2) equivariant map. This maps from a vector representation of the group SO(2), ρ1, to the direct sum of a vector and scalar representation of the group SO(2), ρ1 ⊕ ρ0. This produces a third inferred dimension which we treat as the third dimension of a 3D space and feeds into the SO(3)-equivariant decoder."
2111.13023,data,30,,,"Comparing both models to our EMLP model highlights the value of building useful inductive bias into the model, especially those that reﬂecting some known symmetry in the data."
2111.13023,data,300,,,"The state-of-the art prior methods utilise a GNN decoder with node neighbourhoods chosen to be relatively small in comparison to the entire set of vertices in the output mesh. No consideration is given to the neighbourhood size used in the graph decoder and whether a global update could improve upon the local updates of a GNN. A MLP could be viewed as a fully connected GNN update with separate update on each edge. The main drawback of an MLP is the ﬁxed size outputs of the model and in general GNNs are used due to the variable size of the data. Here when generating 3D meshes the output is of ﬁxed size and hence one beneﬁt of using a GNN is moot. The remaining advantage of using a GNN over an MLP is that the GNN has an inductive bias of locality. We experimentally compare using a GNN and MLP decoder in Table 1 and note that the MLP decoder achieves a lower vertex and Laplacian loss indicating the MLP model learns to generate more accurate and smoother meshes. We further look at the validation mesh error for both the GNN and MLP decoders in Table 4. This shows that the MLP produces a lower mesh error on the ﬁxed orientation images, but a larger mesh error on the rotated hands. This indicates that the inductive bias of locality marginally reduces over ﬁtting over the MLP model. The GNN decoder therefore predicts less accurate and less smooth meshes. As we seek to build the inductive bias of rotational equivariance into our model, which aims to improve the generalisation of the vertex position prediction, we conclude that an MLP decoder is desirable given its improved performance."
2111.13023,data,43,04/21/22,0,"[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017. 2"
2111.13023,data,44,,,"[55] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D steerable CNNs: Learning rotationally equivariant features in volumetric data. Conference on Neural Information Processing Systems (NeurIPS), 2018. 2"
2111.13023,data,54,,,"Equivariance in neural networks has been used for many years in the form of translation equivariance [38], which led to many successes with image data [34]. More recently, equivariance has been considered for rotations of images. Initially this was achieved by looking at group actions for"
2111.13023,data,66,,,"Joshua Mitton is supported by a University of Glasgow Lord Kelvin Adam Smith Studentship. Chaitanya Kaul and Roderick Murray-Smith acknowledge funding from the QuantIC project funded by the EPSRC Quantum Technology Programme (grant EP/MO1326X/1) and the iCAIRD project, funded by Innovate UK (project number 104690). Roderick Murray-Smith acknowledges funding support from EPSRC grant EP/R018634/1, Closed-loop Data Science."
2111.13023,data,89,,,"model for the vertex loss, but outperforms the GNN model. Despite the worse vertex prediction accuracy of the EMLP model, it achieves almost comparable performance to the MLP model for Laplacian loss in Table 1 indicating it is predicting equally smooth meshes. The better performance of the MLP model is due to the model having more trainable parameters than the other two models and being unconstrained through the addition of no inductive bias, which allows the model to over-ﬁt to the training data."
2111.13023,data,94,,,"[16] Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, and Yue Gao. Meshnet: Mesh neural network for 3D shape representation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 8279–8286, 2019. 2 [17] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to Lie groups on arbitrary continuous data. In International Conference on Machine Learning, pages 3165–3176. PMLR, 2020. 2"
2111.13023,"data, dataset",176,,,"to a diverse range of hand meshes. As such, they often do not accurately generate hand meshes of correct shape and pose. The second approach is to use neural network models, which are more general and do not exploit a 3D shape space prior. The neural network model is generally in the form of a convolutional encoder and graph or mesh neural network decoder. These models are able to more accurately predict the correct shape and pose of the hands, although these models currently require large amounts of data to train. This often requires the use of synthetic data as a pre-training step. Despite this, use of synthetic data often results in a model that does not generalise well to real-world examples. On the other hand, collecting large datasets of real-world data is expensive and time consuming. The lack of robustness presented from using synthetic data or the expense of gathering real-world data may limit the number of applications of the approach."
2111.13023,"data, dataset",177,,,"In this work we overcome the issue of requiring large amounts of data by building a neural network model with a suitable inductive bias. We quantitatively demonstrate the effectiveness of this approach on real-world data, outperforming previous state-of-the-art methods. Further, we qualitatively demonstrate the beneﬁt of our choice of inductive bias and compare to alternative models. We follow the approach of using a neural network based decoder, rather than a deformable model with 3D shape prior, to overcome the issue with generalisation. In addition to building a model than can generalise, we utilise suitable inductive biases in each component of the model. This improved weight sharing in the model reduces the need for pre-training on large synthetic datasets and generalises well training only on a small real-world dataset. The resulting model is rotation equivariant, which is a suitable inductive bias for generating 3D meshes from RGB images due to the symmetries present in the data. To the best of our knowledge this is the"
2111.13023,"data, dataset",266,,,"Generating meshes from 2D images has seen increased attention in recent years [6]. Using a convolutional encoder is the approach taken in all prior works to embed the image into a latent space, with differing choices of convolutional architecture. The main difference between prior works comes in the form of the decoder. One approach is to use a deformable model which has been ﬁt to landmarks, exploiting a prior distribution in 3D shape space [1, 3, 26, 36, 59]. This has the advantage that the model will generate realistic looking hands. On the other hand, this approach has a weakness that the model will tend to generate hands only from the prior distribution and therefore the generated meshes may not accurately reﬂect the input image. Another approach is to use more general models as the decoder with no prior distribution guiding the model to generate hands. One such method uses a ResNet-50 encoder and spacial mesh convolutional decoder [35]. Another method uses a stacked hourglass convolutional encoder and graph convolutional decoder utilising Chebyshev polynomials [20]. While these approaches more accurately generate hand meshes with the correct shape and pose of the input image, they generally require large amounts of synthetic data as a pre-training step. This is not advantageous as it requires signiﬁcant effort in creating a synthetic dataset. In addition, these methods are not robust at generating real-world meshes due to the reliance on large datasets."
2111.13023,"data, dataset",73,,,"model outputs predicted meshes of the hand. The decoder predicts the point positions of a mesh, where we follow previous work [20, 35] and output the points of the mesh in a ﬁxed order which makes use of a predeﬁned topology. In this work, we train directly on a small dataset of real-world data and do not have any expensive pre-training step on synthetic data."
2111.13023,dataset,118,,,"provement in mesh generation justiﬁes our choice of using an MLP based model. Further, we demonstrate that rotation equivariance is a suitable inductive bias to build into each component of the model by outperforming other leading methods on a real-world dataset. In addition, this improves robustness by removing undesirable deformations of the generated meshes under rotation of the input image, which we both quantitatively and qualitatively compare with non-rotation equivariant models. The theoretical match of prior knowledge about the problem, and the improved empirical performance compared to the state of the art support the use of these new inductive biases in models generating 3D hand meshes from 2D RGB images."
2111.13023,dataset,119,,,"The encoder takes images deﬁned on Z2 and maps these to latent functions deﬁned on the group C8. We introduce a novel vector mapping function to map the function deﬁned on C8 to a latent point cloud space deﬁned on the group SO(2). Further, we introduce a 3D projection function that learns a 3D function from the SO(2) latent space. Finally, we use an SO(3) equivariant decoder to ensure rotation equivariance. Our rotation equivariant model outperforms state-of-the-art methods on a real-world dataset and we demonstrate that it accurately captures the shape and pose in the generated meshes under rotation of the input hand."
2111.13023,dataset,248,,,"Secondly, we consider three different models two nonrotation equivariant models, named MLP and GNN, and a rotation equivariant model, named EMLP. All three models have an identical training scheme and they are tested on the 83 validation examples. As presented in Table 2 the model without rotation equivariance using an MLP decoder achieves the best validation mesh error on the ﬁxed orientation validation dataset. The GNN decoder performs more poorly indicating worse generalisation to the validation dataset. Further, the rotation equivariant model performs worse still. This result is due to the constraints placed over the GNN and EMLP models via a choice of inductive bias, which results in them having less trainable parameters in the models. In addition to comparing to the ﬁxed orientation validation set, which is likely to contain hands in similar orientations to the training dataset, we consider a rotated orientation validation dataset, where the hands and meshes are correspondingly rotated. Evaluation on this validation dataset highlights the failure of the non-rotation equivariant models, where the rotation equivariant EMLP model drastically outperforms the other methods. Overall, the consistency of the rotation equivariant model across all validation datasets and the improved generalisability results in it outperforming other models. This indicates our choice our choice of model for generating 3D meshes is beneﬁcial and rotation equivariant is a useful inductive bias to build into the model."
2111.13023,dataset,26,,,"Figure 5. Qualitative mesh reconstruction results on rotated validation hand images from the validation dataset for the EMLP, MLP, and GNN models."
2111.13023,dataset,263,,,"Finally, we qualitatively validate our method by comparing the target mesh to the mesh predicted by our model in Figure 4. We present the image input into the model with both the target and predicted meshes projected onto the input image. Further, we show the mesh generated by our model for the image in two different view points. This shows that the generated meshes accurately capture the pose and shape required and the generated meshes are smooth. In addition to qualitative analysis on the validation dataset, we also compare how our model performs to the two nonrotation equivariant models with MLP and GNN decoders on a validation dataset where the hands have been rotated by 90◦. As is demonstrated in Figure 5 our EMLP model generates meshes that accurately capture the pose and and shape of the hand. On the contrary, the MLP and GNN model fail to capture the pose correctly and do not predict realistic looking hand meshes in many cases. This result is an effect of the type of inductive bias that we chose to build into our model. Table 2 shows that the GNN model can accurately reconstruct the meshes when the orientation remains ﬁxed, although in reality this will not be the case and therefore the inductive bias of locality hurts the models ability to generalise. Further, the MLP model with no inductive bias appears to generate more realistic looking hands than the GNN model, despite generating hands of incorrect pose."
2111.13023,dataset,27,,,Table 4. Average mesh error tested on the validation set of the real-world dataset. Results for prior methods are taken from [20].
2111.13023,dataset,30,,,Figure 4. Qualitative mesh reconstruction results on the validation dataset from the real-world dataset of [20] comparing our model’s predictions to the target meshes.
2111.13023,dataset,33,,,"Further, we compare our rotation equivariant model to other prior works in Table 4. This demonstrates that our method produces superior results to previous methods on a real-world validation dataset."
2111.13023,dataset,46,,,Table 1. Training loss split into vertex loss Lv and Laplacian loss Ll for an MLP and GNN decoder in a non-rotation equivariant model and a EMLP model that has rotation equivariance build into the entire model for the real-world dataset [20].
2111.13023,dataset,55,,,"Table 2. Average mesh error tested on the validation set of the real-world dataset [20] in the column ﬁxed orientation mesh error. Also, a rotated version of this validation dataset in the column rotated orientation mesh error, where the rotations used are 90◦, 180◦, and 270◦."
2111.13023,dataset,72,,,"We experiment on the real-world dataset from [20] which we split into 500 training examples of images and meshes, and 83 validation examples of images and meshes. We report the vertex and Laplacian losses and the mesh error as evaluation metrics, where the mesh error is the average error in Euclidean space between corresponding vertices in each generated 3D mesh and its ground truth 3D mesh."
2111.13023,dataset,83,,,"We develop a rotation equivariant model for generating 3D hand meshes from 2D RGB images. This guarantees that as the input image of a hand is rotated the generated mesh undergoes a corresponding rotation. Furthermore, this removes undesirable deformations in the meshes often generated by methods without rotation equivariance. By building a rotation equivariant model, through considering symmetries in the problem, we reduce the need for training on very large datasets to achieve good mesh reconstruction."
2111.13023,dataset,92,,,"Vision based models for 3D hand mesh generation are currently receiving a lot of interest, as they facilitate a wide range of applications including virtual reality (VR), augmented reality (AR), and sign language recognition. Currently, two main approaches exist to tackle the difﬁculties of 3D mesh generation from 2D RGB images. One approach uses deformable models which exploit a prior distribution in 3D shape shape. These models can be trained on relatively small datasets, although typically they struggle to generalise"
2111.14671,code,114,,,"[27] Peter Ukkonen, Robert Pincus, Robin J. Hogan, Kristian Pagh Nielsen, and Eigil Kaas. Accelerating radiation computations for dynamical models with targeted machine learning and code optimization. Journal of Advances in Modeling Earth Systems, 12(12):e2020MS002226, 2020. [28] Menno A. Veerman, Robert Pincus, Robin Stoffer, Caspar M. van Leeuwen, Damian Podareanu, and Chiel C. van Heerwaarden. Predicting atmospheric optical properties for radiative transfer computations using neural networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 379(2194):20200095, 2021."
2111.14671,code,162,,,"CanESM5’s radiative transfer parameterization The radiative transfer parameterization in CanESM5, is representative of the approach used in most modern ESMs. The optical properties of a number of components are accounted for, including the surfaces, aerosols, clouds and gases (represented using a correlated k-distribution model). The solar and thermal radiative transfer is computed using a 2-stream solution [29]. The unresolved, subgrid-scale variability of clouds is treated using the Monte Carlo Independent Column Approximation (McICA) [3]. The subgrid-scale variability of the surface albedo for solar and emissivity for thermal are accounted for in the radiative transfer calculations [26]. The performance of the CanESM radiative transfer code under pristine (gas-only), clear (gas plus aerosols) and all-sky conditions has been documented relative to line-by-line calculations and other radiative transfer models with similar complexity [21, 23]."
2111.14671,code,48,,,"[18] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4): 541–551, 1989."
2111.14671,code,80,,,"• Towards Advancing the State-of-the-Art. ClimART’s scale, unique properties, and ease of access, together with the accompanying code interface and baselines, will lower barriers for the ML community to tackle impactful challenges in climate science. ClimART also presents opportunities for spurring methodological innovation in ML via multiple out-ofdistribution test sets, the scope for building physics-informed ML models, and the accuracy versus inference speed trade-off inherent in the problem setting."
2111.14671,code,89,,,"A complete list of all input variables can be found in Table 3, and for all potential target variables in Table 4. Within a CanESM grid box, the surface can include multiple types. An example of this is a grid box that includes a coast line which includes both land and water. The fraction of the grid box and its optical properties for a particular surface type is passed into the radiative transfer code where it is accounted for in the subsequent calculations."
2111.14671,"code, github, code available",16,04/21/22,2,"∗Equal contribution 2Download instructions, baselines, and code are available at: https://github.com/RolnickLab/"
2111.14671,data,109,,,"• Graph networks, take graph-structured data (with node and edge features), complemented by a so-called global feature vector, as input. Thus, it is the most natural model for our task, since we can map the levels to be the nodes, layers to be the edges connecting the adjacent levels (i.e. a line-graph), and the non-spatial variables to the global feature vector. Essentially, a graph network [4] consists of multiple MLP modules, for which we use 1-layer MLPs with a hidden dimension of 128. We use a three-layered graph network."
2111.14671,data,122,,,"CNN For the CNN model, we use a 3-layer network with kernel sizes (cid:104)20, 10, 5(cid:105) and the corresponding strides set as (cid:104)2, 2, 1(cid:105). The channels parameter is given by (cid:104)200, 400, 100(cid:105), with the last channels setting it equal to the input size. We then apply a global average pooling over the resulting tensor to get the output. To preprocess the data for CNN, we pad the surface and layers variable to match the dimensions of levels variable. Then the result is concatenated and fed to the model."
2111.14671,data,122,,,"Inputs Recall that each example in ClimART consists of three distinct input arrays that correspond to the globals, layers, and levels data subset. All three arrays are stored together in a single Hdf5 ﬁle for each year, which can all be found in the inputs/ sub-directory. The layers array is concatenated along the channel dimension in such a way, that the 14 ﬁrst features are the ones needed for pristine-sky experiments, while the whole array would be used for clear-sky experiments. This avoids storage redundancy, and allows it to access the pristine-sky data by simple slicing of the layers array (see B.5.4 for the exact shape of the input arrays)."
2111.14671,data,124,,,"• 1D CNN: For the CNN model, we use a 3-layer network with kernel sizes (cid:104)20, 10, 5(cid:105) and the corresponding strides set as (cid:104)2, 2, 1(cid:105). The channels parameter is given by (cid:104)200, 400, 100(cid:105), with the last channels setting it equal to the input size. We then apply a global average over the resulting tensor to get the output. To preprocess the data for CNN, we pad the surface and layers variable to match the dimensions of levels variable. Then the result is concatenated and fed to the model."
2111.14671,data,127,,,"Occasionally, a volcanic eruption is large enough to inject material and gases well into the stratosphere. When that happens, the resulting aerosol loading spreads over the globe and can remain suspended for periods of time that are long enough to have measurable impacts on remote sensing data and surface-atmosphere climatic conditions. The overwhelming impact is a reduction of solar radiation absorbed by Earth, and hence a slight, but measurable and attributable, reduction in lower-atmospheric and surface temperatures (with other variables responding according which can both amplify or mitigate the initial cooling). To account for these radiative forcings with some conﬁdence in a global model requires reliable input of height-dependent mass loading and aerosol optical properties. If"
2111.14671,data,129,,,"where N is the data dimensions (i.e. the number of examples of a speciﬁc year), Slay and Slev are the number of layers and levels in a column respectively (49 and 50 in this case), and Dlay, Dlev, Dglob is the number of features/channels for layers, levels, globals respectively. For both pristine-sky and clear-sky conditions, we have that Dlev = 4 and Dglob = 82, while Dlay = 14 for pristine-sky, and Dlay = 45 for clear-sky conditions (see B.5.1 for details on the nature of this). The array for pristine-sky conditions can be easily accessed by slicing the ﬁrst 14 features out of the stored array, e.g.:"
2111.14671,data,149,,,"• GCNs, take graph-structured data as input. To map the columns to a graph, we use a straighforward line-graph structure where each node is a level or layer and is connected to the two layers or levels spatially adjacent to it above and below. To take into account the global information, we add it as an additional node to the graph with connections to all other nodes. The resulting graph structure, in form of an adjacency matrix, is shown in Fig.4a, where the global node has index 0, and the other nodes are spatially indexed for plotting purposes, where 1 corresponds to the TOA level and the last node corresponds to the surface. A more sophisticated graph structure is studied in section 5.2 (L-GCN). The used GCN has three layers of dimension 128."
2111.14671,data,159,,,"Note that prior work usually restricted the ML model to be a multi-layer perceptron (MLP). In light of the structured data in ClimART, we aim to 1) propose more structured neural network architectures that we believe are more suitable to the task and on which we hope follow-up work can build upon; 2) study how these more structured neural network architectures compare to the unstructured MLP. Thus, we benchmark an MLP against a 1-D convolutional neural network (CNN) [18], a graph convolutional network (GCN) [14], and a graph network [4]. We now give a high-level overview over each of the architectures, which all are relatively lightweight as it is important to keep the inherent inference speed versus accuracy trade-off in mind. More details on it and used hyperparameters can be found in Appendix C."
2111.14671,data,17,,,In the following we describe the data split that we recommend to follow for benchmarking purposes.
2111.14671,data,176,,,"while for the edge structure we use a simple line-graph that contains connections between adjacent levels and layers only. Thus, the graph has 49 + 50 + 1 = 100 nodes. As is standard practice, we add self-loops to the adjacency matrix, see Fig. 4a for a visualization of the resulting adjacency matrix. Since layers, levels, and globals are heterogenous data arrays with different numbers of features, we project them to a hidden size of 128 with a separate 1-layer MLP for each of the input types, before passing it to the GCN. The MLP projectors use LayerNorm and GeLU as activation function. The GCN backbone is the same for both GCN and L-GCN, i.e. L-GCN only differs from GCN in its structure learning module, which is identical to the one proposed by [7]. To get predictions we use a 1-layer MLP head that takes as input mean-pooled node embeddings generated by the last GCN layer."
2111.14671,data,183,,,"(more detailed in Fig. 5) and reported in Table 1. Notably all models can be seen to signiﬁcantly deteriorate as a function of the test year (that becomes temporally farther away from the training years). We note that prior work could not observe this phenomenon since the test data did not cover as many years or was randomly split from the training set. We also ﬁnd that the CNN architecture provides the most skillful emulation in terms of RMSE, while the GraphNet provides the least biased errors. This holds for the respective metrics computed at the TOA and surface level only, as well as when averaged out vertically over all levels. We note that the surface and TOA ﬂux predictions are especially important, as they are directly used by the host climate or weather model. We also ﬁnd that the L-GCN, which extends the GCN by a learnable edge structure module, is able to signiﬁcantly outperform it, especially for TOA predictions, see next subsection."
2111.14671,data,191,,,"In physics-based RT models as used in large-scale environmental models, computation of radiative ﬂux proﬁles is a two-step process. The ﬁrst step involves the calculation of optical properties for gases, aerosols, and clouds. The second, more computationally intensive, and arguably most erroneous step is the application of a solution of the RT equation, using the optical properties from the ﬁrst step, leading to vertical proﬁles of radiative ﬂuxes. Work by [27, 28] focused on a hybrid approach in which gaseous optical properties are predicted by an ML model and then passed off to a physics-based RT model for calculation of ﬂuxes. However, [28] generates training data by just using perturbations on input variables of RFMIP[24]. [27] make a more comprehensive effort via generation of training data by combining multiple sources such as RFMIP[24], CKDMIP[12] and CAMS[1], and perturbing them to generate sufﬁcient training data. Such perturbations, however, might lead to unrealistic input values."
2111.14671,data,206,,,"Recall from 4.3, that we use the years 1990, 1999, and 2003 for training, while validating on 2005 and testing on the proposed test set years 2007-2014. For all the models, we normalize the input data by subtracting the mean and dividing by the standard deviation that were computed on the potential training years {1979 − 90, 1994 − 2004}. The targets are not normalized in any form, but directly predicted in their raw form by all models. The batch size used for training all the models was ﬁxed at 128. All models use the GeLU activation function [11]. For the optimizer, Adam, we use a weight decay of 1e-6 and an exponential decay learning rate scheduler (with gamma = 0.98, and a minimum learning rate of 1e-6). We clip the L2 gradient norm of all our models at 1, which is important due to the unnormalized targets. We use LayerNorm [2] for the MLP, while all other models do not use any network normalization – these conﬁgurations were found empirically to be superior for the respective models."
2111.14671,data,240,,,"For evaluating the generalization of our models in OOD data, we run it on historic data (1850-1852) and future data (2097-2099). These experiments are extremely challenging given the limited size of of training set use for baseline models. Apart from this, in historic (pre-industrial) and future conditions, the values of input variables, especially those relating to the concentrations of gases vary quite a lot. For a model to be able to perform well on this data, it has to have understood the role of gas concentrations in prediction of the ﬂux properties. As seen from the results in Fig. 3, all models degrade signiﬁcantly in performance, especially for future climate conditions. However, it is notable how the models that better account for the structure of atmospheric data perform considerably better compared to MLPs: While both, the MLP and GraphNet, perform comparably well for present-day conditions with an RMSE of less than 1 W/m2 (Fig. 5a), the MLP’s RMSE for future conditions is above 100 while the GraphNet’s stays at around 30 (Fig. 6a). Similarly, the CNN degrades “only” from less than 0.5 RMSE on the main test set, to below 18 W/m2 for future-day climate conditions 6 Conclusion & Future Work"
2111.14671,data,33,,,"of the ML emulators on real world unseen data. This can be fatal when the ML emulator is to be used in long-term simulations (e.g., future climate projections)."
2111.14671,data,34,,,"remained in the atmosphere for years after the eruption, we chose to exclude the two subsequent years, 1992 and 1993, from ClimART in order to avoid data leakage during training."
2111.14671,data,55,,,"Training and Validation sets ClimART provides the complete data extracted from CanESM5, as described above, from 1979 to 2006, excluding 1991-93, as suggested data for training and validating ML models. In our experiments we used 1990, 1999, and 2003 for training, while keeping 2005 for validation."
2111.14671,data,57,,,"MLP The MLP used for our experiments is a simple three layer MLP with the following hiddenlayer dimensions: (cid:104)512, 256, 256(cid:105). As an MLP takes unstructured 1D data as input, all the input variables need to be ﬂattened into a single vector for the MLP."
2111.14671,data,59,,,"For the evaluation of ML models, we make use of an instance with 4 CPUs (2x AMD EPYC Zen 2 ""Rome"" 7742), 12GB Memory and Nvidia v100 GPU. The results for different ML models for pristine-sky conditions is shown in 2 excluding the time for data loading. These results are averaged"
2111.14671,data,59,,,"• MLP: The MLP used for our experiments is a simple three layer MLP with the following hidden-layer dimensions: (cid:104)512, 256, 256(cid:105). As an MLP takes unstructured 1D data as input, all the input variables need to be ﬂattened into a single vector for the MLP."
2111.14671,data,68,,,"We provide the complete data for the years 1979 to 2006, excluding the years 1992-93 in order to avoid potential data leakage when using 1991 as an out-of-distribution test set. In total there are thus 10,076,160 samples for this period. Thus, minus the held-out year 1991, this results in up to 9, 732, 096 potential training samples from present-day conditions."
2111.14671,data,70,,,"Main Test set We suggest to use the data from the years 2007 to 2014 as main test set. This relatively long interval allows to test the ML model on a very diverse set of present-day conditions. To make evaluations feasible regardless of the available compute resources, we chose to subsample 15 random snapshots for each year. This results in almost 1 million testing samples."
2111.14671,data,96,,,"• globals: Consists of variables related to boundary conditions (e.g. sun angle), surface type variables, as well as geographical information (as described in B.8), which all do not have a spatial dimension (i.e. one, possibly multi-dimensional, variable per column/data example). • levels: Consists of variables occurring at each level of the column (50 levels in this case). These are only four variables. It is worth recalling that the target radiative ﬂux proﬁles are level variables."
2111.14671,"data available, publicly available, open-source, data open-source , dataset",82,,,"To date, different datasets, setups, and evaluation procedures have made results in this space hard to compare. This can be attributed to the fact that no comprehensive public dataset exists, and the creation of it requires access to, and knowledge of, the relevant climate model. To address these issues and catalyze further work, we introduce a new and comprehensive dataset for the RT problem and open-source it under the Creative Commons license."
2111.14671,"data, code",221,,,"Inputs We saved the exact same inputs used by the CanESM5 radiation code and augmented them by auxiliary variables such as geographical information (see Appendix B.3 for full details and description). Each input corresponds to a column of CanESM5. Its variables can be divided into three distinct types: i) layer variables, ii) level variables, iii) variables not tied to the height/vertical discretization. Examples for the two ﬁrst 1D variables are pressure (occurring at both, levels and layers) and water vapour (only present at the layers). The third type of variables are comprised of optical properties of the surface, boundary conditions, and geographical information related data. We refer to this set as the global variables. The data thus has a unique structure with heterogenous data types, where 1D vertical data is complemented by non-spatial information. We also note that the RT problem is non-local, since the spectral composition, and thus the heating rate, at one level can depend much on attenuation, or production, of radiation at a far-removed layer (e.g., reduced absorption of solar radiation by water vapour near the surface due to the presence of reﬂective high-altitude cirrus clouds)."
2111.14671,"data, dataset",105,,,"For our main dataset, global snapshots of the current atmospheric state were sampled from CanESM5 simulations every 205 hours from 1979 to 2014.3 CanESM5’s horizontal grid discretizes longitude into 128 columns with equal size and latitude into 64 columns using a Gaussian grid (8192 = 128×64 columns in total). This results in 43 global snapshots per year for a total of more than 12 million columns for the period 1979-2014 and a raw dataset size of 1.5TB. Each column of atmospheric 3The choice of 205 hours provides a manageable amount of equally spaced data while also ensuring that"
2111.14671,"data, dataset",117,,,"Secondly, our targets do not include radiation output under all-sky conditions (which, besides aerosols, includes clouds). We believe however that our otherwise comprehensive dataset will serve well as a test-bed for ML emulators under the more simple (yet complex) pristine- and clear-sky conditions. Moreover, we note that pristine- and clear-sky are routinely used in diagnostic analyses of climate and weather model results. That is, while such conditions do not often occur in the atmosphere (mostly above the troposphere), they are nevertheless computed for the entire globe in order to assess a model’s cloud dynamics and compare to satellite data."
2111.14671,"data, dataset",189,,,"Pre-industrial and future This test set probes how well the ML emulator generalizes under challenging distributional shifts. For this purpose, ClimART provides historic data from the years 1850-52 and future data from the years 2097-99. In both cases, the primary challenge for ML models is that they can be expected to encounter surface-atmosphere conditions that are not present in the training dataset. The primary differences between current and pre-industrial conditions involve: reduced atmospheric trace (greenhouse) gas concentrations for pre-industrial times; changes in aerosol emission; and some land surface properties that arise through changes in land usage. The future climate data can test how well ML models extrapolate to conditions that differ from the current climatic state as a result of radiative forcing through increases in greenhouse gas concentrations. Future climatic conditions were simulated by CanESM5 based on increases in atmospheric greenhouse gas concentrations and changes in aerosol emissions that follow well-deﬁned scenarios (see [26]) laid out for the Sixth Coupled Model Intercomparison Project (CMIP6; cf. [10])."
2111.14671,"data, dataset",205,,,"For convenience, we provide pre-computed dataset statistics (mean, standard deviation, minimum and maximum) in the statistics.npz ﬁle that can be found in the root directory of the dataset. All statistics were computed on 1979-1990 + 1994-2004, i.e. on the years that we propose to use for training. Given the large sample size, it is important to use ﬂoat64 precision for the mean and standard deviation in order to avoid numerical overﬂows. The statistics are provided for each input type, in-type ∈ {layers, levels, globals}, separately and the corresponding arrays have the same feature/channel dimensionality so that they can be directly used for normalization. The statistics that follow the naming <statistic>_<in-type> are concatenated scalar statistics for each variable. The statistics that follow the naming spatial_<statistic>_<in-type> were, additionally, computed for each level or layer separately (and are thus 2D arrays). In our experiments we used these statistics to scale the input data to have zero mean and unit standard deviation (""z-scaling""), as is common."
2111.14671,"data, dataset",224,,,"The computational burden of the RT physics motivated early pioneering work to seek out its emulation with shallow multi-layer perceptron (MLP) networks [8, 9, 15, 16], including decadal climate model simulations [17]. More recent work still focuses on using MLPs to emulate (a part of) the RT physics [19, 20, 22, 27, 28]. 2D CNNs have been also used in [19], which however treat the different input variables within the second spatial dimension instead of in the channel dimension. Prior work on such ML emulators, however, employed datasets that simplify Earth (e.g., with Aqua-planet conditions [6, 25, 30]), use a limited subset of climate model variables as predictors [15, 19, 25, 28], use manually perturbed test sets [19, 28], and generally fail to accurately probe the generalization power of ML models [19, 20, 22, 28]. The latter is particularly important, randomly-split test sets [20, 22, 28], and/or test data coming from at most two different years [19, 20, 22] can overestimate the actual skill"
2111.14671,"data, dataset",50,,,"On the dataset side, we plan to extend ClimART to include all-sky data that includes the complexity due to clouds. We hope that ClimART will advance both fundamental ML methodology and climate science, and catalyze greater involvement of ML researchers in problems relevant to climate change."
2111.14671,"data, dataset",93,,,"The present work is closer to [19] that used a small fraction of ERA-Interim data from 1979-85 and 2015-16. Their concern was however, limited, to longwave radiative ﬂux proﬁles for simpliﬁed clear-sky atmospheric conditions (without greenhouse gases like methane). Similarly to [28], they employ a test set that includes manually perturbed atmospheric states. We believe that our dataset that includes data of pre-industrial and future climate drawn from an actual climate model, is more realistic and the better choice."
2111.14671,"data, dataset provided",3,,,B.5.2 Data normalization
2111.14671,"data, dataset, code",165,,,"Our dataset focuses on pristine-sky (no aerosols and no clouds) as well as clear-sky (no clouds) conditions, i.e. it leaves out the most general all-sky condition that includes clouds. These input conditions, which consist of surface properties and proﬁles of pressure, temperature, humidity, and trace gases, were simulated by setting input variables corresponding to clouds (and aerosols for pristine-sky) to zero. These input snapshots, for the respective atmospheric conditions, were then forwarded through CanESM5’s RT physics code. for each atmospheric condition, the outputs are proﬁles of up- and down-welling ﬂuxes for both, shortwave (solar) and longwave (thermal) radiation, plus their respective heating rates. These raw inputs and outputs are stored in separate NetCDF4 ﬁles for each snapshot. All together (for the main dataset of 1979-2014), they amount to over 1.5Tb of data."
2111.14671,"data, dataset, dataset provided",58,,,We introduce a novel dataset ClimART which aims to provide a comprehensive dataset for parameterization of radiative transfer using ML models. We conduct a series of experiments to demonstrate which models are able to perform well under the inherent structure of atmospheric data in Experiments. Future work for improving upon the current baselines could include:
2111.14671,"data, dataset, publicly available",47,,,"• ClimART: Climate Atmospheric Radiative Transfer, is the most comprehensive publicly available dataset for ML emulation of weather and climate model parameterizations. It comes with more than 10 million samples, including three subsets of data for evaluating out-of-distribution (OOD) generalization."
2111.14671,"data, python",55,,,"Using Python, ClimART’s input and target arrays can be accessed as follows (for the example year 2007, and assuming that the user wants to predict longwave heating rates under pristine-sky conditions): # Assume that h5py and numpy are installed and we are in the root data directory."
2111.14671,dataset,111,,,"surface properties was then passed through CanESM5’s RT physics model in order to collect the corresponding RT output: Shortwave and longwave (up- and down-welling) ﬂux and heating rate proﬁles for pristine- and clear-sky conditions. The resulting NetCDF4 datasets were then processed to NumPy arrays stored in Hdf5 format (one per year), with three distinct input arrays as described in the following subsection, and one output array per potential target variable. We proceeded analogously for the pre-industrial and future climate years, 1850-52 and 2097-99 respectively (see section 4.3.1). More details are available in the Appendix B.1."
2111.14671,dataset,125,,,"It is important to note that a ML model trained on our dataset be both veriﬁed and validated before it can be employed ""operational"" in a global climate or NWP model. The veriﬁcation phase is characterized by ""simple"" quantitative assessment of a ML model’s bias and random (conditional) errors. Once one feels that the ML model is ready to go into the dynamical model, its computationsaving aspect can be assessed against any ramiﬁcations it has on the overall forecast of the model. Ideally, a successful ML model (i.e., one that is validated) will simultaneously reduce computation time and have incur only statistically insigniﬁcant impacts on the overall forecast."
2111.14671,dataset,15,,,ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models
2111.14671,dataset,17,,,35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.
2111.14671,dataset,183,,,"Numerical simulations of Earth’s weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. This has made them a popular target for neural network-based emulators. However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking. To ﬁll this gap, we build a large dataset, ClimART, with more than 10 million samples from present, pre-industrial, and future climate conditions, based on the Canadian Earth System Model. ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed. We also present several novel baselines that indicate shortcomings of datasets and network architectures used in prior work.2"
2111.14671,dataset,26,,,"In the following, we introduce background and terminology that is helpful in order to better understand the problem setup and the dataset we present."
2111.14671,dataset,29,04/21/22,0,"In our experiments we focus on pristine shortwave radiation. Our dataset, however, allows the user to choose the desired target variables based on their needs."
2111.14671,dataset,3,,,4 ClimART Dataset
2111.14671,dataset,3,,,4.1 Dataset collection
2111.14671,dataset,3,,,4.2 Dataset interface
2111.14671,dataset,3,,,4.3 Dataset split
2111.14671,dataset,3,,,B Dataset details
2111.14671,dataset,3,,,B.1 Dataset collection
2111.14671,dataset,3,,,B.5 Dataset interface
2111.14671,dataset,32,,,Directory structure The dataset is stored as separate Hdf5 ﬁles for each year (ﬁlenames follow <year>.h5). From the dataset root directory the structure thus follows:
2111.14671,dataset,4,,,B.7 Dataset split sizes
2111.14671,dataset,40,,,"• Applying New Models to ClimART. We propose multiple new models not studied in the related work, which thanks to the comprehensiveness of ClimART, allow us to analyze the limitations of previously used models and datasets."
2111.14671,dataset,63,,,"these can be supplied, simple solar RT models can predict accurate ﬂux perturbations. For an ML model to be able to address them well, however, appropriate inputs and responses must be included in the training dataset. The added challenge is that not all volcanoes are equal and their time and location can result in distinct radiative forcings."
2111.14671,dataset,75,,,"Thanks to their better inference speed, neural network-based surrogates are a promising alternative to computationally slow physics parametrizations. Such hybrid modelling approaches, however, present several challenges, including accurate emulation of complex physical processes, as well as the (out-of-distribution) generalization power of ML models to handle environmental conditions not present in their training datasets (e.g., weather states that are not realized under current conditions)."
2111.14671,dataset,79,,,"To decrease the dataset size as well as mapping the raw variables into a format that is more amenable for ML models, we chose to concatenate the input variables that share the same spatial dimension across the feature/channel dimension. The information about which channel corresponds to which variable was saved, and is provided in the META_INFO.json ﬁle included in the dataset root directory. This results in three distinct input types and arrays per sample:"
2111.14671,"dataset, dataset provided",43,,,"The variable aerin holds information about the aerosols passed into the radiative transfer calculations. In the dataset provided these are aerosol mixing ratios. The third index of the arrays are associated with different aerosols simulated in CanESM5 [29],"
2111.14671,"dataset, dataset provided",44,,,"In order to evaluate how well a ML model generalizes beyond the present-day conditions found in the main dataset, we provide three distinct OOD test sets that cover an anomaly in the atmospheric state, as well as two temporal distributional shifts."
2111.14671,"python, dataset",6,,,B.6 Reading the dataset in Python
2111.14915,code,17,,,"in the code and supplementary material released alongside this article. Here, we provide an overview"
2111.14915,data,10,,,"2019), to address limitations of census data."
2111.14915,data,11,,,"census data’, Urban Spatial. Accessed November 27."
2111.14915,data,12,,,"Learning for Social Science’, Frontiers in Big Data 3."
2111.14915,data,12,,,and those that make simple but intelligent use of existent data.
2111.14915,data,121,,,"Table 1: Regression Results for Two Linear Spatial Simultaneous Autoregressive Models. Observations for both models are cell grids where a2 = X, t =1 year for the year 2018. All independent variables (rows of the matrix) except for % Homes Sold are drawn from the 2018 ACS 5-year data and interpolated from block groups to grid cells. The dependent variable of the ﬁrst model (ﬁrst column) is our primary outcome variable (% Homes Sold), for the second (second column), it is the diﬀerence between the primary outcome and the prediction of our EWS. All indepedent variables are centered and scaled by two standard deviations."
2111.14915,data,13,,,"and ethical boundaries’, Frontiers in Big Data 2, 13."
2111.14915,data,13,,,parcel-level data can be predicted across various granularities of space and time.
2111.14915,data,14,,,"pipeline of development that involves four deeply inter-related challenges: data selection, outcome"
2111.14915,data,15,,,Evaluating predictive models requires that one separate the data used to infer model parameters from
2111.14915,data,15,,,academics have increasingly sought to move beyond these data in their studies of gentriﬁcation and
2111.14915,data,15,,,"forest model makes predictions by randomly sampling the training data, and then constructing decision"
2111.14915,data,15,04/21/22,0,is trained on data from 2000-2010 to predict gentrifying neighborhoods from 2010-2020. The present
2111.14915,data,15,,,"popular option in the study of gentriﬁcation and displacement, namely parcel-level data on home"
2111.14915,data,16,,,Many of the EWS used in practice are constructed by ingesting administrative data and then using
2111.14915,data,16,,,"With respect to operationalization, we argue that while data on home transactions do not directly"
2111.14915,data,16,,,coordinates for each parcel extracted from the county websites. These data were merged with the
2111.14915,data,16,,,"did not have latitude/longitude information in the Buﬀalo Open Data portal, and were thus dropped"
2111.14915,data,16,,,using gathered data. Early EWSs leveraged a variety of approaches to feature identiﬁcation and model
2111.14915,data,17,,,"However, with timestamped parcel-level data, one outstanding question is, what is the correct level"
2111.14915,data,17,,,"Our primary data source begins with a set of 116,438 recorded transactions on 51,425 homes in the"
2111.14915,data,17,,,"With respect to data selection, most existing EWS in the U.S. use traditional, widely-available data"
2111.14915,data,17,,,"a more detailed study-in-study examination of data from 2018, we also discuss how our ﬁndings relate"
2111.14915,data,17,,,"algorithms (including standard statistical approaches) to cluster data, and or work that reduces the"
2111.14915,data,17,,,"future work may incorporate these data productively into EWS, we therefore focus here on a more"
2111.14915,data,17,,,"granularity provides additional training data that can be used to reﬁne model predictions. Second, as"
2111.14915,data,17,,,"model selection, and model evaluation. With respect to data selection, we leverage an increasingly"
2111.14915,data,17,,,predictions each of the decision trees constructed on random subsets of the data are averaged. We
2111.14915,data,17,,,the analysis of gentriﬁcation and/or displacement. This includes ”big data” sources such as Google
2111.14915,data,17,,,tion measure in a given spatio-temporal unit based on an index computed from parcel-level data. They
2111.14915,data,17,,,"via a combination of new and old solutions to these challenges of data selection, operationalization,"
2111.14915,data,18,,,(Carlson 2021) and indicators available through other forms of administrative data (Reades et al.
2111.14915,data,18,,,"In order to create model features, we ﬁrst aggregate our data according to particular values of a2"
2111.14915,data,18,,,"city of Buﬀalo, NY from January 1st, 2000 through December of 2019.3 Our data collection process"
2111.14915,data,18,,,"compare these three diﬀerent proxies both empirically, using survey and census data in New York City,"
2111.14915,data,18,,,for our study. The ﬁnal stage of data collection involved ﬁltering out data that was incomplete or
2111.14915,data,18,,,"myriad methodologies, ranging from deep ethnography to the use of ”big data” and machine learning"
2111.14915,data,18,,,"scholars are increasingly turning to other forms of data (Barton 2016), such as anonymized surveys"
2111.14915,data,18,,,"the features described here on data from 2017, but also 2016, and 2015 as well."
2111.14915,data,19,,,"It is worth noting, however, that more innovative data sources have also shown to be useful in"
2111.14915,data,19,,,"displacement. This includes the use of other ”traditional” forms of data, such as surveys Carlson"
2111.14915,data,19,,,"in the concluding section of this article, one important point to emphasize here is that the data does"
2111.14915,data,19,,,"the data used to evaluate model predictions. Not separating these can lead to what is called overﬁtting,"
2111.14915,data,19,,,"this kind of home purchase data; namely, granularity and availability even in small cities like Buﬀalo."
2111.14915,data,19,,,“memorized” the data it is given. The most common approach to avoiding overﬁtting is a practice
2111.14915,data,20,,,"Third, we geolocate each home to a latitude, longitude pairing. To geolocate the data, we ﬁrst"
2111.14915,data,20,,,property’s value and tax payment history. This data is presented by the county in an HTML web
2111.14915,data,20,,,"transactions after January 1st, 2018, given all data prior to that date. We then vary the train/test"
2111.14915,data,21,,,"2020). These data, in particular those from social media, are a rich source of important social and"
2111.14915,data,21,,,Figure 2 provides summary data for grid cells using the full set of values of a2 and t we consider in
2111.14915,data,21,,,"Street view images (Hwang & Sampson 2014) and social media data (Glaeser et al. 2018, Chapple"
2111.14915,data,21,,,"that is, when homes are bought and sold, and for what amount. These data are of interest because"
2111.14915,data,22,,,"county data via shared SBLs. A small number of homes (98 homes, 0.7% of all residential properties)"
2111.14915,data,22,,,"et al. 2021), as well as more diﬃcult to gather administrative data on credit scores (Hwang & Ding"
2111.14915,data,22,,,grid cells with an area of 1 km2. We would therefore have data for each grid cell for each year from
2111.14915,data,22,,,"to quantify the extent to which model predictions match real data. Following Reades et al. (2019), we"
2111.14915,data,23,,,"at which we will try to make predictions into the future, given all data prior to that date. So, for"
2111.14915,data,23,,,"is to aggregate our data into yearly (because p = 1 year) snapshots, each of which represents a set of"
2111.14915,data,23,,,"when trained on data from, e.g., a speciﬁc city. For example, Kolko (2007) uses census-tract data from"
2111.14915,data,25,,,"(2021), data on foreclosures (Williams et al. 2013), and home transactions (Yonto & Schuch 2020)."
2111.14915,data,25,,,"Glaeser, E. L., Kim, H. & Luca, M. (2018), ‘Nowcasting Gentriﬁcation: Using Yelp Data to Quantify"
2111.14915,data,26,,,"Greene, S. & Pettit, K. L. (2016), ‘What if cities used data to drive inclusive neighborhood change?’,"
2111.14915,data,28,,,"Olteanu, A., Castillo, C., Diaz, F. & Kiciman, E. (2019), ‘Social data: Biases, methodological pitfalls,"
2111.14915,data,29,,,"McFarland, D. A., Lewis, K. & Goldberg, A. (2015), ‘Sociology in the Era of Big Data: The Ascent of"
2111.14915,data,3,,,2.1 Data Selection
2111.14915,data,3,,,4.1 Data Selection
2111.14915,data,3,,,4https://data.buffalony.gov/
2111.14915,data,32,,,"emphasizes the importance of looking at ways of modeling and measuring gentriﬁcation and displace ment that move beyond the granularities of census data, in particular to parcel-level data. Our work"
2111.14915,data,39,,,"available data sources from the City of Buﬀalo and Erie County (where Buﬀalo is located). In Sec tion 4.2, we describe how we operationalize the outcome for our EWS. We then discuss the model"
2111.14915,data,46,,,"Figure 1: Data from the 2019 ACS ﬁve-year estimates, at the census tract level, for total population (left), percent of population identifying as White (center), and the logarithm of the median reported household income (right)"
2111.14915,data,5,,,model match the data.
2111.14915,data,76,,,"Figure 3: Each sub-ﬁgure represents a diﬀerent year of transaction data, from 2011-2019. Overlaid on a map of the city of Buﬀalo are white and red grid cells; areas with no grid cells have fewer than 10 homes and are thus excluded from our analysis. Cells are red if the percent of homes bought/sold in that cell was 1 standard deviation or higher above the mean value in that year."
2111.14915,data,89,,,"Figure 6: Variable importance, computed using SHAP, for an EWS for 2018 with a2 = X and t = 1 year. SHAP values (y-axis) are given for the full training data (each dot) for the 25 most important independent variables in terms of mean absolute SHAP values (x-axis). Color represents the magnitude of the feature value; all features are rescaled from 0 (blue, lowest value) to 1 (orange,highest value)."
2111.14915,"data, code, data available, code available",20,,,"consists of four steps; as with the predictive modeling we describe below, all code used in our data"
2111.14915,"data, data available",104,,,"3While data prior to the year 2000 is available, it is sometimes irregular. More speciﬁcally, pre-2000 data was not cleanly recorded in the system; most notably, certain years appeared to simply be missing. In addition, deed types were often missing or marked as a generic category that prevented us from being conﬁdent the transaction represented a sale. We also set aside data from after 2019, as we expect a potential discontinuity due to COVID. While the latter is of signiﬁcant interest, it presents in our opinion a separate challenge for future study."
2111.14915,"data, data available",16,,,We construct and evaluate a novel early warning system for gentriﬁcation based on data that is
2111.14915,"data, data available",17,,,"We also leverage home transaction data here, because such data have the advantage of being readily"
2111.14915,"data, data available",17,,,that density estimations of parcel-level data were found to be both accurate and useful. Their work
2111.14915,"data, dataset",18,,,"entitled cross-validation, where the data is repeatedly split into a training dataset, used to infer model"
2111.14915,"data, dataset",18,,,utilized a dataset from the city of Buﬀalo’s Open Data Portal4 which provides longitude and latitude
2111.14915,"data, dataset provided",20,,,"As noted above, most EWS in the U.S. rely exclusively on census data to provide forecasts. However,"
2111.14915,"data, publicly available",16,,,"who use machine learning to forecast gentriﬁcation using decennial, publicly available data on the city"
2111.14915,"data, publicly available",22,,,"transactions (Carlson 2021, Ding et al. 2016). We focus on publicly available data on home transactions;"
2111.14915,"data, publicly available, data available",38,,,"Our analysis uses parcel-level transaction data from the city of Buﬀalo to construct an EWS. In Sec tion 4.1, we describe our data selection, that is, how we collect the transaction data from publicly"
2111.14915,dataset,12,,,"parameters, and a testing dataset used to test the model."
2111.14915,dataset,14,,,traditional dataset which harbors a more extablished and direct relationship to the phenomenon of
2111.14915,dataset,16,,,methods are used to better understand datasets that have no clear outcome variable; examples include
2111.14915,dataset,17,,,"not relevant to the present work. First and foremost, the complete dataset covers transactions for"
2111.14915,dataset,18,,,"In sum, then, the present study analyzes a dataset of all 75,160 home transactions captured by"
2111.14915,publicly available,19,,,"the present work from other events recorded in the public record, for example, name changes (which"
2111.14915,publicly available,5,,,collection is available publicly.
2111.14915,python,17,,,"page; to extract the information, we use the BeautifulSoup and pandas libraries in python."
2112.0663,code,166,,,"In the selection step of every iteration we need to ﬁnd the neighborhood N (u) for every node u in the current KNNgraph approximation. The neighborhood N (u) contains every node v ∈ V for which (u, v) ∈ E, i.e. node v is one of the k-nearest neighbors according to the current approximation. More interestingly, it also contains every node w for which (w, u) ∈ E, so every node w which has node u as one of its k-nearest neighbors. In the pseudo code as presented [1] such nodes w are found by ﬁrst inverting all the directed edges in the current KNN-graph G, resulting in a graph G(cid:48) = (V, E(cid:48)) for which E(cid:48) = {(u, v)|(v, u) ∈ E}."
2112.0663,code,92,,,"Experimental setup. All the experiments were conducted on a computer running Ubuntu 20.04 LTS. The CPU is a Intel Core i7-9700K CPU @3.60GHz (turbo boost disabled) with the cache sizes L1: 256 KiB, L2: 2 MiB and L3: 12 MiB. The GCC version 9.3.0 was used in all experiments with the following ﬂags: 03, ffast-math, march=native, ﬂto. The ﬂags p and pg were used to add run time instrumentation to the code for speciﬁc proﬁling tasks."
2112.0663,data,139,,,"Because the heaps incurred a lot of cache misses we further optimized the fused selection function described above in order to get rid of the heaps. Upon every update of the KNN-graph we keep track of how large the neighborhood of every node v is. Since when doing these updates we access the relevant data structures anyway, we do not incur any additional cache misses by these modiﬁcations. Knowing how large each neighborhood is allows us to simplify the sampling process: for every edge e = (u, v) we insert ρ·k v into N (u) with probability |N (u)| . In expectation this is equivalent to the previous sampling procedure, but it works without heaps. This gives a small speedup of around 1.12x."
2112.0663,data,168,,,For each such block we allocate an 256 bit AVX2 accumulator for every distance evaluation (either 10 or 25 in total). We then proceed with the computation of the squared l2-distance 8 features at a time for all combinations. In a block where 25 mutual distances are computed simultaneously only 10 AVX2 vectors of data are loaded per 8 dimensions. Without blocking each of theses would be loaded once for every distance evaluation. (1 vs. 25 loads per component). For high dimensional vectors especially this drastic reduction of data loads has a major impact on the performance (cf. tag blocked). Choosing a blocksize of 5x5 allows for each of the 25 accumulators to be allocated to a register which can be seen by inspecting the assembly. If the number of vectors to be compared to one-another is not divisible by 5 a more ﬂexible but slower function is used for the remaining pairs.
2112.0663,data,185,,,of use and support of custom distance metrics. PyNNDescent is used in the sci-kit learn compatible implementation of UMAP [4] [5]. UMAP is currently gaining popularity as an alternative dimensionality reduction technique to tSNE due to favorable embedding properties and because it is faster than current implementations of t-SNE [6]. The use of NN-Descent in practice makes performance optimized implementations especially desirable. A major challenge in optimizing the performance of NNDescent implementations is the irregular memory access pattern stemming from unordered input data together with the neighbor-of-neighbor heuristic explained in Section 2. This paper presents a single-core implementation which is limited to the l2-distance metric at the beneﬁt of a vast runtime reduction when compared to PyNNDescent. Optimization techniques are discussed which performance for both low or high-dimensional input respectively. In particular a novel heuristic is introduced which increases locality by improving the otherwise irregular memory access pattern. The overall performance advantage over PyNNDescent motivates the use of specialized implementations of NN-Descent for commonly used distance metrics and use cases.
2112.0663,data,30,,,mem-align Data aligned to 256bits (Section 3.3) blocked Blocking of the L2 distance calculations (Section 3.3) greedyheuristic Reordering of memory using heuristic (Section 3.2)
2112.0663,data,4,,,data in memory?
2112.0663,data,45,,,"• recovers most of the clusters. Moreover, it should output a permutation σ : [n] → [n] which we may use in the end to permute our data in memory all at once to bring the clusters together."
2112.0663,data,48,,,The algorithm then returns a permutation σ. We proceed by permuting all of our data in memory using that permutation. Afterwards we continue with the remaining iterations of NN-Descent using the permuted memory layout. The copying itself is done all at once using σ.
2112.0663,data,60,,,"As stated in section 2 the work W (n) is calculated from the number of distance evaluations and the dimensionality of the datapoints. For the sake of the rooﬂine model analysis, we additionally need to reason about the data movement Q(n), the bandwidth β as well as the peak performance π."
2112.0663,data,62,,,Our optimizations in Section 3.2 and 3.3 are motivated by the fact that the bottleneck of our algorithm changes as we change the dimension. For low dimensional data the computation is memory bound whereas for higher dimension we actually become compute bound as shown in ﬁgure 3. In the following the derivation of the rooﬂine model parameters is explained.
2112.0663,data,68,,,"A major improvement of locality when using the greedy clustering heuristic can be seen in terms of the cache misses. Our heuristic nearly halves the number of LL cache data read misses. The increase in operational intensity by the reduction in cache misses moves the computation to the right in the rooﬂine plot (no-heuristic, dim8 to greedyheuristic, dim8 in ﬁgure 3)."
2112.0663,data,76,,,"Kwok, L. G. Ng, F. Ginhoux, and E. W. Newell, “Dimensionality reduction for visualizing single-cell data using UMAP,” Nature Biotechnology, vol. 37, no. 1, pp. 38–44, Jan. 2019, ISSN: 1087-0156, 1546-1696. DOI: 10 . 1038 / nbt . 4314. [Online]. Available: http://www.nature.com/articles/nbt. 4314 (visited on 06/11/2020)."
2112.0663,data,77,,,"CNNNAfter the reverse step, we then proceed by setting N (u) = adjG(u)∪adjG(cid:48)(u) for all u which we refer to as union step. Note that while adjG(u) is bounded in size by k, adjG(cid:48)(u) can contain up to n elements, which requires the usage of a dynamically growing data structure."
2112.0663,data,98,,,Datapoints which are close in the dataspace are frequently accessed together but the underlying data is not usually located closely together. This leads to a difﬁculty in exploiting spatial locality. Our rooﬂine model analysis (cf. Section 4.2) indicates that our implementation is memory-bound for low-dimensional inputs. The major difﬁculty of exploiting locality in spite of the non-uniform memory access pattern is the primary issue to be solved. Introducing an assumption on the data space distribution of the input data allows the development of a heuristic approach to tackle this problem.
2112.0663,"data, code",332,,,"The above requirements inform the design of our greedy clustering heuristic. In the pseudo code (Algorithm 1), the permutations σ and σ−1 are modeled as n-dimensional arrays. We initialize them with the identity function: for each i ∈ [n] we have σ(i) = i. We proceed by looking at node i = 0. In each iteration of the outermost loop we would like to ﬁnd a good candidate for the spot i + 1, meaning whichever node permutation σ maps onto i + 1, it should be close in data space to node i. To achieve that we now sort the adjacency list of i by distance, so ai[j] contains the identiﬁer of the j’th closest node. Now we check whether the spot assigned to by ai[j] by permutation σ is smaller than our current position, if so, we assume that ai[j] already has a good spot where it is close to its (data-space) neighbors in memory space. If not we check whether ai[j] already occupies the spot we would like to have it at (σ(ai[j]) = i + 1) then we conclude the search and break out of the inner loop. Otherwise if σ(ai[j]) > i + 1, we would like to set σ such that σ(ai[j]) = i + 1, such that the node ai[j] will occupy spot i + 1. Note that this is the desired outcome, the nodes on spots i and i + 1 are close together in data space. This speciﬁc sequence of two swaps in the permutation σ and its inverse turn out to give the desired result."
2112.0663,"data, code",345,,,"We initialize them with the identity function: for each i ∈ [n] we have σ(i) = i. We proceed by looking at node i = 0. In each iteration of the outermost loop we would like to ﬁnd a good candidate for the spot i + 1, meaning whichever node permutation σ maps onto i + 1, it should be close in data space to node i. To achieve that we now sort the adjacency list of i by distance, so ai[j] contains the identiﬁer of the j’th closest node. Now we check whether the spot assigned to by ai[j] by permutation σ is smaller than our current position, if so, we assume that ai[j] already has a good spot where it is close to its (data-space) neighbors in memory space. If not we check whether ai[j] already occupies the spot we would like to have it at (σ(ai[j]) = i + 1) then we conclude the search and break out of the inner loop. Otherwise if σ(ai[j]) > i + 1, we would like to set σ such that σ(ai[j]) = i + 1, such that the node ai[j] will occupy spot i + 1. Note that this is the desired outcome, the nodes on spots i and i + 1 are close together in data space. This speciﬁc sequence of two swaps in the permutation σ and its inverse turn out to give the desired result. By creating and updating both the permutation and its inverse at the same time, we save ourselves a costly inversions of the permutation at several steps. This way we can satisfy the second requirement of only doing one pass through the KNN-graph."
2112.0663,"data, data available",138,,,"Recall that during the selection step we iterate over all edges e ∈ E of the current KNN-graph approximation G = (V, E). For one edge e = (u, v) we will access both adjG(u) and adjG(v). Those two lists are likely to be in completely different locations in memory. Since the edge e is part of our current KNN-graph, u and v are likely close in data space according to the given metric. This is why, especially after the initial iteration when our KNN-graph approximation becomes more accurate, closeness in data-space and temporal locality in the access pattern are highly correlated. For the remainder of this section we will consider clustered inputs (clustered assumption)."
2112.0663,"data, dataset",128,,,"Without any assumptions about the input distribution, our access pattern is irregular and we cannot improve locality. This is due to the tight relationship of temporal locality of two nodes and their distance in data space. We proceeded by assuming our input is clustered, meaning for every node all its k nearest neighbors are within the same cluster (clustered assumption). This assumption will allow us to partly recover those clusters from an early approximation of the K-NNG. After reordering memory such that the clusters are close together, we proceed with the remaining iterations of NN-Descent. Experiments on a synthetic dataset and on real world data set are promising (consult section 4 for more details)."
2112.0663,"data, dataset",140,,,"Empirical Evaluation on Synthetic Dataset. To test whether our heuristic leads to a speedup when the clustered assumption is satisﬁed we created such a dataset (Synthetic Clustered Dataset). We ran our algorithm twice on this dataset, once with greedy clustering and subsequent reordering of the data in memory and once without those changes. As reasoned in the previous section, we expect a shorter running time when doing the reordering. One can see in Figure 5 that the ﬁrst iteration takes slightly longer to ﬁnish in the greedy clustering version of our algorithm due to overhead, but we can proﬁt from the reordering in all subsequent iterations where we are consistently faster than the non reordered version. In total this amounts to a speedup of 18.46% over all iterations."
2112.0663,"data, dataset",147,,,"The dimensionality of all datapoints is denoted by d. The k nodes adjacent to some node u in G, denoted as adjG(u), are thus the k nearest neighbors according to the given metric. One trivial way of computing the KNN-Graph is to compute all n(n−1) mutual distances and selecting the best k points. This is not computationally viable for most datasets of practical interest, especially in data science applications due to the O(n2) asymptotic number of distance evaluations. NN-Descent by Wei Dong et al.[1] is a randomized, iterative heuristic which computes an approximation of the K-NNG. The NN-Descent algorithm requires far fewer distance evaluations (empirical cost O(n1.14) [1]) at the expense of the quality of the resulting KNN-Graph."
2112.0663,"data, dataset",174,,,"NN-Descent is an efﬁcient algorithm for computing K-NNGs. The presented fast single-core C implementation incorporates numerous orthogonal optimizations which improve the runtime signiﬁcantly in low-dimensional, as well as high dimensional usecases. The presented optimizations to the selection step which reduce the number of memory passes and simplify the datastructures are particularly primarily for low dimensional data. For high dimensional input data the implementation becomes compute bound and beneﬁcial memory alignment and the use of blocked distance evaluations become paramount. Blocking is made possible by the restriction to the l2-distance while restricing the dimensionality makes memory alignment cheaper. An interesting topic for future work would be to further explore heuristics for reordering the data. The evaluation shows that such heuristics can improve performance even on real world data. Such reordering heuristics are made possible by the iterative nature of the NN-Descent algorithm. The signiﬁcantly lower runtime on real world datasets compared to the popular PyNNDescent implementation illustrates the value of specialized implementations for common usecases."
2112.0663,"data, dataset",178,,,"Fast and reliable K-Nearest Neighbor Graph algorithms are more important than ever due to their widespread use in many data processing techniques. This paper presents a runtime optimized C implementation of the heuristic ”NNDescent” algorithm by Wei Dong et al. [1] for the l2-distance metric. Various implementation optimizations are explained which improve performance for low-dimensional as well as high dimensional datasets. Optimizations to speed up the selection of which datapoint pairs to evaluate the distance for are primarily impactful for low-dimensional datasets. A heuristic which exploits the iterative nature of NN-Descent to reorder data in memory is presented which enables better use of locality and thereby improves the runtime. The restriction to the l2-distance metric allows for the use of blocked distance evaluations which signiﬁcantly increase performance for high dimensional datasets. In combination the optimizations yield an implementation which signiﬁcantly outperforms a widely used implementation of NN-Descent on all considered datasets. For instance, the runtime on the popular MNIST handwritten digits dataset is halved."
2112.0663,"data, dataset",214,,,"PyNNDescent improves over this by only doing one pass over the data but introducing a heap in the sampling process. Instead of building each neighborhood N (u) and then sampling from it, PyNNDescent does both simultaneously in one pass over the KNN-graph. We adopted this change. For each edge r = (u, v) a weight re is drawn uniformly at random (u.a.r.) in [0, 1]. Both N (u) and N (v) are implemented as heaps and we insert the node v in N (u) with weight re, we do the symmetric thing for N (v). This corresponds to both the reverse and union step of the na¨ıve selection implementation. Because the heaps are bounded in size, every neighborhood N (v) ends up containing at most ρ · k elements. Selecting a subset of size ρ · k is equivalent to assigning a random weight u.a.r. to each element and selecting the ρ · k elements with the smallest weights. This gives a considerable speedup; on our synthetic dataset we observed a 16x runtime speedup (see Section 4.1)."
2112.0663,"data, dataset",56,,,"Table 2. Runtimes on the real-world MNIST and Audio datasets. Note how even though the clustered assumption might not hold, we still get a speedup on both datasets using our greedy clustering and subsequent reordering of data in memory. Our ﬁnal implementation greedyclustering is signiﬁcantly faster on both datasets than PyNNDescent."
2112.0663,"data, dataset",56,,,"When the clustered assumption is given, it is intuitive that our heuristic will succeed in clustering most of the data. Consult section 4 for an experimental evaluation on a synthetic data set. More surprisingly we have even seen a small speedup on real world datasets where the clustered assumption does not hold."
2112.0663,"data, dataset",72,,,"Synthetic Gaussian Dataset The input parameter for the generation of this data set is the number of dimensions and the number of points. For the Single Gaussian Dataset all points are drawn from one gaussian distribution centered at the origin. In the non-single variant, for each dimension a gaussian is created and centered around the canonical basis vector. For all evaluations the covariance is 2 · Id."
2112.0663,"data, dataset",75,,,"Real World Data Evaluation. As previously hinted at we observed a speedup on real world data when applying our clustering heuristic. Consult Table 2 to see the runtimes on the MNIST and Audio datasets. According to our observations, the clustering heuristic does not manage to cluster MNIST semantically. Still, the reordering does improve locality which makes sense considering we move nodes together which are close in data space."
2112.0663,"data, dataset",80,,,"Towards the end of the dataset all of the relative frequencies of the eight clusters are around 1 8 - our heuristic stops working. This is expected since our algorithm is restricted by design to a single pass through the data. If a certain cluster was already handled and most of the points belonging to it were moved to some location, then the missing few will end up at the end of our simpliﬁed memory layout."
2112.0663,"data, dataset, code",108,,,"Data movement. The number of bytes transferred from memory to the cache far exceeds the size of the cache, which necessitates repeated loading and rewriting of data. To reason about the bytes transferred, cachegrind (Valgrind extension)[13] was used. Cachegrind enables examination of the code’s cache behaviour by simulating the memory behaviour in terms of ﬁrst and last level (LL) caches. Using this tool we measured the LL cache data read and write misses. Table 1 summarizes the cachegrind output on our Synthetic Clustered Dataset with n = 131072 and 16 clusters."
2112.0663,"data, dataset, code",303,,,"updating data structures remains constant. In such cases optimizing the distance evaluations becomes signiﬁcantly more important than the optimizations described in previous sections. A single l2 distance evaluation for two vectors is computed by summing the component-wise distances and taking the square root. As the actual value of the l2-distance is unimportant, the square root is omitted and the implementation uses the squared l2-distance. This improvement is not signiﬁcantly impactful as for high dimensional vectors the cost of computing and summing the differences is dominant. We decided to limit our implementation to vector dimensions which are divisible by 8 in order to simplify the use of AVX2 SIMD intrinsics. As each AVX2 register can hold 8 single-precision ﬂoating point numbers this alleviates the need for auxiliary code to handle the last (fewer than 8) components. The real-world datasets described in Section 4 happen to fulﬁll this requirement without modiﬁcation. We use a AVX2 vector of accumulators for each distance evaluation and process 8 components at a time by subtracting to compute the difference and using an fmadd instruction to square the difference and add it to the accumulator vector (cf. tag l2intrinsics in Section 4). We noticed that the restriction of the dimensionality to divisibles by 8 allowed for an easy modiﬁcation to the way the datapoints are stored in memory. This modiﬁcation allocates the data neatly aligned to 256 bits at the cost of at most additional 192 bits. This change signiﬁcantly improves the performance since the loadu instructions become faster. We did not observe a speedup by replacing loadu intrinsic instructions by the equivalent load intrinsic (cf. tag memalign). The compute step for a single node can be further"
2112.0663,"data, dataset, data available",62,,,"To test this, we ran our greedy clustering algorithm on the Synthetic Clustered Dataset. We then reordered our data according to the permutation σ found by the greedy clustering algorithm. Figure 4 shows that the algorithm successfully recovers many of the clusters in the beginning of the dataset, where the heuristic can chose from many unassigned nodes."
2112.0663,"data, dataset, used dataset",127,,,"We previously argued that increasing the dimension of our input data makes our implementation compute bound. In low dimensions, our algorithm spends a large portion of its runtime in the selection step. For higher dimensions, the focus changes to the calculation step. Some improvements can therefore only unfold their true potential when we increase the dataset dimensionality. In Figure 7 we kept the number of datapoints n ﬁxed at 16(cid:48)384 and varied the dimension from 8 up to 3144 in increments of 64. We used our Synthetic Single Gaussian Dataset for this evaluation. Note how turbosampling only sees a 3.52x performance gain when increasing the dimension while our optimizations targeted at high-dimensional data proﬁt much"
2112.0663,"data, python",124,,,"K-nearest neighbor graphs (K-NNG), which contain the identities of the closest datapoints for each datum, are fundamental to many data science and machine learning techniques. The rapid growth of attainable data has therefore increased the need for practical K-NNG algorithms and efﬁcient implementations thereof. This paper presents an optimized implementation of the NNDescent algorithm by Wei Dong et al. [1]. NN-Descent is an iterative, randomized heuristic which improves a random guess of the K-NNG. A particularly popular implementation of NN-Descent is PyNNDescent [2] which is implemented in Python and uses the Numba JIT copiler [3] to achieve sufﬁcient runtime performance. Particular strengths of PyNNDescent are its ease"
2112.0663,"data, python",20,,,4. How does our ﬁnal implementation perform to the numba based Python implementation PyNNDescent on real world data?
2112.0663,database,17,,,"database,” 2010. [Online]. Available: http://yann. lecun.com/exdb/mnist/."
2112.0663,dataset,11,,,2. How does performance scale in terms of dataset size
2112.0663,dataset,16,,,For higher dimensional datasets each l2-distance evaluation becomes more costly while the overhead of sampling and
2112.0663,dataset,23,,,tion investigates both how well the greedy clustering algorithm recovers the clusters of the underlying dataset and how this causes the speedup.
2112.0663,dataset,39,,,Audio Dataset The audio dataset used in the NN-Descent [1]. Each of the publication by Wei Dong et al. 54’387 points consists 192 features which were extracted from recordings of English sentences.
2112.0663,dataset,39,,,Synthetic Clustered Dataset A dataset designed to fulﬁll our clustered assumption. For every cluster we draw its points from a multivariate Gaussian. Mean and covariance are chosen such that the clustered assumption holds with high probability.
2112.0663,dataset,45,,,"Fig. 5. Time spent on each iteration on Synthetic Clustered Dataset with 16’384 points, 16 clusters and 8 dimensions. While the ﬁrst iteration is slower due to the overhead of the heuristic, we proﬁt in later iterations."
2112.0663,dataset,51,,,"Four datasets were used to evaluate the performance and recall of our modiﬁed implementation. To investigate scaling behaviours, synthetically generated datasets were used. Additionally real-world datasets including the MNIST handwritten digits [10] were used to verify the generalization of the performance from the synthetic datasets."
2112.0663,dataset,55,,,Quality of Clustering with Greedy Heuristic. This sec Fig. 3. Rooﬂine plot with peak performance π = 24 [ﬂops/cycle] and bandwidth β = 4.77 [bytes/cycle]. The Synthetic Gaussian Dataset was used with n = 131(cid:48)072 and the dimensions 8 and 256 respectively.
2112.0663,dataset,6,,,n and dataset dimension d?
2112.0663,dataset,60,,,Table 1. Cachegrind results for versions of our implementation with (greedyheuristic) or without (no-heuristic) memory reordering on the Synthetic Clustered Dataset (n = 131(cid:48)072 and 16 clusters) for the speciﬁed dimension. Increasing d by a factor of 32 increases the last-level read misses by a smaller factor.
2112.0663,dataset,90,,,"2624222022242628I(n) [flops / byte]24222022242628P(n) [flops / cycle]bandwithpeak performanceRoofline Plotbandwithpeak performancegreedyheuristic, dim8no-heuristic, dim8no-heuristic, dim256Fig. 4. Each line represents the fraction of datapoints belonging to single cluster in a 2000 sized window (y-axis) starting at the position given in the x-axis. Plot represents the cluster distribution greedyclustering reordering retrieved on our Synthetic Clustered Dataset with parameters n = 16(cid:48)384, d = 8 and 8 clusters."
2112.0663,"dataset, code",128,,,"validate both the quality of the returned K-NNG as well as the computational cost. Recall is used to measure how close the K-NNG approximation is to the true K-NNG. Our implementation achieved a recall of over 99% on all examined datasets. Multiple parameters could if desired be altered to change the runtime-quality trade-off. To calculate the number of ﬂoating point operations, the number of distance evaluations is counted. This enables performance comparisons between different code versions which have small but varying amounts of additional ﬂoating point comparisons. Together with the dimensionality of the datapoints the number of operations can be computed. For each l2-distance evaluation d subtractions, d multiplications, and d − 1 additions are performed."
2112.0663,"dataset, code",46,,,02000400060008000100001200014000position in memory0.00.10.20.30.40.50.60.7cluster distributionGreedy Clustering Heuristic Evaluation123456Iteration0.00.51.01.52.02.5runtime[s]Runtime per Iterationno-heuristicgreedyheuristicFig. 7. Performance for the various versions of our code on the Synthetic Single Gaussian Dataset with n = 16(cid:48)384 and increasing dimension along the x-axis.
2112.0663,"dataset, code",55,,,"Fig. 6. Performance of the discussed improvements on the Synthetic Gaussian Dataset. The dimensions is ﬁxed at 256, while the n increases along the x-axis. Lines correspond to speciﬁc tags of our code where every versions contains the improvements of previous versions to showcase the effect of speciﬁc changes."
2112.0663,"dataset, code, used dataset",48,,,"To show how the performance scales with number of input points n, we used the Synthetic Gaussian Dataset with dimension 256. Every line in Figure 6 corresponds to a speciﬁc version of our code, building on top of the improvements of the lines below."
2112.0663,"dataset, database",26,,,MNIST Dataset [10] The MNIST database contains 70’000 images of handwritten digits given as 784 dimensional vectors of pixel intensity values.
2112.0663,"dataset, used dataset",138,,,"Every iteration of NN-Descent consists of a selection and computation step (cf. Section 2). Here we evaluate the improvements detailed in Section 3.1. As previously mentioned the PyNNDescent inspired sampling is up to 16 times faster than the na¨ıve sampling implementation in C. Our improved sampling, which we call turbosampling, gives a speedup on top of that of up to 1.12x. The speedup was measured in terms of runtime since the ﬂop count varies across those three implementations and a performance plot would be misleading. We used the Synthetic Gaussian Dataset with parameters n = 16(cid:48)384, d = 8 for said measurements. Due to space constraints, we omit further evaluations of improvements inspired by PyNNDescent and focus on our own contributions."
2112.0663,github,33,,,"[2] L. McInnes, Lmcinnes/pynndescent, original-date: 201802-07T23:23:54Z, Jun. 7, 2020. [Online]. Available: https://github.com/lmcinnes/pynndescent (visited on 06/09/2020)."
2112.0663,github,33,,,"[4] Umap implementation, original-date: 2017-07-02T01:11:17Z, Jun. 11, 2020. [Online]. Available: https://github. com/lmcinnes/umap (visited on 06/11/2020)."
2112.0663,github,34,,,"[9] E. Skomski, Eskomski/nn descent, original-date: 201901-10T05:35:25Z, Apr. 22, 2020. [Online]. Available: https://github.com/eskomski/nn_descent (visited on 06/09/2020)."
2112.0663,python,32,,,[3] Numba: A high performance python compiler. [Online]. Available: https : / / numba . pydata . org/ (visited on 06/11/2020).
2112.0663,"python, code",79,,,"We started out with a C implementation[9] which adheres closely to the pseudo code found in [1] (NNDescentFull). We then studied the most popular implementation of NN-Descent, PyNNDescent [2], which was introduced in Section 1. Although asymptotically the two implementations behave the same, the Python implementation was considerably faster than the straightforward C implementation. We identiﬁed the main differences and adopted the improvements."
2201.02733,code,137,,,"7 RELATED WORKS There have been several studies that evaluated the impact of noise on machine learning systems in various domains such as image classification (e.g. [1, 22]), code intelligence (e.g. [14]), and others. To the best of our knowledge, there have not been similar works for models in the domain of counterfeit news detection. Horne et al. [7] evaluate the robustness of a fake news classifier in the presence of specific adversarial attacks in the input space, however, they do not analyze the behaviour of the model with randomized noise, as was done in this work. Below we discuss some contributions by the ML community in the domain of fake content detection."
2201.02733,code,32,,,"[14] Md Rafiqul Islam Rabin, Aftab Hussain, Vincent J. Hellendoorn, and Mohammad Amin Alipour. 2021. Memorization and Generalization in Neural Code Intelligence Models. arXiv:2106.08704"
2201.02733,data,138,,,"The framework we choose for evaluation, HDFS, is based on Bi-directional Long-Short-Term Models, that learns the context of news documents by capturing structural information of the constituent sentences of the documents. It has been found to outperform other standard models (e.g. N-Grams) in the field of fake news detection. In addition, it has been shown that robust BiLSTM models, the building block of HDFS, can be built in the domain of disfluency detection in speech transcripts [3]. Our work takes a stepping stone in the direction of determining how accurate are the predictions of the ML models that have been trained with noisy data in the domain of fake content detection. Contributions. Overall we make the following contributions in this paper:"
2201.02733,data,42,,,[4] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’16). 785–794.
2201.02733,data,49,,,"[17] Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. DEFEND: Explainable Fake News Detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’19)."
2201.02733,"data, data repository",51,,,"[18] Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. FakeNewsNet: A Data Repository with News Content, Social Context and Spatial Temporal Information for Studying Fake News on Social Media. arXiv preprint arXiv:1809.01286 (2019)."
2201.02733,"data, dataset",129,,,"Driven by question, “Are ML models in the domain of counterfeit news detection smart enough to bypass perturbations in the training data and make quality predictions based on what they learn from the structure of the data?"", in this work, we evaluate the impact of noise on a recent ML technique used in fake news detection: Karimi and Tang’s Hierarchical Discourse-level Structure for Fake News Detection (HDSF) framework [9]. In particular, we explore how the prediction accuracy of HDFS is impacted when HDFS is trained with noise-induced datasets? Here, we focus on the problem where noise entails news article data points in the training set being mis-labelled as fake or real."
2201.02733,"data, dataset",162,,,"different versions of the dataset. We observe that the prediction accuracy of the HDFS model on test data remains the same at different stages of the training process, when the model is being trained with the untampered dataset. With the noise-induced datasets, we see several trends: (1) At several stages, the performance with the noise-induced datasets are as good as that with the original dataset, (2) at certain stages, HDFS trained with some of the noisy datasets outperformed the model when trained with the original dataset (for e.g. the model trained by the 50% noise induced dataset at steps 120, 180, and 200, and the model trained by the 25% noise induced dataset at step 160), and lastly (3) the fluctuations in performance from stage-to-stage are more pronounced with the noise-induced datasets than that with the original dataset."
2201.02733,"data, dataset",190,,,"5.1 Impact of Dataset Noise on Training Loss Figure 3 shows HDFS’s loss scores obtained while training it with the five different datasets (original/no-noise data, 25%-, 50%-, 75%-, 100%-noise induced data). We see that over the 200 steps of training, the training loss is the lowest with the original dataset. For the other four datasets we see three interesting trends: (1) The training loss is around 0.7 when HDFS is trained with the 25% and the 50% noiseinduced datasets, throughout the training process, (2) the training loss is more erratic with the 75% and 100% noise-induced datasets, with the 100% noise-induced dataset incurring less loss than all the other noisy datasets, and (3) as the number of steps increases, training with the 100%-noise induced dataset shows improvement in the loss score, unlike when using the other noise-induced datasets, where the training losses tend to hold a constant horizontal trend."
2201.02733,"data, dataset",58,,,"Observation. Adding high-levels of noise (100% and 75%) significantly reduced HDFS’s prediction accuracy on unseen (test) data, however, HDFS made better predictions for some moderate noise levels (25% and 50%) at certain stages than it did when trained with the original dataset."
2201.02733,"data, dataset, code",146,,,"In future, evaluation studies for understanding the memorization and generalization properties of HDSF and other models in fake news detection can be carried out, as has been done in other domains, such as the study by Arpit et al. [1] which leveraged findings based on training their models with noise-induced image datasets. Such studies are likely to shed more light on the learning behavior of such models. Another direction is to build and train more sophisticated models with richer datasets that can predict what parts of a news article are potentially fake. This could be beneficial for interpreting how a model reached an authenticity conclusion about an article. Lastly, it is interesting to investigate the potential of HDSF in performing prediction tasks in other domains that involve structured data, such as software code."
2201.02733,"data, dataset, code",168,,,"believe this can exhibit the generalizability [1] potential of HDFS, whereby it is relying on the structure of the code, rather than blindly following the labels in the training set – which is a positive sign as it conversely shows the model is relying less on memorizing [1] the data. However, there is a limit as to how much noise the model can take. For example, we observed that the accuracy of the model drops significantly when trained at a 100% noise level. Thus, further investigations are necessary to investigate the memorization and generalization behaviour of the model. Lastly, in our experiments the size of the test set was relatively very small (134 samples). A larger variety of test samples may be tried to obtain more generality in the results. It also remains to be seen whether the trends revealed in these experiments can extend to other datasets."
2201.02733,dataset,118,,,"1 INTRODUCTION With the growth of social media, such as Twitter and Facebook, and various online news media outlets, the widespread reach of fake news has become a very detrimental problem for society, triggering the growth of numerous online platforms for fact-checking. Towards checking counterfeit content, a great deal of interest as also grown in the Machine Learning (ML) field from where various software systems for detecting fake content have emerged over the past few years. Given the vast datasets that are used to train such ML-based systems, we underscore that there is a need to evaluate the robustness of these systems in the presence of noise."
2201.02733,dataset,129,,,"a deep neural network that uses a Convolutional Neural Network (CNN) based classifier. They implement their approach on Twitter and Weibo datasets [12, 13]. An earlier work by Zhou et al. [23] also focuses on the problem of early fake news detection, but applies a theory based approach. They investigate news content at multiple levels including, lexicon-level, syntax-level, semantic-level, and discourse-level, where well-established theories in social and forensic psychology are used to represent the news at each level. They utilize a supervised machine learning framework, deploying several supervised classifiers with five-fold cross-validation, namely, Support Vector Machine (with linear kernel), Random Forest (RF),"
2201.02733,dataset,141,,,"Hakak et al. [6] aim to improve training and testing accuracy by proposing an ensemble-based model. The model, built on Random Forest, Decision Tree, and Extra Tree Classifier, focuses on extracting important features from the dataset. In total, they extracted 26 features including number of words, number of characters, number of sentences, average sentence length and others. They implement their approach on the LIAR dataset [20]. Aslam et al. [2] also proposed an ensemble-based machine learning model that combines two deep learning models based on the properties of attributes: Bi-LSTM-GRU-dense deep learning model for a textual attribute (statement) and a dense deep learning model for other attributes. They also apply their approach on the LIAR dataset."
2201.02733,dataset,15,,,Figure 3: HDFS training loss when trained with different levels of noise-induced datasets.
2201.02733,dataset,20,,,RQ.1 What is the impact of dataset noise on the prediction accu racy of HDFS? (Subsection 5.2)
2201.02733,dataset,23,,,"Figure 4: Test Accuracy of the HDFS Model at 10 different stages of training, for different levels of noise-induced datasets."
2201.02733,dataset,23,,,"datasets we used. In Subsection 4.2, we detail the configuration of our system and the HDFS model that we deployed."
2201.02733,dataset,25,,,and XGBoost10 [4]. They used well established datasets of news articles that were collected from PolitiFact and BuzzFeed [17–19].
2201.02733,dataset,42,,,"Observation. While training with the original (noise-less) dataset exhibited the lowest training loss, adding more noise to the dataset does not increase the loss in training in accordance with how much noise we add to the dataset."
2201.02733,dataset,46,,,"Figure 2: Overview of our approach for evaluating the impact of training HDFS with noisy datasets. (a) Approach for the generation of noise induced datasets. (b) Training HDFS with datasets, and evaluating it with the test set."
2201.02733,dataset,47,,,"5 RESULTS In this section, we present the results of our experiments, where we evaluate the impact of noise on HDFS. In particular, we address the following research questions: RQ.1 What is the impact of dataset noise on the training loss of"
2201.02733,dataset,58,,,"[19] Kai Shu, Suhang Wang, and Huan Liu. 2018. Beyond News Contents: The Role of Social Context for Fake News Detection. arXiv preprint arXiv:1712.07709 (2018). [20] William Yang Wang. 2017. ""Liar, Liar Pants on Fire"": A New Benchmark Dataset"
2201.02733,dataset,7,,,5.2 Impact of Dataset Noise on Prediction
2201.02733,"dataset, github",89,04/21/22,2,"3.2 Training HDFS Once the train sets have been generated, we deploy HDSF available in HDSF’s authors’ Github repository [8]. We trained HDSF with each of the five training sets (one original and four noisy dataset) over 20 steps, and generated models, on which we performed our tests using a separate test set (Figure 2 (b)). Thereafter we collected loss and accuracy scores for each model obtained for the same test set."
2201.02733,"dataset, used dataset",99,,,"4.1 Datasets We use the same dataset to train the HDSF model as that used in [9]. In total, the dataset consists of 6,452 examples. For investigating the impact of noise on HDSF, we generated four different noisy datasets from this dataset using the approach mentioned in Section 3 at noise levels of 25%, 50%, 75%, and 100%, respectively. All datasets have the same number of samples. After training, we tested each obtained model with a test set of 134 samples."
2201.02733,github,15,04/21/22,0,[8] Hamid Karimi. 2020. HDSF Source Repository. https://github.com/hamidkarimi/
2201.02733,"python, dataset",157,,,"3.1 Generation of Noise-Induced Datasets We first produce the noisy datasets using our Noise Inducer module (Figure 2 (a)), which is written in Python. The Noise Inducer first shuffles the train set and randomly picks a sample of the training set with 50% probability, and flips its news status or authenticity label: changing “Fake"" to “Real"", and vice versa. The Noise Inducer also keeps track of which samples have been already changed, to avoid re-altering an already altered sample. The Noise Inducer repeats this process on the training set until 𝑋 % of the samples have been modified, where 𝑋 is a user-defined parameter. In this manner, we generated four different levels of noise-induced datasets from the original dataset: training sets at 25%, 50%, 75%, and 100% noise."
2201.0347,code,181,,,"Once we increase the message size to 20 bytes for Bluetooth Mesh, we see the results of the transmission delays between packets discussed in the introduction. A 20 byte message with a 3-byte opcode (3 bytes needed for vendor-speciﬁc opcodes, see section 2), will need 3 packets to be transmitted with Bluetooth Mesh. We have set parameters NTC = 3 and NTI = 20, giving an absolute minimum latency of 120 ms by eq. (1), assuming all random delays are zero. However, looking at the source code of the Zephyr Project’s implementation of Bluetooth Mesh, we see that the actual time used to send a packet is 30+(NTI + 10) · NTC [11], indicating that 240 ms is the minimum expected latency, which is also clear from the 20 byte plot in ﬁg. 3. OpenThread maintains a low latency, and is still within 10ms with > 99.9% probability for 20 byte messages."
2201.0347,data,187,,,"Thread’s [3] physical and media access control (MAC) layers are based on the IEEE 802.15.4 wireless standard [5]. The physical layer uses the 2.4GHz wireless band with a data rate of 250 kbit/s and a frame size of up to 127 bytes. Routing in a Thread network is handled by nodes assigned the routing role. These nodes maintain routing tables and forward frames based on the routing information protocol (RIP) algorithm. Every device capable of routing can be assigned the router role when the network deems it necessary, and this happens automatically without user intervention. If a router is no longer needed, it will automatically be demoted to a regular end device. Only keeping the necessary number of routers allows the entire network to remain connected without overcrowding it with relayed traﬃc. Even though the network reacts to changes dynamically, there can be signiﬁcant delay before the network reacts, meaning that, e.g., a node leaving the network, can result in long outages."
2201.0347,data,218,,,"any device designed for Bluetooth LE can also run Bluetooth Mesh with only software modiﬁcations. Bluetooth Mesh makes use of the Bluetooth LE advertising channels for communication, which can be set to 1Mbit/s, 500Kbit/s, or 125Kbit/s mode depending on the conﬁguration. The advertising packets have 11 bytes available for the application layer, however 1, 2, or 3 bytes must be used for an opcode, leaving 8 to 10 bytes for application data. Routing is performed by relay nodes in the network, who do not maintain routing tables, but re-transmits any messages that are not intended for them and that meet certain requirements. This approach to routing is referred to as network ﬂooding. Not all nodes need to have relay capability, and among those that do have the relay capability it can be enabled or disabled. As opposed to Thread, enabling and disabling relay capability does not happen automatically in Bluetooth Mesh. Instead this must be manually conﬁgured when deploying the network. As shown in [6], latency in large Bluetooth Mesh networks can be signiﬁcantly improved by optimizing the number of relay nodes, meaning manual conﬁguration might be necessary to achieve the best possible latency."
2201.0347,github,16,,,[11] Zephyr Project. Zephy RTOS. https://github.com/zephyrproject rtos/zephyr. 2020.
2201.05314,"code, code available",33,,,mance in discovering activities in all evaluation parameters compared to the other state-of-the-art methods and has increased accuracy of at least 4 % on average. The code is available here: Human-Activity-Discovery-HPGMK
2201.05314,data,111,,,"A new method is introduced to extract a set of statistical displacements, angles, and orientation features for encoding key aspects of activities. These important features are extracted from selected (informative) joints in the data to fully describe the shape and movement of humans. Selected joints include both left and right hand, foot, hip, shoulder, elbow and knee. In this way, we use information related to the position and movement of joints, the orientation and angle between a pair of bones and activity variations over time. The normalization procedure [30] is performed on all features."
2201.05314,data,145,,,"In this work, we use 3D skeleton-based data as it does not have the problems of the other two data types. Such data show three-dimensional coordinates of main body joints obtained in each frame. Skeletal information is valuable to show human movements and activities [25]. Here, we review the 3D skeleton-based approaches for human activity discovery. The ﬁrst work proposed used k-means to cluster unlabeled activities into groups and estimate the number of groups using cluster validity indices [26]. A feature extraction approach using a range of human movements as features was presented in [27]. An incremental clustering approach to deal with undeﬁned number of activities and noisy data was proposed in [28]. In [24], an autonomous learning technique was developed based on mixture of"
2201.05314,data,179,,,"Many studies in HAR used supervised approaches. They focus on feature representation and activities classiﬁcation [10, 11, 12, 13, 14, 15, 16]. These approaches require activity labels to learn. These labels are given by humans during data preparation. On the other hand, human activity discovery automatically recognizes human activity in a fully unsupervised way and the challenge is dealing with unlabeled data. Majority of existing methods were developed for sensor-based [17, 18] and RGB video data [19, 20, 21, 22, 23]. The challenges of sensor-based approach are diﬃcult to implement in the environment and take long time to install [24]. Furthermore, it is impractical for people to wear sensors everywhere. With RGB videos, the problems faced are millions of pixel values, illumination variations, viewpoint changes, and cluttered backgrounds [6]. It is not the objective of this work to investigate in such data."
2201.05314,data,18,,,"Input: D={d1,d2,. . .,dn} //Set of data points"
2201.05314,data,207,,,"We have proposed a novel Hybrid Particle Swarm Optimization with Gaussian Mutation and k-means (HPGMK) approach to solve human activity discovery on skeleton-based data with no prior knowledge of the activities in the data, apart from the number of activities. A method based on kinetic energy was used to select important frames to overcome the data redundancy problem. A novel feature extraction method was introduced to extract joint displacement, joint orientation, joint angle, and statistical time domain features from 3D skeleton data. In this method, instead of using all joints to extract the features, informative joints were used to prevent the creation of redundant features that can cause errors in the identiﬁcation of activities. Using such features, activities with high intra-class variables were well clustered using HPGMK. A segmentation method of sampling overlapping frames was employed to divide the entire incoming video into instances of the activities and retain important characteristics of them. A hybrid evolutionary clustering was proposed to discover human activity. It exploited the advantages of PSO, Gaussian mutation, and KM algorithm. This clustering has improved the convergence speed of PSO and prevented it from falling"
2201.05314,data,29,,,"[43] P. Agarwal, S. Mehta, A. Abraham, A meta-heuristic density-based subspace clustering algorithm for high-dimensional data, Soft Computing (2021) 1–20."
2201.05314,data,29,,,"aSchool of Digital Science, Universiti Brunei Darussalam, Jalan Tungku Link, Brunei bInstitute of Applied Data Analytics, Universiti Brunei Darussalam, Jalan Tungku Link, Brunei"
2201.05314,data,327,,,"Research in Human Activity Recognition (HAR) has attracted much attention due to its applications in ﬁelds such as human-computer interaction, intelligent transportation systems, and monitoring applications [1]. Despite its spectacular progress, obtaining high accuracy is a serious challenge. The purpose of activity recognition is to automatically identify actions and activities humans perform in diﬀerent environments. The input to a HAR system is a sequence of frames of a person performing diﬀerent movements. The output is a set of labels that represent the actions taken or activities in those movements. Many existing works use visual data as input. But such data have considerable complexity detrimental to the HAR systems performance. These complexities include cluttered background, changes in brightness and points of view [2]. Using 3D skeleton data partially overcome these complexities [3, 4] and protect people’s privacy [5]. Each frame represented by 3D coordinates of the main body joints is appropriate for representing human actions [5] and can be easily obtained in real-time with low-cost depth sensors [6]. As shown in Fig. 1, there are at least 7 steps in vision-based HAR systems. Activities performed by a person are captured by vision sensors. The skeletal information comprising of joints coordinates are then extracted from captured videos, containing image sequences called frames. Meaningful features are then extracted for more accurate activity discovery. The system discovers activities by clustering them without prior knowledge of the activities. It starts the learning process to model each cluster of activity. The learned model is used to recognize future activities. Signiﬁcant progress has been made in the supervised learning of activity models [7, 8], illustrated in block (f) and (g) of Fig. 1."
2201.05314,data,338,,,"any knowledge of activity labels or any information that characterise an activity, making this step particularly challenging. In other words, activity discovery is like a child learning to move where there is no prior information to deﬁne a speciﬁc sequence of movements to mean a speciﬁc activity such as crawling or waving and so forth to the child learner. Using the ability to diﬀerentiate, they learn from unlabeled data and based on that training, form a model that can post-label new data. In human activity discovery, there is no known information or knowledge that the ﬂow of a particular movement including its start to end is an activity such as picking up something. This means the input is a series of movements where there is no prior knowledge of which start and end points indicate an activity. In some existing work, the input data are segmented by activity [5]. Thus, the start and end point of the activities are already known, though the techniques to elucidate these activities may be unsupervised. In this paper, we focus on the less developed activity discovery steps, illustrated in block (d) to (e) of Fig. 1. To reduce computational load and increase accuracy [9], keyframe selection and PCA are used to remove redundant frames and features respectively. A new feature extraction methodology is applied to extract features from the most informative joints and bones which include joint displacement, joint orientation, and statistical time domain. A hybrid Particle Swarm Optimization (PSO) clustering technique is proposed to ﬁnd activities. PSO can reach the optimal solution independent of initial population states. Although PSO is faster than evolutionary algorithms, swarm particles can get trapped in local optima. One reason is the particles converge to a speciﬁc point between the best global position and the best personal position."
2201.05314,data,34,,,// Refining the centroids Calculate distances of data points to centroids Assign data points to the closest cluster Centroids are updated using following equation centroidi = 1 ∀di∈Ci ni points in the cluster i
2201.05314,data,343,,,"Thus, the start and end point of the activities are already known, though the techniques to elucidate these activities may be unsupervised. In this paper, we focus on the less developed activity discovery steps, illustrated in block (d) to (e) of Fig. 1. To reduce computational load and increase accuracy [9], keyframe selection and PCA are used to remove redundant frames and features respectively. A new feature extraction methodology is applied to extract features from the most informative joints and bones which include joint displacement, joint orientation, and statistical time domain. A hybrid Particle Swarm Optimization (PSO) clustering technique is proposed to ﬁnd activities. PSO can reach the optimal solution independent of initial population states. Although PSO is faster than evolutionary algorithms, swarm particles can get trapped in local optima. One reason is the particles converge to a speciﬁc point between the best global position and the best personal position. To address this, a new hybrid PSO with Gaussian mutation is proposed. Then, K-means is applied to the centroids obtained by PSO to reﬁne their location and get the best possible solution. Our methodology performs activity discovery using unsegmented input data and the proposed techniques used are purely unsupervised, with no prior knowledge of what comprise the different activities except the number of clusters. The aim of this paper is to present a new and eﬃcient methodology to cluster diﬀerent activities using un-segmented skeleton data with no prior information about the activities. The input data holds no information regarding the start and end of each of the activities. The main contributions are: (1) A method based on kinetic energy is applied to select the most important frames and remove redundant information (with the assistance of kinetic energy); (2) a sampling method is employed with assistance of overlapping sliding windows to segment frames"
2201.05314,data,344,,,"Despite its spectacular progress, obtaining high accuracy is a serious challenge. The purpose of activity recognition is to automatically identify actions and activities humans perform in diﬀerent environments. The input to a HAR system is a sequence of frames of a person performing diﬀerent movements. The output is a set of labels that represent the actions taken or activities in those movements. Many existing works use visual data as input. But such data have considerable complexity detrimental to the HAR systems performance. These complexities include cluttered background, changes in brightness and points of view [2]. Using 3D skeleton data partially overcome these complexities [3, 4] and protect people’s privacy [5]. Each frame represented by 3D coordinates of the main body joints is appropriate for representing human actions [5] and can be easily obtained in real-time with low-cost depth sensors [6]. As shown in Fig. 1, there are at least 7 steps in vision-based HAR systems. Activities performed by a person are captured by vision sensors. The skeletal information comprising of joints coordinates are then extracted from captured videos, containing image sequences called frames. Meaningful features are then extracted for more accurate activity discovery. The system discovers activities by clustering them without prior knowledge of the activities. It starts the learning process to model each cluster of activity. The learned model is used to recognize future activities. Signiﬁcant progress has been made in the supervised learning of activity models [7, 8], illustrated in block (f) and (g) of Fig. 1. Unlike learning and recognition steps, which rely on human-labelled training data to categorize activities and do not perform block (e). Human activity discovery (see block (e) of Fig. 1) is a HAR process where activities are categorized based on their similarities without"
2201.05314,data,36,,,"[40] S. Gaglio, G. L. Re, M. Morana, Human activity recognition process using 3-d posture data, IEEE Transactions on Human-Machine Systems 45 (5) (2014) 586–597."
2201.05314,data,36,,,"[44] P. Agarwal, S. Mehta, Analyzing subspace clustering approaches for high dimensional data, in: Artiﬁcial Intelligence for a Sustainable Industry 4.0, Springer, 2021, pp. 169–195."
2201.05314,data,37,,,"[42] Y. Lu, S. Wang, S. Li, C. Zhou, Particle swarm optimizer for variable weighting in clustering high-dimensional data, Machine learning 82 (1) (2011) 43–70."
2201.05314,data,42,,,"[18] P. Gupta, R. McClatchey, P. Caleb-Solly, Tracking changes in user activity from unlabelled smart home sensor data using unsupervised learning methods, Neural Computing and Applications 32 (16) (2020) 12351– 12362."
2201.05314,data,49,,,"xi is a data point belonging to the cluster Ck and µk is the mean of the cluster Ck. To avoid in local optimum, a proposed Gaussian mutation operator based on [35] and [36] is applied to the global particle as follows:"
2201.05314,data,54,,,"[25] P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, N. Zheng, View adaptive recurrent neural networks for high performance human action recognition from skeleton data, in: Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2117–2126."
2201.05314,data,9,,,"di, where ni is the number of data"
2201.05314,data,92,,,"Gaussian hidden Markov model with assistance of incremental k-means clustering. State-of-the-art approaches include Spectral Clustering (SC), Elastic Net Subspace Clustering (ENSC) and Sparse Subspace Clustering (SSC), which use covariance descriptor to solve HAR without labels [5]. They used an aﬃnity matrix to ﬁnd similarities and then applied spectral clustering. A time stamp pruning approach was used to remove redundant data. Although they achieved impressive results, the data used were already segmented by activity before applying clustering."
2201.05314,data,94,,,"Figure 1: Overview of HAR system: (a) performed activities are (b) captured by a Kinect sensor. (c) After that, pose of humans are estimated by extracting joints. (d) To make raw data more usable, their salient and deﬁning features are identiﬁed. (e) Based on the similarities and diﬀerences, activities are discovered. (f) Afterwards, the system begins to learn from the discovered activities and (g) ﬁnally human activities are recognized."
2201.05314,"data, dataset",251,,,"into the local optimum. three diﬀerent datasets were used to assess the performance of our method. The results obtained show that HPGMK achieved an average overall accuracy of 77.53 %, 56.54 %, and 66.84 in datasets CAD60, UTK, and F3D respectively and validate the signiﬁcant superiority of HPGMK over other methods. In activities with high intra-class variables and corrupted data with sitting and standing positions, HPGMK has been eﬀective in activity discovery compared to other state-of-the-art methods. Because Gaussian mutation in HPGMK maintain good, diverse and meaningful particles to evolve, without losing the meaningful hidden structure during exploration. Moreover, the advantage of KM in exploitation has not only increased the eﬃciency of discovery, but also improved the convergence rate and reduced clustering time. This work paves the way towards the implementation of fully unsupervised human activity discovery in practical applications using skeleton-based data. There are various factors in the proposed methods can impact the eﬃcacy of the ﬁnal clusters. One factor is the number of clusters that ﬁts performed activities. This indicates the proposed HPGMK can be further extended to address the task of human activity discovery automatically by estimating the number of activities. Another factor is detecting outlier or noisy data. Outliers shift the cluster centers towards itself, thus aﬀecting optimal cluster formation. It can be beneﬁcial to use outlier detection methods in HPGMK to reject outliers."
2201.05314,"data, dataset",254,,,"Human activity discovery aims to distinguish the activities performed by humans, without any prior information of what deﬁnes each activity. Most methods presented in human activity recognition are supervised, where there are labeled inputs to train the system. In reality, it is diﬃcult to label data because of its huge volume and the variety of activities performed by humans. In this paper, a novel unsupervised approach is proposed to perform human activity discovery in 3D skeleton sequences. First, important frames are selected based on kinetic energy. Next, the displacement of joints, set of statistical, angles, and orientation features are extracted to represent the activities information. Since not all extracted features have useful information, the dimension of features is reduced using PCA. Most human activity discovery proposed are not fully unsupervised. They use pre-segmented videos before categorizing activities. To deal with this, we used the fragmented sliding time window method to segment the time series of activities with some overlapping. Then, activities are discovered by a novel hybrid particle swarm optimization with a Gaussian mutation algorithm to avoid getting stuck in the local optimum. Finally, k-means is applied to the outcome centroids to overcome the slow rate of PSO. Experiments on three datasets have been presented and the results show the proposed method has superior perfor ∗Corresponding author. Email addresses: 20h8561@ubd.edu.bn (Parham Hadikhani),"
2201.05314,dataset,112,,,"Three benchmark datasets were used to evaluate the eﬀectiveness of proposed method; Cornell Activity Dataset (CAD-60) [37], UTKinect-Action3D (UTK) [38], and Florence3D (F3D) [39]. Moreover, Other two datasets including Kinect Activity Recognition Dataset (KARD) [40] and MSR DailyActivity3D (MSR) [41] are also used to evaluate the proposed method that their results are provided as supplementary material to this paper. These datasets have diﬀerent dimensions, features, and activities. Table 1 shows the statistical information of these datasets. They are discussed as follows."
2201.05314,dataset,119,,,"CAD-60: This dataset includes 14 activities; rinsing mouth, brushing teeth, wearing contact lens, talking on the phone, drinking water, opening pill container, cooking (chopping), cooking (stirring), talking on couch, relaxing on couch, writing on whiteboard, still (standing), random, and working on computer. Each activity was performed by 4 subjects including one lefthanded person. They were performed in 5 diﬀerent environments: bathroom, It contains activities of cyclic bedroom, kitchen, living room, and oﬃce. nature such as brushing teeth and similar activities such as drinking water and talking on the phone."
2201.05314,dataset,137,,,"among the activities in the results of the proposed method. Moreover, instead of using all the joints to extract the relevant features which can increase redundancy and overlapping between activities, this method uses features from informative joints to diﬀerentiate the activities. In addition, the Gaussian mutation, with exploration capability has improved solution pool in the swarm that help to diﬀerentiate the diﬀerent similar activities. On the other hand, cluster overlapping appears relatively high in the other methods especially KM and SC in all datasets. In CAD-60 the largest error in the activity assignment occured between brushing teeth and random due to similar body gestures. There was also activity overlapping between brushing teeth, and wearing contact lenses, because of similar hand movement near the head."
2201.05314,dataset,183,,,"Fig. 10 to 12 show the average F-score for each activity for all subjects. By examining the average F-scores in all datasets , it shows HPGMK outperforming other methods in all datasets used. HPGMK achieved slightly under 75 % on CAD-60, just over 45 % on UTK, and almost 60 % on F3D. In Fig. 10, for HPGMK although F-score is high in most of the activities compared with other methods, Cooking (chopping) and Talking on the phone were discovered with low F-scores. This was due to the high similarity between Cooking (chopping) with Cooking (stirring) and Talking on the phone with Wearing contact lenses. In Fig. 11, a few actions were discovered with low F-scores. For example, throw samples were easily mistakenly regarded as push ones due to similar hand movements, resulting in low F-scores in both activities. In Fig. 12, we observe HPGMK outperformed other methods except for activities wave, clap and tight lace."
2201.05314,dataset,258,,,"Fig. 8 shows the accuracy of HPGMK with the state-of-the-art techniques for all subjects of each dataset based on the maximum, minimum and average accuracies. The average overall accuracy of the HPGMK was 77.53 % for CAD-60, 56.54 % for UTK, and 66.84 % for F3D. As seen in Fig. 8, HPGMK has the best performance in terms of maximum and average accuracy in all datasets. This proves the robustness and eﬀectiveness of the HPGMK for human activity discovery. By utilizing the Gaussian mutation and KM along with PSO, our approach brings signiﬁcant performance improvement compared to the other methods. As PSO sometimes gets stuck in local optimum, Gaussian mutation mitigate this. Additionally, applying KM enhances the exploration ability of PSO. Moreover, ENSC and SSC, which are subspace clustering algorithms, do not achieve good cluster quality compared to PSO and HPGMK because these algorithms do not use an eﬃcient search strategy [42]. These approaches did not maintain the balance between exploitation and exploration well and sometimes get stuck in the local solutions. Parameters are required to be set and ﬁnding the right values for them is tricky and complex such as size of subspace [43, 44]. In contrast, HPGMK uses the power of PSO in exploration, KM in exploitation, and proposed Gaussian mutation to prevent premature convergence which improve the exploration and exploitation capability as a whole."
2201.05314,dataset,3,,,4.1. Datasets
2201.05314,dataset,4,,,Frequency/Dataset CAD-60 UTK F3D
2201.05314,dataset,50,,,"[10] A. Shahroudy, J. Liu, T.-T. Ng, G. Wang, Ntu rgb+ d: A large scale dataset for 3d human activity analysis, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 1010– 1019."
2201.05314,dataset,64,,,"UTK: There are 10 activities in this dataset: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, and clap hands. These activities were performed by 10 subjects and repeated twice by each subject. The signiﬁcant intra-class and viewpoint variations are the main challenges of this dataset."
2201.05314,dataset,76,,,"F3D: This dataset includes 9 activities: wave, drink from a bottle, answer phone, clap, tight lace, sit down, stand up, read watch, and bow. These activities are repeated twice or thrice by 10 subjects. The challenge with this dataset is that activities are performed at high speed. This provides a small number of frames for the algorithm to sample and learn from."
2201.05314,"dataset, used dataset",15,,,"Table 1: Number of activities, subjects and videos in the three datasets used"
2201.06463,data,104,,,"The WK3 model is more ﬂexible and can ﬁt the observed data better, but it can lead to overestimation of the total arterial compliance, C (Segers et al., 2008). When the aortic valve is closed (Q(t) = 0), both models decay exponentially and become equivalent. Furthermore, an important connection between the two models is that in WK2 model R equals the ratio of mean pressure over mean ﬂow, while for the WK3 model this ratio is equal to R1 + R2 (Westerhof et al., 2009)."
2201.06463,data,111,,,"We have brieﬂy presented a fully Bayesian data-driven approach for quantifying the uncertainty in physical model parameters. We showed that extending the idea of physics-informed priors in a Bayesian framework accounts for imperfect models through the inclusion of discrepancy in the model formulation. The approach produces reasonable quantiﬁcation of uncertainty for model parameters and also reduces the uncertainty in model predictions. A potential problem is identiﬁability between the model parameters and the discrepancy function. This might be tackled by introducing constraints in the discrepancy prior (Brynjarsdóttir and O’Hagan, 2014; Riihimäki and Vehtari, 2010; Wang and Berger, 2016)."
2201.06463,data,245,,,"In this work we introduce a computational efﬁcient data-driven framework suitable for quantifying the uncertainty in physical parameters of computer models, represented by differential equations. We construct physics-informed priors for differential equations, which are multi-output Gaussian process (GP) priors that encode the model’s structure in the covariance function. We extend this into a fully Bayesian framework which allows quantifying the uncertainty of physical parameters and model predictions. Since physical models are usually imperfect descriptions of the real process, we allow the model to deviate from the observed data by considering a discrepancy function. For inference Hamiltonian Monte Carlo (HMC) sampling is used. This work is motivated by the need for interpretable parameters for the hemodynamics of the heart for personal treatment of hypertension. The model used is the arterial Windkessel model, which represents the hemodynamics of the heart through differential equations with physically interpretable parameters of medical interest. As most physical models, the Windkessel model is an imperfect description of the real process. To demonstrate our approach we simulate noisy data from a more complex physical model with known mathematical connections to our modeling choice. We show that without accounting for discrepancy, the posterior of the physical parameters deviates from the true value while when accounting for discrepancy gives reasonable quantiﬁcation of physical parameters uncertainty and reduces the uncertainty in subsequent model predictions."
2201.06463,data,28,,,"The observed pressure, yP and inﬂow, yQ data are modelled by the physics-informed prior corrupted by Gaussian i.i.d. noise εP and εQ respectively as follows"
2201.06463,data,337,,,"R1751001251501750.000.250.500.751.00timepressuremodelWK2WK3observed data we add i.i.d. noise to both inﬂow and pressure as follows, yQ = Q(t) + εQ, where εQ ∼ N (0, 42) and yP = PWK3(t) + εP , where εP ∼ N (0, 32). We develop physics-informed priors for the WK2 model as detailed in Section 2 (for more details on the priors see the Appendix). The ﬁrst model does not account for discrepancy (WK2 in Figures 2 and 3) while the second does (WK2 + δ(t) in Figures 2 and 3). We present the posterior distributions of the parameters and the predictions for the two models. In Figure 2 top row, we observe that if we don’t account for model discrepancy in the imperfect model (WK2), the posterior distributions of the physical parameters (R, blue and C, green) become over-conﬁdent about the wrong value and also the marginal standard deviation of the pressure noise, σP is overestimated. Accounting for discrepancy in the model formulation via a GP prior on the model discrepancy, results in more reasonable quantiﬁcation of uncertainty of physical parameters and also the noise parameters (see Figure 2, bottom row). Furthermore, in Figure 3 we see the model predictions for both pressure and inﬂow. The model without accounting for bias-correction (WK2) can not ﬁt the data well. This results in overestimated noise (Figure 2, ﬁrst row, third plot) and consequently increased uncertainty in model predictions. While accounting for model discrepancy (WK2 + δ(t)) the model estimates the noise accurately (Figure 2, second row, third plot) which means that it has learned the discrepancy between the two models and consequently reduces the uncertainty in model predictions."
2201.06463,data,39,,,We use physics-informed priors in a Bayesian Calibration framework where we allow the physical model to deviate from the observed data by incorporating a discrepancy function δu(t) for u(t). We have that
2201.06463,data,68,,,"For this experimental setup, we simulate noisy data from the WK3 model and use the simpler WK2 model as our modeling choice. More speciﬁcally, we use a given inﬂow, Q(t) to simulate three cycles of pressure from the WK3 model, PWK3(t, R1 = 0.05, R2 = 1, C = 1.1). To create the"
2201.06463,data,96,,,"Physical models are simpliﬁed representations of the real phenomenon under study and are commonly described by (systems of) differential equations. These models typically consist of observable inputs that can be measured through ﬁeld experiments and unknown model calibration parameters which are usually estimated by using the observed data and have concrete physical meaning. In this paper, our primary interest is to quantify the uncertainty of physical parameters by acknowledging that the model is an imperfect description of reality. Implementation of physical models is often referred to as computer models."
2201.06463,data,96,,,"This paper is motivated by the need for interpretable simpliﬁed personalized hemodynamic models for digital twins. Thus, the physical model we consider is the arterial Windkessel model which is a linear differential equation with two physically interpretable parameters, the total arterial compliance, C and the total peripheral resistance R. In a synthetic case study we simulate noisy data from a more complex model with known mathematical connections to our modeling choice, where the goal is to learn and quantify the uncertainty of the physical parameters and also produce bias-corrected predictions."
2201.08452,code,101,,,"5.1.1 in docker allows all the code being analyzed to be run in a sandbox, protecting the host machine. To allow input to npm-ﬁlter and access to the results ﬁles from running in docker, we have some special directories that the docker container has access to. All input ﬁles to running npm-ﬁlter in docker must be in a directory docker_configs in the npm-ﬁlter home directory (any user scripts, CodeQL queries, or custom conﬁguration ﬁles). Results ﬁles end up in the npm_filter_docker_results directory, which is also in the npm-ﬁlter home directory."
2201.08452,code,48,,,"CodeQL is a semantic code analysis language: with it, users can write static analyses for a variety of languages, including (most relevantly for npm-ﬁlter) JavaScript/TypeScript. In Section 6.2, we describe how this features was already used in an existing tool."
2201.08452,code,95,,,"6.3 Desynchronizer Gokhale et al. used npm-ﬁlter to collect projects to evaluate their tool for automatically migrating projects that use synchronous JavaScript APIs to use their asynchronous equivalents [11]. The tool, called Desynchronizer, statically detects calls to synchronous JavaScript APIs that have asynchronous equivalents (e.g., calls to readFileSync, rather than readFile)—then infers a call graph, and automatically refactors the code. In the evaluation, authors automatically applied every refactoring, and ran test suites post refactoring to establish any behavioural diﬀerences."
2201.08452,"code, github",53,,,"Thus, the evaluation undertaken in the paper requires running test suites, and npm-ﬁlter was used to ﬁlter a list of 40K JavaScript Github repositories with asynchronous JavaScript code to a much more manageable 450 projects that had running/passing tests. This work is being presented concurrently at ICSE Technical Track."
2201.08452,"code, package",121,,,"3.5 Results The results of all phases of npm-ﬁlter are output to a JSON ﬁle. This JSON results object is organized in a hierarchical structure corresponding to the aforedescribed phases of execution. Any errors in an execution phase are reported in the corresponding ﬁeld of the results. The output ﬁle is named [package name]__results.json. If the user speciﬁes CodeQL queries to be run over the package source code, the output of each of these queries is output to a CSV ﬁle, named [package name]__[query name]__results.csv. Any errors in the CodeQL query execution would be reported in the CodeQL ﬁeld of the JSON results."
2201.08452,"code, package",183,,,"npm provides a wealth of metadata for all of the projects it hosts, including the number of weekly downloads, dependencies, dependents, and a link to the associated code repository. This said, running application code can reveal yet more useful information, such as if the package is equipped with a test suite, passing, failing, ﬂaky tests, etc. But even though it is relatively straightforward to install, build, and test an npm package, in our anecdotal experience conducting JavaScript tool evaluations, we found that only (roughly) <5% of npm packages have running test suites with no failing tests. npm-ﬁlter can be used in any scenario where metadata about the execution of JavaScript code is required. A list of npm projects or JavaScript repositories (e.g., from GHTorrent [12], CodeDJ [18], or scraping npm), can be fed into npm-ﬁlter to gather dynamic metadata by trying to install, build, and run package tests."
2201.08452,"code, package",27,,,• rm_after_cloning: delete the package source code after the tool is done analyzing it. Strongly recommended if running over a large batch of packages.
2201.08452,"code, package",4,,,package source code.
2201.08452,"code, package",5,,,the package source code.
2201.08452,"code, package",93,,,"3.1 Package Setup and Installation Supplied with an npm package name, npm-ﬁlter scrapes the repository link from the npm package page. The source code is then downloaded (with a git clone). If the user speciﬁed a particular commit to be analyzed, then the source code is checked out at this commit. If there is no repository link found on the page or if there is an issue with the cloning, then npm-ﬁlter bails out at this stage and reports the error to the user."
2201.08452,"code, package, code available, data, code package",88,,,"Once the code has been downloaded, the package dependencies are installed 1. The list of transitive dependencies can be a useful piece of data: for example, [30] show that transitive dependencies can contain vulnerabilities that compromise the package itself. npm-ﬁlter computes this list by reporting the list of all packages in the node_modules directory after the install phase has completed. There is also an option to exclude devDependencies, which are dependencies excluded from production distributions of the package."
2201.08452,"code, package, code available, github, code package",111,,,"ABSTRACT The static properties of code repositories, e.g., lines of code, dependents, dependencies, etc. can be readily scraped from code hosting platforms such as GitHub, and from package management systems such as npm for JavaScript; Although no less important, information related to the dynamic properties of programs, e.g., number of tests in a test suite that pass or fail, is less readily available. The ability to easily collect this dynamic information could be immensely useful to researchers conducting corpus analyses, as they could differentiate projects based on properties that can only be observed by running them."
2201.08452,"code, package, code package, code available",140,,,"npm-ﬁlter is a tool for automatically installing, building, and testing sets of npm packages. npm is a major repository for JavaScript library code, and already contains a wealth of static metadata about JavaScript projects (and, if available, a link to the code). npm-ﬁlter complements this by generating information such as how a project is built, if and how it is tested, the number of passing/failing tests, and the list of transitive dependencies. npm-ﬁlter runs package code in a sandbox for added security and to ensure reproducibility of results. Users can also specify custom scripts to run over the source code of the package. As far as we know, there is no similar framework to automatically build, run, and test npm packages."
2201.08452,"code, package, code package, code available",34,,,"comprises various phases of execution, which correspond to the tasks required to set up and test an npm package, and running any user-speciﬁed scripts over the package’s source code."
2201.08452,"code, package, code package, code available",71,,,"3.4 Running Custom Scripts and CodeQL In addition to the metadata collected about the package build and test suite, users can also specify shell scripts and CodeQL [10] static analysis queries to be run over the source code of the package. The scripts are run in the sequence speciﬁed, and any terminal output of each of them is included in the results, including errors."
2201.08452,"code, package, download, code available",61,,,"download, install, build, test, and run custom user scripts over the source code of JavaScript projects available on npm, the most popular JavaScript package manager. In addition to describing the implementation and usage of npm-ﬁlter, we also show that it has already been useful in developing evaluation suites for three separate JavaScript tools."
2201.08452,"code, package, download, code available",70,,,"In this paper, we present npm-ﬁlter, an automated tool that can download, install, build, test, and run custom user scripts over the source code of JavaScript projects available on npm, the most popular JavaScript package manager. We outline this tool, describe its implementation, and show that npm-ﬁlter has already been useful in developing evaluation suites for multiple JavaScript tools."
2201.08452,"code, repo",54,,,"5 NPM-FILTER USAGE In this section, we explain how to use npm-ﬁlter and give some examples of usage. To follow along, clone the source code linked above; all example commands are run from the root of the repo. We have also included a minimal example usage tutorial here3."
2201.08452,"data, code, github",131,,,"1 INTRODUCTION Many code hosting platforms contain a wealth of useful metadata: e.g., GitHub lists code authors, commits, and general project history, and library repositories (such as npm for JavaScript) often contain information on dependencies and dependents. Although it can be readily scraped from the web, this metadata is static, and does not tell you much about running the actual code. We thus deﬁne dynamic metadata to be information gleaned from program executions: e.g., number of running tests, code coverage of tests, performance, memory usage, etc. Making said dynamic metadata available can enable new corpus analyses, focused on data pertaining to program executions—this is the purpose of our tool, npm-ﬁlter."
2201.08452,dataset,56,,,"[12] Georgios Gousios. 2013. The GHTorrent dataset and tool suite. In Proceedings of the 10th Working Conference on Mining Software Repositories (San Francisco, CA, USA) (MSR ’13). IEEE Press, Piscataway, NJ, USA, 233–236. http://dl.acm.org/citation.cfm?id=2487085.2487132"
2201.08452,github,137,,,"6.2 Nessie Arteca et. al built a test generator for JavaScript APIs with callback arguments [2]. In this project, they wrote a static analysis in CodeQL, to identify pairs of nested calls to functions that were part of the APIs the test generator was targeting. Then they used the CodeQL plugin feature of npm-ﬁlter to run this analysis on 13.6K JavaScript projects on GitHub. The results of this CodeQL query, amalgamated across all 13.6K projects, was used to inform the test generator of common pairs of nested API calls, to generate tests more representative of developers’ use of the APIs. They also used npm-ﬁlter to select projects to evaluate the test generator. This work is being presented concurrently at ICSE Technical Track."
2201.08452,github,15,04/21/22,0,-- repo_link_a nd _S HA https :// github . com / streamich / memfs
2201.08452,github,21,04/21/22,0,"5.2 Basic usage This tool can either take JavaScript packages speciﬁed as GitHub repository links, or as npm packages."
2201.08452,github,25,04/21/22,0,""" repo_link "" : "" https :// github . com / streamich / memfs "" , "" repo_commit_S HA "" : REDACTED FOR LENGTH"
2201.08452,github,6,,,3https://github.com/emarteca/npm-ﬁlter/blob/master/Tutorial.md 4https://hub.docker.com/r/emarteca/npm-ﬁlter
2201.08452,github,66,,,"8 CONCLUSION npm and GitHub contain a wealth of metadata related to static JavaScript project properties, but augmenting this static information with dynamic properties such as the number of tests in a test suite that pass or fail is immensely useful to researchers conducting corpus analyses or testing program transformation tools. In this paper, we presented npm-ﬁlter, an automated tool that can"
2201.08452,github,7,,,5Default conﬁguration: https://github.com/emarteca/npm-ﬁlter/tree/master/conﬁgs.
2201.08452,github,91,,,"[8] Facebook. 2022. jest. https://jestjs.io/. Accessed: 2022-01-20. [9] OpenJS Foundation. [n.d.]. Node.js. https://nodejs.org/en/. Accessed 2020-08-27. [10] GitHub. 2022. CodeQL. https://github.com/github/codeql. Accessed: 2022-01-20. [11] Satyajit Gokhale, Alexi Turcotte, and Frank Tip. 2021-10-20. Automatic migration from synchronous to asynchronous JavaScript APIs. Proceedings of the ACM on programming languages. 5, OOPSLA (2021-10-20)."
2201.08452,"github, open-source",31,,,"npm-ﬁlter is open source and includes a detailed Readme, with more examples than are included in this paper. npm-ﬁlter is available at https://github.com/emarteca/npm-ﬁlter/ [3]."
2201.08452,"github, package",64,,,"package information because of the rate limiter, but if a user is analyzing a large number of packages they will see a signiﬁcant performance hit compared to running on the GitHub repos directly. Thus, we also provide an option for users to pass a list of GitHub repos instead of npm packages to be analyzed, skipping the scraping entirely."
2201.08452,"github, repo",11,04/21/22,0,More examples are included in the npm-ﬁlter GitHub repo Readme.
2201.08452,"github, repo",12,04/21/22,0,"To run npm-ﬁlter over GitHub repo links, use the following:"
2201.08452,"github, repo",24,04/21/22,0,• repo_link: a link to a single GitHub repo to be analyzed • repo_link_and_SHA: link to a GitHub repo followed by a
2201.08452,"github, repo",42,04/21/22,0,"• repo_list_file: a ﬁle containing a list of GitHub repo links to be analyzed. Each line of the input ﬁle must specify one repo link, with an optional whitespace delimited commit SHA to check the repo out at."
2201.08452,"github, repo, package",39,,,"5.2.1 Example Usage. What follows is an example of basic usage. This example runs on a single package, speciﬁed by GitHub repo and at a speciﬁc commit (to ensure consistency of expected output)."
2201.08452,"github, repo, package",43,,,"7 NPM-FILTER LIMITATIONS We currently only support packages hosted on GitHub: if there is no GitHub repo link available on the package page, then npm-ﬁlter will not work. In our use cases we have found this to be rare."
2201.08452,package,10,,,• track_deps: speciﬁes to compute the package dependen cies
2201.08452,package,110,,,"3.3 Testing a Package Next, npm-ﬁlter determines if the package has a test suite, and if so computes some dynamic metadata—this is the test phase. package.json is further parsed, this time to ﬁnd the test commands. By default, these are the common ones we observed: “test”, “unit’ ’, “cov”, “ci”, “integration”, “lint”, “travis”, “e2e”, “bench”, “mocha”, “jest”, “ava”, “tap”, “jasmine”2."
2201.08452,package,113,,,"• packages: list of npm packages to analyze. Required argu ment, and at least one package must be passed. • config: path to a conﬁguration ﬁle for the tool • html: path to an html ﬁle that represents the npm page for the package that is speciﬁed to be analyzed. This option only works for one package, so if you want to use this option on multiple packages you’ll need to call the tool in sequence. • output_dir: path to a directory in which to output the results ﬁles (note: this only works when not running in docker)"
2201.08452,package,14,,,[22] prettier. 2022. prettier. https://www.npmjs.com/package/prettier. Accessed:
2201.08452,package,14,,,[28] webpack. 2022. webpack. https://www.npmjs.com/package/webpack. Accessed:
2201.08452,package,150,,,"To determine the build commands, npm-ﬁlter looks at the package’s package.json ﬁle and ﬁnds the available commands matching our tracked build commands. By default, these are “build”, “compile”, and “init” (the most common build commands in our experience). However, users can also customize the build commands tracked with a custom conﬁguration ﬁle (discussed in Section 5.3). If there is an error running a particular build command, the problematic command is added to the end of the command list; this way, the command can be run after potentially prerequisite commands. If all the build commands in a list have errors, then npm-ﬁlter bails out (to avoid inﬁnite cycling) but continues to the testing phase anyway, reporting the build error in the results."
2201.08452,package,16,,,[29] xx. 2022. xx. https://www.npmjs.com/package/xx. Accessed: 2022-01-20.
2201.08452,package,17,,,[19] mocha. 2022. mocha. https://www.npmjs.com/package/mocha. Accessed: 2022 01-20.
2201.08452,package,17,,,[7] eslint. 2022. eslint. https://www.npmjs.com/package/eslint. Accessed: 2022-01 20.
2201.08452,package,18,,,[23] Rollup. 2022. Rollup. https://www.npmjs.com/package/rollup. Accessed: 2022 01-20.
2201.08452,package,22,,,"Dependencies. package dependency tracking (this is the libraries the current package depends on, both directly and transitively)."
2201.08452,package,26,,,3 NPM-FILTER DESIGN We will describe the overall design of npm-ﬁlter by way of describing the steps involved in analyzing a given npm package. Analysis
2201.08452,package,28,,,lab. https://www.npmjs.com/package/@hapi/lab. Accessed: 2022 [16] Istanbul. 2022. nyc. https://www.npmjs.com/package/nyc. Accessed: 2022-01 20.
2201.08452,package,30,,,[24] scrapy. 2022. scrapy. https://scrapy.org/. Accessed: 2022-03-21. [25] standard. 2022. standard. https://www.npmjs.com/package/standard. Accessed:
2201.08452,package,34,,,[13] gulp. 2022. gulp. https://www.npmjs.com/package/gulp. Accessed: 2022-01-20. [14] gulp eslint. 2022. gulp-eslint. https://www.npmjs.com/package/gulp-eslint. Ac cessed: 2022-01-20.
2201.08452,package,35,,,[20] npm. [n.d.]. npm. https://www.npmjs.com/. Accessed 2020-08-27. [21] palantir. 2022. tslint. https://www.npmjs.com/package/tslint. Accessed: 2022 01-20.
2201.08452,package,46,,,[4] ava. 2022. ava. https://www.npmjs.com/package/ava. Accessed: 2022-01-20. [5] c8. 2022. c8. https://www.npmjs.com/package/c8. Accessed: 2022-01-20. [6] coveralls. 2022. coveralls. https://www.npmjs.com/package/coveralls. Accessed:
2201.08452,package,5,,,Install. package installation.
2201.08452,package,51,,,"[26] tap. 2022. tap. https://www.npmjs.com/package/tap. Accessed: 2022-01-20. [27] Alexi Turcotte, Michael D. Shah, Mark W. Aldrich, and Frank Tip. 2022. DrAsync: Identifying and Visualizing Anti-Patterns in Asynchronous JavaScript. In ICSE ’22."
2201.08452,package,6,,,Build. package compile/build stage.
2201.08452,package,6,,,Test. package test stage.
2201.08452,package,68,,,"3.2 Building a Package Once installed, some npm packages have additional commands that need to be run before the package is operational: we call this the build phase. For instance, packages written in TypeScript need to be compiled to JavaScript, and another common build step is the application of a bundler such as rollup [23] or webpack [28]."
2201.08452,package,7,,,supports both npm and yarn package managers
2201.08452,package,8,,,jasmine. https://www.npmjs.com/package/jasmine. Accessed:
2201.08452,package,86,,,"If the package uses a testing tool that we have not implemented output parsing for, then it might not be properly tracked. That said, we have covered the most popular JavaScript test ecosystems. Also, if the package uses build/test commands that don’t include the substrings we expect, then they won’t be run. Note, however, that users can customize their npm-ﬁlter conﬁguration to add or remove as many tracked commands as they want."
2201.08452,"package, download",151,,,"2 BACKGROUND & MOTIVATION Node.js [9] is an eminently popular JavaScript runtime, particularly for server-side JavaScript, and while JavaScript is best known as a front-end, client-side language, it is rapidly gaining in popularity for server-side development [1]. npm [20] is the most popular package ecosystem for Node.js applications: with the npm command-line interface (CLI) installed, a developer needs only navigate to the root of their project and npm install <package-name> to download and install any package they desire. JavaScript packages have a package.json ﬁle in which users can specify commands that can be run by npm, e.g., many developers will specify a test command that describes how a package’s test suite is run, then a user can execute the package’s tests with npm run test."
2201.08452,python,26,,,./ runDocker . sh python3 src / d i a g n o s e _ g i t h u b _r ep o. py
2201.08452,python,26,,,./ runDocker . sh python3 src / d i a g n o s e _ g i t h u b_ r ep o. py
2201.08452,python,26,,,./ runDocker . sh python3 src / d i a g n o s e _ n p m _ p a ck ag e. py
2201.08452,python,53,,,"4 IMPLEMENTATION npm-ﬁlter is written in Python. All the npm commands we run are done by dispatching with the Python subprocess library; this allows us to parse the output, and specify a timeout. It also doesn’t crash npm-ﬁlter if there is any error in the subprocess."
2201.08452,"python, code, package",130,,,"The back end of npm-ﬁlter’s npm package analyzer is a web scraper: given the name of an npm package, it ﬁnds the associated repository link on the npm page so that it can analyze the package’s source code. The scraper is built using Python’s scrapy library [24], which allows us to include custom middleware to run if the scraper gets an error code as a response from the site. We implemented some middleware to deal with errors caused by the rate limiting on the npm site: if the site returns an error indicating that too many requests were received, the scraper pauses and then retries. This middleware ensures that the scraper will not miss"
2201.08452,repo,19,,,"All arguments are optional, although npm-ﬁlter will not do any thing if no repo links are speciﬁed."
2201.08452,repo,8,,,space-delimited commit SHA to analyze the repo at
2201.09144,data,15,,,Background Invariant Classiﬁcation on Infrared Imagery by Data Efﬁcient Training and Reducing Bias in CNNs
2201.09144,data,19,,,"Figure 4: Synthetic IR data samples of APC,tank and truck without background (Masked)."
2201.09144,data,207,,,"Even though convolutional neural networks can classify objects in images very accurately, it is well known that the attention of the network may not always be on the semantically important regions of the scene. It has been observed that networks often learn background textures which are not relevant to the object of interest. In turn this makes the networks susceptible to variations and changes in the background which negatively affect their performance. We propose a new two-step training procedure called split training to reduce this bias in CNNs on both Infrared imagery and RGB data. Our split training procedure has two steps: using MSE loss ﬁrst train the layers of the network on images with background to match the activations of the same network when it is trained using images without background; then with these layers frozen, train the rest of the network with cross-entropy loss to classify the objects. Our training method outperforms the traditional training procedure in both a simple CNN architecture, and deep CNNs like VGG and Densenet which use lots of hardware resources, and learns to mimic human vision which focuses more on shape and structure than background with higher accuracy."
2201.09144,data,29,,,"to efﬁciently learn from limited available data on infrared imagery and RGB data without relying on deep models, at the same time reducing background bias in CNNs."
2201.09144,data,33,,,Table 1: Test accuracies for each architecture and training method on synthetic IR (infrared) data. Numbers in parentheses are the standard deviation of the accuracies over 10 runs.
2201.09144,data,337,,,"IEEE. Chen, Y.; Li, J.; Xiao, H.; Jin, X.; Yan, S.; and Feng, J. 2017. Dual path networks. arXiv preprint arXiv:1707.01629. Everingham, M.; Eslami, S. A.; Van Gool, L.; Williams, C. K.; Winn, J.; and Zisserman, A. 2015. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1): 98–136. Geirhos, R.; Rubisch, P.; Michaelis, C.; Bethge, M.; Wichmann, F. A.; and Brendel, W. 2018. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231. Huang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger, K. Q. 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4700–4708. Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and Chen, L.-C. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4510–4520. Sehwag, V.; Oak, R.; Chiang, M.; and Mittal, P. 2020. Time for a Background Check! Uncovering the impact of Background Features on Deep Neural Networks. arXiv preprint arXiv:2006.14077. Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, 618–626. Simonyan, K.; and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556."
2201.09144,data,343,,,"References Arif, M.; and Mahalanobis, A. 2021. Few Shot Learning For Infra-Red Object Recognition Using Analytically Designed Low Level Filters For Data Representation. In 2021 IEEE International Conference on Image Processing (ICIP), 1079–1083. IEEE. Chen, Y.; Li, J.; Xiao, H.; Jin, X.; Yan, S.; and Feng, J. 2017. Dual path networks. arXiv preprint arXiv:1707.01629. Everingham, M.; Eslami, S. A.; Van Gool, L.; Williams, C. K.; Winn, J.; and Zisserman, A. 2015. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1): 98–136. Geirhos, R.; Rubisch, P.; Michaelis, C.; Bethge, M.; Wichmann, F. A.; and Brendel, W. 2018. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231. Huang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger, K. Q. 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4700–4708. Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and Chen, L.-C. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4510–4520. Sehwag, V.; Oak, R.; Chiang, M.; and Mittal, P. 2020. Time for a Background Check! Uncovering the impact of Background Features on Deep Neural Networks. arXiv preprint arXiv:2006.14077. Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization."
2201.09144,data,59,,,"The premise is that by design the primary network learns and responds to target information (since there is no background). Forcing the activations of the primary and secondary network to be similar encourages the latter to re Figure 3: Synthetic IR data samples of APC,tank and truck with background (Unmasked)."
2201.09144,"data, dataset",106,,,"In the introduction section, we mentioned that the GradCAM output from the model trained on images without background yielded a more appealing output compared to the Grad-CAM output from the model trained using background. Figure 9 shows the Grad-CAM outputs for the model trained with our split training method on the infrared dataset. We see that since our method trains the model to match the activations of the primary model, the Grad-CAM/heatmaps closely match. We also observe the method’s performance on MS-COCO (RGB) data as demonstrated in Figure 8. Bottom row shows test images and top"
2201.09144,"data, dataset",151,,,"Introduction Deep CNNs with millions of parameters are able to learn complex patterns by developing low-level feature understanding leading to high-level scene understanding. However, training such deep networks requires huge amounts of training data that in turn requires too many computing resources. The very popular VGG (Simonyan and Zisserman 2014) was trained using 4 NVIDIA Titan Black GPUs,and it took 2–3 weeks to train the architecture on 1.3 million images of the ImageNet dataset. When the dataset is limited in size, the network is often unable to learn the semantically important regions of the scene for object classiﬁcation. An example of this occurs for infrared object recognition where the data sets are relatively small, and applying transfer learning to networks trained with RGB data does not work well. Motivated by such issues, we propose a split training method"
2201.09144,"data, dataset",165,,,"spond to the target and thereby learn to ignore the background. The proposed approach is also shown in Algorithm 1. Our method differs from transfer learning methods in the following way. When ﬁnetuning a model pretrained on some dataset D1 with another target dataset D2, the convolutional layers are frozen with features learned from D1 using large amounts of data, while the fully connected layers are ﬁnetuned to classify samples from D2. In our split training method, all of the layers are trained using samples from the target dataset D2. The model ﬁrst starts with randomly initialized weights. Then the convolutional layers of the secondary model are trained to learn features from D2 to match the activations of the primary model trained on D1. Then, with those convolutional layers frozen, the fully connected layers are trained to learn weights to classify samples from D2 given the features from the frozen convolutional layers."
2201.09144,"data, dataset",180,,,"Datasets Synthetic IR data - (Infrared Imagery): The ﬁrst dataset we use for our experiments is a synthetic infrared dataset of military vehicles. The dataset contains three classes/targets: APC, tank, and truck. Each of the targets are in 18 different backgrounds and thermal signatures, and viewed in various angles, with azimuth angles between 0◦ and 359◦, and elevation angles of 15◦, 30◦, 45◦, 60◦, 75◦, and 90◦. This gives a total of 38880 images per class, and thus a total of 116640 images for the entire dataset. Examples from the dataset are shown in Figure 3. Our dataset also contains masks that we can use to remove the background of each target. Figure 4 shows the same images as Figure 3, but with the background removed by using the mask. The masked dataset will be used to train our primary model, while the dataset including the background will be used to train the secondary model."
2201.09144,"data, dataset",205,,,"Related Work Different works have tried to address the background bias problem. (Chen et al. 2017) propose a dual path network (DPN) which outperforms both ResNET and DenseNet deep CNNs. In particular, a shallow DPN with 26% smaller model size surpasses ResNNXt-101 performance on ImageNet dataset which shows that just because a network is deep, it still might not be learning the best representations of the data. Further, it also uses 25% less computational power and 8% lower memory hence reducing the overall carbon footprint. (Xiao et al. 2020) study conﬁrms that standard image classiﬁcation models rely too much on signals from background and ignore foreground. Background correlations are largely predictive and inﬂuence model decisions to a great extent. Models often misclassify images even when correct foregrounds are present, up to 87.5% of the time when random adversarial backgrounds are chosen. The backgrounds become a strong point of correlation between images and their labels which the authors show comprehensively via experiments involving ImageNet dataset with modiﬁed backgrounds. The backgrounds are created in a very interesting fashion. Some subsets include unmodiﬁed"
2201.09144,"data, dataset",212,,,"Ablations and Comparisons On synthetic IR dataset , we compare our method with the conventional training method using only cross entropy loss, and also transfer learning. For our method, we experimented with the last feature layer for the MSE loss, along with an arbitrary chosen layer between the input layer and the last feature layer. We benchmarked our method on 4 different models: a simple CNN, Mobilenet (Sandler et al. 2018), VGG (Simonyan and Zisserman 2014), and DenseNet (Huang et al. 2017). To clarify, we only used the architecture of these deep models which are known to work really well on RGB data. When we say we did transfer learning, we trained the deep model ﬁrst using masked images of our IR dataset only and then ﬁnetuned using unmasked images and reported those results. We did not use pre-trained weights, trained on RGB - ImageNet data since they do not work well on Infrared imagery (Arif and Mahalanobis 2021). Further, we do not want to employ a method that adds to the carbon footprint during training and uses lots of hardware resources/GPUs and memory."
2201.09144,"data, dataset",221,,,"Computation Time: We also want to note the computation time for the different methods and training strategies. Table 4 shows these statistics. A simple network using our method takes less than 36000 s ˜10 hrs to train on 3 classes of the infrared data. MobileNet is known to be nearly as accurate as VGG and is 32 times smaller and 27 times less computationally expensive (Sandler et al. 2018). VGG network alone took around 2-3 weeks when being trained on ImageNet, so whatever dataset it is ﬁnetuned on will cumulatively take longer than 1,209,600 s ˜336 hours. In our exclusive case, since we did not use pretrained weights and trained on VGG11 architecture from scratch, the depth and additional parameters meant longer training time than 10 hrs. Densenet networks are the most memory hungry networks among all popular deep models. Further, they are much deeper than VGG or MobileNet architectures. Computation time to train ImageNet dataset on Densenet is similar to VGG networks or more . Hence cumulative time to ﬁnetune on another dataset would be greater than 14 days ˜1,209,600 s ˜336 hours also. Training from scratch took longer than a simple CNN network that we proposed for our split training method."
2201.09144,"data, dataset",226,,,"MS-COCO On MS-COCO (RGB data) , we perform 2 experiments. First, we use only 3 classes as a starting point. We use 6k training images and 300 test images. We use a simple CNN as our shallow base network. It consists of 4 Conv-BatchNorm-ReLU-MaxPool blocks as also illustrated in Figure 2. We ﬁrst train this simple CNN using our masked images for 3 classes to obtain ground truth activations and also obtain the best model which focuses only on object since there is no background to get confused with. We use Adam optimizer with a learning rate of 1e-3 for the ﬁrst 40 epochs and then 1e-4 for the last 20 epochs. We use a batch size of 32. We train it for 60 epochs. Since images in the dataset are of varying sizes , we resize all of them to 160 x 120 before training. To know what our baseline classiﬁcation score is i.e. training via conventional method , we also train the same network using unmasked images.We keep all other hyperparameters the same for consistency. Lastly, we perform the experiment using our split training method. We ﬁrst train the convolutional blocks using our proposed approach till the last feature layer and mean square error loss function."
2201.09144,"data, dataset",25,,,Table 2: Test accuracies on synthetic IR (infrared) data for each architecture and training method on the random background test dataset.
2201.09144,"data, dataset",288,,,"MS-COCO (RGB data) : We also illustrate our work using RGB data to show our method’s validity. MS-COCO is a main-stream computer vision dataset for object detection, segmentation and classiﬁcation. Figure 5 and 6 show examples of unmasked and masked images which we use in our experiments. MS-COCO has 91 categories in all, with 81 suitable for segmentation. These object categories are grouped into 11 super categories. We make use of 10 of the 81 classes for instance segmentation and perform 2 experiments - one involving 3 classes and one involving 10. These classes include airplane, dogs, elephant, motorcycle, bus, giraffe, umbrella, tie, tennis racket and clock. In order to avoid confusion between classes, we selected training images which have only one object. However, we do end up with few images having multiple instances of the same class (and multiple classes) as can be seen in Figure 5. We use a total of 20k training images (2k per class) and 1k test images. To create the set of masked images i.e. with background removed (Figure 6 ), we use Pycoco (VP 2020) library and the ground truth annotations provided with the dataset. We mask the background by replacing the corresponding pixel values by zero. MS-COCO is a complex dataset with much of the images containing a lot of background, as a result of which the object of interest appears to be very small and not in the center. This becomes a good challenging point for our proposed method."
2201.09144,"data, dataset",296,,,"Random background Test Data For each class, we take 3 subsets of thermal signatures to use for our test set. When choosing our subsets, since some backgrounds may look similar, we visually check them in order to avoid validating on images that may look similar to the training set. This gives us about a 83- 17 training-test split. Figure 7 shows the 9 subsets of the data we choose, which are viewed with 0◦ azimuth and 90◦ elevation. From top to bottom, the rows correspond to APC, tank, and truck class. Since our goal is to have the model focus on the target and ignore the background, we also validate on the synthetic IR dataset, by replacing the background with a random solid color. That way, we hope that if our method does actually learn to ignore the background, the model should also do well on the dataset with random background. We initialize each model per training method with the same weights for uniformity. We ﬁnd average of test accuracies on this random background data for each model across 10 runs, and report the mean and standard deviation in Table 2. Here we show the validation accuracy for the architectures and training methods on the random background test set. We see similar results to Table 1. The ﬁnetuning validation accuracy for the random background test set is higher than the ﬁnetuning accuracy on the regular validation set. This could be due to the pretrained model being trained on images with no background and random test set (mix set) also containing images with no background(random solid color)."
2201.09144,"data, dataset",343,,,"Different works have tried to address the background bias problem. A very recent work by (Sehwag et al. 2020) explicitly studies the effects of background on the classiﬁcation performance of deep CNNs. Their focus is on background invariance and background inﬂuence , which they observe by switching backgrounds and by masking the foreground, and then performing classiﬁcation respectively. They show using a series of experiments on deep CNNs (like ResNet18) and diverse datasets (like ImageNet), that even though the foreground is masked/removed, the network can still correctly identify the class. Similarly, when the background is switched and foreground remains intact, test accuracy decreases. On the VOC12 dataset (Everingham et al. 2015) , test accuracy decreases from 75% to 46% when different backgrounds are introduced and object of interest or foreground is kept the same. So the question is, are the current training paradigms being used in deep learning the only way to train a deep CNN? What is the CNN actually using when it makes a prediction? And how can available data be used efﬁciently for better test accuracy? To answer these questions, we propose a novel training strategy called ‘Split training’ which aims to teach the model to focus on the object (and ignore the background) by matching its activations at some speciﬁed layer to that of another model which was trained on masked (background removed) images and consequently produces ideal activations as shown in Figure 2. Our method outperforms the standard training method and transfer learning methods, and produces activation maps that show the object is used to make predictions rather than texture and background. In the following sections, we describe some related studies, our synthetic Infrared (grayscale) and RGB dataset which we use in our research, our proposed method and then various experimental results and ablation studies."
2201.09144,"data, dataset, dataset provided",213,,,"Synthetic IR data We perform 2 sets of experiments using the synthetic IR data - one set of experiments use a train/test split involving 120k original images using the data set as provided, and the second using images in random background. We use a simple CNN architecture which is a feedforward network with 4 Conv-BatchNorm-ReLU-MaxPool blocks. We train our network in 3 ways, one using masked images which gives us our ground truth activations, second using unmasked images and standard training procedure and then lastly, using unmasked images together with our split training method. For our split training method, we use the last feature layer as well as an intermediate layer for comparison. We use Adam optimizer and learning rate of 1e-3 to train our initial layers using Mean Square Error(MSE) loss function. Once this part of the network is trained, we freeze the weights and train the top classiﬁer part using Cross Entropy(CE) loss. We also compare our results with other architectures, details of which are ahead under section Ablations and Comparisons. This is followed by another set of experiments where we modify our test set and make it more challenging."
2201.09144,"data, dataset, github",109,,,"We have shown that our split training method outperforms the standard(conventional) training and ﬁnetuning methods on Infrared and RGB data. When we split the training into two steps: minimize the difference between the activations of a primary model and a secondary model using MSE, and minimize the ﬁnal probabilities using CE loss, our model learns to focus on the object, which is shown by the GradCAM outputs. For our datasets, learning to focus on the object yielded better classiﬁcation performance subsequently. Acknowledgements The authors would gratefully like to acknowledge the technical feedback of Dr.Nazanin Rah https://github.com/virafpatrawala/"
2201.09144,dataset,113,,,"Preprocessing : The infrared images have a different pixel range compared to color(RGB) images. Infrared images use 16 bits per pixel, and in the dataset, the pixel values range from about 400 to 4000. For preprocessing, we cen Figure 5: MS-COCO sample images with background (Unmasked) - First row : Airplane(Left and second last to Right), Dog (Second to left and Right); 2nd row (L to R): Elephant, Motorcycle,Bus,Giraffe ; 3rd row (L to R): Umbrellas, Tie,Tennis racket,Clock."
2201.09144,dataset,29,,,Figure 8: Comparision of Grad-CAM heatmaps obtained using a CNN trained with our split training method and conventional methods on masked and unmasked versions of MS-COCO dataset.
2201.09144,dataset,332,,,"(Zhu, Xie, and Yuille 2016) exploit the visual hints in an image by learning them explicitly and adding them in a conventionally trained CNN model to increase test classiﬁcation. They also show through a series of experiments how deep CNNs achieve high performance by relying on visual contents of the whole image, mostly the background. By contrast humans tend to learn and classify objects based on foreground which is shown by conducting human recognition experiments on either pure background or pure foreground (created using bounding boxes). The results indicate that human beings are outperformed by networks trained on background but are able to beat deep CNNs when trained on foreground implying that deep CNNs fail to classify images based on object shape and structure. A recent work that indicates how CNNs are biased by texture is (Geirhos et al. 2018). The authors demonstrate texture bias using all major deep CNN models and experiment with stylized versions of ImageNet dataset, similar to the way done in (Xiao et al. 2020). They create grayscale versions of objects which contain both shape and texture, silhouette images where object outline is ﬁlled with black color, images with only edges and lastly images with contrasting shapes and textures example a cat with texture of an elephant. All popular deep CNN for image classiﬁcation fail to recognize shape and do better with texture, VGG-16 gives 17.2% accuracy using shape vs. 82.8% when using texture; GoogleNet gives 31.2% using shape vs. 68.8% when using only texture and ResNet-50 gives 22.1% accuracy using shapes as compared to 77.9% when using texture. Human observers do much better as expected with 95.9% accuracy when shapes are clearly deﬁned. These results conﬁrm our theory and are a motivation behind our proposed training method which aims to guide a network"
2201.09144,dataset,344,,,"background but black foreground and some include unmodiﬁed background but tiled foreground. Some subsets also involve black background and unmodiﬁed foreground which we will use in our proposed training method. (Zhu, Xie, and Yuille 2016) exploit the visual hints in an image by learning them explicitly and adding them in a conventionally trained CNN model to increase test classiﬁcation. They also show through a series of experiments how deep CNNs achieve high performance by relying on visual contents of the whole image, mostly the background. By contrast humans tend to learn and classify objects based on foreground which is shown by conducting human recognition experiments on either pure background or pure foreground (created using bounding boxes). The results indicate that human beings are outperformed by networks trained on background but are able to beat deep CNNs when trained on foreground implying that deep CNNs fail to classify images based on object shape and structure. A recent work that indicates how CNNs are biased by texture is (Geirhos et al. 2018). The authors demonstrate texture bias using all major deep CNN models and experiment with stylized versions of ImageNet dataset, similar to the way done in (Xiao et al. 2020). They create grayscale versions of objects which contain both shape and texture, silhouette images where object outline is ﬁlled with black color, images with only edges and lastly images with contrasting shapes and textures example a cat with texture of an elephant. All popular deep CNN for image classiﬁcation fail to recognize shape and do better with texture, VGG-16 gives 17.2% accuracy using shape vs. 82.8% when using texture; GoogleNet gives 31.2% using shape vs. 68.8% when using only texture and ResNet-50 gives 22.1% accuracy using shapes as compared to 77.9% when using texture. Human observers do much better as expected with 95.9% accuracy when shapes are clearly deﬁned."
2201.09144,dataset,55,,,"Preprocessing : The RGB pixel values range from 0 to 255 unlike infrared images. We perform our experiments with and without mean target subtraction and ﬁnd out that without mean target subtraction, our model performs better. Hence we stick to only normalizing our dataset by scaling the images to unit variance."
2201.09144,dataset,64,,,"to focus more on object shape than background and texture. Lastly, as mentioned before deep networks like VGG-16 fail to work well with Infrared images due to different domain types. VGG-16, ResNet and Densenet have been trained on ImageNet(an RGB dataset) hence transfer learning does not work well when such networks are ﬁnetuned on infrared imagery."
2201.09144,dataset,67,,,"tered the image by subtracting the mean value of the target from each pixel, then scaled the image to unit variance. To generate the images with no background, we take the preprocessed dataset with background, and set the background pixels to 0. This ensures that the target values are the same between the dataset with background and the dataset without background."
2201.09144,dataset,71,,,"Choice of Intermediate Layer Table 3 shows the validation accuracies for Mobilenet on the infrared dataset with background for varying output activation sizes. The activation sizes are denoted by (c;h;w), where c is the number of channels, h is the height, and w is the width. The table shows that the intermediate feature layer that had an output size of"
2201.09144,dataset,8,,,Table 5: Test accuracies on MS-COCO dataset
2201.11218,data,106,,,"Hardware Resources. Spatial DNN accelerators [5] comprise an array of processing elements (PEs). Each PE has a MAC to compute partial sums and a scratchpad to store weights, activations, and partial sums. The accelerators also house a shared on-chip (global) buffer to pre-fetch activations and weights from off-chip memory for the next tile of computation that will be mapped over the PEs. Networks-on-Chip (NoCs) are used to distribute data from the on-chip buffer to PEs, collect the partial or full outputs, and write them back to the on-chip buffer."
2201.11218,data,120,,,"We call a layer-wise micro-batching strategy — layer fusion strategy, since it is essentially dividing DNN layers into multiple fusedlayers. For example, in Fig. 1 we have a layer fusion strategy for a 5-layer DNN workload. The strategy dictates the output microbatch sizes of each layer, where the first value represents the input micro-batch size. We use ""-1"" to represent a signal to synchronize and stream data back to off-chip memory before proceeding to the next layer. This synchronization divides the dataflow into two groups of fused-layers, as shown in Fig. 1. For an N layer DNN workload, the layer fusion strategy is represented as follows"
2201.11218,data,167,,,"3 PROBLEM FORMULATION In the DNN accelerator, the on-chip buffer is often limited and not able to stage the full output activation. The key idea of layer fusion is that, rather than streaming back the full output activation to the off-chip memory like in [8, 9, 13], we stage ""partial"" output activation on-chip to exploit data locality and thus boost data reuse. Practitioners start to use ""micro-batching"" as the go-to strategy. With micro-batching, we will divide a batch of activations into multiple micro-batches. In practice, We search for the largest acceptable micro-batch size that allows us to stage all intermediate activations on-chip, which becomes the most naive micro-batching strategy. However, this naive strategy cannot maximize the reuse opportunity. Different layers produce different sizes of output activations, and a naive unified micro-batch size could leave the on-chip buffer under-utilized. A more sophisticated layer-wise micro-batching"
2201.11218,data,18,,,CCS CONCEPTS • Computer systems organization → Data flow architectures; • Computing methodologies → Reinforcement learning.
2201.11218,data,21,,,[10] Andrei Ivanov et al. 2020. Data Movement Is All You Need: A Case Study on
2201.11218,data,213,,,"We use ""requesting/conditioning on-chip memory usage"" as the 𝑟𝑡 ). This formulation brings conditioning (desired) reward (noted as (cid:98) three benefits: 1) The current available on-chip memory sizes are known parameters, therefore we could use it as a reasonable condition. 2) The heuristics tell that a layer fusion strategy that maximizes the on-chip memory usage often achieves better runtime performance. Therefore, we can expect a high-performance solution if conditioning on the total available memory. 3) By setting memory usage as a condition, it also set up a potential use case of generalizing to unseen memory sizes, discussed later in Section 4.6. 4.4 Training Data Collections Imitation Learning and Teacher Model. An DNN learning 4.4.1 trick, ""imitation learning"", is found useful in training Transformers [2, 3, 20]. ""Imitation learning"" could be imitating and learning 1) from self previous experiences as used in related works [2, 3, 20] or 2) from another well-trained teacher model2, which we use in this work and find it critical for Transformers to learn well in our layer fusion problem."
2201.11218,data,229,,,"Figure 3: The layer-fusion solutions found by DNNFuser and G-Teacher on ResNet18 with batch size 64 conditioning on memory size of 20MB. Values represent output micro-batch size of each layer, with Layer 0 being the input to Layer 1. 5.4 Transfer Learning To demonstrate the ability to execute transfer learning, we train DNNFuser on both VGG16 and Resnet18 to form a pre-trained general model. Then we use this general model to execute transfer learning on new DNN workloads: Resnet50, Mobilenet-V2, and Mnasnet. We fine-tune on new workloads (Transfer-DFs) with only 10% of the training epochs compared to training-from-scratch (DirectDFs). With only 10% of training epochs, Transfer-DFs can achieve compatible performance with G-Teacher and better performance than Direct-DFs (looking at some edge cases where Direct-DFs fail (N/A)). This tells that prior knowledge gained from pre-training actually increases the training speed and quality, which is especially important when training large DNN workloads such as Resnet50, Mobilenet-V2, or Mnasnet, which would otherwise require more training data. I.e., Direct-DFs need more teacher demonstrations from the Teacher model than Tranfer-DFs. This observation showcases the usefulness of transfer learning—with a pre-trained general model of DNNFuser, we can learn new workloads faster and better."
2201.11218,data,65,,,"Note that we empirically found that G-Teacher works well as a layer fusion mapper, which beats many optimization methods. However, G-Teacher is still a search-based method, which is inevitably slower than most of the inference-based DNN models. Therefore, G-Teacher only serves as a training data generator, which samples several good solutions (trajectories) in the layer-fusion environment"
2201.11218,data,80,,,"Instead of sticking to layer-by-layer execution, layer fusion opens the discussion of mapping multiple layers to the DNN accelerators simultaneously to leverage the immediate data reuse of intermediate activations. Fused-layer CNN [1] and TGPA [24] showcased huge potential benefits, however relying on their manual-designed layer fusion strategies. While many mappers are proposed to automate the search in intra-layer map-space, automated mappers for the layer fusion map-space have been rarely discussed."
2201.11218,data,82,,,"1 INTRODUCTION Accelerators for Deep Neural Network (DNN) models are commonplace today across the system stacks from cloud clusters [11] to edge devices [5, 6, 19]. For computation and energy efficiency, different dataflows/mappings [5, 19] have been proposed to optimize the data movement and compute utilization inside DNN accelerators. Many mappers have been proposed to automate this mapping optimization problem [8, 9, 13]."
2201.11218,"data, dataset",157,,,"4.5 Model Training and Inference 4.5.1 Training. With all the above-mentioned preparation and setup, we can now train our model. As shown in Fig. 2, the steps are as follows. 1) Data collection with teacher model. We take G-Teacher and ask it to search for several (4-10) sets of optimized mapping in different conditions (on-chip memory sizes). 2) Formulating into RL state transition. We take those solutions, which is a sequence of actions in RL terminology, and decorate them with the state and reward information to make it a state transition trajectory. These decorated trajectories will be stored in the replay buffer (essentially a place to house the training dataset). 3) Model training. We sample the training data from the replay buffer and train our model in an ""imitation learning"" fashion."
2201.11218,dataset,165,,,"4.3.2 Discussion. There is two high-level intuition for why this formulation is specifically useful in our problem. 1) Sparse and distant reward. The correlation between action and reward is often sparse and distant in the layer fusion scenario. The optimal micro-batch and the corresponding impact on the overall memory usage for the current layer often do not depend closely on the last action made (last micro-batch decision), but those several time-steps back. The transformer is expected to efficiently deal with the long-sequence scenario and excavate the relation between each action pair, near and far. 2) The transformer structure, specifically GPT [21] as we leverage, is capable of finding shortcuts between individual trajectories [3]. This means that our DNNFuser can find paths that are missing from the dataset, and more specific to our layer-fusion scenario, some peak memory usages that it has never seen."
2201.11218,dataset,95,,,"In previous work, such as Decision Transformer (DT) [3], the ""imitation learning"" dataset is collected by an RL agent sampling in the targeting Atari or OpenAI environment. However, we find that popular RL algorithms, including Advantage Actor-Critic (A2C) [17] and Deep Deterministic Policy Gradient (DDPG) [16], suffer from inefficient sampling when interacting with the layer fusion mapspace, and their trajectories (self experiences) are therefore inefficient demonstrations for ""imitation learning""."
2201.11218,github,2,,,//GitHub.com/FacebookResearch/Nevergrad.
2202.02272,code,65,,,"We are ready now to summarize in pseudo-code the two proposed versions of the MM-EnKF, as Algorithms 1 and 2. To maintain generality, we deﬁne the following DA_step function, which represents the analysis step for any EnKF, and in which the observation operator H is kept as possibly nonlinear, since many ensemble Kalman ﬁlters allow nonlinear observation operators:"
2202.02272,"code, open-source, code available, python, github, open-source code",80,,,"We implemented the method in the Julia language, with the open-source code available on GitHub[81]. The code is modular, making it easy to add diﬀerent DA methods and models. The CRPS error metric was computed with the properscoring library [82]. We used the parasweep library for Python of [83] to facilitate the running in parallel of multiple experiments at diﬀerent lead times and with diﬀerent parameter values."
2202.02272,data,10,,,A multi-model ensemble Kalman ﬁlter for data assimilation and forecasting
2202.02272,data,106,,,"b. Hybrid forecasting and DA. Hybrid methods combining statistical or ML forecasts with a dynamical model of a system are a promising approach for improving on pure dynamical forecasts. Ref. 86 demonstrated the advantage of hybrid models in forecasting high-dimensional chaotic systems, showing that a hybrid that combines an ML forecast with a forecast from an imperfect dynamical model can be skillful for longer than either one individually. In [70], the authors also demonstrated the advantage of combining a data-driven model with a dynamical model in leveraging the predictability of a system’s oscillatory modes."
2202.02272,data,107,,,"Model error can be handled by estimating the model error covariance matrix Q and using it to inﬂate the forecast covariance (additive inﬂation), inﬂating the forecast covariance with a scalar (multiplicative inﬂation), or attempting to directly correct model error (bias correction). Additive inﬂation generally works better than multiplicative inﬂation in accounting for model errors [41–44], since multiplicative inﬂation assumes that model errors will have the same structure as errors due to initial conditions, which is not generally the case. Multiplicative inﬂation, however, is more feasible in high-dimensional and data-scarce settings."
2202.02272,data,109,,,"The paper is laid out as follows. In sections I A and I B, we review the literature on combining forecasts and on multi-model DA, respectively. In section II, we discuss the development and implementation of MM-EnKFs. In section IV, we apply MM-EnKFs to chaotic systems. In section V, we draw conclusions and provide an outlook, including on applications to data-driven models. Finally, in Appendix A, we prove optimality of MM-DA in the linear minimum variance sense, and in Appendix B we detail the simple model error estimation method used in the numerical experiments."
2202.02272,data,128,,,"Data assimilation (DA) aims to optimally combine model forecasts and noisy observations. Multimodel DA generalizes the variational or Bayesian formulation of the Kalman ﬁlter, and we prove here that it is also the minimum variance linear unbiased estimator. However, previous implementations of this approach have not estimated the model error, and have therewith not been able to correctly weight the separate models and the observations. Here, we show how multiple models can be combined for both forecasting and DA by using an ensemble Kalman ﬁlter with adaptive model error estimation. This methodology is applied to the Lorenz96 model, and it results in signiﬁcant error reductions compared to the best model and to an unweighted multi-model ensemble."
2202.02272,data,142,,,"Data assimilation (DA) is the process of combining model forecasts with observations to obtain a state estimate of a system. DA is an essential part of forecasting in a wide variety of scientiﬁc and engineering ﬁelds, with most methods based nowadays on the Kalman ﬁlter [11]. In the meteorological literature, the need for methods to take noisy, possibly sparse observations and produce an initial condition suitable for a numerical model has been recognized since the ﬁrst numerical weather forecast [12], and the Kalman ﬁlter was proposed for this purpose in [13]. Ensemble Kalman ﬁlters (EnKFs), which approximate the evolution of the probability distribution using a Monte Carlo approach [14], have become popular for geophysical and other problems [15]."
2202.02272,data,29,,,"[84] T. Berry and T. Sauer, Correlation Between System and Observation Errors in Data Assimilation, Monthly Weather Review 146, 2913 (2018)."
2202.02272,data,30,,,"[5] L. Xue and D. Zhang, A Multimodel Data Assimilation Framework via the Ensemble Kalman Filter, Water Resources Research 50, 4197 (2014)."
2202.02272,data,31,,,"[16] A. Narayan, Y. Marzouk, and D. Xiu, Sequential Data Assimilation With Multiple Models, Journal of Computational Physics 231, 6401 (2012)."
2202.02272,data,31,,,"[42] T. M. Hamill and J. S. Whitaker, Accounting for the Error Due to Unresolved Scales in Ensemble Data Assimilation: A Comparison of Diﬀerent Approaches, Monthly"
2202.02272,data,33,,,"[43] J. S. Whitaker and T. M. Hamill, Evaluating Methods to Account for System Errors in Ensemble Data Assimilation, Monthly Weather Review 140, 3078 (2012)."
2202.02272,data,33,,,"[76] S. G. Penny, Mathematical Foundations of Hybrid Data Assimilation From a Synchronization Perspective, Chaos: An Interdisciplinary Journal of Nonlinear Science 27, 126801 (2017)."
2202.02272,data,36,,,"[41] H. Li, E. Kalnay, T. Miyoshi, and C. M. Danforth, Accounting for Model Errors in Ensemble Data Assimilation, Monthly Weather Review 137, 3407 (2009)."
2202.02272,data,37,,,"[11] M. Asch, M. Bocquet, and M. Nodet, Data Assimilation: Methods, Algorithms, and Applications, Fundamentals of Algorithms (Society for Industrial and Applied Mathematics, 2016)."
2202.02272,data,37,,,"[36] L. Yang, A. Narayan, and P. Wang, Sequential Data Assimilation With Multiple Nonlinear Models and Applications to Subsurface Flow, Journal of Computational Physics 346, 356 (2017)."
2202.02272,data,37,,,"[69] Y. Chen and S. N. Stechmann, Multi-Model Communication and Data Assimilation for Mitigating Model Error and Improving Forecasts, Chinese Annals of Mathematics, Series B 40, 689 (2019)."
2202.02272,data,38,,,"[39] A. Carrassi, S. Vannitsem, and C. Nicolis, Model Error and Sequential Data Assimilation: A Deterministic Formulation, Quarterly Journal of the Royal Meteorological Society 134, 1297 (2008)."
2202.02272,data,38,,,"[73] H. Hoel, G. Shaimerdenova, and R. Tempone, Multilevel Ensemble Kalman Filtering Based on a Sample Average of Independent EnKF Estimators, Foundations of Data Science 2, 351 (2020)."
2202.02272,data,42,,,"[13] M. Ghil, S. Cohn, J. Tavantzis, K. Bube, and E. Isaacson, Applications of Estimation Theory to Numerical Weather Prediction, in Dynamic Meteorology: Data Assimilation Methods, Applied Mathematical Sciences, edited by"
2202.02272,data,45,,,"[60] L. M. Stewart, S. L. Dance, and N. K. Nichols, Data Assimilation With Correlated Observation Errors: Experiments With a 1-D Shallow Water Model, Tellus A: Dynamic Meteorology and Oceanography 65, 19546 (2013)."
2202.02272,data,45,,,"[62] A. Carrassi, M. Bocquet, L. Bertino, and G. Evensen, Data Assimilation in the Geosciences: An Overview of Methods, Issues, and Perspectives, Wiley Interdisciplinary Reviews: Climate Change 9, e535 (2018)."
2202.02272,data,47,,,"[33] K. Ide, P. Courtier, M. Ghil, and A. C. Lorenc, Uniﬁed Notation for Data Assimilation: Operational, Sequential and Variational, Journal of the Meteorological Society of Japan. Ser. II 75, 181 (1997)."
2202.02272,data,48,,,"[74] A. Carrassi, M. Ghil, A. Trevisan, and F. Uboldi, Data Assimilation as a Nonlinear Dynamical Systems Problem: Stability and Convergence of the Prediction-Assimilation System, Chaos: An Interdisciplinary Journal of Nonlinear Science 18, 023112 (2008)."
2202.02272,data,50,,,"[71] A. Chattopadhyay, M. Mustafa, P. Hassanzadeh, E. Bach, and K. Kashinath, Towards Physically Consistent Data-Driven Weather Forecasting: Integrating Data Assimilation With Equivariance-Preserving Spatial Transformers in a Case Study With ERA5, Geoscientiﬁc Model Development Discussions , 1 (2021)."
2202.02272,data,52,,,"[22] G. S. Duane, C. Grabow, F. Selten, and M. Ghil, Introduction to Focus Issue: Synchronization in Large Networks and Continuous Media extemdash Data, Models, and Supermodels, Chaos: An Interdisciplinary Journal of Nonlinear Science 27, 126601 (2017)."
2202.02272,data,53,,,"a. Weighting distinct forecasts. Several approaches for weighting distinct model forecasts have been developed, with Bayesian model averaging [18] being one of the most common. This methodology estimates the posterior model probabilities based on past data, and assigns the models scalar weights based on these probabilities."
2202.02272,data,54,,,"[37] P. Tandeo, P. Ailliot, M. Bocquet, A. Carrassi, T. Miyoshi, M. Pulido, and Y. Zhen, A Review of Innovation-Based Methods to Jointly Estimate Model and Observation Error Covariance Matrices in Ensemble Data Assimilation, Monthly Weather Review 148, 3973 (2020)."
2202.02272,data,55,,,"[75] H. D. I. Abarbanel, S. Shirman, D. Breen, N. Kadakia, D. Rey, E. Armstrong, and D. Margoliash, A Unifying View of Synchronization for Data Assimilation in Complex Nonlinear Networks, Chaos: An Interdisciplinary Journal of Nonlinear Science 27, 126802 (2017)."
2202.02272,data,55,,,"[77] S. G. Penny, E. Bach, K. Bhargava, C.-C. Chang, C. Da, L. Sun, and T. Yoshida, Strongly Coupled Data Assimilation in Multiscale Media: Experiments Using a Quasi-Geostrophic Coupled Model, Journal of Advances in Modeling Earth Systems 11, 1803 (2019)."
2202.02272,data,56,,,"[34] O. G. Logutov and A. R. Robinson, Multi-Model Fusion and Error Parameter Estimation, Quarterly Journal of the Royal Meteorological Society 131, 3397 (2005). [35] E. Kalnay, Atmospheric Modeling, Data Assimilation and Predictability (Cambridge University Press, New York, 2002)."
2202.02272,data,6,,,(a) Single-model data assimilation
2202.02272,data,6,,,(b) Multi-model data assimilation
2202.02272,data,7,,,B. Multi-model data assimilation (MM-DA)
2202.02272,data,70,,,"MM-DA could be tested for hybrid DA and forecasting. As demonstrated in sections IV B 2 b and IV C, MMEnKF is able to successfully combine models of diﬀerent accuracy and resolution. This feature could be used for combining physical and data-driven forecasts: namely, one of the model ensembles could be generated by a physical model and the other one by a data-driven model."
2202.02272,data,70,,,"[27] S. Gerding and B. Myers, Adaptive Data Fusion of Mete orological Forecast Modules, in 3rd Conference on Artiﬁcial Intelligence Applications, 83rd American Meteorological Society Annual Meeting (Long Beach, CA, 2003). [28] G. Anandalingam and L. Chen, Linear Combination of Forecasts: A General Bayesian Model, Journal of Forecasting 8, 199 (1989)."
2202.02272,data,75,,,"[58] B. R. Hunt, E. J. Kostelich, and I. Szunyogh, Eﬃcient Data Assimilation for Spatiotemporal Chaos: A Local Ensemble Transform Kalman Filter, Physica D: Nonlinear Phenomena Data Assimilation, 230, 112 (2007). [59] P. L. Houtekamer and H. L. Mitchell, A Sequential Ensemble Kalman Filter for Atmospheric Data Assimilation, Monthly Weather Review 129, 123 (2001)."
2202.02272,data,97,,,"In the synchronization-based supermodelling approach of [23], connection terms between model states are introduced into the model equations. The connection coeﬃcients are gathered into matrices C, which can be identiﬁed with the gain matrices Ki in Eq. 23. We note, however, that MM-DA diﬀers from the approach of [23], as the latter directly estimates the connection coeﬃcients by minimizing a cost function with training data. Additionally, their approach uses static and diagonal C, does not allow for diﬀerent model spaces, and"
2202.02272,data,99,,,"In this paper, we proposed and implemented a multimodel ensemble Kalman ﬁlter (MM-EnKF), based on the framework of [16]. We addressed several gaps in previous work, including the formulation of an appropriate EnKF algorithm for high-dimensional systems and incorporation of model error estimation. Using numerical experiments with several versions of a chaotic model [78], we showed that the MM-EnKF is a robust and versatile method for making use of multiple imperfect models of a system in data assimilation (DA), as well as forecasting."
2202.02272,"data, data https",47,,,"[45] A. Farchi, M. Bocquet, P. Laloyaux, M. Bonavita, and Q. Malartic, A Comparison of Combined Data Assimilation and Machine Learning Methods for Oﬄine and Online Model Error Correction, Journal of Computational Science , 101468 (2021)."
2202.02272,github,73,,,"[79] G. Gaspari and S. E. Cohn, Construction of Correlation Functions in Two and Three Dimensions, Quarterly Journal of the Royal Meteorological Society 125, 723 (1999). [80] H. Hersbach, Decomposition of the Continuous Ranked Probability Score for Ensemble Prediction Systems, Weather and Forecasting 15, 559 (2000). [81] https://github.com/eviatarbach/mmda. [82] The"
2202.02272,github,9,,,"Corporation, https://github.com/TheClimateCorporation/properscoring (2015)."
2202.02272,provide implementation,78,,,"weight, and allow the weights to diﬀer for diﬀerent variables; (ii) we formulate several possible implementations of deterministic EnKFs for MM-DA (MM-EnKFs) and discuss computational issues; (iii) we provide an opensource software implementation of MM-EnKFs; (iv) we test MM-EnKF with chaotic models for DA and forecasting in various scenarios; and, ﬁnally, (v) we prove linear minimum variance optimality of MM-DA."
2202.04041,code,12,,,boundary conditions are automatically satisﬁed. Sec. 3.4 describes the code
2202.04041,code,15,,,function is obtained using backpropagation [117]. Our code is deployed on a
2202.04041,code,27,,,each element and the B-ﬁeld is constant inside each element. The maxi mum triangle area is 0.04 mm2. Our in-house FE code has been validated
2202.04041,data,13,,,as follows. We denote the minimum available B-H curve data point with
2202.04041,data,13,,,"noisy data, Journal of Computational Physics 425 (2021) 109913."
2202.04041,data,16,,,"For values outside the range of the available data, we extrapolate νsteel(B2)"
2202.04041,data,16,,,"curve data point. Then, for B < Bmin, the reluctivity is assumed constant"
2202.04041,data,17,,,training loss (with a window size of 500 data points) for the last iterations.
2202.04041,data,17,,,"uncertainty quantiﬁcation without labeled data, Journal of Computa tional Physics 394 (2019) 56–81."
2202.04041,data,19,,,"in between known data points [116]. The antiderivative of νsteel(B2) (i.e.,"
2202.04041,data,20,,,the magnetic ﬁeld energy density given by Eq. (9). We use the data in Table 2
2202.04041,data,28,,,"[58] T. Qin, K. Wu, D. Xiu, Data driven governing equations approxima tion using deep neural networks, Journal of Computational Physics 395"
2202.04041,data,31,,,"P. Perdikaris, Machine learning in cardiovascular ﬂows modeling: Pre dicting arterial blood pressure from non-invasive 4D ﬂow MRI data us ing physics-informed neural networks, Computer Methods in Applied"
2202.04041,data,33,,,"surrounding the device. The device’s core is made of steel, which is charac terized by a nonlinear B-H curve. Table 2 lists the B-H curve data used in"
2202.04041,data,45,,,"1 0 . Thus, the reluctivity is given by νsteel(B2) = linearly with a slope of ν− (Hmax + ν0(B − Bmax))/B. Fig. 4(a) depicts the B-H curve data points"
2202.04041,data,5,,,500 data points).
2202.04041,data,9,,,wc2wwwebxbybycwwidwwbcdgyxaxisofsymmetryLxLyTable 2: B-H curve data points.
2202.04041,python,27,,,"[91] A. Koryagin, R. Khudorozkov, S. Tsimfer, PyDEns: A python frame work for solving diﬀerential equations with neural networks, arXiv"
2202.06466,data,47,,,"Xu Chen1 , Yongfeng Zhang2 , Ji-Rong Wen1 1Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artiﬁcial Intelligence, Renmin University of China 2Department of Computer Science, Rutgers University xu.chen@ruc.edu.cn, yongfeng.zhang@rutgers.edu, jrwen@ruc.edu.cn"
2202.06466,data,50,,,"[Wang et al., 2018] Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. Explainable recommendation via multi-task learning in opinionated text data. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 165–174, 2018."
2202.06466,data,52,,,"[Cheng et al., 2019] Weiyu Cheng, Yanyan Shen, Linpeng Incorporating interpretability Huang, and Yanmin Zhu. into latent factor models via fast inﬂuence analysis. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 885–893, 2019."
2202.06466,"data, dataset",118,,,"stakeholder utilities, one can automatically learn the metric functions, where one can incorporate a small amount of human labeled data as the ground truth. And then, for benchmarking this ﬁeld, one can build a large scale dataset, which incorporates the ground truth on the user effectiveness, recommendation persuasiveness and so on. At last, existing methods mostly evaluate the explanations within a speciﬁc task. However, we believe human intrinsic preferences should be stable and robust across different tasks. Thus, in order to evaluate whether the derived explanations can indeed reveal such human intrinsic preferences, developing task-agnostic evaluation methods should be an interesting research direction."
2202.06466,"data, dataset, publicly available",185,,,"• Crowdsourcing by injecting annotator data into public dataset. A major problem in the above method is that the annotator preferences may deviate from the real users’, which may introduce noises into the evaluation. As a remedy [Gao et al., 2019] designs a novel crowdfor this problem, sourcing method by combining the annotator generated data with the public dataset. In speciﬁc, the evaluations are conducted based on the Yelp dataset1, which is collected from yelp.com. The authors recruit 20 annotators, and require each annotator to write at least 15 reviews on yelp.com. Then, the reviews written by the annotators are infused into the original Yelp dataset, and the explainable models are trained based on the augmented dataset. In the evaluation process, the annotators are required to score from 1 to 5 on the explanations according to their usefulness. Since the data generated by the annotators is also incorporated into the model training process, the feedback from the annotators are exactly the real user feedback."
2202.06466,"data, dataset, publicly available",75,,,"ﬁrst one is fully based on the public datasets, the second one is based on the combination between the annotator generated data and the public dataset, and the last one is completely based on the annotator generated data. Comparing with the case studies and quantitative metrics, crowdsourcing is directly based on real human feelings. However, it can be much more expensive due to the cost of recruiting annotators."
2202.06466,dataset,177,,,"• Crowdsourcing with public datasets. In this method, the recommender models are trained based on the public datasets. And then, many annotators are recruited to evaluate the model generated explanations based on a series of questions. There are two key points in the evaluation process. (i) Annotation quality control. In order to accurately evaluate the explanations, controlling the annotation quality is a necessary step. In the previous work, there are two major strategies: one is based the voting mechanism, for example, in [Chen et al., 2019], there are three annotators for each labeling task, and the ﬁnal result is available only when more than two annotators have the same judgment. The other strategy is based on computing certain statistical quantities. For example, in [Chen et al., 2018], Cohen’s Weighted κ [Cohen, 1968] is leveraged to assess the inter-annotator agreements and remove the outlier annotations."
2202.06466,dataset,3,,,1https://www.yelp.com/dataset
2202.06466,dataset,309,,,"• Crowdsourcing with fully constructed datasets. In this category, the datasets are fully constructed by the annotators. In general, there are four steps in the evaluation process, that is, (i) recruiting annotators with different background (e.g., age, sex, nationality and etc.), (ii) collecting annotator preferences, (iii) generating recommendations and explanations for the annotators, and (iv) evaluating the explanations by asking questions on different evaluation perspectives. Usually, The number of annotators in this category is much larger than above two methods, and the experiment settings and studied problems are quite diverse. For example, in [Musto et al., 2019], there are 286 annotators (with 76.3% males, and 48.4% PhDs), and they are asked to evaluate the explanation transparency and effectiveness. In [Naveed, 2020], the authors aim to study whether featurebased collaborative ﬁltering (CF) models can lead to better explanations than the conventional CF’s. There are 20 annotators, among which 14 are females with the age ranges from 21 to 40. In [Hernandez-Bocanegra, 2020], the authors investigate whether the type and justiﬁcation level of the explanations inﬂuence the user perceptions. To achieve this goal, 152 annotators are recruited, where there are 87 females, and the average age is 39.84. In [Balog and Radlinski, 2020], the authors recruit 240 annotators to study the relations between the explanation effectiveness, transparency, persuasiveness and scrutability. In [Tsukuda, 2020], the authors recruit 622 annotators to evaluate the inﬂuence of the explanation styles on the explanation persuasiveness."
2202.06466,dataset,87,,,"In the above questions, the ﬁrst category is the most difﬁcult, since the annotators have to make decisions from a large amount of candidates. However, the labeled datasets are model-agnostic, which can be reused for different models and experiments. In the second and third categories, the annotators only have to answer yes or no, or select from a small amount of candidate answers, but the annotations cannot be reused, which is costly and not scalable."
2202.06466,package,29,,,"[Lin, 2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81, 2004."
2202.09669,data,20,,,"linear relationship between paired data [32]. For feeder f , the Pearson’s coefﬁcient is calculated"
2202.09669,data,22,,,"storage on residential energy consumption: An Australian case study based on smart meter data,” Renewable Energy, vol."
2202.09669,data,23,,,"of Smart Meter Data,” in 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA), Dec."
2202.09669,data,29,,,"[23] W. Luan, J. Peng, M. Maras, J. Lo, and B. Harapnuk, “Smart Meter Data Analytics for Distribution Network Connectivity"
2202.09669,data,30,,,"[17] S. Tindemans, G. Strbac, J. R. Schoﬁeld, M. Woolf, R. Carmichael, and M. Bilton, “Low carbon london project: Data"
2202.09669,data,4,,,AVAILABLE SMART METER DATA
2202.09669,data,4,,,data on controllable loads
2202.09669,data,5,,,data 3 − φ measurements
2202.09669,data,7,,,OS SM data with tariff infor mation
2202.09669,"data, data https, data available",17,,,IEEE P1159.1: Guide For Recorder and Data Acquisition Requirements For Characterization of Power Quality Events.
2202.09669,"data, dataset",35,,,"[19] S. Barker, A. Mishra, D. Irwin, E. Cecchet, P. Shenoy, J. Albrecht et al., “Smart*: An open data set and tools for enabling"
2202.09669,dataset,26,,,"distribution network dataset,” Int. Journal of Sustainable Energy, vol. 36, no. 8, pp. 787–806, 2017."
2202.09669,open-source,25,,,"[35] D. M. Fobes, S. Claeys, F. Geth, and C. Coffrin, “Powermodelsdistribution.jl: An open-source framework for exploring"
2202.09669,"open-source data, open-source",6,,,Open source (OS) SMs
2202.10603,code,15,,,"3. Since the training code of LFEPICNN [20] is unavailable, we"
2202.10603,data,114,,,"Apart from these aforementioned networks, many architectures have also been developed for LF image processing, such as bi-directional recurrent structure [41], 4D convolution [12], [55], [56] and cost volumes [28], [68]. The key difference between existing schemes and our disentangling mechanism is, we can fully use the information from all angular views and incorporate the LF structure prior. With our mechanism, high-dimensional LF data can be disentangled into different low-dimensional subspaces. Consequently, the difﬁculty for learning deep CNNs is reduced and several LF image processing tasks can be beneﬁted."
2202.10603,data,125,,,"During the training phase, we randomly cropped SAIs into patches of size 64×64, and converted them into grayscale images. We performed a large variety of data augmentation, including random ﬂipping and rotation, color channel re-distribution, brightness and contrast adjustment, refocusing and downsampling. Our network was trained using an L1 loss and optimized using the Adam method [78] with β1=0.9, β2=0.999 and a batch size of 12. The initial learning rate was set to 1×10−3 and was decreased by a factor of 0.5 for every 3000 epochs. The training was stopped after 15000 epochs. Our model was implemented in PyTorch and trained on a PC with an Nvidia RTX 3090 GPU."
2202.10603,data,13,,,mechanism is generic and more effective in handling LF data than LF-InterNet.
2202.10603,data,130,,,"The aforementioned feature extractors can fully incorporate the LF structure prior and disentangle LFs into different subspaces. Since each feature extractor only processes LF features within a 2D subspace, the complexity of the 4D LF data is effectively reduced, making LF representation learning much easier. In practice, we combine these feature extractors into different modules for different tasks. By stacking multiple modules (i.e., deepening the networks), our feature extractors can work jointly to incorporate the information from different domain, and enlarge the receptive ﬁeld to cover the spatially varying disparities. In the following sections, we apply our disentangling mechanism to three typical LF image processing tasks, and introduce the details of our modules and networks."
2202.10603,data,146,,,"[2] T. E. Bishop and P. Favaro, “The light ﬁeld camera: Extended depth of ﬁeld, aliasing, and superresolution,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 5, pp. 972–986, 2011. [3] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, “Depth estimation with occlusion modeling using light-ﬁeld cameras,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 11, pp. 2170–2181, 2016. I. K. Park, K. M. Lee et al., “Robust light ﬁeld depth estimation using occlusion-noise aware data costs,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484– 2497, 2017."
2202.10603,data,151,,,"2) Distg-Block v.s. 4D Convolution. Since several recent works [12], [55], [56] used 4D convolutions to handle LF data and have achieved promising performance, we compare our disentangling mechanism with 4D convolutions by replacing our Distg-Block with a series of 4D residual blocks. As shown in Table 1, stacking 4D convolutions can result in a very large model size (i.e., 22.3M for 2×SR) but cannot introduce performance improvements. By using 4D convolutions, the variant achieves an average PSNR of 38.61dB for 2×SR, which is 0.31dB lower than our DistgSSR. That is because, by using our proposed Distg-Block, the high-dimensional LF data can be disentangled into different subspaces and the inherent structural characteristics of input LF images can be efﬁciently learned by our speciﬁcally designed feature extractors."
2202.10603,data,170,,,"Recently, deep CNNs have been widely used for disparity estimation and achieved superior performance over traditional methods. Heber et al. [65] proposed a CNN to learn an end-to-end mapping between a 4D LF and its corresponding depths. Subsequently, Heber et al. [66] proposed a U-Net with 3D convolutions to extract geometric information for robust disparity estimation. Shin et al. [24] proposed a multi-stream network (i.e., EPINET) and a speciﬁcally designed data augmentation approach for fast and accurate disparity estimation. EPINET signiﬁcantly improves the accuracy of disparity estimation and achieves top performance on the HCInew benchmark [67] at that time. Tsai et al. [68] proposed an attention-based view selection network (i.e., LFattNet) to adaptively incorporate all angular views for disparity estimation. Chen et al. [28] proposed an attentionbased multi-level fusion network to handle the occlusion problem for disparity estimation."
2202.10603,data,47,,,"of size 32 × 32. We performed random horizontal ﬂipping, vertical ﬂipping, and 90-degree rotation to augment the training data by 8 times. Note that, the spatial and angular dimension need to be ﬂipped or rotated jointly to maintain LF structures."
2202.10603,data,50,,,"[6] Williem, I. K. Park, and K. M. Lee, “Robust light ﬁeld depth estimation using occlusion-noise aware data costs,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484–2497, 2018."
2202.10603,data,72,,,"consideration, and reduce the dimension of LF images by performing convolutions on neighboring views [18], [19], epipolar plane images (EPIs) [20]–[23], or 3D sub-LFs [24]– [28]. Although these methods are effective in handling LF data, their performance is limited due to the underexploitation of the rich angular information."
2202.10603,data,93,,,"Compared to existing LF image processing frameworks, our disentangling mechanism has three remarkable properties. First, our mechanism can well incorporate the LF structure prior and fully use the information from all angular views. Second, since the 4D data can be disentangled into several subspaces by our mechanism, the convolution layers in our networks only need to process features in a single subspace, which makes LF representation learning easier. Third, our mechanism is generic and can be applied to different LF image processing tasks."
2202.10603,"data, dataset",240,,,"Following [49], we use both synthetic LF datasets (i.e., the HCInew [67] and HCIold [76] datasets) and real-world LF datasets (i.e., the 30scenes [47] and STFlytro [81] datasets) for experiments. The split of training and test sets was identical to that in [49]. That is, 20 synthetic scenes and 100 real-world scenes were used for training, while 4 scenes from the HCInew dataset [67], 5 scenes from the HCIold dataset [76], 30 scenes from the 30scenes dataset [47], 25 scenes from the Occlusion category and 15 scenes from the Reﬂective category in the STFlytro dataset [81] were used for test. For simplicity, we followed most existing works [49], [53], [54], [71] to perform 2×2→7×7 angular SR. To generate training and test samples, we angularly cropped the central 7×7 SAIs and used their 2×2 corner views as inputs to reconstruct the remaining views. During training phase, we spatially cropped each SAI into patches of 64×64, and generated around 1.5×104 training samples for each of the synthetic and real-world datasets. Similar to spatial SR, we performed random horizontal ﬂipping, vertical ﬂipping and 90-degree rotation for data augmentation."
2202.10603,"data, github",197,,,"Abstract—Light ﬁeld (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Speciﬁcally, we ﬁrst design a class of domain-speciﬁc convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-speciﬁc modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efﬁciency, and generality of our disentangling mechanism. Project page: https://yingqianwang.github.io/DistgLF/."
2202.10603,dataset,103,,,"We followed [45] to use 5 public LF datasets (i.e., EPFL [75], HCInew [67], HCIold [76], INRIA [77], STFgantry [15]) for both training and test. The division of training and test set was set identical to that in [45]. All LFs in these datasets have an angular resolution of 9 × 9. In the training stage, we cropped each SAI into patches with a stride of 32, and used the bicubic downsapling approach to generate LF patches"
2202.10603,dataset,104,,,"RCAN [74], ESRGAN [80]) and 7 LF image SR methods (i.e., LFBM5D [33], GB [34], resLF [25], LFSSR [44], LFATO [40], LF-InterNet [29], LF-DFnet [45]). We also include bicubic interpolation as a baseline method. Note that, all deep learning-based SR methods have been retrained on the same training datasets as our method for fair comparison. For simplicity, we only present the results on 5×5 LFs for 2× and 4×SR."
2202.10603,dataset,111,,,"Following [25], [39], [45], [74], we used PSNR and SSIM calculated on the Y channel images as quantitative metrics for performance evaluation. Note that, to obtain the metric score (e.g., PSNR) for a dataset with M test scenes (each scene with an angular resolution of A × A), we ﬁrst calculated the metric on A×A SAIs on each scene separately, then obtained the score for each scene by averaging its A2 scores, and ﬁnally obtained the score for this dataset by averaging the scores of all M scenes."
2202.10603,dataset,113,,,"Kalantari et al. [47] synthesize novel views by performing disparity estimation and feature warping, and achieve much better performance than EPI-based methods [20], [22], [53]. Among all the compared methods, our DistgASR achieves the highest SSIM scores on all the 5 datasets and the highest PSNR scores on 4 of the 5 datasets (less competitive than FS-GAF on the HCInew dataset). By disentangling input LFs into spatial, angular and EPI subspaces, our DistgASR can fully incorporate the multi-dimensional information and learn the LF structure to accurately reconstruct missing views without explicit disparity estimation."
2202.10603,dataset,118,,,"using EPI branches, model-3 suffers a decrease of 0.30dB in PSNR as compared to DistgSSR (38.62 v.s. 38.92). Note that, the PSNR drop is more signiﬁcant on the datasets with large disparity variations (e.g., 0.41 dB PSNR drop on the STFgantry [15] dataset). That is because, the proposed angular feature extractor cannot effectively incorporate angular information under large disparity variations since pixels of an object among different views can locate at different macropixels. In contrast, the EPI feature extractor can disentangle LFs into EPI subspaces, and its powerful spatial-angular correlation modeling capability makes our DistgSSR more robust to disparity variations."
2202.10603,dataset,164,,,"1) Quantitative results: Quantitative results are presented in Table 3. Our DistgSSR achieves the highest PSNR and SSIM scores on all the 5 datasets for both 2× and 4×SR. It is worth noting that the improvement of our method is very signiﬁcant on the HCInew, HCIold and STFgantry datasets for 2×SR (i.e., 0.51dB, 0.47dB, and 0.73dB higher than the second top-performing method on these three datasets, respectively). That is because, these three datasets were either synthetically rendered [67], [76] or captured by a moving camera mounted on a gantry [15], and thus have LFs with more complex structures and larger disparity variations than the Lytro datasets [75], [77]. By processing features in different subspaces with the disentangling mechanism, our DistgSSR can well handle these complex scenarios while maintaining promising performance for LFs with small baselines."
2202.10603,dataset,31,,,"[75] M. Rerabek and T. Ebrahimi, “New light ﬁeld image dataset,” in International Conference on Quality of Multimedia Experience (QoMEX), 2016."
2202.10603,dataset,33,,,"In this section, we ﬁrst introduce the datasets and implementation details, then compare our DistgDisp to state-of 5. The MSE scores reported in this section are multiplied with 100 for"
2202.10603,dataset,42,,,"In this section, we ﬁrst introduce the datasets and our implementation details. Then, we conduct ablation studies to investigate our design choices. Finally, we compare our network to several state-of-the-art SISR and LF image SR methods."
2202.10603,dataset,44,,,"[76] S. Wanner, S. Meister, and B. Goldluecke, “Datasets and benchmarks for densely sampled 4d light ﬁelds.” in Vision, Modelling and Visualization (VMV), vol. 13, 2013, pp. 225–226."
2202.10603,dataset,47,,,"[67] K. Honauer, O. Johannsen, D. Kondermann, and B. Goldluecke, “A dataset and evaluation methodology for depth estimation on 4d light ﬁelds,” in Asian Conference on Computer Vision (ACCV), 2016, pp. 19–34."
2202.10603,dataset,5,,,4.2.1 Datasets and Implementation Details
2202.10603,dataset,5,,,5.2.1 Datasets and Implementation Details
2202.10603,dataset,5,,,6.2.1 Datasets and Implementation Details
2202.10603,dataset,54,,,"TABLE 4: Comparisons of the number of parameters (#Param.) and FLOPs for 2× and 4×SR. Note that, FLOPs is calculated on an input LF with an angular resolution of 5×5 and a spatial resolution of 32×32. The PSNR scores are averaged over 5 test datasets."
2202.10603,dataset,60,,,26.64/0.744 31.43/0.850 33.66/0.918 32.72/0.924 34.76/0.930 31.84/0.898 37.61/0.942 39.17/0.975 34.41/0.955 36.38/0.944 29.61/0.819 35.73/0.898 38.22/0.970 35.42/0.962 35.96/0.942 32.85/0.909 38.58/0.944 41.40/0.982 37.25/0.972 38.09/0.953 32.30/0.900 39.69/0.941 42.77/0.986 38.88/0.980 38.33/0.960 34.60/0.937 40.84/0.960 42.53/0.985 38.36/0.977 38.20/0.955 37.14/0.966 41.80/0.974 42.75/0.986 38.51/0.979 38.35/0.957 34.70/0.974 42.18/0.978 43.67/0.995 39.46/0.991 39.11/0.978 Note: All compared methods except LFEPICNN [20] were retrained on the same datasets as our method.
2202.10603,dataset,66,,,"We used PSNR and SSIM values calculated on the Y channel images for quantitative evaluation. We ﬁrst calculated the PSNR and SSIM values on the reconstructed views (totally 45 views for 2×2→7×7 ASR), and then averaged these values to obtain the score of the scene. The score of a dataset is calculated by further averaging the scores of its scenes."
2202.10603,dataset,69,,,"We compare our method to 7 state-of-the-art methods including LFEPICNN [20], ShearedEPI [22], P4DCNN [53], Kalantari et al. [47], Yeung et al. [54], LFASR-geo [71] and FS-GAF [49]. All the compared methods except LFEPICNN3 were trained on the same datasets as our method for fair comparison."
2202.12194,data,173,,,"generation to make use of the underutilized mmWave spectrum between 24.25 GHz and 52.6 GHz, which has been deﬁned by the 3GPP standard as Frequency Range 2 (FR2) [1], [2]. At the same time, the large amount of bandwidth availability in the unlicensed mmWave band at 60 GHz has led to the formulation of next generation WiFi standards like IEEE 802.11ad and 802.11ay, also known as Wireless Gigabit (WiGig) [3]. The mmWave spectrum is extremely attractive for two main reasons: (i) it allows to accommodate large antenna arrays in very compact systems, thus obtaining high antenna gain and beamforming capability; and (ii) it provides large channel bandwidths. These points make mmWave suitable to accommodate a rapidly growing number of intelligent devices and services and to address highly demanding applications, where very low latencies and very high data rates are required [4], [5]."
2202.12194,data,187,,,"The Donor gNB consists of a central unit (CU) and one or more distributed units (DUs). A CU is a logical node hosting radio resource control (RRC), service data adaptation protocol (SDAP) and packet data convergence protocol (PDCP) and controls the operation of one or more DUs. A DU is a logical node hosting radio link control (RLC), medium access control (MAC) and physical (PHY) layers. IAB nodes exploit the CU/DU functional split by hosting a DU and a mobile termination (MT) function. The IAB-DU interfaces to the Donor gNB CU via the F1 interface (with relevant extensions). The MT replicates the UE functionality to establish connections to a gNB and the core network. In this manner, IAB-MTs can connect to DUs on other IAB nodes or on the Donor gNB thus realizing multi-hop backhaul. The CU on the Donor gNB becomes the central control function for all IAB-DUs and IAB-MTs deﬁning an interconnected IAB topology."
2202.12194,data,283,,,"To mitigate these limitations, mmWave applications typically resort to high gain and narrow beamforming antenna arrays combined with high transmit power, in other words high EIRP systems. While this solution is sufﬁcient for adequately recovering the propagation losses in line-of-sight (LoS) or near-LoS scenarios, it fails in delivering acceptable performance in urban areas characterized by deep non-LoS (NLoS) conditions. For this cases, a natural solution would be to resort to network densiﬁcation. A denser deployment of 5G mmWave base stations (gNBs according to the 5G NR terminology) would be preferred from the performance point of view since it would guarantee the desired minimum signal strength in the served area, but it may not be always a feasible or economically viable solution, e.g., due to the lack of wired/wireless backhaul, the higher costs for the acquisition of new sites, rental fees, maintenance and power supply. As a matter of fact, alternative solutions to the approach ”more information and data through more power and higher costs” are mandatory. Aligned to this view, standardization bodies, mobile operators and ICT industries are now evaluating a number of trade-off solutions in terms of network performance and complexity, total cost of ownership (TCO) and electromagnetic ﬁeld (EMF) levels. Such solutions include, among the others, the deployment of new types of network nodes, much cheaper and easy to be deployed than traditional Macro and Micro BSs, that allow to achieve an homogeneous coverage in the radio access network without necessarily resorting to the installation"
2202.12194,data,61,,,"Comment Mature technology, few suppliers at microwave and mmWave frequencies Few suppliers in volume production Suitable for RFIC integration, less as standalone device Suitable for MMIC integration, less as standalone device Suitable for system integration, less as standalone device Data refer to GeTe. V02 still in early research phase Either standalone device or integrated into systems"
2202.12194,data,89,,,"2) Phase change materials: Phase Change Materials (PCMs) are receiving considerable attention as reliable candidates for next generation non-volatile memory devices, glass ﬁbers, data storage media, high density optical recording applications, microchips, and reprogrammable photonic circuits [55]–[57]. These materials are characterized by the capability to change from amorphous to crystalline phase, in order to display different electrical features. The transformation between states could be triggered by a thermal excitation generated by"
2202.12194,package,202,,,"[34] X. Ding et al., “A wide-angle scanning planar phased array with pattern reconﬁgurable magnetic current element,” IEEE Transactions on Antennas and Propagation, vol. 65, no. 3, pp. 1434–1439, 2017. [35] G. Yang and J. Li, “Study on wide-angle scanning linear phased array,” in 2017 IEEE International Symposium on Antennas and Propagation USNC/URSI National Radio Science Meeting, 2017, pp. 491–492. [36] R. Askar et al., “Interference handling challenges toward full duplex evolution in 5g and beyond cellular networks,” IEEE Wireless Communications, vol. 28, no. 1, pp. 51–59, 2021. [37] MEST2GFC-010-25 AlGaAs PIN diode, Macom. [38] MASW-010646 bare die, Macom. [39] ADGM1304 SP4T MEMS switch (inlcuding package), Analog Devices. [40] D. Kelly, C. Brindle, C. Kemerling, and M. Stuber, “The state-ofthe-art of silicon-on-sapphire cmos rf switches,” in IEEE Compound Semiconductor Integrated Circuit Symposium, 2005. CSIC ’05., 2005."
2202.13728,data,139,,,"We investigate the error of the (semidiscrete) Galerkin method applied to a semilinear subdiﬀusion equation in the presence of a nonsmooth initial data. The diﬀusion coeﬃcient is allowed to depend on time. It is well-known that in such parabolic problems the spatial error increases when time decreases. The rate of this time-dependency is a function of the fractional parameter and the regularity of the initial condition. We use the energy method to ﬁnd optimal bounds on the error under weak and natural assumptions on the diﬀusivity. First, we prove the result for the linear problem and then use the ”frozen nonlinearity” technique coupled with various generalizations of Gr¨onwall inequality to carry the result to the semilinear case. The paper ends with numerical illustrations supporting the theoretical results."
2202.13728,data,16,,,Error of the Galerkin scheme for a semilinear subdiﬀusion equation with time-dependent coeﬃcients and nonsmooth data
2202.13728,data,17,,,"Keywords: subdiﬀusion, Caputo derivative, L1 scheme, nonlinearity, nonsmooth data, timedependent coeﬃcient"
2202.13728,data,172,,,"By using the energy method coupled with two generalizations of the classical Gr¨onwall’s inequality we have been able to signiﬁcantly relax assumptions on the diﬀusivity D(x, t) and show L2 error estimates of the Galerkin method in the case of nonsmooth data. The linear result could then be carried over into the semilinear case with the use of frozen nonlinearity technique and a recently proved lemma concerning scalar product and the fractional derivative. In our further work we would proceed to the quasilinear subdiﬀusion equations, that is when the diﬀusivity is a function of the solution u. This example is suﬃciently general to model many real-world phenomena for instance, these found in hydrology. This problem could produce interesting questions concerning regularity requirements and estimates on (cid:107)∇uh(cid:107). Our numerical calculations conducted to support the theory indicated a further need of theoretical analysis of the fully discrete schemes for the time-dependent nonlinear subdiﬀusion equation."
2202.13728,data,313,,,"As until the time of writing this paper, the majority of results concerning subdiﬀusion problems investigated mostly either constant or space-dependent diﬀusivity in both analytical and numerical settings. There is a vast literature on these topics and we mention only several of them. In [41] a complete characterisation of solutions to the constant diﬀusivity case with a possibly nonsmooth initial condition was given. The numerical treatment and Galerkin method error estimates was carried over for example in [11]. The case of only space-dependent diﬀusivity has been studied for example in [28] from analytical and in [6] from numerical points of view. Many fully discrete schemes were proposed to solve the subdiﬀusion equation with time-independent diﬀusivity. The Caputo derivative can be discretized in diﬀerent ways, from which probably the convolution quadrature [27] and the L1 method [34] are the most popular. Some recent developments in these fully discrete schemes were given in [13, 4] (convolution quadrature) and [12, 19, 42, 26] (L1 method). Some interesting results concerning the L1 scheme for the semilinear subdiﬀusion equation has been published in [1] where the nonsmoothness of the initial data has been taken into account. We would also like to mention our previous integral equation approach to devise numerical methods for (1) in the case of degenerate porous medium equation, that is when D(x, t, u) ≈ um with m > 1 (see [38, 40, 33]). The reader is also invited to consult [14] (and references therein) for a concise survey of numerical methods in the presence of nonsmooth data."
2202.13728,data,34,,,"[1] Mariam Al-Maskari and Samir Karaa. Numerical approximation of semilinear subdiﬀusion equations with nonsmooth initial data. SIAM Journal on Numerical Analysis, 57(3):1524–1544, 2019."
2202.13728,data,37,,,"We consider the following semilinear subdiﬀusion equation on a convex domain Ω ⊂ Rd (d ≥ 1) with smooth boundary ∂Ω, initial data ϕ ∈ L2(Ω), and Dirichlet boundary condition"
2202.13728,data,39,,,"[14] Bangti Jin, Raytcho Lazarov, and Zhi Zhou. Numerical methods for time-fractional evolution equations with nonsmooth data: a concise overview. Computer Methods in Applied Mechanics and Engineering, 346:332–358, 2019."
2202.13728,data,41,,,"[12] Bangti Jin, Raytcho Lazarov, and Zhi Zhou. An analysis of the L1 scheme for the subdiﬀusion equation with nonsmooth data. IMA Journal of Numerical Analysis, 36(1):197–221, 2016."
2202.13728,data,42,,,"[13] Bangti Jin, Raytcho Lazarov, and Zhi Zhou. Two fully discrete schemes for fractional diﬀusion and diﬀusion-wave equations with nonsmooth data. SIAM Journal on Scientiﬁc Computing, 38(1):A146–A170, 2016."
2203.02458,data,117,,,"comprehension question, we know the time when the necessary piece of information is uttered in the source speech document. Based on this timing information, we can relate comprehension and the reported continuous feedback. In Figure 2, we plot the number of Continuous rating button clicks divided according to whether the information at that time was understood acceptably (“OK/OK-”), spotted but forgotten (“forgot”), missed by the user (“unknown”), or misunderstood (“wrong”). This data aggregates observations for all documents and all setups excluding the ofﬂine MT and the oracle online MT without ﬂicker."
2203.02458,data,41,,,"The source and target languages in our study are German and Czech, respectively. This is an interesting example of two neighbouring countries, distinct language families and yet a relatively well studied pair with sufﬁcient direct training data."
2203.04613,data,195,,,"Similarly to keypoint-based methods, where a 2D-3D matching between image keypoints and landmarks is sought, we also need to associate the predicted ellipses with their corresponding ellipsoidal models available in the pre-built map. The process to reconstruct this map beforehand is explained in subsection 4.1. Also, the ellipse regression network is trained separately for each object of our scene model, and thus, the correct version of the network should be used for each detected object. We can only partly rely on the class label predicted by the object detector, because the scene might contain several instances of the same object class (Also, we can not leverage temporal consistency by using associated data from previous frames, as the goal of our method is to estimate the camera pose from a single image.) Inspired by [12], we use a robust RANSAC-based method, in which a score is computed for each association hypothesis. This score is computed using the object-wise Intersection-over-Union (IOU) between a detected ellipse in the image and the projection of its associated ellipsoid."
2203.04613,data,25,,,the projection of the BBs. Data augmentation with a random background is performed during training to reduce the inﬂuence of the scene context.
2203.04613,data,3,,,3.5 Data association
2203.04613,data,3,,,4.2 Data augmentation
2203.04613,data,33,,,Data augmentation plays an important role in the training of the ellipse prediction network and its generalization with a relatively limited number of annotated images. Several strategies were performed during training:
2203.04613,data,40,,,We interpret them as being normalized with respect to the size of the crop input image (256 × 256). The center and dimensions of the ground truth ellipses in the training data are thus also normalized.
2203.04613,data,5,,,4 Data acquisition and augmentation
2203.04613,data,51,,,"– When only one object is detected, it is still possible to estimate the camera position if we have access to orientation data [11]. In practice, this can be obtained using an external sensor (IMU) or with an automatic vanishing point detection algorithm."
2203.04613,data,54,,,"The scene reconstruction and the generation of training data for our network only requires a set of calibrated images and a small amount of manual annotations. This makes the method easy to deploy in a new environment, which is very interesting from a practical point-of-view. The procedure works as follows:"
2203.04613,data,7,,,4.1 3D model and training data generation
2203.04613,data,70,,,"– When only two objects are detected, we use the P2E method described in [12]. Assuming that the camera roll is null, this method transforms the 6-DoF problem into a reduced problem with only one remaining degree-of-freedom which corresponds to an angular parameter. This makes it possible to review all the possible solutions in the same RANSAC that is used for data association."
2203.04613,data,75,,,"the other hand, the predicted ellipses seem more robust. This is especially true for the objects marked with the arrows in Figure 19. Even though the crop passed to the prediction network does not contain the whole object, the inferred ellipses are still correct. This robustness is mostly achieved thanks to our data augmentation which randomly shifts the image (equivalent to shifting the detection box before cropping)."
2203.04613,"data, code, github, data available",232,,,"Abstract In this paper, we propose a method for initial camera pose estimation from just a single image which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses ﬁtted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learningbased method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoids in terms of perspective projection. Experiments prove that the accuracy of the computed pose signiﬁcantly increases thanks to our method. This is achieved with very little eﬀort in terms of training data acquisition – a few hundred calibrated images of which only three need manual object annotation. Code and models are released at https://gitlab.inria.fr/tangram/ 3d-aware-ellipses-for-visual-localization."
2203.04613,"data, data available",150,,,"– A network for an improved 3D-aware object detection, which predicts ellipses around objects that are coherent with the projection of their 3D ellipsoidal abstractions. Its goal is to overcome the weaknesses of directly ﬁtting the ellipses to the axis-aligned bounding boxes. Our data augmentation procedure allows for robustness to box boundaries variability. – We show how the concept of ellipsoidal abstractions of objects and 3D-coherent ellipse predictions can be used for robust pose computation when only a small amount of data is available on the scene. We show that the pose accuracy little depends on the choice of this ellipsoidal abstraction, which makes the method ﬂexible and easy to use in practice. Only three calibrated images need to be annotated by hand to build the ellipsoid cloud. Annotations of the object are then obtained by projection in the training images."
2203.04613,"data, data available",203,,,"In this paper, we propose an object-based method for initial pose estimation from just a single image, which leverages the robustness of object detectors to large changes of viewpoints and environmental conditions. We model objects in 3D with ellipsoids, which can be interpreted as higher-level semantic landmarks for pose computation. A scene model made of ellipsoids is thus reconstructed in an initial step and is assumed to remain static for the localization. We use the term ”object” here in a broad sense, i.e. any elements that can be detected by a speciﬁcally trained object detector. It is also conceivable to model larger objects by parts with multiple ellipsoids. The major interest of the proposed method is its ﬂexibility, provided by our rough ellipsoidal modeling that can be applied to any objects. In particular, we show that the ﬁtting accuracy between the ellipsoidal model and the real object is not important. Our method thus meets the growing need of easy deployment for robotics or augmented reality applications in any environments, especially those for which no accurate model nor huge amount of ground truth data are available."
2203.04613,"data, data https",50,,,"54. Yang, C., Simon, G., See, J., Berger, M.O., Wang, W.: WatchPose: A View-Aware Approach for Camera Pose Data Collection in Industrial Environments. Sensors 20(11) (2020). URL https://hal.inria.fr/ hal-02735272"
2203.04613,database,96,,,"Image-retrieval methods estimate the camera pose from a query image by ﬁnding the most similar image in a database. They combine global descriptors (BoW [47], Fisher vector [33] or VLAD [8, 16]), with eﬃcient and scalable retrieval methods [28, 34]. With the emergence of convolutional neural networks, learned descriptors appeared [2]. The NetVLAD architecture was introduced in [1] and showed remarkable results, outperforming the state-of-the-art non-learnt image representations and oﬀ-the-shelf CNN descriptors."
2203.04613,dataset,109,,,"Full pose estimation. In order to evaluate the robustness to new viewpoints of the full camera pose estimation, we created a synthetic dataset. This virtual scene is composed of ten objects taken from the YCB benchmark and rendered with Blender. This enabled us to completely control the camera viewpoints between training and testing. Figure 13 shows the scene with our reconstructed object models. The training images were generated from three camera trajectories taken at approximately 4 m around the center of the scene, for a total of 192 images. We then generated other camera trajectories for evaluation with more diverse viewpoints"
2203.04613,dataset,48,,,"ple objects taken from the T-LESS dataset [15], where we simulated noisy detections. We randomly shifted the corners deﬁning each detection box. Figures 19 and 20 compare the inﬂuence of these noisy boxes on the inscribed ellipses and on the predicted ones."
2203.04613,dataset,53,,,"15. Hodaˇn, T., Haluza, P., Obdrˇz´alek, ˇS., Matas, J., Lourakis, M., Zabulis, X.: T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects. IEEE Winter Conference on Applications of Computer Vision (WACV) (2017)"
2203.04613,dataset,79,,,"However, all these methods assume to have access to a detailed textured model of the objects. NOCS [52] is an interesting category-level approach, in which a normalized coordinate space is used to represent diﬀerent objects from a same category. Their large real and syntehtic dataset enable them to generalize to unseen objects from known categories. The objects poses are recovered by combining the predicted coordinate maps with a measured depth map."
2203.04613,"dataset, dataset provided",90,,,"portion of correctly estimated positions wrt. the reprojection and ADD errors. These last two metrics compute the error between the object point cloud (provided in the dataset) transformed once with the estimated position and once with the ground truth value, either projected in the image (projection error) or directly in 3D (ADD). These experiments show a signiﬁcant improvement compared to the inscribed ellipses. Examples of predicted ellipses for some objects of the dataset are provided in Figure 11."
2203.04613,"dataset, used dataset",132,,,"Position estimation. We used the WatchPose dataset [54], which provides ten industrial scenes with images taken at diﬀerent distances (near at around 60cm and far at around 1.4m). Unfortunately, only one object per scene can be used for localization, and thus, only the camera position was evaluated. We tested two scenarios: an easy one, where a subset of near and far images were used for training and testing, and a hard one, where training was done only on near images and testing on far images. Examples of predicted ellipses are available in Figure 12. The results in Table 5 show the beneﬁts oﬀered by the the 3D-coherent ellipses, even in the far case."
2203.04613,"dataset, used dataset",239,,,"In some situations, it is possible to obtain the camera orientation using an external sensor (IMU) or a vanishing-point detection algorithm. In such scenarios, the camera position is determined from only one object. We evaluate here its accuracy and show the beneﬁts offered by our improved object observation method compared to the axis-aligned ellipse. We used the LINEMOD dataset [14], which provides RGB-D images of 15 objects in cluttered environments with ground truth pose information. We split the available images in two, leading to around 200 images for testing and 200 for training. A few of them were used to reconstruct the ellipsoidal model of each object. Rather than assuming that we know the ground truth camera orientation at test time, we added a random noise, uniformly sampled in between −2° and 2° on each of its Euler angle. This was done to be more realistic with what an external measurement could provide. Note that we did not ﬁne-tune the object detection part of our system, but only evaluate the impact of the 3D-coherent ellipse prediction. Figure 10 and Tables 2, 3 show the pro 020040060080010000.000.250.500.751.001.251.501.752.00Position error (m)Ou)s (3D­cohe)ent elli(ses)Reloc O(enVSLAM (RGB­D ma()PoseLSTM02004006008001000F)ames0510Nb. det. objects10"
2203.04613,"dataset, used dataset",97,,,"We used the 7-Scenes dataset to evaluate our method for camera pose estimation. This dataset is a collection of seven indoor scenes scanned with an RGB-D camera. For each scene, several scanned sequences are provided with color and depth frames as well as ground truth pose annotations. We used the scene called Chess, as it illustrates a typical environment where object-based methods can be used. We split the six available sequences as follows: sequences 1, 4, 6 for training and 2, 3, 5 for testing."
2203.05084,data,105,,,"However, one could still choose a relatively large truncation bound to ensure that no data is discarded by Transform. As a result, query errors under such circumstances will be caused primarily by unsynchronized cached view tuples. Typically, less unsynchronized cached data that satisfy the requesting query implies better accuracy and vice versa. For instance, when IncShrink has just completed a new round of view update, the amount of cached data tends to be relatively small, and thus less data is missing from the materialized view. Queries issued at this time usually have better accuracy."
2203.05084,data,105,,,"Observation 1. View-based query answering provides a significant performance improvements over NM method. As per Table 2, we observe that the non-materialization method is the least efficient group among all groups. In terms of the average QET, the DP protocols achieve performance improvements of up to 1.5e+5× and 7888× on TPC-ds and CPDB data, respectively, in contrast to the NM approach. Even the EP method provides a performance edge of up to 1366× over NM approach. This result further demonstrates the necessity of adopting view-based query answering mechanism. A similar observation can be learned from"
2203.05084,data,106,,,"Proof. In this section, we focus on proving the simulator provided in Table 1 yields computationally indistinguishable outputs compared to the execution of the real view update protocols. Let 𝑓𝑡 (𝑥, 𝑦) to be the functionality of truncated view transformation and 𝑓𝑠 (𝑥, 𝑦) to be the functionality of synchronizing data from secure cache to materialized view. 𝜋𝑡 and 𝜋𝑠 are the protocols that securely computes these two functionalities (Transform and Shrink). In general, we assume the secure cache and the materialized view are the secret-shared objects across the 2-PC participants."
2203.05084,data,114,,,"Observation 8. The average Shrink execution time increases along with the growth of 𝜔, while the average execution time of Transform tends to be approximately the same. The reason for this tendency is fairly straightforward. The execution time of both Transform and Shrink protocols is dominated by the oblivious input sorting. The input size of the Transform protocol is only related to the size of data batches submitted by the users. Therefore, changing 𝜔 does not affect the efficiency of Transform execution. However, the input size of Shrink is tied to 𝜔, so as 𝜔 grows, the execution time of Shrink increases."
2203.05084,data,118,,,"This design pattern helps us to address the extra leakage, but also raises new challenges. The transformation from outsourced data to a view instance may have unbounded stability, i.e., an input record may contribute to the generation of multiple rows in the transformed output, which could cause unbounded privacy loss. To address this, we enforce that any individual data outsourced by the owner only contributes to the generation of a fixed number of view tuples. As the transformation after applying this constraint has bounded stability, thus we obtain a fixed privacy loss with respect to each insertion (logical update) to the owner’s logical data."
2203.05084,data,119,,,"Observation 5. sDPTimer and sDPANT show accuracy advantages in processing Sparse and Burst data, respectively. According to Figure 6, sDPTimer shows a relatively lower L1 error in the Sparse group than sDPANT. It is because it can take a relatively long time to have a new view entry when processing Sparse data. Applying sDPANT will cause some data to be left in the secure cache for a relatively long time. However, sDPTimer’s update schedule is independent of the data workload type, so when the load becomes very sparse, the data will still be synchronized on time. This explains why sDPTimer shows a better accuracy guarantee against sDPANT"
2203.05084,data,12,,,thus the total number of deferred data is bounded by 𝑂 (
2203.05084,data,124,,,"In our design, we adopt an innovative ""Transform-and-Shrink"" paradigm, where the protocol is composed of two sub-protocols, Transform and Shrink, that coordinate with each other. Transform generates corresponding view entries based on newly outsourced data, and places them to an exhaustively padded secure cache to avoid information leakage. Shrink, runs independently, periodically synchronizes the cached data to the materialized view according to its internal states. To prevent the inclusion of a large amount of dummy data, Shrink shrinks the cached data into a DP-sized secure array such that a subset of the dummy data is removed whereas the true cardinality is still preserved. As a result, the resulting protocol"
2203.05084,data,126,,,"𝑖=1 𝑌𝑖 , where each 𝑌𝑖 is an i.i.d Laplace rnadom variable drawn from the distribution Lap( 𝑏 𝜖 ), where 𝑏 is the contribution upper bound for each record. Given 𝛽 ∈ (0, 1), according to Corollary 11 we obtain that Pr [𝑐𝑘 − ˜𝑐𝑘 ≥ 𝛼] ≤ 𝛽, such that 𝛼 ← 2𝑏 . Knowing that ˜𝑐𝑘 − 𝑐𝑘 computes the total 𝜖 number of records delayed after 𝑘-th updates, thus the theorem holds. Knowing that 𝑘 ′ ← ⌊ 𝑘𝑇 , and according to Corol𝑖 𝑌𝑖 is bounded by 𝑂 ( 2𝑏 lary 11, (cid:205)𝑘 𝜖 data after 𝑘-th updates is bounded by 𝑂 ( 2𝑏 𝜖"
2203.05084,data,13,,,"Input: data stream 𝑋 , privacy budget 𝜖, threshold 𝜃 ."
2203.05084,data,14,,,[59] Yanbin Lu. 2012. Privacy-preserving Logarithmic-time Search on Encrypted Data
2203.05084,data,141,,,"bounded privacy loss as max𝑢,D ≤ 𝑞 max(𝜖𝑖 ). Inspired by this, the following steps could help to obtain fixed privacy loss over time: First we assign a total contribution budget 𝑏 to each outsourced data 𝑑𝑠𝑖 ∈ DS. As long as a record is used as input to Transform (regardless of whether it contributes to generating a real view entry), it is consumed with a fixed amount of budget (equal to the truncation limit 𝜔). Then Transform keeps track of the available contribution budget for each record over time and ensures that only data with a remaining budget is used. According to this design, the “life time” contribution of each outsourced data to the materialized view object is bounded by 𝑏."
2203.05084,data,142,,,"Figure 3: Cache read operation. Next, sDPTimer obliviously sorts the exhaustively padded cache 𝝈 based on the 𝑖𝑠𝑉 𝑖𝑒𝑤 bit, moving all real tuples to the head and all dummy data to the tail, then cuts off the first sz elements from the sorted array and stores them as a separate structure o. sDPTimer then updates the materialized view V by appending o to the old view instance. Figure 3 shows an example of the aforementioned operation. Such secure array operation ensures that the real data is always fetched before the dummy elements, which allows us to eliminate a subset of the dummy data and shrink the size of the updated view entries. Finally, after each update, sDPTimer resets 𝑐 to 0 and re-shares it secretly to both participating servers."
2203.05084,data,170,,,"Observation 3. sDPTimer and sDPANT exhibit different privacyaccuracy trade-off. The accuracy-privacy trade-off evaluation is summarized in Figure 5a and 5c. In general, as 𝜖 increases from 0.01 to 50, we observe a consistent decreasing trend in the average L1 error for sDPTimer, while the mean L1 error for sDPANT first increases and then decreases. According to our previous discussion, the error upper bound of sDPTimer is given by 𝑐∗ + 𝑂 ( 2𝑏 ), 𝜖 where 𝑐∗ denotes the cached new entries since last update, and 𝑂 ( 2𝑏 ) is the upper bound for the deferred data (Theorem 4). As 𝜖 sDPTimer has a fixed update frequency, thus 𝑐∗ is independent of 𝜖. However, the amount of deferred data is bounded by 𝑂 ( 2𝑏 ), 𝜖 which leads to a decreasing trend in the L1 error of sDPTimer as 𝜖 increases. On the other hand, the update frequency of sDPANT"
2203.05084,data,171,,,"5.2 Shrink Protocol We propose two secure protocols that synchronize tuples from the secure cache to the materialized view. Our main idea originates from the private synchronization strategy proposed in DP-Sync [83], but with non-trivial variations and additional design. Typically, DPSync enforces trusted entities to execute private synchronization strategies, whereas in our scenario, the framework is supposed to allow untrusted servers to evaluate the view update protocol. Therefore, naïvely adopting their techniques could lead to additional leakage and exposure to untrusted servers, such as the internal states (i.e., randomness) during protocol execution. Furthermore, DP-Sync considers that the subjects evaluating the synchronization strategies have direct access to a local cache in the clear, whereas in our setup the protocol must synchronize cached view tuples from an exhaustively padded (with dummy entries) secure array without knowing how the real data is distributed. To address these problems, we incorporate the following techniques:"
2203.05084,data,174,,,"It’s not hard to bound 𝑐∗ by picking a smaller interval 𝑇 , however the upper bound for deferred data accumulates when 𝑘 increases. In addition, at each update, the server only fetches a batch of DP-sized data, leaving a large number of dummy tuples in the cache. Thus, to ensure bounded query errors and prevent the cache from growing too large, we apply an independent cache flushing mechanism to periodically clean the cache. To flush the cache, the protocol first sorts it, then fetches a set of data by cutting off a fixed number of tuples from the head of the sorted array. The fetched data is updated to the materialized view immediately and the remaining array is recycled (i.e. freeing the memory space). As per Theorem 4, we can set a proper flush size, such that with at most (a relatively small) probability 𝛽 there is non-dummy data been recycled."
2203.05084,data,179,,,"8 EXTENSIONS We discuss potential extensions of the original IncShrink design. Connecting with DP-Sync. For ease of demonstration, in the prototype design, we assume that data owners submit a fixed amount of data at fixed intervals. However, IncShrink is not subject to this particular record synchronization strategy. Owners can choose other private update policies such as the ones proposed in DP-Sync, and can also adapt our framework. Additionally, the view update protocol requires no changes or recompilation as long as the view definition does not change. On the other hand, privacy will still be ensured under the composed system that connects IncShrink with DP-Sync. For example, assume the owner adopts a record synchronization strategy that ensures 𝜖1-DP and the server is deployed with IncShrink that guarantees 𝜖2-DP with respect to the owner’s data. By sequential composition theorem [31], revealing their combined leakage ensures (𝜖1 + 𝜖2)-DP over the owner’s data. Similarly, such"
2203.05084,data,183,,,"Figure 5: Trade-off experiment. is variable and will be affected accordingly when 𝜖 changes. For example, a relatively small 𝜖 (large noise) will result in more frequent updates. As large noises can cause sDPANT to trigger an update early before enough data has been placed in the secure cache. As a result, a relatively small 𝜖 will lead to a correspondingly small 𝑐∗, which essentially produces smaller query errors. Additionally, when 𝜖 reaches a relatively large level, its effect on sDPANT’s update frequency becomes less significant. Increasing 𝜖 does not affect 𝑐∗ much, but causes a decrease in the amount of 16 log 𝑡 deferred data (bouned by 𝑂 ( ) as shown in Theorem 6). This 𝜖 explains why there is a decreasing trend of sDPANT’s L1 error after 𝜖 reaches a relatively large level. Nevertheless, both protocols show a privacy-accuracy trade-off, meaning that users can actually adjust privacy parameters to achieve their desired accuracy goals."
2203.05084,data,191,,,"4 PRIVACY MODEL In general, we consider our framework supports dynamic updating of the materialized view while hiding the corresponding update leakage. More specifically, we consider the participants involved in the outsourcing phase are a set of data owners and two servers S0, and S1. We assume there exists a semi-honest probabilistic polynomial time (p.p.t.) adversary A who can corrupt any subset of the owners and at most one of the two servers. Previous work [64] refers to this type of adversary as the admissible adversary, which captures the property of two non-colluding servers, i.e., if one is compromised by the adversary, the other one behaves honestly. Our privacy definition requires that the knowledge A can obtain about any single data of the remaining honest owners, by observing the view updates, is bounded by differential privacy. In this section, we first provide key terminologies and notations (Section 4.1) then formalize our privacy model (Section 4.2) using simulation-based computational differential privacy (SIM-CDP) [63]."
2203.05084,data,203,,,"Definition 4 defines the secure protocol for maintaining the materialized view such that as long as there exists at least one honest owner, the privacy of her data’s individual records is guaranteed. In addition, the remaining entities’ knowledge about honest owner’s data is preserved by differential privacy. In other words, any p.p.t. adversary’s knowledge of such protocol Π is restricted to the outputs of an 𝜖-DP mechanism 𝐹 . We refer to the mechanism 𝐹 as the leakage profile of protocol Π, and a function related to the update pattern, i.e., 𝐹 (D) = 𝑓 (UpdtPatt(D)). In the rest of this paper, we focus mainly on developing view update protocols that satisfy this definition. Moreover, Definition 4 is derived from the SIM-CDP definition which is formulated under the Universal Composition (UC) framework [17]. Thus Definition 4 leads to a composability property such that if other protocols (i.e., Query protocol) defined by the underlying databases also satisfy UC security, then privacy guarantee holds under the composed system."
2203.05084,data,206,,,"𝑐 = 0 and secret shares it to both servers (Alg 1:1-2). Transform uses trans_truncate (Alg 1:3) operator to obliviously compute the new view tuples and truncate the contribution of each record at the same time. More specifically, we assume the output of this operator is stored in an exhaustively padded secure array ΔV, where each ΔV [𝑖] is a transformed tuple with an extra isView bit to indicate whether the corresponding tuple is a view entry (isView=1) or a dummy tuple (isView=0). Additionally, the operator ensures that ∀𝑑𝑠𝑖 ∈ DS𝑡 , ||𝑔𝜔 (DS𝑡 ) − 𝑔𝜔 (DS𝑡 − {𝑑𝑠𝑖 }) || ≤ 𝜔 (3) where 𝑔𝜔 (·) ← truncate (new_entry(·), 𝜔). This indicates any input data only affects at most 𝜔 rows in the truncated ΔV. Once the truncated outputs are available, Transform updates and re-shares the cardinality counter 𝑐, then appends the truncated outputs to the secure cache 𝝈 (Alg 1:5-7). 𝑞-stable transformation. We now provide the following lemmas with respect to the 𝑞-stable transformation."
2203.05084,data,212,,,"1 INTRODUCTION There is a rapid trend of organizations moving towards outsourcing their data to cloud providers to take advantages of its costeffectiveness, high availability, and ease of maintenance. Secure outsourced databases are designed to help organizations outsource their data to untrusted cloud servers while providing secure query functionalities without sacrificing data confidentiality and privacy. The main idea is to have the data owners upload the encrypted or secret-shared data to the outsourcing servers. Moreover, the servers are empowered with secure protocols which allow them to process queries over such securely provisioned data. A series of works such as CryptDB [69], Cipherbase [4], EnclaveDB [70], and HardIDX [33] took the first step in the exploration of this scope by leveraging strong cryptographic primitives or secure hardware to accomplish the aforementioned design goals. Unfortunately, these solutions fail to provide strong security guarantees, as recent works on leakageabuse attacks [10, 19, 49, 88] have found that they are vulnerable to a variety of reconstruction attacks that exploit side-channel leakages. For instance, an adversary can fully reconstruct the data distribution after observing the query processing transcripts."
2203.05084,data,216,,,"From then on, for each time step, the protocol gets the secret shares ⟦𝑐⟧𝑚 (true cardinality) and ⟦ ˜𝜃 ⟧𝑚 from S0 and S1, which are subsequently recovered inside the protocol. sDPANT distorts the recovered cardinality ˜𝑐 ← Lap( 4𝑏 ) and compares the noisy 𝜖1 cardinality ˜𝑐 with ˜𝜃 . A view update is posted if ˜𝑐 ≥ ˜𝜃 . By issuing updates, sDPANT distorts 𝑐 with another Laplace noise Lap( 𝑏 ) to 𝜖2 obtain the read size sz. Similar to sDPTimer, it obliviously sorts the secure cache and fetches as many tuples as specified by sz from the head of the sorted cache. Note that, each time when an update is posted, sDPANT must re-generate the noisy threshold with fresh randomness. Therefore, after each updates, sDPANT resets 𝑐 = 0, produces a new ˜𝜃 , and updates the corresponding secret shares stored on the two servers (Alg 3:11-13). In addition, the same cache flush method in sDPTimer can be adopted by sDPANT as well. The following theorem provides an upper bound on the cached data at each time, which can be used to determine the cache flush size."
2203.05084,data,216,,,"Table 1: Simulator construction (sDPTimer) S appends 𝐵2 to 𝐵 and then samples 𝐵3 from 𝐵 such that the resulting cardinality of 𝐵3 equals to 𝑣𝑡 (if 𝑣𝑡 = 0 then S sets 𝐵3 = {∅}). This step simulates the data synchronized by Shrink protocol. Finally, if 𝑡 (mod 𝑇 ) = 0, S generates one additional random value 𝑥 to simulate the secret share of the cardinality counter and reveals 𝑥 together with the generated data batches to the adversary (2.iii). If 0 ≡ 𝑓 (mod 𝑡), then S performs another random sample from internal storage 𝐵 to fetch 𝐵′ such that |𝐵′| = 𝑠, followed by resetting 𝐵 to empty. S reveals 𝐵1, 𝐵2, 𝐵3, 𝐵′ and one additional random value 𝑥 to the adversary. These steps (2.iv) simulate the cache flush. Otherwise S only reveals 𝐵1, 𝐵2 and 𝐵3. The computational indistinguishability between the 𝐵1, 𝐵2, 𝐵3, 𝑥 and the messages the adversary can obtain from the real protocol follows the security of (2, 2)-secret-sharing scheme and the security of secure 2PC [57]. □"
2203.05084,data,22,,,• View-based query answering. IncShrink enables viewbased query answering for a class of specified queries over secure outsourced growing data.
2203.05084,data,220,,,"Example 5.1 (𝑏-truncated oblivious sort-merge join). Assume two tables 𝑇1, 𝑇2 to be joined, the algorithm outputs the join table between 𝑇1 and 𝑇2 such that each data owns at most 𝑏 rows in the resulting join table. The first step is to union the two tables and then obliviously sort [5] them based on the join attributes. To break the ties, we consider 𝑇1 records are ordered before 𝑇2 records. Then similar to a normal sort-merge join, where the operator linearly scans the sorted merged table then joins 𝑇1 records with the corresponding records in 𝑇2. There are some variations to ensure obliviousness and bounded contribution. First, the operator keeps track of the contribution of each individual tuple. If a tuple has already produced 𝑏 join entries, then any subsequent joins with this tuple will be automatically discarded. Second, for linear scan, the operator outputs 𝑏 entries after accessing each tuple in the merged table, regardless of how many true joins are generated. If there are fewer joins, then pad them with additional dummy data, otherwise truncate the true joins and keep only the 𝑏 tuples. Figure 2 illustrates the aforementioned computation workflow."
2203.05084,data,263,,,"Observation 2. DP protocols provide a balance between the two dimensions of accuracy and efficiency. According to Table 2, the DP protocols demonstrate at least 50× and 107× accuracy advantages (in terms of L1-error), respectively for TPC-ds and CPDB, over the OTM method. Meanwhile, in terms of performance, the DP protocols show a significant improvement in contrast to the EP method. For example, in TPC-ds group, the average QETs of both sDPTimer (0.051s) and sDPANT (0.052s) are almost 120× smaller than that of EP method (5.84s). Such performance advantage is even evident (up to 302×) over the CPDB data as testing query Q2 has join multiplicity greater than 1. Although, the DP approaches cannot achieve a complete accuracy guarantee, the average relative errors of all tested queries under DP protocols are below 4.3%. These results are sufficient to show that DP approaches do provide a balance between accuracy and efficiency. This conclusion can be better illustrated with Figure 4, where we can observe that EP and OTM are located in the upper left and lower right corners of each plot, respectively, which indicates that they either completely sacrifice efficiency (EP) or accuracy (OTM) guarantees. However, both DP methods lie at the bottom-middle position of both figures, which further reveals that the DP protocols are optimized for the dual objectives of accuracy and efficiency."
2203.05084,data,27,,,• We adopt a truncated view transformation functionality to ensure that each outsourced data contributes to a bounded number of rows in the transformed view instance.
2203.05084,data,270,,,"(c) Avg. Transform execution time (d) Avg. Shrink execution time Figure 8: Evaluations with different truncation bound 𝜔. Observation 7. As 𝜔 grows from a small value, query accuracy increases and query efficiency decreases quickly. After 𝜔 reaches a relatively large value, sDPTimer and sDPANT exhibit different trends in accuracy, but the same tendency in efficiency. As per Figure 8, the average L1 error decreases when 𝜔 grows from 𝜔 = 2. This is because when 𝜔 is small, many true view entries are dropped by the Transform protocol due to truncation constraint, which leads to larger L1 query errors. However, when 𝜔 reaches a relatively large value, i.e., greater than the maximum record contribution, then no real entries are discarded. At this point, increasing 𝜔 only leads to the growth of injected DP noises. As we have analyzed before, the accuracy under sDPANT can be better for relatively large noise, but the accuracy metric will be worse under sDPTimer method. On the other hand, dropping a large number of real entries (when 𝜔 is small) leads to a smaller materialized view, which consequently improves query efficiency. When 𝜔 is greater than the maximum record contribution, based on our analysis in Observation 4, keep increasing 𝜔 leads to both methods to introduce more dummy data to the view and causes its size to keep growing. As such, the efficiency continues decreasing."
2203.05084,data,33,,,• Privacy against untrusted server. Our framework allows untrusted servers to continuously update the materialized view while ensuring that the privacy of the owners’ data is preserved against outsourcing servers.
2203.05084,data,334,,,"Apparently, one can directly replicate the design of this paper to support complex queries by first compiling a Transform protocol that produces and caches the corresponding view tuples based on the specified query plan, while a Shrink protocol is used independently to continuously synchronize the cached data. However, there exists another design pattern that utilizes multi-level “Transform-and-Shrink” protocol. For example, we can disassemble a query into a series of operators and then construct an independent ""Transform-and-Shrink"" protocol for each individual operator. Moreover, the output of one ""Transform-andShrink"" protocol can be the input of another one, which eventually forms a multi-level view update protocol. There are certain benefits of the multi-level design, for instance, one can optimize the system efficiency via operator level privacy allocation [7]. Recall that in Section 5.2 we discussed that the choice of privacy budget affects the number of dummy records processed by the system, with a higher proportion of dummy records reducing overall performance and vice versa. To maximize performance, one can construct an optimization problem that maximizes the efficiency of all operators in a given query, while maintaining the desired accuracy level. With a privacy budget allocation determined by the optimization problem, each operator can carry out its own instance of IncShrink, minimizing the overall computation cost while satisfying desired privacy and accuracy constraints. Note that optimization details are beyond the scope of this paper but may be of independent interest and we leave the design of these techniques to future work. Expanding to multiple servers. Although in our prototype IncShrink design we assume to leverage 2 non-colluding servers, the system architecture can be modified to work with multiple servers. In what follows, we summarize the major modifications that would extend our current design to a 𝑁 servers setup such that 𝑁 ≥ 2."
2203.05084,data,334,,,"composability can also be obtained in terms of the accuracy guarantee. For instance, let’s denote the error bound for the selected record synchronization policy as 𝛼𝑟 (total number of records not uploaded in time). Then by Theorem 4 and 6, the combined system √ ensures error bounds 𝑂 (𝑏𝛼𝑟 + 2𝑏 ) under 𝜖 sDPTimer and sDPANT protocol, respectively. Interested readers may refer to our full version for the complete utility proofs. Support for complex query workloads. Now we describe how to generalize the view update protocol for complex query workloads, i.e. queries that can be written as a composite of multiple relational algebra operators. Apparently, one can directly replicate the design of this paper to support complex queries by first compiling a Transform protocol that produces and caches the corresponding view tuples based on the specified query plan, while a Shrink protocol is used independently to continuously synchronize the cached data. However, there exists another design pattern that utilizes multi-level “Transform-and-Shrink” protocol. For example, we can disassemble a query into a series of operators and then construct an independent ""Transform-and-Shrink"" protocol for each individual operator. Moreover, the output of one ""Transform-andShrink"" protocol can be the input of another one, which eventually forms a multi-level view update protocol. There are certain benefits of the multi-level design, for instance, one can optimize the system efficiency via operator level privacy allocation [7]. Recall that in Section 5.2 we discussed that the choice of privacy budget affects the number of dummy records processed by the system, with a higher proportion of dummy records reducing overall performance and vice versa. To maximize performance, one can construct an optimization problem that maximizes the efficiency of all operators in a given query, while maintaining the desired accuracy level."
2203.05084,data,336,,,"To maximize performance, one can construct an optimization problem that maximizes the efficiency of all operators in a given query, while maintaining the desired accuracy level. With a privacy budget allocation determined by the optimization problem, each operator can carry out its own instance of IncShrink, minimizing the overall computation cost while satisfying desired privacy and accuracy constraints. Note that optimization details are beyond the scope of this paper but may be of independent interest and we leave the design of these techniques to future work. Expanding to multiple servers. Although in our prototype IncShrink design we assume to leverage 2 non-colluding servers, the system architecture can be modified to work with multiple servers. In what follows, we summarize the major modifications that would extend our current design to a 𝑁 servers setup such that 𝑁 ≥ 2. Firstly, the owners need to share their local data using the (𝑁 , 𝑁 )secret-sharing scheme, and disseminate one share per participating server. In addition, for all outsourced objects, such as the secure cache, the materialized view, and parameters passed between view update protocols, must be stored on the 𝑁 servers in a secret shared manner. Secondly, both Transform and Shrink protocol will be compiled as a general MPC protocol where 𝑁 parties (servers) provide their confidential input and evaluate the protocol altogether. Finally, when generating DP noises, each server needs to contribute a random bit string to the MPC protocol, which subsequently aggregates the 𝑁 random strings to obtain the randomness used for noise generation. Note that our joint noise addition mechanism ensures to produce only one instance of DP noise, thus expanding to 𝑁 servers setting does not lead to injecting more noise. According to [51, 52], such design can tolerate up to 𝑁 − 1 server corruptions."
2203.05084,data,34,,,"[50] Georgios Kellaris, George Kollios, Kobbi Nissim, and Adam O’Neill. 2017. Ac cessing data while preserving privacy. arXiv preprint arXiv:1706.01552 (2017)."
2203.05084,data,34,,,[53] Daniel Kifer and Ashwin Machanavajjhala. 2011. No free lunch in data privacy. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of data. 193–204.
2203.05084,data,35,,,"𝛽 , where 𝛽 ∈ (0, 1). The number of deferred data 𝑐𝑑 after 𝑘-th updates satisfies Pr [𝑐𝑑 ≥ 𝛼] ≤ 𝛽, where 𝛼 = 2𝑏 𝜖"
2203.05084,data,37,,,[62] Frank D McSherry. 2009. Privacy integrated queries: an extensible platform for privacy-preserving data analysis. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data. 19–30.
2203.05084,data,37,,,"[76] Shuang Song, Yizhen Wang, and Kamalika Chaudhuri. 2017. Pufferfish privacy mechanisms for correlated data. In Proceedings of the 2017 ACM International Conference on Management of Data. 1291–1306."
2203.05084,data,38,,,"[42] Bijit Hore, Sharad Mehrotra, and Gene Tsudik. 2004. A privacy-preserving index for range queries. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. 720–731."
2203.05084,data,41,,,"[2] Rakesh Agrawal, Jerry Kiernan, Ramakrishnan Srikant, and Yirong Xu. 2004. Order preserving encryption for numeric data. In Proceedings of the 2004 ACM SIGMOD international conference on Management of data. 563–574."
2203.05084,data,42,,,"[18] Yang Cao, Masatoshi Yoshikawa, Yonghui Xiao, and Li Xiong. 2017. Quantifying differential privacy under temporal correlations. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE). IEEE, 821–832."
2203.05084,data,44,,,"2.3 IncShrink Workflow We now briefly review the framework architecture and illustrate its workflow with a running example (as shown in Figure 1), where an analyst is interested in a join query over the outsourced data from two data owners."
2203.05084,data,44,,,"Theorem 17. Applying IncShrink over the outsourced data uploaded by an (𝛼, 𝛽)-accurate private synchronization strategy 𝑟 _𝑠𝑦𝑛𝑐, √ results in error bounds 𝑂 (𝑏𝛼 + 2𝑏 ), respec𝜖 tively for sDPTimer and sDPANT protocol."
2203.05084,data,44,,,• IncShrink imposes constraints on the record contribution to view tuples which ensures the entire transformation from outsourced data to the view object over time has bounded stability. This further implies a bounded privacy loss with respect to each individual logical update.
2203.05084,data,45,,,"Finally, since 𝑓2 obliviously sorts then fetches from the recovered input data, thus the output of 𝑓2 should be computational indistinguishable from the random sampling over the recovered input data. This also applies to cache flush, and therefore we can obtain"
2203.05084,data,45,,,"𝛽 , where 𝛽 ∈ (0, 1). Suppose the cache flush interval is 𝑓 with flush size 𝑠. Then the number of data entries inserted to the materialized view after the 𝑘-th update is bounded by 𝑂 ( 2𝑏 𝜖"
2203.05084,data,46,,,"according to Eq. 7, with probability at most 𝛽, the number of de16𝑏 (log 𝑡 +log (2/𝛽)) ferred data, (cid:205)𝑡 . (𝑐𝑖 −𝑎𝑖 ) is greater than 𝛼 ≥ 𝜖 16𝑏 log 𝑡 𝜖"
2203.05084,data,47,,,"In general, as the logical gap of materialized view is resulted by (i) the total data delayed by 𝑟 _𝑠𝑦𝑛𝑐 and (ii) the total view entries delayed by Shrink protocol. Thus the logical gaps of the two mechanisms are additive."
2203.05084,data,47,,,"[75] Elaine Shi, John Bethencourt, TH Hubert Chan, Dawn Song, and Adrian Perrig. 2007. Multi-dimensional range query over encrypted data. In 2007 IEEE Symposium on Security and Privacy (SP’07). IEEE, 350–364."
2203.05084,data,47,,,"[7] Johes Bater, Xi He, William Ehrich, Ashwin Machanavajjhala, and Jennie Rogers. 2018. Shrinkwrap: efficient sql query processing in differentially private data federations. Proceedings of the VLDB Endowment 12, 3 (2018), 307–320."
2203.05084,data,48,,,"[67] Sarvar Patel, Giuseppe Persiano, Kevin Yeo, and Moti Yung. 2019. Mitigating leakage in secure cloud-hosted data structures: Volume-hiding for multi-maps via hashing. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 79–93."
2203.05084,data,49,,,"[29] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. 2006. Our data, ourselves: Privacy via distributed noise generation. In Annual International Conference on the Theory and Applications of Cryptographic Techniques. Springer, 486–503."
2203.05084,data,49,,,"[33] Benny Fuhry, Raad Bahmani, Ferdinand Brasser, Florian Hahn, Florian Kerschbaum, and Ahmad-Reza Sadeghi. 2017. HardIDX: Practical and secure index with SGX. In IFIP Annual Conference on Data and Applications Security and Privacy. Springer, 386–408."
2203.05084,data,50,,,"2002 ACM SIGMOD international conference on Management of data. 216–227. [41] Bijit Hore, Sharad Mehrotra, Mustafa Canim, and Murat Kantarcioglu. 2012. Secure multidimensional range queries over outsourced data. The VLDB Journal 21, 3 (2012), 333–358."
2203.05084,data,51,,,"[20] David Cash, Joseph Jaeger, Stanislaw Jarecki, Charanjit S Jutla, Hugo Krawczyk, Marcel-Catalin Rosu, and Michael Steiner. 2014. Dynamic searchable encryption in very-large databases: data structures and implementation.. In NDSS, Vol. 14. Citeseer, 23–26."
2203.05084,data,52,,,"Observation 6. When 𝜖 is small, two DP protocols have different biases in terms of accuracy and performance. According to Figure 7a and 7d, when 𝜖 = 0.1, the data points for the sDPANT locate in the upper left corner of both figures, while the sDPTimer"
2203.05084,data,55,,,"𝑗 is computed from the secure 2PC protocol 𝜋, thus by Thec orem 5, there must exists simulator such that S(𝑥 𝑗 , 𝑓𝑗 (𝑥 𝑗 , 𝑥1−𝑗 )) = 𝑗 . In addition, since 𝛾 𝑣 𝛾 𝑣 𝑗 is assumed to be secret-shared data, thus"
2203.05084,data,56,,,"According to the testing query, we set the materialized view as a join table for all products that returned within 10 days. Furthermore, as Q1 has multiplicity 1, thus we set the truncation bound as 𝜔 = 1, and the total contribution budget for each data as 𝑏 = 10."
2203.05084,data,58,,,"Theorem 16 ((𝛼, 𝛽)-accurate sync strategy). A record synchronization strategy 𝑟 _𝑠𝑦𝑛𝑐 over growing data D is (𝛼, 𝛽)-accurate if there exists 𝛼 > 0, and 0 < 𝛽 < 1, such that the logical gap when outsourcing D with 𝑟 _𝑠𝑦𝑛𝑐 satisfies, ∀𝑡"
2203.05084,data,58,,,"Theorem 6. Given 𝜖, 𝑏, and let the cache flushes every 𝑓 time steps with fixed flushing size 𝑠, the number of deferred data at any time 𝑡 is bounded by 𝑂 ( ) and the total number of dummy data inserted to the materialized view is bounded by 𝑂 ( 𝑓 ⌋."
2203.05084,data,58,,,"• We utilize a joint noise-adding mechanism to generate DP noise, which ensures that no admissible adversary can obtain or tamper with the randomness used to generate the noise. • We implement a secure cache read operation that enforces the real data is always fetched before the dummy tuples when a cache read is performed."
2203.05084,data,62,,,"where 𝛾𝑢 𝑗 , 𝑐 𝑗 , and 𝜃 𝑗 denotes the corresponding secret shares party 𝑗 obtains for user uploaded data, transformed view tuples, synchronized data, flushed data, cardinality counter and the noisy threshold (this parameter is not included in the view of sDPTimer protocol). Since 𝛾𝑢 𝑗 is the secret shared data generated"
2203.05084,data,67,,,"Thus, Munit satisfies 𝜖-DP. Moreover, mechanism NANT can be expressed as the composition of a Munit and a Laplace mechanism, each with a privacy parameter of 𝜖 2 . Therefore by sequential composition, NANT satisfies 𝜖-DP. Similar, Mant can be treated as repeatedly running NANT over disjoint data, thus by Eq. 11. Mant □ satisfies 𝜖-DP."
2203.05084,data,67,,,"• We evaluate IncShrink on use cases inspired by the Chicago Police Data and the TPC-ds benchmark. The evaluation results show at least 7800× and up to 1.5e+5× query efficiency improvement over standard SOGDB. Moreover, our evaluation shows that IncShrink provides a 3-way trade-off between privacy, efficiency, and utility while allowing users to adjust the configuration to obtain their desired guarantees."
2203.05084,data,68,,,"𝑡 −𝑇 D = D𝑡 −D𝑡 −𝑇 denotes the logical updates (records) received between time (𝑡 − 𝑇 , 𝑡]. Since output a constant is independent of the input data, thus proving the privacy of Mtimer (D) is reduced to proving the privacy of the composed mechanism M (D) = {Munit (Δ(𝑖+1)𝑇"
2203.05084,data,69,,,"Initially, S initializes the internal storage 𝐵. Then for each time step, S randomly samples 2 batches of data 𝐵1, 𝐵2 from Z𝑚, where the cardinality of 𝐵1, and 𝐵2 equals to 𝐶𝑟 , and 𝑏𝐶𝑟 , respectively. These two batches simulate the secret shared data uploaded by owners and the transformed tuples placed in cache at time 𝑡. Next,"
2203.05084,data,73,,,"[72] Bharath Kumar Samanthula, Wei Jiang, and Elisa Bertino. 2014. Privacypreserving complex query evaluation over semantically secure encrypted data. In European Symposium on Research in Computer Security. Springer, 400–418. [73] Zhiwei Shang, Simon Oya, Andreas Peter, and Florian Kerschbaum. 2021. Obfuscated Access and Search Patterns in Searchable Encryption. arXiv preprint arXiv:2102.09651 (2021)."
2203.05084,data,83,,,"Note that query errors of IncShrink are caused by two factors, namely, the valid data discarded by the Transform due to truncation constraints and the total amount of unsynchronized data in the cache. When the truncation bound is set too small, then a large number of valid view entries are dropped by Transform, resulting in inaccurate query answers. We further investigate how truncation bound would affect query errors in a later section (Section 7.4)."
2203.05084,data,87,,,"7.4 Evaluation with Different 𝜔 In this section, we investigate the effect of truncation bounds by evaluating IncShrink under different 𝜔 values. Since the multiplicity of Q1 is 1, the 𝜔 for answering Q1 is fixed to 1. Hence, in this evaluation, we focus on Q2 over the CPDB data. We pick different 𝜔 values from the range of 2 to 32 and set the contribution budget as 𝑏 = 2𝜔. The result is reported in Figure 8."
2203.05084,data,88,,,"10 CONCLUSION In this paper, we have presented a framework IncShrink for outsourcing growing data onto untrusted servers while retaining the query functionalities over the outsourced data. IncShrink not only supports an efficient view-based query answering paradigm but also ensures bounded leakage in the maintenance of materialized view. This is achieved by (i) utilizing incremental MPC and differential privacy to architect the secure view update protocol and (ii) imposing constraints on record contributions to the transformation of materialized view instance."
2203.05084,data,88,,,"There is no doubt one can benefit in many aspects from the viewbased query answering paradigm. However, designing a practical view-based SOGDB is fraught with challenges. First, the servers that maintain the materialized view is considered to be potentially untrusted. Hence, we must explore the possibility of updating such materialized view without a trusted curator. A typical way is to leverage secure multi-party computation (MPC). However, naïvely applying MPC for updating view instances over growing data would"
2203.05084,data,89,,,"the entire cache to the client, or scanning the cache while processing the query so that no data is missing. This will not break the security promise, however, will either increase the communication cost or the runtime overhead as the secure cache is exhaustively padded with dummy data. Moreover, in later evaluations (Section 7.1), we observe that even without using the cache for query processing, the relative query errors are small (i.e. bounded by 0.04)."
2203.05084,data,91,,,"For CPDB data, we consider only Allegation table is private and is delegated to a client program. The Award table will be treated as a public relation. Again, we use the investigation case end time to indicate when the client program received this record, and we assume that the client outsource data once every 5 days (minimum time span is 5 days), and the data is padded to maximum possible size as well. For evaluation, we select the following query."
2203.05084,data,92,,,"Although some recent efforts, such as [7, 11, 28, 32, 46, 65, 67, 68, 87, 89], have shown potential countermeasures against leakageabuse attacks, the majority of these works focus primarily on static databases. A more practical system often requires the support of updates to the outsourced data [1, 35, 78, 83], which opens up new challenges. Wang et al. [83] formulate a new type of leakage called"
2203.05084,data,94,,,"As per Theorem 4, we can derive the upper bound for total cached data at any time as 𝑐∗ + 𝑂 ( 2𝑏 ), where 𝑐∗ refers to the 𝜖 number of newly cached entries since last update, and the second term captures the upper bound for deferred data. The data in the cache, although stored on the server, is not used to answer queries. Therefore large amounts of cached data can lead to inaccurate query results. Although one may also adopt strategies such as returning"
2203.05084,data,95,,,"Additionally, we also compare the two protocols with varying non-privacy parameters, i.e. 𝑇 and 𝜃 , where we fix the 𝜖 then change 𝑇 from 1-100, and correspondingly set 𝜃 according to 𝑇 (As mentioned before, the average new view entries per moment are 2.7 and 9.8 for TPC-ds and CPDB data, respectively, thus we set 𝜃 to 3𝑇 and 10𝑇 ). We test the protocols with three privacy levels 𝜖 = 0.1, 1 and 10 and report their comparison results in Figure 7."
2203.05084,data,95,,,"Algorithm 1 provides an overview of Transform protocol. At the very outset, Transform is tasked to: (i) convert the newly submitted data into its corresponding view entry from time to time; (ii) cache the new view entries to an exhaustively padded secure array; and (iii) maintain a cardinality counter of how many new view entries have been cached since the last view update. This counter is then privately passed (through secret shares) to the Shrink protocol. Algorithm 1 Transform protocol"
2203.05084,data,96,,,"A.1.1 Oblivious selection (filter). Since each input record can only contribute to the output of the selection operator at most once. Therefore, it does not require us to have additional implementations to constraint the record contributions. To ensure obliviousness, the operator will return all input data as the output. However, only records that satisfy the selection predicate will have its isView bit set to 1. As a result, records that do not satisfy the selection predicate are treated as dummy tuples (isView=0)."
2203.05084,"data, database",184,,,"In this work, we take the next step towards designing a secure outsourced growing database (SOGDB) architecture with a more efficient query answering mechanism. Our proposed framework employs a novel secure query processing method in which the servers maintain a growing size materialized view corresponding to the owner’s outsourced data. The upcoming queries will be properly answered using only the materialized view object. This brings in inherent advantages of view-based query answering [77] paradigm, such as allowing the servers to cache important intermediate outputs, thus preventing duplicated computation. For instance, with our view-based SOGDB architecture, one can address the performance issues in the aforementioned use case by requiring the servers to maintain a materialized join table between the outsourced sales and delivery data. Moreover, whenever the underlying data changes, the materialized join table is updated accordingly. To this end, the servers only need to perform secure filtering over the materialized join table for processing queries, which avoids duplicated computation of join relations."
2203.05084,"data, database",184,,,"expose extra information leakage (i.e., update pattern [83]). For example, consider the use case we mentioned before where the servers maintain a join table over the sales and the delivery data. Even with MPC, one can still obtain the true cardinality of newly inserted entries to the join table by looking at the output size from MPC. This would allow the adversary to learn the exact number of packages requested for delivery by the local retail store at any given time. Naïve methods, such as always padding newly generated view tuples to the maximum possible size or choosing never to update the materialized view, could prevent the aforementioned leakage. However, such an approach either introduces a large performance burden or does not provide us with the functionality of database updates. To combat this, we propose a novel view update methodology that leverages incremental MPC and differential privacy (DP), which hides the corresponding update leakage using DP while balancing between the efficiency and accuracy."
2203.05084,"data, database",198,,,"According to Lemma 1, it’s clear that protocol Transform is 𝑞stable, and thus by Lemma 2, applying an 𝜖-DP mechanism over the outputs of Transform protocol (the cached data) implies 𝑞𝜖-DP over the original data. Therefore, if 𝑞 is constant, then the total privacy loss with respect to the input of Transform is bounded. Contribution over time. According to the overall architecture, Transform is invoked repeatedly for transforming outsourced data into view entries at each time step. Thus having a 𝑞-stable Transform does not immediately imply bounded privacy loss with respect to the logical database. There are certain cases where one record may contribute multiple times over time as the input to Transform. For example, suppose the servers maintain a join table on both Alice’s and Bob’s data. When Alice submits new data, the servers need to compute new join tuples between her new data and Bob’s entire database, which results in some of Bob’s data being used multiple times. This could eventually lead to unbounded privacy loss."
2203.05084,"data, database",219,,,"Materialized view. We use V to denote the materialized view instance which is a collection of secret-shared tuples. Each tuple in V is transformed from the outsourced data DS or in combination with public information. We define V = {V𝑡 }𝑡 ≥0, where V𝑡 denotes the materialized view structure at time 𝑡, and ΔV𝑡 denotes the changes between (newly generated view entries) V𝑡 and V𝑡 −1 Query. Given a growing database D and a corresponding materialized view V, we define the logical query posted at time 𝑡 as 𝑞𝑡 (D𝑡 ) and the re-written view-based query as ˜𝑞𝑡 (V𝑡 ). We refer the L1 norm of the difference between ˜𝑞𝑡 (V𝑡 ) and 𝑞𝑡 (D𝑡 ) as the L1 query error, denoted as 𝐿𝑞𝑡 ← || ˜𝑞𝑡 (V𝑡 ) − 𝑞𝑡 (D𝑡 )||1, which measures the difference between the server responded outputs and their corresponding logical results. Additionally, we call the elapsed time for processing ˜𝑞𝑡 (V𝑡 ) as the query execution time (QET) of 𝑞𝑡 . In this work, we use L1 error and QET as the main metrics to evaluate the accuracy and efficiency of our framework, respectively."
2203.05084,"data, database",226,,,"These designs differ in the types of supported queries and the provided security guarantees. Although the initial goal was to conceal the record values [2, 6, 9, 12, 13, 15, 40, 42, 59, 66, 69, 69, 75], researchers soon discovered the shortcomings of this security assurance. Recent work has revealed that these methods may be subject to certain leakage through query patterns [84, 88], access patterns [27, 49] and query response volume [37–39, 49], which makes them vulnerable to leakage-abuse attacks [10, 19]. Therefore, more recent works on secure outsourced databases not only consider concealing record values but also hiding associated leakages [3, 6, 7, 11, 20, 24, 28, 32, 35, 43, 45, 46, 50, 65, 67, 78, 87, 89]. Unfortunately, few of the aforementioned efforts consider the potential leakage when underlying data is dynamic [3, 20, 35, 50]. Wang et al. [83] formalize a general leakage named update pattern that may affect many existing secure database schemes when outsourcing dynamic data."
2203.05084,"data, database",228,,,"KI-3. Fixed privacy loss through constraints on record contributions. When IncShrink releases noisy cardinalities, it ensures 𝜖-DP with respect to the view instance. However, this does not imply 𝜖-DP to the logical data where the view is derived. This is because an individual data point in the logical database may contribute to generating multiple view entries. As a result, the framework either incurs an unbounded privacy loss or has to stop updating the materialized view after sufficiently many synchronizations. This leads us to our third key idea, where we bound the privacy loss by imposing constraints on the contributions made by each individual record to the generation of the view object. Each data point in the logical database is allocated with a contribution budget, which is consumed whenever the data is used to generate a new view entry. Once the contribution budget for a certain record is exhausted, IncShrink retires this data and will no longer use it to generate view entries. With such techniques, IncShrink is able to constantly update the materialized view with a bounded privacy loss. On the other hand, despite such constraints over record contributions, IncShrink is still able to support a rich class of queries with relatively small errors (Section 7)."
2203.05084,"data, database",228,,,"a secret event, therefore such mechanism 𝐹 achieves 𝜖-event level DP [30]. However, due to the group-privacy property [53, 58, 86] of DP, one can achieve privacy for properties across multiple updates as well as at the user level as long as the property depends on a finite number of updates. An overall 𝜖-user level DP can be achieved by setting the privacy parameter in Definition 3 to 𝜖 ℓ , where ℓ is the maximum number of tuples in the growing database owned by a single user. In practice, if the number of tuples owned by each user is unknown, a pessimistic large value can be chosen as 𝑘. Moreover, recent works [18, 76] have provided methods for deriving an 𝜖 < 𝜖 ′ ≤ ℓ ×𝜖, such that 𝜖-event DP algorithms provide an 𝜖 ′ bound on privacy loss when data are correlated. For certain correlations, 𝜖 ′ can be even close to 𝜖 and much smaller than ℓ ×𝜖. In general, we emphasize that for the remainder of the paper, we focus exclusively on developing algorithms that ensure event-level privacy with parameter 𝜖, while simultaneously satisfying all the aforementioned privacy guarantees, with possibly a different privacy parameter."
2203.05084,"data, database",229,,,"5.1 Transform Protocol Whenever owners submit new data, Transform is invoked to convert the newly outsourced data to its corresponding view instance based on a predefined view definition. Although, one could simply reuse the query capability of the underlying database to generate the corresponding view tuples. There are certain challenges in order to achieve our targeted design objectives. Here are two examples: (i) The view transformation might have unbounded stability which further leads to an unbounded privacy loss; (ii) While existing work [7] implements a technique similar to first padding the output and then reducing its size, they compile the functionality as a one-time MPC protocol, which makes it difficult for them to handle dynamic data. Our design of constructing “Transform” and “Shrink” as independent MPC protocols overcome this problem and introduce flexibility in the choice of view update policy, but it raises a new challenge in that the two independently operating protocols still need to collaborate with each other. By default, the Shrink protocol is unaware of how much data can be removed from the secure cache, therefore Transform must privately inform Shrink how to eliminate the dummy records while ensuring DP. To address these challenges, we employ the following techniques:"
2203.05084,"data, database",232,,,"2.2 Framework Components Underlying database. IncShrink does not create a new secure outsourced database but rather builds on top of it. Therefore, as one of the major components, we assume the existence of an underlying secure outsourced database scheme. Typically, secure outsourced databases can be implemented according to different architectural settings, such as the models utilizing server-aided MPC [6, 7, 47, 64, 79], homomorphic encryption [22], symmetric searchable encryption [3, 9, 20, 26, 35, 48, 78] or trusted hardware [32, 70, 81, 89]. For the ease of demonstration, we focus exclusively on the outsourced databases built upon the server-aided MPC model, where a set of data owners secretly share their data to two untrusted but non-colluding servers S0 and S1. The two outsourcing servers are able to perform computations (i.e., query processing) over the secret shared data by jointly evaluating a 2party secure computation protocol. More details about this setting and its corresponding security definitions are provided in Section 4. We stress that, although the protocols described in this paper assumes an underlying database architected under the server-aided MPC setup, these protocols can be adapted to other settings as well."
2203.05084,"data, database",274,,,"The first category consists of works that enable DP query answering over securely provisioned (and potentially dynamic) data. Since these efforts typically focus solely on query outputs, side-channel leakages are not considered or assumed to be eliminable by existing techniques. Works in the second group focus on hiding side-channel information with DP, which is pertinent to our study. Among those, [7] and [83] are the two most relevant works to our study. [7] extends the work of [6], both of which use MPC as the main tool to architect secure outsourced databases. However, [6] fails to address some important leakages associated with intermediate computation results (i.e., the size of some intermediate outputs may leak sensitive information about the underlying data). Thus, [7] is proposed to fill this gap. [7] implements a similar resizing technique as IncShrink that ensures the volume leakage per secure operator is bounded by differential privacy, however, their system is restrictively focused on processing static data. [83] considers hiding update patterns when outsourcing growing data with private update strategies. However, they mandate that the update strategies must be enforced by trusted entities, while IncShrink allows untrusted servers to privately synchronize the materialized view. Additionally, [83] considers the standard mode that processes queries directly over outsourced data, which inevitably incurs additional performance overhead. Interested readers may refer to Sections 5.1 and 5.2, where we provide"
2203.05084,"data, database",294,,,"Initially, the analyst obtains authentications from the owners and registers the query with the outsourcing servers. The servers decide the view definition as a join table, set up the initial materialization structure, the secure cache, and then compile the corresponding secure protocols Transform and Shrink for maintaining the view instance. Since then, the owners periodically update the outsourced data, securely provisioning the newly received data since the last update (through the update functionality defined by the underlying database). For demonstration purposes, we assume that the owners submit a fixed-size data block (possibly padded with dummy records) at predetermined intervals. We discuss potential extensions to support other update behaviors in a later section. Whenever owners submit new data, the servers invoke Transform to securely compute new joins. The join outputs will be padded to the maximum size then placed into a secure cache. Next, Shrink is executed independently, where it periodically synchronizes data from the secure cache to the materialized join table with DP resized cardinalities. Note that the DP noise used to distort the true cardinality can be either positive or negative. If the noise is negative, some of the real tuples in the cache are not fetched by Shrink. We refer to the real view tuples left in the secure cache as the “deferred data”. On the contrary, if the noise is positive, some deferred data or additional dummy tuples will be included to synchronize with the view. On the other hand, the analyst can issue query requests to the servers, which process the issued queries over the materialized join"
2203.05084,"data, database",32,,,"[40] Hakan Hacigümüş, Bala Iyer, Chen Li, and Sharad Mehrotra. 2002. Executing SQL over encrypted data in the database-service-provider model. In Proceedings of the"
2203.05084,"data, database",334,,,"9 RELATED WORK Secure outsourced database and leakage abuse attacks. There have been a series of efforts under the literature of secure outsourcing databases. Existing solutions utilize bucketization [40–42], predicate encryption [59, 75], property and order preserving encryption [2, 9, 12, 13, 66, 68, 69], symmetric searchable encryption (SSE) [3, 11, 20, 26, 35, 45, 46, 48, 50, 67, 78], functional encryption [15, 74], oblivious RAM [6, 24, 28, 43, 65, 89], multi-party secure computation (MPC) [6, 7, 14, 79], trusted execution environments (TEE) [32, 70, 81, 87] and homomorphic encryption [16, 22, 34, 72]. These designs differ in the types of supported queries and the provided security guarantees. Although the initial goal was to conceal the record values [2, 6, 9, 12, 13, 15, 40, 42, 59, 66, 69, 69, 75], researchers soon discovered the shortcomings of this security assurance. Recent work has revealed that these methods may be subject to certain leakage through query patterns [84, 88], access patterns [27, 49] and query response volume [37–39, 49], which makes them vulnerable to leakage-abuse attacks [10, 19]. Therefore, more recent works on secure outsourced databases not only consider concealing record values but also hiding associated leakages [3, 6, 7, 11, 20, 24, 28, 32, 35, 43, 45, 46, 50, 65, 67, 78, 87, 89]."
2203.05084,"data, database",342,,,"Differentially-private leakage. Existing studies on hiding database leakage with DP can be divided into two main categories: (i) safeguarding the query results from revealing sensitive information [1, 22, 25, 56, 60], and (ii) obscuring side-channel leakages such as access pattern [7, 21, 50, 61, 73, 82], query volume [11, 67] and update patterns [83]. The first category consists of works that enable DP query answering over securely provisioned (and potentially dynamic) data. Since these efforts typically focus solely on query outputs, side-channel leakages are not considered or assumed to be eliminable by existing techniques. Works in the second group focus on hiding side-channel information with DP, which is pertinent to our study. Among those, [7] and [83] are the two most relevant works to our study. [7] extends the work of [6], both of which use MPC as the main tool to architect secure outsourced databases. However, [6] fails to address some important leakages associated with intermediate computation results (i.e., the size of some intermediate outputs may leak sensitive information about the underlying data). Thus, [7] is proposed to fill this gap. [7] implements a similar resizing technique as IncShrink that ensures the volume leakage per secure operator is bounded by differential privacy, however, their system is restrictively focused on processing static data. [83] considers hiding update patterns when outsourcing growing data with private update strategies. However, they mandate that the update strategies must be enforced by trusted entities, while IncShrink allows untrusted servers to privately synchronize the materialized view. Additionally, [83] considers the standard mode that processes queries directly over outsourced data, which inevitably incurs additional performance overhead."
2203.05084,"data, database",58,,,"Outsourced data. The outsourced data is denoted as DS, which stores the secret-shared entries corresponding to the records in the logical database, with the possibility to include additional dummy data. Similarly, we write DS = {DS𝑡 }𝑡 ≥0, where DS𝑡 ⊆ DS is the outsourced data at time 𝑡."
2203.05084,"data, database",60,,,"In general, Definition 1 defines the transcript of entire update history for outsourcing a growing database D. It may include information about the volume of the outsourced data and their corresponding insertion times. Moreover, if 𝑇𝑡 (D) ← D𝑡 − D𝑡 −1, then this simply indicates the record insertion pattern [83]."
2203.05084,"data, database",79,,,"Materialized view. A materialized view is a subset of a secure outsourced database, which is typically generated from a query and stored as an independent object (i.e., an encrypted or secretshared data structure). The servers can process queries over the view instance just as they would in a persistent secure database. Additionally, changes to the underlying data are reflected in the entries shown in subsequent invocations of the materialized view."
2203.05084,"data, database",83,,,"Definition 3 ensures that, by observing the output of 𝐹 , the information revealed by any single logical update posted by the owner is differentially private. Moreover, if the logical update corresponds to different entity’s (owner’s) data, the same holds for 𝐹 over each owner’s logical database (privacy is guaranteed for each entity). Additionally, in this work, we assume each logical update 𝑢𝑖 ∈ D as"
2203.05084,"data, database",84,,,"[80] Yuchao Tao, Xi He, Ashwin Machanavajjhala, and Sudeepa Roy. 2020. Computing Local Sensitivities of Counting Queries with Joins. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 479–494. [81] Dhinakaran Vinayagamurthy, Alexey Gribov, and Sergey Gorbunov. 2019. Stealthdb: a scalable encrypted database with full SQL query support. Proceedings on Privacy Enhancing Technologies 2019, 3 (2019), 370–388."
2203.05084,"data, database",96,,,"𝑡 differ by the addition or removal of a single logical update. Definition 3 (DP mechanism over growing data). Let 𝐹 to be a mechanism applied over a growing database. 𝐹 is said to satisfy 𝜖-DP if for any neighboring growing databases D and D ′, and any 𝑂 ∈ O, where O is the universe of all possible outputs, it satisfies: Pr [𝐹 (D) ∈ 𝑂] ≤ 𝑒𝜖 Pr (cid:2)𝐹 (D ′) ∈ 𝑂 (cid:3)"
2203.05084,"data, database, github",15,,,[23] cpdp.co. 2006. Chicago Police Database. https://github.com/invinst/chicago police-data
2203.05084,"data, database, package",235,,,"update pattern that affects many existing outsourced database designs when the underlying data is dynamically growing. To mitigate such weakness, their solution dictates the data owners’ update behavior to private record synchronization strategies, with which it perturbs the owners’ logical update pattern. However, their solution only considers a naïve query answering mode such that each query is processed independently and evaluated directly over the entire outsourced data. This inevitably leads to a substantial amount of redundant computation by the servers. For example, consider the following use case where a courier company partners with a local retail store to help deliver products. The retail store has its sales data, and the courier company has its delivery records, both of which are considered to be the private property of each. Now assume the retail store owner wants to know how many of her products are delivered on time (i.e., within 48 hours of the courier accepting the package). With secure outsourced databases, the store owner and the courier company have the option to securely outsource their data and its corresponding computations to cloud servers. However, in a naïve query processing mode, the servers have to recompute the entire join relation between the outsourced data whenever a query is posted, which raises performance concerns."
2203.05084,"data, dataset",120,,,"Figure 6: DP protocols under different workloads. for sparse data. On the contrary, when the data becomes very dense, i.e., there is a burst workload, the fixed update rate of sDPTimer causes a large amount of data to be stagnant in the secure cache. And thus causes significant degradation of the accuracy guarantee. However, sDPANT can adjust the update frequency according to the data type, i.e., the denser the data, the faster the update. This feature gives sDPANT an accuracy edge over sDPTimer when dealing with burst workloads. On the other hand, both methods show similar efficiency for all types of test datasets."
2203.05084,"data, dataset",154,,,"7.5 Scaling Experiments We continue to evaluate our framework with scaling experiments. To generate data with different scales, we randomly sample or replicate the original TPC-ds and CPDB data (We assign new primary key values to the replicated rows to prevent conflicts). According to Figure 9, for the largest dataset, i.e., the 4× groups, the total MPC time are around 24 and 6 hours, respectively for TPC-ds and CPDB. However, it is worth mentioning that for the 4× group, TPC-ds has 8.8 million and 1.08 million records in the two testing tables, and CPDB has 800K and 2.6 million records for 𝐴𝑙𝑙𝑒𝑔𝑎𝑡𝑖𝑜𝑛 and 𝐴𝑤𝑎𝑟𝑑 tables, respectively. This shows the practical scalability of our framework. In addition, the total query time for 4× TPC-ds and 4× CPDB groups are within 400 and 630 seconds, respectively."
2203.05084,"data, dataset",175,,,"Similarly, the materialized view is a join table that process Q2. We set the truncation bound 𝜔 = 10 and budget for each data as 𝑏 = 20. Default setting. Unless otherwise specified, we assume the following default configurations. For both DP protocols, we set the default privacy parameter 𝜖 = 1.5, and cache flush parameters as 𝑓 = 2000 (flush interval) and 𝑠 = 15 (flush size). We fix the sDPANT threshold 𝜃 as 30 for evaluating both datasets. Since the average number of new view entries added at each time step is 2.7 and 9.8, respectively for TPC-ds and CPDB, thus for consistency purpose we set the timer 𝑇 to 10 ← ⌊ 30 2.7 ⌋ and 3 ← ⌊ 30 9.8 ⌋. For each test group, we issue one test query at each time step and report the average L1 error and query execution time (QET) for all issued testing queries."
2203.05084,"data, dataset",196,,,"Observation 4. DP protocols have similar privacy-efficiency trade-off. Both DP protocols show similar trends in terms of efficiency metrics (Figure 5b and 5d), that is when 𝜖 increases, the QET decreases. It is because with a relatively large 𝜖, the number of dummy data included in the view will be reduced, thus resulting in a subsequent improvement in query efficiency. Thus, similar to the accuracy-privacy trade-off, the DP protocols also provide a privacy-efficiency trade-off that allow users to tune the privacy parameter 𝜖 in order to obtain their desired performance goals. 7.3 Comparison Between DP Protocols We address Question-3 by comparing the two DP protocols over different type of workloads. In addition to the standard one, for each dataset, we create two additional datasets. First, we sample data from the original data and create a Sparse one, where the total number of view entries is 10% of the standard one. Second, we process Burst data by adding data points to the original dataset, where the resulting data has 2× more view entries."
2203.05084,"data, dataset provided",116,,,"3 PRELIMINARIES Multi-party secure computation (MPC). MPC utilizes cryptographic primitives to enable a set of participants 𝑃1, 𝑃2, ..., 𝑃𝑛 to jointly compute a function 𝑓 over private input data 𝑥𝑖 supplied by each party 𝑃𝑖 , without using a trusted third party. The theory of MPC offers strong security guarantee similar as what can be achieved with a trusted third party [36], i.e., absolutely no information leak to each participant 𝑃𝑖 beyond the desired output of 𝑓 (𝑥1, 𝑥2, ..., 𝑥𝑛) and their input 𝑥𝑖 . In this work we focus mainly on the 2-party secure computing setting."
2203.05084,"data, dataset provided",174,,,"KI-2. Incremental MPC with DP update leakage. A key design goal of IncShrink is to allow the untrusted servers to privately update the view instance while also ensuring the privacy of owners’ logical data. As we mentioned before, compiling the view update functionality into the MPC protocol is not sufficient to ensure data privacy, as it still leaks the true cardinality of newly inserted view entries at each time, which is directly tied with the owners’ record update patterns [83]. Although naïve approaches such as exhaustive padding (EP) of MPC outputs or maintaining a one-time materialized view (OTM) could alleviate the aforementioned privacy risk. They are known to either incorporate a large amount of dummy data or provide poor query accuracy due to the lack of updates to the materialized view. This motivates our second key idea to design an incremental MPC protocol for view updates while balancing the privacy, accuracy, and efficiency guarantees."
2203.05084,"data, dataset, database",207,,,"ABSTRACT In this paper, we consider secure outsourced growing databases that support view-based query answering. These databases allow untrusted servers to privately maintain a materialized view, such that they can use only the materialized view to process query requests instead of accessing the original data from which the view was derived. To tackle this, we devise a novel view-based secure outsourced growing database framework, IncShrink. The key features of this solution are: (i) IncShrink maintains the view using incremental MPC operators which eliminates the need for a trusted third party upfront, and (ii) to ensure high performance, IncShrink guarantees that the leakage satisfies DP in the presence of updates. To the best of our knowledge, there are no existing systems that have these properties. We demonstrate IncShrink’s practical feasibility in terms of efficiency and accuracy with extensive empirical evaluations on real-world datasets and the TPC-ds benchmark. The evaluation results show that IncShrink provides a 3-way trade-off in terms of privacy, accuracy and efficiency guarantees, and offers at least a 7,800× performance advantage over standard secure outsourced databases that do not support view-based query paradigm."
2203.05084,"data, dataset, database",78,,,"Growing database. A growing database is a dynamic relational dataset with insertion only updates, thus we define it as a collection of (logical) updates, D = {𝑢𝑖 }𝑖 ≥0, where 𝑢𝑖 is a time stamped data. We write D = {D𝑡 }𝑡 ≥0, such that D𝑡 denotes the database instance of the growing database D at time 𝑡 and ∀ D𝑡 , D𝑡 ⊆ D."
2203.05084,"data, provide implementation",179,,,"Implementation of trans_truncate operator. Naïvely, this operator can be implemented via two separate steps. For example, one may first apply an oblivious transformation (i.e. oblivious filter [7, 22], join [32, 89], etc.) without truncation over the input data. The results are stored to an exhaustively padded array. Next, truncation can be implemented by linearly scanning the array and resets the isView bit from 1 to 0 for a subset of the data in the array such that the resulting output satisfies Eq 3. In practice, truncation can be integrated with the view transformation so that the protocol does not have to run an extra round of linear scan. In what follows, we provide an instantiated implementation of oblivious sort-merge join where the output is truncated with a contribution bound 𝑏, and we continue to provide more implementations for other operators such as filter, nested-loop join, etc. in our complete full version."
2203.05084,"data, publicly available",133,,,"KI-1. View-based query processing over secure outsourced growing data. IncShrink employs materialized view for answering pre-specified queries over secure outsourced growing data. The framework allows untrusted outsourcing servers to securely build and maintain a growing-size materialized view corresponding to the selected view definition. A typical materialized view can be either transformed solely based on the data provisioned by the owners, i.e., a join table over the outsourced data, or in combination with public information, i.e., a join table between the outsourced data and public relations. Queries posed to the servers are rewritten as queries over the defined view and answered using only the view object. Due to the existence of materialization, the outsourcing servers are exempted from performing redundant computations."
2203.05084,"data, publicly available",227,,,"where 𝜎𝑡 ∗<𝑡𝑡𝑖𝑑 ≤𝑡 is a filter that selects all data received since last update. In general, Mant is a mechanism utilizes sparse vector techniques (SVT) to periodically release a noisy count. According to [83] (Theorem 11), this mechanism satisfies 𝜖 -DP (interested 𝑏 readers may refer to our full version for complete privacy proof of Mant). As per Lemma 2 we can obtain the same conclusion that Mant ( ˆ𝑇 (D)) achieves 𝜖-DP, if ˆ𝑇 is 𝑞-stable and 𝑞 = 𝑏. Similarly, we abstract Mant’s output as {(𝑡, 𝑣𝑡 )}𝑡 ≥0 and we assume the following parameters are publicly available: 𝜖, 𝐶𝑟 , 𝑏, 𝑠, 𝑓 , 𝜃 (threshold). For simulator construction one can reuse most of the components in Table 1 but with the following modifications: (i) For step 2.iii, one should replace the condition check as whether 𝑣𝑡 > 0. (ii) For step 2.iii and 2.iv, the simulator outputs one additional random rd ←−− Z𝑚) which simulates the secret shares of refreshed value 𝑦 noisy threshold. Similar, the indistinguishability follows the security property of XOR-based secret-sharing scheme."
2203.05084,"data, publicly available",235,,,"where 𝑡𝑡𝑖𝑑 denotes the time stamp when tuple 𝑟𝑖𝑑 is inserted to D, and 𝜎𝑡 −𝑇 <𝑡𝑡𝑖𝑑 is a filter operator that selects all tuples inserted within the time interval (𝑡 − 𝑇 , 𝑡]. In general, Mtimer can be formulated as a series of 𝜖 -DP Laplace mechanisms that applies over 𝑏 disjoint data (tuples inserted in non-overlapping intervals). Thus by parallel composition theorem [31], Mtimer satisfies 𝜖 -DP. Moreover, 𝑏 by Lemma 2 given a 𝑞-stable transformation ˆ𝑇 such that 𝑞 = 𝑏 (i.e. the Transform protocol), then Mtimer ( ˆ𝑇 (D)) achieves 𝑏 × 𝜖 𝑏 = 𝜖DP. We abstract Mtimer’s output as {(𝑡, 𝑣𝑡 )}𝑡 ≥0, where 𝑣𝑡 is the number released by Mtimer at time 𝑡. Also we assume the following parameters are publicly available: 𝜖, Z𝑚, 𝐶𝑟 (batch size of owner uploaded data), 𝑏 (contribution bound), 𝑠 (cache flush size), 𝑓 (cache flush rate), 𝑇 (view update interval). In what follows, we construct a p.p.t. simulator S that simulates the protocol execution with only access to Mtimer’s outputs and public parameters (Table 1)."
2203.05084,database,135,,,"We start with a modified mechanism Munit of NANT such that where it outputs ⊤ once the condition 𝑐 +𝑐𝑡 > ˜𝜃 is satisfied (Alg 5:6), and outputs ⊥ for all other cases (Alg 5:9). We write the output of Munit as 𝑂 = {𝑜1, 𝑜2, ..., 𝑜𝑚 }, where ∀ 1 ≤ 𝑖 < 𝑚, 𝑜𝑖 = ⊥, and 𝑜𝑚 = ⊤. Now given two neighboring database 𝑋 and 𝑋 ′, and for all 𝑖, Pr [ ˜𝑐𝑖 < 𝑥] ≤ Pr (cid:2) ˜𝑐 ′ 𝑖 < 𝑥 + 1(cid:3) is satisfied, where ˜𝑐𝑖 and ˜𝑐 ′ 𝑖 denotes the 𝑖𝑡ℎ noisy count when applying Munit over 𝑋 and 𝑋 ′"
2203.05084,database,22,,,table (through the query functionality defined by the underlying database) and return the resulting outputs back to the analyst.
2203.05084,database,35,,,[27] Jonathan L Dautrich Jr and Chinya V Ravishankar. 2013. Compromising privacy in precise query protocols. In Proceedings of the 16th International Conference on Extending Database Technology. 155–166.
2203.05084,database,39,,,"[70] Christian Priebe, Kapil Vaswani, and Manuel Costa. 2018. Enclavedb: A secure database using SGX. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 264–278."
2203.05084,database,40,,,"Definition 1 (Update pattern). Given a growing database D, the update pattern for outsourcing D is the function family of UpdtPatt(D) = {UpdtPatt𝑡 (D)}𝑡 ∈N+ , with:"
2203.05084,database,50,,,"Theorem 15 (Logical gap [83]). For each time 𝑡, the logical gap, 𝐿𝐺𝑡 between the outsourced and logical database is defined as the total number of records that have been received by the owner but have not been outsourced to the server."
2203.05084,database,50,,,"[37] Paul Grubbs, Marie-Sarah Lacharité, Brice Minaud, and Kenneth G Paterson. 2018. Pump up the volume: Practical database reconstruction from volume leakage on range queries. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 315–331."
2203.05084,database,59,,,"• IncShrink is a first of its kind, secure outsourced growing database framework that supports view-based query processing paradigm. Comparing with the standard SOGDB [83] that employs naïve query answering setting, IncShrink improves query efficiency, striking a balance between the guarantees of privacy, efficiency and accuracy, at the same time."
2203.05084,database,71,,,"Definition 4 (SIM-CDP view update protocol). A view update protocol Π is said to satisfy 𝜖-SIM-CDP if there exists a p.p.t. simulator S with only access to a set of public parameters pp and the output of a mechanism 𝐹 that satisfies Definition 3. Then for any growing database instance D, and any p.p.t. adversary A, the adversary’s advantage satisfies:"
2203.05084,database,71,,,"In [83], logical gap is used as the major utility metric and typically a large logical gap indicates a relatively large error for queries to the outsourced database. Similar we derive a logical gap at time 𝑡 for the materialized view as 𝐿𝐺 V , which denotes the number of view tuples delayed by the respective mechanisms (record synchronization strategy and view update protocol)."
2203.05084,database,78,,,"[68] Rishabh Poddar, Tobias Boelter, and Raluca Ada Popa. 2016. Arx: A Strongly Encrypted Database System. IACR Cryptol. ePrint Arch. 2016 (2016), 591. [69] Raluca Ada Popa, Catherine MS Redfield, Nickolai Zeldovich, and Hari Balakrishnan. 2012. CryptDB: processing queries on an encrypted database. Commun. ACM 55, 9 (2012), 103–111."
2203.05084,database,88,,,"[38] Paul Grubbs, Marie-Sarah Lacharité, Brice Minaud, and Kenneth G Paterson. 2019. Learning to reconstruct: Statistical learning theory and encrypted database attacks. In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 1067–1083. [39] Zichen Gui, Oliver Johnson, and Bogdan Warinschi. 2019. Encrypted databases: New volume attacks against range queries. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 361–378."
2203.05084,dataset,1,,,cvut.cz/dataset/TPCDS
2203.05084,"dataset, data repository, data https",17,,,[71] RELATIONAL DATASET REPOSITORY. [n.d.]. TPCDS. https://relational.fit.
2203.05084,provide implementation,34,,,A.2 Generating secret shares inside MPC We provide an implementation example on how to generate 𝑘-outof-𝑘 XOR secret shares inside an MPC protocol follow by a sketch proof for it’s security.
2203.05084,"publicly available, package, database, data, dataset",338,,,"Implementation and configuration. We implement a prototype IncShrink, and evaluate it with real-world datasets. We build the prototype IncShrink based on Shrinkwrap [7], a typical secure outsourced database scheme under the server-aided MPC setting. Shrinkwrap only supports static data and a standard query answering method. IncShrink extends it to a view-based SOGDB that handles growing data. In addition to the prototype IncShrink, we also implement client programs that consume data from the given datasets and outsource them to the server, which simulate how realworld data owner devices would receive and outsource new data. We implement all secure 2PC protocols using EMP-Toolkit-0.2.1 package and conduct all experiments on the GCP instance with 3.8GHz Xeon CPU, 32Gb RAM, and 64 bit Ubuntu 18.04.1 OS. Data. We evaluate the system using two datasets: TPC Data Stream (TPC-ds) [71], and Chicago Police Database (CPDB) [23]. TPC-ds collects the retail records for several product suppliers over a fiveyear period. In our evaluation, for TPC-ds, we mainly use two relational tables, the Sales and the Return table. After eliminating invalid data points with incomplete or missing values, the Sales and Return tables contain 2.2 million and 270,000 records, respectively. CPDB is a living repository of public data about Chicago’s police officers and their interactions with the public. We primarily use two relations, the Allegation table, which documents the results of investigations into allegations of police misconduct, and the Award table, which collects information on awards given to certain officers. The cleaned data (after eliminating invalid entries) contains 206,000 and 656,000 records for Allegation and Award table, respectively. Execution scenario & Testing query. For TPC-ds data, we delegate each relational table to a client program, which then independently outsources the data to the servers."
2203.05084,"publicly available, package, database, data, dataset",344,,,"In addition to the prototype IncShrink, we also implement client programs that consume data from the given datasets and outsource them to the server, which simulate how realworld data owner devices would receive and outsource new data. We implement all secure 2PC protocols using EMP-Toolkit-0.2.1 package and conduct all experiments on the GCP instance with 3.8GHz Xeon CPU, 32Gb RAM, and 64 bit Ubuntu 18.04.1 OS. Data. We evaluate the system using two datasets: TPC Data Stream (TPC-ds) [71], and Chicago Police Database (CPDB) [23]. TPC-ds collects the retail records for several product suppliers over a fiveyear period. In our evaluation, for TPC-ds, we mainly use two relational tables, the Sales and the Return table. After eliminating invalid data points with incomplete or missing values, the Sales and Return tables contain 2.2 million and 270,000 records, respectively. CPDB is a living repository of public data about Chicago’s police officers and their interactions with the public. We primarily use two relations, the Allegation table, which documents the results of investigations into allegations of police misconduct, and the Award table, which collects information on awards given to certain officers. The cleaned data (after eliminating invalid entries) contains 206,000 and 656,000 records for Allegation and Award table, respectively. Execution scenario & Testing query. For TPC-ds data, we delegate each relational table to a client program, which then independently outsources the data to the servers. We multiplex the sales time (Sales table) or return time (Return table) associated with each data as an indication of when the client received it. In addition, we assume that the client program uploads a batch of data every single day and the uploaded data is populated to the maximum size. In addition, we pick the following query for evaluating with TPC-ds."
2203.05573,data,58,,,"We use ViT as the backbone architecture for both pre-training and downstream tasks. A ViT is composed of the patch embedding layer, position embedding, and Transformer blocks. Patch Embedding: Since a ViT takes a sequence as input, the patch embedding layer needs to transform any high-dimensional data into sequences. For a"
2203.05573,"data, dataset",195,,,"Our experiments are implemented on PyTorch [17] and MONAI [16]. We use ViT-B/16 as the backbone and utilize the AdamW optimizer in all the experiments. The patch size is 16 × 16 for 2D images and 16 × 16 × 16 for 3D volumes. Data Preprocessing and Augmentation. For ChestX-ray14, we perform a histogram equalization on all the X-ray images. During training, we randomly ﬂip and crop a 224 × 224 region out of the original 256 × 256 image. For BTCV, we clip the raw values between -175 and 250, and re-scale the range between 0 and 1. During training, we randomly ﬂip and crop a 96×96×96 3D volume as the input. For BRATS. we perform an instance-wise normalization over the non-zero region per channel. During training, we randomly ﬂip and crop a 128 × 128 × 128 3D volume as the input. MAE Pre-training. We use a learning rate of 1.5e-4 and weight decay of 0.05 for all datasets. The pre-training epochs vary. MAE pre-training is run for 800"
2203.05573,"data, dataset",264,,,"Abstract. Masked Autoencoder (MAE) has recently been shown to be eﬀective in pre-training Vision Transformers (ViT) for natural image analysis. By performing the pretext task of reconstructing the original image from only partial observations, the encoder, which is a ViT, is encouraged to aggregate contextual information to infer content in masked image regions. We believe that this context aggregation ability is also essential to the medical image domain where each anatomical structure is functionally and mechanically connected to other structures and regions. However, there is no ImageNet-scale medical image dataset for pretraining. Thus, in this paper, we investigate a self pre-training paradigm with MAE for medical images, i.e., models are pre-trained on the same target dataset. To validate the MAE self pre-training, we consider three diverse medical image tasks including chest X-ray disease classiﬁcation, CT abdomen multi-organ segmentation and MRI brain tumor segmentation. It turns out MAE self pre-training beneﬁts all the tasks markedly. Speciﬁcally, the mAUC on lung disease classiﬁcation is increased by 9.4%. The average DSC on brain tumor segmentation is improved from 77.4% to 78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset (N=30), the average DSC improves from 78.8% to 83.5% and the HD95 is reduced by 60%, indicating its eﬀectiveness in limited data scenarios. The segmentation and classiﬁcation results reveal the promising potential of MAE self pre-training for medical image analysis."
2203.05573,"data, dataset",294,,,"Abdomen Multi-organ Segmentation. The results are shown in Table 2. First, we reproduce the UNETR baseline with an average DSC of 78.83% on BTCV. Then, after MAE self pre-training, we initialize the encoder with the pre-trained weights and the decoder randomly. The whole UNETR is ﬁnetuned to perform the multi-organ segmentation task. Both UNETR and UNETR with MAE surpass the other SOTA methods. Most importantly, MAE self pre-training improves upon the UNETR baseline by a large margin, from 78.8% to 83.5% on average DSC. It is also superior to the ImageNet pre-training paradigm. As MAE is previously only applied on large-scale datasets like ImageNet (N=1000,000), it is interesting to observe its notable performance on a small-scale dataset (N=30); this further demonstrates the promising potential of applying MAE self pretraining to medical images in limited data scenarios. Brain Tumor Segmentation. Similar to multi-organ segmentation, we also use UNETR as our segmentation baseline for the brain tumor segmentation task. Results are listed in Table 3. UNETR achieves an average DSC of 77.4% and a HD95 of 7.78mm. With the help of MAE (12.5% mask ratio) self pre-training, the performance of UNETR is improved further achieving a 78.91% DSC and a HD95 of 7.22mm. Ablation Study. In Table 4, we conduct experiments with diﬀerent MAE pretraining epochs and mask ratios. First, the performance of MAE on BRATS beneﬁts from longer training though this is not the case for BTCV. Second, unlike the high mask ratio [11] adopted in natural images, the two segmentation"
2203.05573,"data, dataset",39,,,Our results indicate that MAE self pre-training can signiﬁcantly improve medical image segmentation and classiﬁcation performance and surpass the ImageNet pre-training paradigm on all the datasets. We also demonstrate that MAE pretraining aids in limited data scenarios.
2203.05573,database,115,,,"23. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2097–2106 (2017) 24. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886 (2021)"
2203.05573,dataset,1,,,Dataset
2203.05573,dataset,190,,,"In this paper, we investigate a MAE-based self pre-training paradigm for medical image analysis tasks. As there is no ImageNet-scale universal medical image dataset, MAE pre-training is performed on the same dataset as the downstream task. We term this self pre-training. We conduct experiments on three diverse medical image tasks including lung disease classiﬁcation on chest X-ray14 (CXR14 [23]), CT multi-organ segmentation on BTCV [14] and MRI brain tumor segmentation (BRATS) from Medical Segmentation Decathlon Challenge [1]. These three datasets are diverse in task types (classiﬁcation and segmentation) and imaging modalities (X-rays, CT, and MRI). For all tasks, we take ViTB/16 [7] as the backbone network. After MAE pre-training, ViT-B/16 is added with a task-speciﬁc head. For classiﬁcation, the head is a simple linear classiﬁer. For segmentation, we follow the decoder design of UNETR [10] which is a recently-proposed segmentation framework with a ViT encoder and a “Ushaped”[19] architecture."
2203.05573,dataset,26,,,Table 4. Ablation Study on Mask Ratios and Pre-training Epochs. Left: MSD Brain Tumor Segmentation Dataset. Right: BTCV Multi-organ Segmentation Dataset
2203.05573,dataset,40,,,"8. El-Nouby, A., Izacard, G., Touvron, H., Laptev, I., Jegou, H., Grave, E.: Are large-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740 (2021)"
2203.05573,dataset,45,,,"Here we have demonstrated that MAE pre-training improves SOTA classiﬁcation and segmentation performance on a diverse set of medical image analysis tasks. Of relevance to medical imaging, MAE self pre-training outperforms existing methods on small datasets, including ImageNet-transfer learning. Furthermore,"
2203.05573,dataset,68,,,"Fig. 2. MAE reconstruction results. The ﬁrst row is the original image, the second row is the masked image where masked region is denoted by gray or black patches. The third row the reconstruction of MAE from the unmasked patches. Every two columns show the results of one dataset, i.e., CXR, BTCV and BRATS from left to right."
2203.05573,"dataset, database",344,,,"Experiments on all tasks include two stages: the MAE pre-training and the downstream task. Speciﬁcally, a ViT is ﬁrst pre-trained by MAE on the target dataset. Then, the pre-trained ViT is appended with a task-speciﬁc head, i.e., linear layer for classiﬁcation and UNETR decoder for segmentation. The whole network is ﬁnetuned further to perform the downstream task. We consider three diverse tasks for validating MAE for medical images, introduced next. Lung Disease Classiﬁcation on ChestX-ray14. ChestX-ray14 [23] is a largescale CXR database consisting of 112,120 frontal-view CXRs from 32,717 patients. We conduct the classiﬁcation task based on the oﬃcial split which consists of trainval(∼ 80%) and testing (∼ 20%) sets. Because each image can have multiple labels, we adopt the multi-class AUC as the performance metric. Abdomen Multi-organ Segmentation on BTCV. The BTCV [14] (Multi Atlas Labeling Beyond The Cranial Vault) dataset consists of 30 subjects with abdominal CT scans where 13 organs were annotated under the supervision of clinical radiologists. Each CT volume has 85 ∼ 198 slices of 512×512 pixels, with a voxel spatial resolution of (0.54 × 0.98 × [2.5 ∼ 5.0] mm3). For the convenience of comparison, we follow [5] to split the 30 cases into 18 for training and 12 for validation in the same way as [4]. We report the average DSC and 95% Hausdorﬀ Distance on 8 abdominal organs (aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen, stomach) to align with [5]. Brain Tumor Segmentation on MSD. Brain tumor segmentation is one of the 10 tasks in Medical Segmentation Decathlon (MSD) Challenge [1]. The entire set has 484 multi-modal (FLAIR, T1w, T1-Gd and T2w) MRI brain scans."
2203.05573,"dataset, database",345,,,"We consider three diverse tasks for validating MAE for medical images, introduced next. Lung Disease Classiﬁcation on ChestX-ray14. ChestX-ray14 [23] is a largescale CXR database consisting of 112,120 frontal-view CXRs from 32,717 patients. We conduct the classiﬁcation task based on the oﬃcial split which consists of trainval(∼ 80%) and testing (∼ 20%) sets. Because each image can have multiple labels, we adopt the multi-class AUC as the performance metric. Abdomen Multi-organ Segmentation on BTCV. The BTCV [14] (Multi Atlas Labeling Beyond The Cranial Vault) dataset consists of 30 subjects with abdominal CT scans where 13 organs were annotated under the supervision of clinical radiologists. Each CT volume has 85 ∼ 198 slices of 512×512 pixels, with a voxel spatial resolution of (0.54 × 0.98 × [2.5 ∼ 5.0] mm3). For the convenience of comparison, we follow [5] to split the 30 cases into 18 for training and 12 for validation in the same way as [4]. We report the average DSC and 95% Hausdorﬀ Distance on 8 abdominal organs (aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen, stomach) to align with [5]. Brain Tumor Segmentation on MSD. Brain tumor segmentation is one of the 10 tasks in Medical Segmentation Decathlon (MSD) Challenge [1]. The entire set has 484 multi-modal (FLAIR, T1w, T1-Gd and T2w) MRI brain scans. The ground-truth labels of segmentation includes peritumoral edema, GD-enhancing tumor and the necrotic/non-enhancing tumor core. The performance is measured on three recombined regions, i.e., tumor core, whole tumor and enhancing tumor. We randomly split the dataset into training (80%) and validation (20%) sets. Reported metrics include average DSC and 95% Hausdorﬀ Distance."
2203.05573,github,67,,,"4. Chen, J.N.: Transunet, https://github.com/Beckschen/TransUNet 5. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)"
2203.05573,github,92,,,"16. MONAI Consortium: MONAI: Medical Open Network for AI (3 2020). https:// doi.org/10.5281/zenodo.4323058, https://github.com/Project-MONAI/MONAI 17. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems 32 (2019)"
